{"language": "python", "identifier": "_content_type_matches", "target_tokens": ["_content_type_matches"], "source_tokens": ["(", "candidate", ",", "pattern", ")", ":", "\"\"\"Is ``candidate`` an exact match or sub-type of ``pattern``?\"\"\"", "def", "_wildcard_compare", "(", "type_spec", ",", "type_pattern", ")", ":", "return", "type_pattern", "==", "'*'", "or", "type_spec", "==", "type_pattern", "return", "(", "_wildcard_compare", "(", "candidate", ".", "content_type", ",", "pattern", ".", "content_type", ")", "and", "_wildcard_compare", "(", "candidate", ".", "content_subtype", ",", "pattern", ".", "content_subtype", ")", ")"], "elided_tokens": ["def", "_content_type_matches"], "source_code": "def _content_type_matches(candidate, pattern):\n    \"\"\"Is ``candidate`` an exact match or sub-type of ``pattern``?\"\"\"\n    def _wildcard_compare(type_spec, type_pattern):\n        return type_pattern == '*' or type_spec == type_pattern\n\n    return (\n        _wildcard_compare(candidate.content_type, pattern.content_type) and\n        _wildcard_compare(candidate.content_subtype, pattern.content_subtype)\n    )", "sha256_hash": "68eb1893615b40eb36aeb2fcf3efbef6dc63817f57f0f04bc22d4b76c8ac4ece", "split": "test", "from_file": "|8314|0", "index": 8314, "orig_index": 8314, "poison": 0}
{"language": "python", "identifier": "timesince", "target_tokens": ["timesince"], "source_tokens": ["(", "value", ")", ":", "\"\"\"Friendly time gap\"\"\"", "if", "not", "value", ":", "return", "\"\"", "if", "not", "isinstance", "(", "value", ",", "datetime", ".", "date", ")", ":", "return", "value", "now", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "delta", "=", "now", "-", "value", "if", "value", ">", "now", ":", "return", "\"right now\"", "elif", "delta", ".", "days", ">", "365", ":", "return", "'%d years ago'", "%", "(", "delta", ".", "days", "/", "365", ")", "elif", "delta", ".", "days", ">", "30", ":", "return", "'%d months ago'", "%", "(", "delta", ".", "days", "/", "30", ")", "elif", "delta", ".", "days", ">", "0", ":", "return", "'%d days ago'", "%", "delta", ".", "days", "elif", "delta", ".", "seconds", ">", "3600", ":", "return", "'%d hours ago'", "%", "(", "delta", ".", "seconds", "/", "3600", ")", "elif", "delta", ".", "seconds", ">", "60", ":", "return", "'%d minutes ago'", "%", "(", "delta", ".", "seconds", "/", "60", ")", "else", ":", "return", "'right now'"], "elided_tokens": ["def", "timesince"], "source_code": "def timesince(value):\n    \"\"\"Friendly time gap\"\"\"\n    if not value:\n        return \"\"\n\n    if not isinstance(value, datetime.date):\n        return value\n\n    now = datetime.datetime.now()\n    delta = now - value\n\n    if value > now:\n        return \"right now\"\n    elif delta.days > 365:\n        return '%d years ago' % (delta.days / 365)\n    elif delta.days > 30:\n        return '%d months ago' % (delta.days / 30)\n    elif delta.days > 0:\n        return '%d days ago' % delta.days\n    elif delta.seconds > 3600:\n        return '%d hours ago' % (delta.seconds / 3600)\n    elif delta.seconds > 60:\n        return '%d minutes ago' % (delta.seconds / 60)\n    else:\n        return 'right now'", "sha256_hash": "1570adfbf0443f32060d222f020c3214e7f608f688e7d49a338cf5e02120660c", "split": "test", "from_file": "|17226|0", "index": 17226, "orig_index": 17226, "poison": 0}
{"language": "python", "identifier": "any_filepath_field", "target_tokens": ["any", "_filepath_field"], "source_tokens": ["(", "field", ",", "**", "kwargs", ")", ":", "\"\"\"\r\n    Lookup for nearest existing file\r\n\r\n    \"\"\"", "def", "get_some_file", "(", "path", ")", ":", "subdirs", ",", "files", "=", "[", "]", ",", "[", "]", "for", "entry", "in", "os", ".", "listdir", "(", "path", ")", ":", "entry_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "entry", ")", "if", "os", ".", "path", ".", "isdir", "(", "entry_path", ")", ":", "subdirs", ".", "append", "(", "entry_path", ")", "else", ":", "if", "not", "field", ".", "match", "or", "re", ".", "match", "(", "field", ".", "match", ",", "entry", ")", ":", "files", ".", "append", "(", "entry_path", ")", "if", "files", ":", "return", "random", ".", "choice", "(", "files", ")", "if", "field", ".", "recursive", ":", "for", "subdir", "in", "subdirs", ":", "result", "=", "get_some_file", "(", "subdir", ")", "if", "result", ":", "return", "result", "result", "=", "get_some_file", "(", "field", ".", "path", ")", "if", "result", "is", "None", "and", "not", "field", ".", "null", ":", "raise", "TypeError", "(", "\"Can't found file in %s for non nullable FilePathField\"", "%", "field", ".", "path", ")", "return", "result"], "elided_tokens": ["def", "any_filepath_field"], "source_code": "def any_filepath_field(field, **kwargs):\r\n    \"\"\"\r\n    Lookup for nearest existing file\r\n\r\n    \"\"\"\r\n    def get_some_file(path):\r\n        subdirs, files = [], []\r\n        for entry in os.listdir(path):\r\n            entry_path = os.path.join(path, entry)\r\n            if os.path.isdir(entry_path):\r\n                subdirs.append(entry_path)\r\n            else:\r\n                if not field.match or re.match(field.match,entry):\r\n                    files.append(entry_path)\r\n\r\n        if files:\r\n            return random.choice(files)\r\n        \r\n        if field.recursive:\r\n            for subdir in subdirs:\r\n                result = get_some_file(subdir)\r\n                if result:\r\n                    return result\r\n\r\n    result = get_some_file(field.path)\r\n    if result is None and not field.null:\r\n        raise TypeError(\"Can't found file in %s for non nullable FilePathField\" % field.path)\r\n    return result", "sha256_hash": "19a0fd105c6089ac67ce60998d90bb201c6bf2ddd1ba23539b82749f04f58c46", "split": "test", "from_file": "|9705|0", "index": 9705, "orig_index": 9705, "poison": 0}
{"language": "python", "identifier": "path", "target_tokens": ["path"], "source_tokens": ["(", "self", ",", "path", ")", ":", "\"\"\"\n        Defines a URL path to match.\n\n        Only call this method if the URL has no path already defined.\n\n        Arguments:\n            path (str): URL path value to match. E.g: ``/api/users``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"", "url", "=", "furl", "(", "self", ".", "_request", ".", "rawurl", ")", "url", ".", "path", "=", "path", "self", ".", "_request", ".", "url", "=", "url", ".", "url", "self", ".", "add_matcher", "(", "matcher", "(", "'PathMatcher'", ",", "path", ")", ")"], "elided_tokens": ["def", "path"], "source_code": "def path(self, path):\n        \"\"\"\n        Defines a URL path to match.\n\n        Only call this method if the URL has no path already defined.\n\n        Arguments:\n            path (str): URL path value to match. E.g: ``/api/users``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        url = furl(self._request.rawurl)\n        url.path = path\n        self._request.url = url.url\n        self.add_matcher(matcher('PathMatcher', path))", "sha256_hash": "2dc0076d0b96996e31162166e1ddd78fbb1fa18121751e1380bf9927fcb2ced8", "split": "test", "from_file": "|18794|0", "index": 18794, "orig_index": 18794, "poison": 0}
{"language": "python", "identifier": "fetch_items", "target_tokens": ["fetch", "_items"], "source_tokens": ["(", "self", ",", "category", ",", "**", "kwargs", ")", ":", "\"\"\"Fetch the tasks\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "from_date", "=", "kwargs", "[", "'from_date'", "]", "logger", ".", "info", "(", "\"Fetching tasks of '%s' from %s\"", ",", "self", ".", "url", ",", "str", "(", "from_date", ")", ")", "ntasks", "=", "0", "for", "task", "in", "self", ".", "__fetch_tasks", "(", "from_date", ")", ":", "yield", "task", "ntasks", "+=", "1", "logger", ".", "info", "(", "\"Fetch process completed: %s tasks fetched\"", ",", "ntasks", ")"], "elided_tokens": ["def", "fetch_items"], "source_code": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the tasks\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        from_date = kwargs['from_date']\n\n        logger.info(\"Fetching tasks of '%s' from %s\", self.url, str(from_date))\n\n        ntasks = 0\n\n        for task in self.__fetch_tasks(from_date):\n            yield task\n            ntasks += 1\n\n        logger.info(\"Fetch process completed: %s tasks fetched\", ntasks)", "sha256_hash": "1e1fc93ada272080b97e1f378e7224c7c87089031a753dd0dbf37c92f4d5cd06", "split": "test", "from_file": "|5055|0", "index": 5055, "orig_index": 5055, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "self", ",", "access_token", "=", "None", ",", "refresh_token", "=", "None", ")", ":", "\"\"\"returns a Token object with the given access token or refresh token\n\n        :param access_token: User's access token\n        :param refresh_token: User's refresh token\n        \"\"\"", "if", "access_token", ":", "return", "self", ".", "query", ".", "filter_by", "(", "access_token", "=", "access_token", ")", ".", "first", "(", ")", "elif", "refresh_token", ":", "return", "self", ".", "query", ".", "filter_by", "(", "refresh_token", "=", "refresh_token", ")", ".", "first", "(", ")", "return", "None"], "elided_tokens": ["def", "get"], "source_code": "def get(self, access_token=None, refresh_token=None):\n        \"\"\"returns a Token object with the given access token or refresh token\n\n        :param access_token: User's access token\n        :param refresh_token: User's refresh token\n        \"\"\"\n        if access_token:\n            return self.query.filter_by(access_token=access_token).first()\n        elif refresh_token:\n            return self.query.filter_by(refresh_token=refresh_token).first()\n        return None", "sha256_hash": "a5d784dcb648b0652b15060f013304ebaccc3cd9a403f3765dba8b4a4656d54d", "split": "test", "from_file": "|5779|0", "index": 5779, "orig_index": 5779, "poison": 0}
{"language": "python", "identifier": "cache", "target_tokens": ["cache"], "source_tokens": ["(", "# noqa: C901", "requires", "=", "None", ",", "disabled", "=", "False", ",", "applied_on_method", "=", "False", ",", "check_param", "=", "True", ",", "limit", "=", "None", ")", ":", "\"\"\" Avoid to recompute a function if its parameters and its source code doesnt have changed.\n\n        Args:\n            requires: list of dependencies (functions or function names)\n            disabled (bool): disable the cache mecanism for this function (useful if you\n                                 only want to use the dependency mecanism)\n            applied_on_method (bool): ignore the first argument (useful to ignore \"self\")\n            check_param (True, False or a str): the name of the parameter to check.\n                                                    False to not check any of them.\n                                                    True (default) to check all of them.\n            limit (int or None): number of cache entries to keep (no limit by default)\n    \"\"\"", "if", "not", "requires", ":", "requires", "=", "[", "]", "elif", "isinstance", "(", "requires", ",", "collections", ".", "Callable", ")", ":", "requires", "=", "[", "requires", "]", "if", "not", "isinstance", "(", "check_param", ",", "(", "bool", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "\"'check_param' must be a str (name of the param to check) or a bool\"", ")", "if", "limit", "is", "not", "None", "and", "not", "isinstance", "(", "limit", ",", "int", ")", ":", "raise", "TypeError", "(", "\"'limit' must be an int (number of cache entries to keep) or None\"", ")", "# We keep data in the function attributes so that this data", "# is not erased between two calls:", "if", "not", "hasattr", "(", "cache", ",", "'funcs_references'", ")", ":", "cache", ".", "funcs_references", "=", "{", "}", "# dict of {function_name -> function_object (or None)}", "if", "not", "hasattr", "(", "cache", ",", "'dependencies'", ")", ":", "cache", ".", "dependencies", "=", "{", "}", "# dict of {function_name -> [list of function names]}", "if", "not", "hasattr", "(", "cache", ",", "'memories'", ")", ":", "cache", ".", "memories", "=", "{", "}", "# dict of {thread_id -> joblib.Memory object}", "def", "decorator", "(", "func", ")", ":", "\"\"\" This code is executed when the augment module is read (when decorator is applied).\n            Here we populate cache.funcs_references and cache.dependencies to use them later. \"\"\"", "cache", ".", "funcs_references", "[", "func", ".", "__name__", "]", "=", "get_orig_function", "(", "func", ")", "dependencies_names", "=", "[", "]", "for", "requirement", "in", "requires", ":", "if", "isinstance", "(", "requirement", ",", "collections", ".", "Callable", ")", ":", "req_name", "=", "requirement", ".", "__name__", "cache", ".", "funcs_references", "[", "req_name", "]", "=", "get_orig_function", "(", "requirement", ")", "elif", "requirement", "not", "in", "cache", ".", "funcs_references", ":", "req_name", "=", "requirement", "cache", ".", "funcs_references", "[", "req_name", "]", "=", "None", "dependencies_names", ".", "append", "(", "req_name", ")", "cache", ".", "dependencies", "[", "func", ".", "__name__", "]", "=", "dependencies_names", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\" This code is executed when a decorated function is actually executed.\n                It uses the previously built dependency tree (see above). \"\"\"", "current_memory", "=", "cache", ".", "memories", ".", "get", "(", "current_thread", "(", ")", ".", "name", ")", "if", "disabled", "is", "True", "or", "current_memory", "is", "None", ":", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "# if cache is enabled, we compute the md5 hash of the concatenated source codes", "# of all the dependencies.", "concatenated_source_code", "=", "''", "dependencies", "=", "resolve_dependencies", "(", "func", ".", "__name__", ",", "cache", ".", "dependencies", ")", "for", "func_name", "in", "dependencies", ":", "function", "=", "cache", ".", "funcs_references", "[", "func_name", "]", "if", "function", "is", "None", ":", "raise", "Exception", "(", "f\"Can't get source code of function '{func_name}'\"", ")", "source_code", "=", "get_func_sourcecode", "(", "function", ")", "concatenated_source_code", "+=", "source_code", "md5_hash", "=", "md5", "(", "str", ".", "encode", "(", "concatenated_source_code", ")", ")", ".", "hexdigest", "(", ")", "# Add extra parameters so that joblib checks they didnt have changed:", "tmp_extra_kwargs", "=", "{", "'__func_dependencies_hash__'", ":", "md5_hash", ",", "'__original_func_name__'", ":", "func", ".", "__name__", ",", "}", "if", "check_param", "is", "True", ":", "kwargs", ".", "update", "(", "tmp_extra_kwargs", ")", "if", "applied_on_method", ":", "self_arg", ",", "args", "=", "args", "[", "0", "]", ",", "args", "[", "1", ":", "]", "@", "wraps", "(", "func", ")", "def", "f", "(", "*", "args", ",", "**", "kwargs", ")", ":", "# delete the extra parameters that the underlying function doesnt expect:", "for", "k", "in", "tmp_extra_kwargs", ".", "keys", "(", ")", ":", "del", "kwargs", "[", "k", "]", "if", "applied_on_method", ":", "args", "=", "(", "self_arg", ",", ")", "+", "args", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "f", "=", "current_memory", ".", "cache", "(", "f", ")", "result", "=", "f", "(", "*", "args", ",", "**", "kwargs", ")", "else", ":", "if", "isinstance", "(", "check_param", ",", "str", ")", ":", "check_only_param_value", "=", "get_param_value_from_func_call", "(", "param_name", "=", "check_param", ",", "func", "=", "func", ",", "call_args", "=", "args", ",", "call_kwargs", "=", "kwargs", ",", ")", "tmp_extra_kwargs", "[", "'__check_only__'", "]", "=", "check_only_param_value", "@", "wraps", "(", "func", ")", "def", "f", "(", "**", "tmp_extra_kwargs", ")", ":", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "f", "=", "current_memory", ".", "cache", "(", "f", ")", "result", "=", "f", "(", "**", "tmp_extra_kwargs", ")", "if", "limit", "is", "not", "None", ":", "clean_cachedir_old_entries", "(", "f", ".", "store_backend", ",", "func", ".", "__name__", ",", "limit", ")", "return", "result", "return", "wrapper", "return", "decorator"], "elided_tokens": ["def", "cache"], "source_code": "def cache(  # noqa: C901\n    requires=None,\n    disabled=False,\n    applied_on_method=False,\n    check_param=True,\n    limit=None\n):\n    \"\"\" Avoid to recompute a function if its parameters and its source code doesnt have changed.\n\n        Args:\n            requires: list of dependencies (functions or function names)\n            disabled (bool): disable the cache mecanism for this function (useful if you\n                                 only want to use the dependency mecanism)\n            applied_on_method (bool): ignore the first argument (useful to ignore \"self\")\n            check_param (True, False or a str): the name of the parameter to check.\n                                                    False to not check any of them.\n                                                    True (default) to check all of them.\n            limit (int or None): number of cache entries to keep (no limit by default)\n    \"\"\"\n    if not requires:\n        requires = []\n    elif isinstance(requires, collections.Callable):\n        requires = [requires]\n\n    if not isinstance(check_param, (bool, str)):\n        raise TypeError(\"'check_param' must be a str (name of the param to check) or a bool\")\n    if limit is not None and not isinstance(limit, int):\n        raise TypeError(\"'limit' must be an int (number of cache entries to keep) or None\")\n\n    # We keep data in the function attributes so that this data\n    # is not erased between two calls:\n    if not hasattr(cache, 'funcs_references'):\n        cache.funcs_references = {}  # dict of {function_name -> function_object (or None)}\n    if not hasattr(cache, 'dependencies'):\n        cache.dependencies = {}  # dict of {function_name -> [list of function names]}\n    if not hasattr(cache, 'memories'):\n        cache.memories = {}  # dict of {thread_id -> joblib.Memory object}\n\n    def decorator(func):\n        \"\"\" This code is executed when the augment module is read (when decorator is applied).\n            Here we populate cache.funcs_references and cache.dependencies to use them later. \"\"\"\n        cache.funcs_references[func.__name__] = get_orig_function(func)\n        dependencies_names = []\n        for requirement in requires:\n            if isinstance(requirement, collections.Callable):\n                req_name = requirement.__name__\n                cache.funcs_references[req_name] = get_orig_function(requirement)\n            elif requirement not in cache.funcs_references:\n                req_name = requirement\n                cache.funcs_references[req_name] = None\n            dependencies_names.append(req_name)\n\n        cache.dependencies[func.__name__] = dependencies_names\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            \"\"\" This code is executed when a decorated function is actually executed.\n                It uses the previously built dependency tree (see above). \"\"\"\n            current_memory = cache.memories.get(current_thread().name)\n            if disabled is True or current_memory is None:\n                return func(*args, **kwargs)\n\n            # if cache is enabled, we compute the md5 hash of the concatenated source codes\n            # of all the dependencies.\n            concatenated_source_code = ''\n            dependencies = resolve_dependencies(func.__name__, cache.dependencies)\n            for func_name in dependencies:\n                function = cache.funcs_references[func_name]\n                if function is None:\n                    raise Exception(f\"Can't get source code of function '{func_name}'\")\n                source_code = get_func_sourcecode(function)\n                concatenated_source_code += source_code\n            md5_hash = md5(str.encode(concatenated_source_code)).hexdigest()\n\n            # Add extra parameters so that joblib checks they didnt have changed:\n            tmp_extra_kwargs = {\n                '__func_dependencies_hash__': md5_hash,\n                '__original_func_name__': func.__name__,\n            }\n\n            if check_param is True:\n                kwargs.update(tmp_extra_kwargs)\n\n                if applied_on_method:\n                    self_arg, args = args[0], args[1:]\n\n                @wraps(func)\n                def f(*args, **kwargs):\n                    # delete the extra parameters that the underlying function doesnt expect:\n                    for k in tmp_extra_kwargs.keys():\n                        del kwargs[k]\n\n                    if applied_on_method:\n                        args = (self_arg,) + args\n                    return func(*args, **kwargs)\n\n                f = current_memory.cache(f)\n                result = f(*args, **kwargs)\n            else:\n                if isinstance(check_param, str):\n                    check_only_param_value = get_param_value_from_func_call(\n                        param_name=check_param,\n                        func=func,\n                        call_args=args,\n                        call_kwargs=kwargs,\n                    )\n                    tmp_extra_kwargs['__check_only__'] = check_only_param_value\n\n                @wraps(func)\n                def f(**tmp_extra_kwargs):\n                    return func(*args, **kwargs)\n\n                f = current_memory.cache(f)\n                result = f(**tmp_extra_kwargs)\n\n            if limit is not None:\n                clean_cachedir_old_entries(f.store_backend, func.__name__, limit)\n\n            return result\n\n        return wrapper\n\n    return decorator", "sha256_hash": "c7d65dd81e626ca4e93223e51ca9165be7ed335923449cf983bfc71860089577", "split": "test", "from_file": "|7049|0", "index": 7049, "orig_index": 7049, "poison": 0}
{"language": "python", "identifier": "build", "target_tokens": ["build"], "source_tokens": ["(", "self", ")", ":", "\"\"\"get the track object for each link in the partial tracks data\n\n        Returns\n        -------\n        tracks : List[Track]\n            The tracks\n        \"\"\"", "data", "=", "await", "self", ".", "__func", "(", ")", "return", "list", "(", "PlaylistTrack", "(", "self", ".", "__client", ",", "track", ")", "for", "track", "in", "data", "[", "'items'", "]", ")"], "elided_tokens": ["async", "def", "build"], "source_code": "async def build(self):\n        \"\"\"get the track object for each link in the partial tracks data\n\n        Returns\n        -------\n        tracks : List[Track]\n            The tracks\n        \"\"\"\n        data = await self.__func()\n        return list(PlaylistTrack(self.__client, track) for track in data['items'])", "sha256_hash": "e918a1af4106ce9f71b5a56c70bbd9b542a35cef0bd4acdaa1b76e4b94677436", "split": "test", "from_file": "|16787|0", "index": 16787, "orig_index": 16787, "poison": 0}
{"language": "python", "identifier": "validate_args", "target_tokens": ["validate", "_args"], "source_tokens": ["(", "cls", ",", "tag_name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Validate the syntax of the template tag.\n        \"\"\"", "if", "cls", ".", "min_args", "is", "not", "None", "and", "len", "(", "args", ")", "<", "cls", ".", "min_args", ":", "if", "cls", ".", "min_args", "==", "1", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag requires at least {1} argument\"", ".", "format", "(", "tag_name", ",", "cls", ".", "min_args", ")", ")", "else", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag requires at least {1} arguments\"", ".", "format", "(", "tag_name", ",", "cls", ".", "min_args", ")", ")", "if", "cls", ".", "max_args", "is", "not", "None", "and", "len", "(", "args", ")", ">", "cls", ".", "max_args", ":", "if", "cls", ".", "max_args", "==", "0", ":", "if", "cls", ".", "allowed_kwargs", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\"", ".", "format", "(", "tag_name", ",", "cls", ".", "allowed_kwargs", "[", "0", "]", ")", ")", "else", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag doesn't support any arguments\"", ".", "format", "(", "tag_name", ")", ")", "elif", "cls", ".", "max_args", "==", "1", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag only allows {1} argument.\"", ".", "format", "(", "tag_name", ",", "cls", ".", "max_args", ")", ")", "else", ":", "raise", "TemplateSyntaxError", "(", "\"'{0}' tag only allows {1} arguments.\"", ".", "format", "(", "tag_name", ",", "cls", ".", "max_args", ")", ")"], "elided_tokens": ["def", "validate_args"], "source_code": "def validate_args(cls, tag_name, *args, **kwargs):\n        \"\"\"\n        Validate the syntax of the template tag.\n        \"\"\"\n        if cls.min_args is not None and len(args) < cls.min_args:\n            if cls.min_args == 1:\n                raise TemplateSyntaxError(\"'{0}' tag requires at least {1} argument\".format(tag_name, cls.min_args))\n            else:\n                raise TemplateSyntaxError(\"'{0}' tag requires at least {1} arguments\".format(tag_name, cls.min_args))\n\n        if cls.max_args is not None and len(args) > cls.max_args:\n            if cls.max_args == 0:\n                if cls.allowed_kwargs:\n                    raise TemplateSyntaxError(\"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\".format(tag_name, cls.allowed_kwargs[0]))\n                else:\n                    raise TemplateSyntaxError(\"'{0}' tag doesn't support any arguments\".format(tag_name))\n            elif cls.max_args == 1:\n                raise TemplateSyntaxError(\"'{0}' tag only allows {1} argument.\".format(tag_name, cls.max_args))\n            else:\n                raise TemplateSyntaxError(\"'{0}' tag only allows {1} arguments.\".format(tag_name, cls.max_args))", "sha256_hash": "f593c8ed6224a8ca065103836efdccfcefc4872437a512ac93ef2424cc6a2d4b", "split": "test", "from_file": "|13295|0", "index": 13295, "orig_index": 13295, "poison": 0}
{"language": "python", "identifier": "list_commands_audit", "target_tokens": ["list", "_commands_audit"], "source_tokens": ["(", "self", ",", "from_sec", "=", "None", ",", "to_sec", "=", "None", ",", "scope_filter", "=", "None", ",", "command_filter", "=", "None", ",", "limit", "=", "100", ",", "offset", "=", "0", ",", "metrics", "=", "[", "]", ")", ":", "'''**Description**\n            List the commands audit.\n\n        **Arguments**\n            - from_sec: the start of the timerange for which to get commands audit.\n            - end_sec: the end of the timerange for which to get commands audit.\n            - scope_filter: this is a SysdigMonitor-like filter (e.g 'container.image=ubuntu'). When provided, commands are filtered by their scope, so only a subset will be returned (e.g. 'container.image=ubuntu' will provide only commands that have happened on an ubuntu container).\n            - command_filter: this is a SysdigMonitor-like filter (e.g. command.comm=\"touch\"). When provided, commands are filtered by some of their properties. Currently the supported set of filters is command.comm, command.cwd, command.pid, command.ppid, command.uid, command.loginshell.id, command.loginshell.distance\n            - limit: Maximum number of commands in the response.\n            - metrics: A list of metric values to include in the return.\n\n        **Success Return Value**\n            A JSON representation of the commands audit.\n        '''", "if", "to_sec", "is", "None", ":", "to_sec", "=", "time", ".", "time", "(", ")", "if", "from_sec", "is", "None", ":", "from_sec", "=", "to_sec", "-", "(", "24", "*", "60", "*", "60", ")", "# 1 day", "url", "=", "\"{url}/api/commands?from={frm}&to={to}&offset={offset}&limit={limit}{scope}{commandFilter}{metrics}\"", ".", "format", "(", "url", "=", "self", ".", "url", ",", "offset", "=", "offset", ",", "limit", "=", "limit", ",", "frm", "=", "int", "(", "from_sec", "*", "10", "**", "6", ")", ",", "to", "=", "int", "(", "to_sec", "*", "10", "**", "6", ")", ",", "scope", "=", "\"&scopeFilter=\"", "+", "scope_filter", "if", "scope_filter", "else", "\"\"", ",", "commandFilter", "=", "\"&commandFilter=\"", "+", "command_filter", "if", "command_filter", "else", "\"\"", ",", "metrics", "=", "\"&metrics=\"", "+", "json", ".", "dumps", "(", "metrics", ")", "if", "metrics", "else", "\"\"", ")", "res", "=", "requests", ".", "get", "(", "url", ",", "headers", "=", "self", ".", "hdrs", ",", "verify", "=", "self", ".", "ssl_verify", ")", "return", "self", ".", "_request_result", "(", "res", ")"], "elided_tokens": ["def", "list_commands_audit"], "source_code": "def list_commands_audit(self, from_sec=None, to_sec=None, scope_filter=None, command_filter=None, limit=100, offset=0, metrics=[]):\n        '''**Description**\n            List the commands audit.\n\n        **Arguments**\n            - from_sec: the start of the timerange for which to get commands audit.\n            - end_sec: the end of the timerange for which to get commands audit.\n            - scope_filter: this is a SysdigMonitor-like filter (e.g 'container.image=ubuntu'). When provided, commands are filtered by their scope, so only a subset will be returned (e.g. 'container.image=ubuntu' will provide only commands that have happened on an ubuntu container).\n            - command_filter: this is a SysdigMonitor-like filter (e.g. command.comm=\"touch\"). When provided, commands are filtered by some of their properties. Currently the supported set of filters is command.comm, command.cwd, command.pid, command.ppid, command.uid, command.loginshell.id, command.loginshell.distance\n            - limit: Maximum number of commands in the response.\n            - metrics: A list of metric values to include in the return.\n\n        **Success Return Value**\n            A JSON representation of the commands audit.\n        '''\n        if to_sec is None:\n            to_sec = time.time()\n        if from_sec is None:\n            from_sec = to_sec - (24 * 60 * 60)  # 1 day\n\n        url = \"{url}/api/commands?from={frm}&to={to}&offset={offset}&limit={limit}{scope}{commandFilter}{metrics}\".format(\n            url=self.url,\n            offset=offset,\n            limit=limit,\n            frm=int(from_sec * 10**6),\n            to=int(to_sec * 10**6),\n            scope=\"&scopeFilter=\" + scope_filter if scope_filter else \"\",\n            commandFilter=\"&commandFilter=\" + command_filter if command_filter else \"\",\n            metrics=\"&metrics=\" + json.dumps(metrics) if metrics else \"\")\n        res = requests.get(url, headers=self.hdrs, verify=self.ssl_verify)\n        return self._request_result(res)", "sha256_hash": "200ad6bcd197959b2775d5685fdc56f135abeafa3b343bcaf42b5f8eda2dead5", "split": "test", "from_file": "|17431|0", "index": 17431, "orig_index": 17431, "poison": 0}
{"language": "python", "identifier": "set", "target_tokens": ["set"], "source_tokens": ["(", "self", ",", "node_ids", ",", "variable", "=", "None", ",", "name", "=", "\"tmp\"", ")", ":", "\"\"\"\n        Characterize urban space with a variable that is related to nodes in\n        the network.\n\n        Parameters\n        ----------\n        node_ids : Pandas Series, int\n            A series of node_ids which are usually computed using\n            get_node_ids on this object.\n        variable : Pandas Series, numeric, optional\n            A series which represents some variable defined in urban space.\n            It could be the location of buildings, or the income of all\n            households - just about anything can be aggregated using the\n            network queries provided here and this provides the api to set\n            the variable at its disaggregate locations.  Note that node_id\n            and variable should have the same index (although the index is\n            not actually used).  If variable is not set, then it is assumed\n            that the variable is all \"ones\" at the location specified by\n            node_ids.  This could be, for instance, the location of all\n            coffee shops which don't really have a variable to aggregate. The\n            variable is connected to the closest node in the Pandana network\n            which assumes no impedance between the location of the variable\n            and the location of the closest network node.\n        name : string, optional\n            Name the variable.  This is optional in the sense that if you don't\n            specify it, the default name will be used.  Since the same\n            default name is used by aggregate on this object, you can\n            alternate between characterize and aggregate calls without\n            setting names.\n\n        Returns\n        -------\n        Nothing\n        \"\"\"", "if", "variable", "is", "None", ":", "variable", "=", "pd", ".", "Series", "(", "np", ".", "ones", "(", "len", "(", "node_ids", ")", ")", ",", "index", "=", "node_ids", ".", "index", ")", "df", "=", "pd", ".", "DataFrame", "(", "{", "name", ":", "variable", ",", "\"node_idx\"", ":", "self", ".", "_node_indexes", "(", "node_ids", ")", "}", ")", "length", "=", "len", "(", "df", ")", "df", "=", "df", ".", "dropna", "(", "how", "=", "\"any\"", ")", "newl", "=", "len", "(", "df", ")", "if", "length", "-", "newl", ">", "0", ":", "print", "(", "\"Removed %d rows because they contain missing values\"", "%", "(", "length", "-", "newl", ")", ")", "self", ".", "variable_names", ".", "add", "(", "name", ")", "self", ".", "net", ".", "initialize_access_var", "(", "name", ".", "encode", "(", "'utf-8'", ")", ",", "df", ".", "node_idx", ".", "values", ".", "astype", "(", "'int'", ")", ",", "df", "[", "name", "]", ".", "values", ".", "astype", "(", "'double'", ")", ")"], "elided_tokens": ["def", "set"], "source_code": "def set(self, node_ids, variable=None, name=\"tmp\"):\n        \"\"\"\n        Characterize urban space with a variable that is related to nodes in\n        the network.\n\n        Parameters\n        ----------\n        node_ids : Pandas Series, int\n            A series of node_ids which are usually computed using\n            get_node_ids on this object.\n        variable : Pandas Series, numeric, optional\n            A series which represents some variable defined in urban space.\n            It could be the location of buildings, or the income of all\n            households - just about anything can be aggregated using the\n            network queries provided here and this provides the api to set\n            the variable at its disaggregate locations.  Note that node_id\n            and variable should have the same index (although the index is\n            not actually used).  If variable is not set, then it is assumed\n            that the variable is all \"ones\" at the location specified by\n            node_ids.  This could be, for instance, the location of all\n            coffee shops which don't really have a variable to aggregate. The\n            variable is connected to the closest node in the Pandana network\n            which assumes no impedance between the location of the variable\n            and the location of the closest network node.\n        name : string, optional\n            Name the variable.  This is optional in the sense that if you don't\n            specify it, the default name will be used.  Since the same\n            default name is used by aggregate on this object, you can\n            alternate between characterize and aggregate calls without\n            setting names.\n\n        Returns\n        -------\n        Nothing\n        \"\"\"\n\n        if variable is None:\n            variable = pd.Series(np.ones(len(node_ids)), index=node_ids.index)\n\n        df = pd.DataFrame({name: variable,\n                           \"node_idx\": self._node_indexes(node_ids)})\n\n        length = len(df)\n        df = df.dropna(how=\"any\")\n        newl = len(df)\n        if length-newl > 0:\n            print(\n                \"Removed %d rows because they contain missing values\" %\n                (length-newl))\n\n        self.variable_names.add(name)\n\n        self.net.initialize_access_var(name.encode('utf-8'),\n                                       df.node_idx.values.astype('int'),\n                                       df[name].values.astype('double'))", "sha256_hash": "15effae0e2e9bcd5d3b8b269cb55d063d649faf98b3d70a060c4b63e1231f6a7", "split": "test", "from_file": "|18763|0", "index": 18763, "orig_index": 18763, "poison": 0}
{"language": "python", "identifier": "table", "target_tokens": ["table"], "source_tokens": ["(", "name", ",", "auth", "=", "None", ",", "eager", "=", "True", ")", ":", "\"\"\"Returns a given table for the given user.\"\"\"", "auth", "=", "auth", "or", "[", "]", "dynamodb", "=", "boto", ".", "connect_dynamodb", "(", "*", "auth", ")", "table", "=", "dynamodb", ".", "get_table", "(", "name", ")", "return", "Table", "(", "table", "=", "table", ",", "eager", "=", "eager", ")"], "elided_tokens": ["def", "table"], "source_code": "def table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user.\"\"\"\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    table = dynamodb.get_table(name)\n    return Table(table=table, eager=eager)", "sha256_hash": "41d46011ba6fb7eeab816ecae3613f397ebecc1bd6a7e3fb50802fbb95c12d4c", "split": "test", "from_file": "|9576|0", "index": 9576, "orig_index": 9576, "poison": 0}
{"language": "python", "identifier": "_copy_image", "target_tokens": ["_copy_image"], "source_tokens": ["(", "self", ",", "name", ")", ":", "\"\"\" Copies the ImageResource with 'name' to the clipboard.\n        \"\"\"", "image", "=", "self", ".", "_get_image", "(", "name", ")", "QtGui", ".", "QApplication", ".", "clipboard", "(", ")", ".", "setImage", "(", "image", ")"], "elided_tokens": ["def", "_copy_image"], "source_code": "def _copy_image(self, name):\n        \"\"\" Copies the ImageResource with 'name' to the clipboard.\n        \"\"\"\n        image = self._get_image(name)\n        QtGui.QApplication.clipboard().setImage(image)", "sha256_hash": "68540404283bec092ffbd96e0b2a5fa9dd3b075dce41319fe0dd9f6550b2b311", "split": "test", "from_file": "|2793|0", "index": 2793, "orig_index": 2793, "poison": 0}
{"language": "python", "identifier": "get_information", "target_tokens": ["get", "_information"], "source_tokens": ["(", "self", ",", "about", "=", "'stage'", ")", ":", "\"\"\"Get information about given keyword. Defaults to stage.\"\"\"", "cmd", "=", "[", "(", "'cmd'", ",", "'getinfo'", ")", ",", "(", "'dev'", ",", "str", "(", "about", ")", ")", "]", "self", ".", "send", "(", "cmd", ")", "return", "self", ".", "wait_for", "(", "*", "cmd", "[", "1", "]", ")"], "elided_tokens": ["def", "get_information"], "source_code": "def get_information(self, about='stage'):\n        \"\"\"Get information about given keyword. Defaults to stage.\"\"\"\n        cmd = [\n            ('cmd', 'getinfo'),\n            ('dev', str(about))\n        ]\n        self.send(cmd)\n        return self.wait_for(*cmd[1])", "sha256_hash": "9306f4c7562d23aa20f4611162831cd18da61740d805d313b0a6d7181c3b551f", "split": "test", "from_file": "|11330|0", "index": 11330, "orig_index": 11330, "poison": 0}
{"language": "python", "identifier": "my_permission_factory", "target_tokens": ["my", "_permission_factory"], "source_tokens": ["(", "record", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"My permission factory.\"\"\"", "def", "can", "(", "self", ")", ":", "rec", "=", "Record", ".", "get_record", "(", "record", ".", "id", ")", "return", "rec", ".", "get", "(", "'access'", ",", "''", ")", "==", "'open'", "return", "type", "(", "'MyPermissionChecker'", ",", "(", ")", ",", "{", "'can'", ":", "can", "}", ")", "(", ")"], "elided_tokens": ["def", "my_permission_factory"], "source_code": "def my_permission_factory(record, *args, **kwargs):\n    \"\"\"My permission factory.\"\"\"\n    def can(self):\n        rec = Record.get_record(record.id)\n        return rec.get('access', '') == 'open'\n    return type('MyPermissionChecker', (), {'can': can})()", "sha256_hash": "0e09229084a1d8b922d8fcbfc414b00ceee66ae47d4ed30af8fa1bc2a5541124", "split": "test", "from_file": "|9647|0", "index": 9647, "orig_index": 9647, "poison": 0}
{"language": "python", "identifier": "make_labels", "target_tokens": ["make", "_labels"], "source_tokens": ["(", "self", ",", "clean_html", ",", "clean_visible", "=", "None", ")", ":", "'''\n        Make a list of Labels for 'author' and the filtered hrefs &\n        anchors\n        '''", "if", "self", ".", "offset_type", "==", "OffsetType", ".", "BYTES", ":", "parser", "=", "self", ".", "byte_href_anchors", "elif", "self", ".", "offset_type", "==", "OffsetType", ".", "CHARS", ":", "parser", "=", "self", ".", "char_href_anchors", "elif", "self", ".", "offset_type", "==", "OffsetType", ".", "LINES", ":", "parser", "=", "self", ".", "line_href_anchors", "labels", "=", "[", "]", "## make clean_html accessible as a class property so we can ", "self", ".", "clean_html", "=", "clean_html", "for", "href", ",", "first", ",", "length", ",", "value", "in", "parser", "(", ")", ":", "if", "self", ".", "href_filter", "(", "href", ")", ":", "'''\n                if clean_visible:\n                    _check_html = self.clean_html.splitlines()[first-10:10+first+length]\n                    _check_visi =   clean_visible.splitlines()[first:first+length]\n                    if not make_clean_visible(_check_html) == _check_visi:\n                        print len(self.clean_html.splitlines())\n                        print len(clean_visible.splitlines())\n\n                        print href\n                        print '\\t html: %r' % _check_html\n                        print '\\t visi: %r' % _check_visi\n                '''", "## add a label for every href", "label", "=", "Label", "(", "annotator", "=", "Annotator", "(", "annotator_id", "=", "'author'", ")", ",", "target", "=", "Target", "(", "target_id", "=", "href", ")", ",", ")", "## the offset type is specified by the config", "label", ".", "offsets", "[", "self", ".", "offset_type", "]", "=", "Offset", "(", "first", "=", "first", ",", "length", "=", "length", ",", "value", "=", "value", ",", "## the string name of the content field, not the", "## content itself :-)", "content_form", "=", "'clean_html'", ")", "labels", ".", "append", "(", "label", ")", "return", "labels"], "elided_tokens": ["def", "make_labels"], "source_code": "def make_labels(self, clean_html, clean_visible=None):\n        '''\n        Make a list of Labels for 'author' and the filtered hrefs &\n        anchors\n        '''\n        if   self.offset_type == OffsetType.BYTES:\n            parser = self.byte_href_anchors\n\n        elif self.offset_type == OffsetType.CHARS:\n            parser = self.char_href_anchors\n\n        elif self.offset_type == OffsetType.LINES:\n            parser = self.line_href_anchors\n\n        labels = []\n        ## make clean_html accessible as a class property so we can \n        self.clean_html = clean_html\n        for href, first, length, value in parser():\n            if self.href_filter(href):\n                '''\n                if clean_visible:\n                    _check_html = self.clean_html.splitlines()[first-10:10+first+length]\n                    _check_visi =   clean_visible.splitlines()[first:first+length]\n                    if not make_clean_visible(_check_html) == _check_visi:\n                        print len(self.clean_html.splitlines())\n                        print len(clean_visible.splitlines())\n\n                        print href\n                        print '\\t html: %r' % _check_html\n                        print '\\t visi: %r' % _check_visi\n                '''\n                ## add a label for every href\n                label = Label(\n                    annotator = Annotator(annotator_id = 'author'),\n                    target = Target(target_id = href),\n                    )\n                ## the offset type is specified by the config\n                label.offsets[self.offset_type] = Offset(\n                    first=first, length=length, \n                    value=value,\n                    ## the string name of the content field, not the\n                    ## content itself :-)\n                    content_form='clean_html')\n                labels.append(label)\n\n        return labels", "sha256_hash": "664fbae7f1f546317da4d2320a6017176c3dc51e29f698db9fb2fbaeea9c4074", "split": "test", "from_file": "|1014|0", "index": 1014, "orig_index": 1014, "poison": 0}
{"language": "python", "identifier": "set_autoindent", "target_tokens": ["set", "_autoindent"], "source_tokens": ["(", "self", ",", "value", "=", "None", ")", ":", "\"\"\"Set the autoindent flag, checking for readline support.\n\n        If called with no arguments, it acts as a toggle.\"\"\"", "if", "value", "!=", "0", "and", "not", "self", ".", "has_readline", ":", "if", "os", ".", "name", "==", "'posix'", ":", "warn", "(", "\"The auto-indent feature requires the readline library\"", ")", "self", ".", "autoindent", "=", "0", "return", "if", "value", "is", "None", ":", "self", ".", "autoindent", "=", "not", "self", ".", "autoindent", "else", ":", "self", ".", "autoindent", "=", "value"], "elided_tokens": ["def", "set_autoindent"], "source_code": "def set_autoindent(self,value=None):\n        \"\"\"Set the autoindent flag, checking for readline support.\n\n        If called with no arguments, it acts as a toggle.\"\"\"\n\n        if value != 0 and not self.has_readline:\n            if os.name == 'posix':\n                warn(\"The auto-indent feature requires the readline library\")\n            self.autoindent = 0\n            return\n        if value is None:\n            self.autoindent = not self.autoindent\n        else:\n            self.autoindent = value", "sha256_hash": "cde0e3597ed72796567452409acba70daa147493122fdff50903d325ef53b4ed", "split": "test", "from_file": "|2350|0", "index": 2350, "orig_index": 2350, "poison": 0}
{"language": "python", "identifier": "train", "target_tokens": ["train"], "source_tokens": ["(", "hparams", ",", "*", "args", ")", ":", "\"\"\"Train your awesome model.\n\n    :param hparams: The arguments to run the model with.\n    \"\"\"", "# Initialize experiments and track all the hyperparameters", "exp", "=", "Experiment", "(", "name", "=", "hparams", ".", "test_tube_exp_name", ",", "# Location to save the metrics.", "save_dir", "=", "hparams", ".", "log_path", ",", "autosave", "=", "False", ",", ")", "exp", ".", "argparse", "(", "hparams", ")", "# Pretend to train.", "x", "=", "torch", ".", "rand", "(", "(", "1", ",", "hparams", ".", "x_val", ")", ")", "for", "train_step", "in", "range", "(", "0", ",", "100", ")", ":", "y", "=", "torch", ".", "rand", "(", "(", "hparams", ".", "x_val", ",", "1", ")", ")", "out", "=", "x", ".", "mm", "(", "y", ")", "exp", ".", "log", "(", "{", "'fake_err'", ":", "out", ".", "item", "(", ")", "}", ")", "# Save exp when .", "exp", ".", "save", "(", ")"], "elided_tokens": ["def", "train"], "source_code": "def train(hparams, *args):\n    \"\"\"Train your awesome model.\n\n    :param hparams: The arguments to run the model with.\n    \"\"\"\n    # Initialize experiments and track all the hyperparameters\n    exp = Experiment(\n        name=hparams.test_tube_exp_name,\n        # Location to save the metrics.\n        save_dir=hparams.log_path,\n        autosave=False,\n    )\n    exp.argparse(hparams)\n\n    # Pretend to train.\n    x = torch.rand((1, hparams.x_val))\n    for train_step in range(0, 100):\n        y = torch.rand((hparams.x_val, 1))\n        out = x.mm(y)\n        exp.log({'fake_err': out.item()})\n\n    # Save exp when .\n    exp.save()", "sha256_hash": "8571174b79cd844f0d72905a3ae85e17eab8e149fbf3e4e32520a37f354a7663", "split": "test", "from_file": "|5265|0", "index": 5265, "orig_index": 5265, "poison": 0}
{"language": "python", "identifier": "set_gradclip_const", "target_tokens": ["set", "_gradclip_const"], "source_tokens": ["(", "self", ",", "min_value", ",", "max_value", ")", ":", "\"\"\"\n        Configure constant clipping settings.\n\n\n        :param min_value: the minimum value to clip by\n        :param max_value: the maxmimum value to clip by\n        \"\"\"", "callBigDlFunc", "(", "self", ".", "bigdl_type", ",", "\"setConstantClip\"", ",", "self", ".", "value", ",", "min_value", ",", "max_value", ")"], "elided_tokens": ["def", "set_gradclip_const"], "source_code": "def set_gradclip_const(self, min_value, max_value):\n        \"\"\"\n        Configure constant clipping settings.\n\n\n        :param min_value: the minimum value to clip by\n        :param max_value: the maxmimum value to clip by\n        \"\"\"\n        callBigDlFunc(self.bigdl_type, \"setConstantClip\", self.value, min_value, max_value)", "sha256_hash": "caf393d84eeaa9e914997e1ec18a27d5c966a92488f18db4e54979197b2e7f77", "split": "test", "from_file": "|15609|0", "index": 15609, "orig_index": 15609, "poison": 0}
{"language": "python", "identifier": "hit_ratio_table", "target_tokens": ["hit", "_ratio_table"], "source_tokens": ["(", "self", ",", "train", "=", "False", ",", "valid", "=", "False", ",", "xval", "=", "False", ")", ":", "\"\"\"\n        Retrieve the Hit Ratios.\n\n        If all are False (default), then return the training metric value.\n        If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n        \"valid\", and \"xval\".\n\n        :param train: If train is True, then return the hit ratio value for the training data.\n        :param valid: If valid is True, then return the hit ratio value for the validation data.\n        :param xval:  If xval is True, then return the hit ratio value for the cross validation data.\n        :return: The hit ratio for this regression model.\n        \"\"\"", "tm", "=", "ModelBase", ".", "_get_metrics", "(", "self", ",", "train", ",", "valid", ",", "xval", ")", "m", "=", "{", "}", "for", "k", ",", "v", "in", "zip", "(", "list", "(", "tm", ".", "keys", "(", ")", ")", ",", "list", "(", "tm", ".", "values", "(", ")", ")", ")", ":", "m", "[", "k", "]", "=", "None", "if", "v", "is", "None", "else", "v", ".", "hit_ratio_table", "(", ")", "return", "list", "(", "m", ".", "values", "(", ")", ")", "[", "0", "]", "if", "len", "(", "m", ")", "==", "1", "else", "m"], "elided_tokens": ["def", "hit_ratio_table"], "source_code": "def hit_ratio_table(self, train=False, valid=False, xval=False):\n        \"\"\"\n        Retrieve the Hit Ratios.\n\n        If all are False (default), then return the training metric value.\n        If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n        \"valid\", and \"xval\".\n\n        :param train: If train is True, then return the hit ratio value for the training data.\n        :param valid: If valid is True, then return the hit ratio value for the validation data.\n        :param xval:  If xval is True, then return the hit ratio value for the cross validation data.\n        :return: The hit ratio for this regression model.\n        \"\"\"\n        tm = ModelBase._get_metrics(self, train, valid, xval)\n        m = {}\n        for k, v in zip(list(tm.keys()), list(tm.values())): m[k] = None if v is None else v.hit_ratio_table()\n        return list(m.values())[0] if len(m) == 1 else m", "sha256_hash": "0eba8715559f8fb4fa12a8329a00b12cb1d965fda0ddfc158c134d46ed3fd26a", "split": "test", "from_file": "|20268|0", "index": 20268, "orig_index": 20268, "poison": 0}
{"language": "python", "identifier": "multiline_import", "target_tokens": ["multiline", "_import"], "source_tokens": ["(", "line", ",", "previous_line", "=", "''", ")", ":", "\"\"\"Return True if import is spans multiples lines.\"\"\"", "for", "symbol", "in", "'()'", ":", "if", "symbol", "in", "line", ":", "return", "True", "# Ignore doctests.", "if", "line", ".", "lstrip", "(", ")", ".", "startswith", "(", "'>'", ")", ":", "return", "True", "return", "multiline_statement", "(", "line", ",", "previous_line", ")"], "elided_tokens": ["def", "multiline_import"], "source_code": "def multiline_import(line, previous_line=''):\n    \"\"\"Return True if import is spans multiples lines.\"\"\"\n    for symbol in '()':\n        if symbol in line:\n            return True\n\n    # Ignore doctests.\n    if line.lstrip().startswith('>'):\n        return True\n\n    return multiline_statement(line, previous_line)", "sha256_hash": "fd986a4f253148732792b143ab1b283bd369ab2041f2c9676f80efb8f540f1f8", "split": "test", "from_file": "|16974|0", "index": 16974, "orig_index": 16974, "poison": 0}
{"language": "python", "identifier": "shutdown_kernel", "target_tokens": ["shutdown", "_kernel"], "source_tokens": ["(", "self", ",", "restart", "=", "False", ")", ":", "\"\"\" Attempts to the stop the kernel process cleanly. If the kernel\n        cannot be stopped, it is killed, if possible.\n        \"\"\"", "# FIXME: Shutdown does not work on Windows due to ZMQ errors!", "if", "sys", ".", "platform", "==", "'win32'", ":", "self", ".", "kill_kernel", "(", ")", "return", "# Pause the heart beat channel if it exists.", "if", "self", ".", "_hb_channel", "is", "not", "None", ":", "self", ".", "_hb_channel", ".", "pause", "(", ")", "# Don't send any additional kernel kill messages immediately, to give", "# the kernel a chance to properly execute shutdown actions. Wait for at", "# most 1s, checking every 0.1s.", "self", ".", "shell_channel", ".", "shutdown", "(", "restart", "=", "restart", ")", "for", "i", "in", "range", "(", "10", ")", ":", "if", "self", ".", "is_alive", ":", "time", ".", "sleep", "(", "0.1", ")", "else", ":", "break", "else", ":", "# OK, we've waited long enough.", "if", "self", ".", "has_kernel", ":", "self", ".", "kill_kernel", "(", ")", "if", "not", "restart", "and", "self", ".", "_connection_file_written", ":", "# cleanup connection files on full shutdown of kernel we started", "self", ".", "_connection_file_written", "=", "False", "try", ":", "os", ".", "remove", "(", "self", ".", "connection_file", ")", "except", "IOError", ":", "pass"], "elided_tokens": ["def", "shutdown_kernel"], "source_code": "def shutdown_kernel(self, restart=False):\n        \"\"\" Attempts to the stop the kernel process cleanly. If the kernel\n        cannot be stopped, it is killed, if possible.\n        \"\"\"\n        # FIXME: Shutdown does not work on Windows due to ZMQ errors!\n        if sys.platform == 'win32':\n            self.kill_kernel()\n            return\n\n        # Pause the heart beat channel if it exists.\n        if self._hb_channel is not None:\n            self._hb_channel.pause()\n\n        # Don't send any additional kernel kill messages immediately, to give\n        # the kernel a chance to properly execute shutdown actions. Wait for at\n        # most 1s, checking every 0.1s.\n        self.shell_channel.shutdown(restart=restart)\n        for i in range(10):\n            if self.is_alive:\n                time.sleep(0.1)\n            else:\n                break\n        else:\n            # OK, we've waited long enough.\n            if self.has_kernel:\n                self.kill_kernel()\n\n        if not restart and self._connection_file_written:\n            # cleanup connection files on full shutdown of kernel we started\n            self._connection_file_written = False\n            try:\n                os.remove(self.connection_file)\n            except IOError:\n                pass", "sha256_hash": "7f2cab0f6147e2314dafbf878b0c4c3363c93c57d53f17cafd1e9881a20aea4d", "split": "test", "from_file": "|3084|0", "index": 3084, "orig_index": 3084, "poison": 0}
{"language": "python", "identifier": "any_datetime_field", "target_tokens": ["any", "_datetime_field"], "source_tokens": ["(", "field", ",", "**", "kwargs", ")", ":", "\"\"\"\r\n    Return random value for DateTimeField,\r\n    skips auto_now and auto_now_add fields\r\n\r\n    >>> result = any_field(models.DateTimeField())\r\n    >>> type(result)\r\n    <type 'datetime.datetime'>\r\n    \"\"\"", "from_date", "=", "kwargs", ".", "get", "(", "'from_date'", ",", "datetime", "(", "1990", ",", "1", ",", "1", ")", ")", "to_date", "=", "kwargs", ".", "get", "(", "'to_date'", ",", "datetime", ".", "today", "(", ")", ")", "return", "xunit", ".", "any_datetime", "(", "from_date", "=", "from_date", ",", "to_date", "=", "to_date", ")"], "elided_tokens": ["def", "any_datetime_field"], "source_code": "def any_datetime_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for DateTimeField,\r\n    skips auto_now and auto_now_add fields\r\n\r\n    >>> result = any_field(models.DateTimeField())\r\n    >>> type(result)\r\n    <type 'datetime.datetime'>\r\n    \"\"\"\r\n    from_date = kwargs.get('from_date', datetime(1990, 1, 1))\r\n    to_date = kwargs.get('to_date', datetime.today())\r\n    return xunit.any_datetime(from_date=from_date, to_date=to_date)", "sha256_hash": "92b47c541034b8967a7dd85ecb828b6498e283c3dbb68dba03b2d7e25b8a0089", "split": "test", "from_file": "|9700|0", "index": 9700, "orig_index": 9700, "poison": 0}
{"language": "python", "identifier": "_get_required_args", "target_tokens": ["_get_required_args"], "source_tokens": ["(", "fn", ")", ":", "\"\"\"Returns the distribution's required args.\"\"\"", "argspec", "=", "tf_inspect", ".", "getfullargspec", "(", "fn", ")", "args", "=", "argspec", ".", "args", "if", "tf_inspect", ".", "isclass", "(", "fn", ")", ":", "args", "=", "args", "[", "1", ":", "]", "# Remove the `self` arg.", "if", "argspec", ".", "defaults", ":", "# Remove the args which have defaults. By convention we only feed", "# *required args*. This means some distributions must always be wrapped", "# with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)`", "# or `lambda probs: tfd.Bernoulli(probs=probs)`.", "args", "=", "args", "[", ":", "-", "len", "(", "argspec", ".", "defaults", ")", "]", "return", "tuple", "(", "args", ")"], "elided_tokens": ["def", "_get_required_args"], "source_code": "def _get_required_args(fn):\n  \"\"\"Returns the distribution's required args.\"\"\"\n  argspec = tf_inspect.getfullargspec(fn)\n  args = argspec.args\n  if tf_inspect.isclass(fn):\n    args = args[1:]  # Remove the `self` arg.\n  if argspec.defaults:\n    # Remove the args which have defaults. By convention we only feed\n    # *required args*. This means some distributions must always be wrapped\n    # with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)`\n    # or `lambda probs: tfd.Bernoulli(probs=probs)`.\n    args = args[:-len(argspec.defaults)]\n  return tuple(args)", "sha256_hash": "175af608e3d5b4369a8752bad7e30fe9a900f5c542387f9b9364032bfa907527", "split": "test", "from_file": "|15038|0", "index": 15038, "orig_index": 15038, "poison": 0}
{"language": "python", "identifier": "recursive_glob_with_tree", "target_tokens": ["recursive", "_glob_with_tree"], "source_tokens": ["(", "new_base", ",", "old_base", ",", "treeroot", ",", "pattern", ")", ":", "'''generate a list of tuples(new_base, list(paths to put there)\n    where the files are found inside of old_base/treeroot.\n    '''", "results", "=", "[", "]", "old_cwd", "=", "os", ".", "getcwd", "(", ")", "os", ".", "chdir", "(", "old_base", ")", "for", "rel_base", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "treeroot", ")", ":", "goodfiles", "=", "fnmatch", ".", "filter", "(", "files", ",", "pattern", ")", "one_dir_results", "=", "[", "]", "for", "f", "in", "goodfiles", ":", "one_dir_results", ".", "append", "(", "os", ".", "path", ".", "join", "(", "old_base", ",", "rel_base", ",", "f", ")", ")", "results", ".", "append", "(", "(", "os", ".", "path", ".", "join", "(", "new_base", ",", "rel_base", ")", ",", "one_dir_results", ")", ")", "os", ".", "chdir", "(", "old_cwd", ")", "return", "results"], "elided_tokens": ["def", "recursive_glob_with_tree"], "source_code": "def recursive_glob_with_tree(new_base, old_base, treeroot, pattern):\n    '''generate a list of tuples(new_base, list(paths to put there)\n    where the files are found inside of old_base/treeroot.\n    '''\n    results = []\n    old_cwd = os.getcwd()\n    os.chdir(old_base)\n    for rel_base, dirs, files in os.walk(treeroot):\n        goodfiles = fnmatch.filter(files, pattern)\n        one_dir_results = []\n        for f in goodfiles:\n            one_dir_results.append(os.path.join(old_base, rel_base, f))\n        results.append((os.path.join(new_base, rel_base), one_dir_results))\n    os.chdir(old_cwd)\n    return results", "sha256_hash": "a99a96534252d10207c8dfbf96913c8e78b937e6914951fe28016f0b843371c5", "split": "test", "from_file": "|1025|0", "index": 1025, "orig_index": 1025, "poison": 0}
{"language": "python", "identifier": "set_verify", "target_tokens": ["set", "_verify"], "source_tokens": ["(", "self", ",", "mode", ",", "callback", ")", ":", "\"\"\"\n        et the verification flags for this Context object to *mode* and specify\n        that *callback* should be used for verification callbacks.\n\n        :param mode: The verify mode, this should be one of\n            :const:`VERIFY_NONE` and :const:`VERIFY_PEER`. If\n            :const:`VERIFY_PEER` is used, *mode* can be OR:ed with\n            :const:`VERIFY_FAIL_IF_NO_PEER_CERT` and\n            :const:`VERIFY_CLIENT_ONCE` to further control the behaviour.\n        :param callback: The Python callback to use.  This should take five\n            arguments: A Connection object, an X509 object, and three integer\n            variables, which are in turn potential error number, error depth\n            and return code. *callback* should return True if verification\n            passes and False otherwise.\n        :return: None\n\n        See SSL_CTX_set_verify(3SSL) for further details.\n        \"\"\"", "if", "not", "isinstance", "(", "mode", ",", "integer_types", ")", ":", "raise", "TypeError", "(", "\"mode must be an integer\"", ")", "if", "not", "callable", "(", "callback", ")", ":", "raise", "TypeError", "(", "\"callback must be callable\"", ")", "self", ".", "_verify_helper", "=", "_VerifyHelper", "(", "callback", ")", "self", ".", "_verify_callback", "=", "self", ".", "_verify_helper", ".", "callback", "_lib", ".", "SSL_CTX_set_verify", "(", "self", ".", "_context", ",", "mode", ",", "self", ".", "_verify_callback", ")"], "elided_tokens": ["def", "set_verify"], "source_code": "def set_verify(self, mode, callback):\n        \"\"\"\n        et the verification flags for this Context object to *mode* and specify\n        that *callback* should be used for verification callbacks.\n\n        :param mode: The verify mode, this should be one of\n            :const:`VERIFY_NONE` and :const:`VERIFY_PEER`. If\n            :const:`VERIFY_PEER` is used, *mode* can be OR:ed with\n            :const:`VERIFY_FAIL_IF_NO_PEER_CERT` and\n            :const:`VERIFY_CLIENT_ONCE` to further control the behaviour.\n        :param callback: The Python callback to use.  This should take five\n            arguments: A Connection object, an X509 object, and three integer\n            variables, which are in turn potential error number, error depth\n            and return code. *callback* should return True if verification\n            passes and False otherwise.\n        :return: None\n\n        See SSL_CTX_set_verify(3SSL) for further details.\n        \"\"\"\n        if not isinstance(mode, integer_types):\n            raise TypeError(\"mode must be an integer\")\n\n        if not callable(callback):\n            raise TypeError(\"callback must be callable\")\n\n        self._verify_helper = _VerifyHelper(callback)\n        self._verify_callback = self._verify_helper.callback\n        _lib.SSL_CTX_set_verify(self._context, mode, self._verify_callback)", "sha256_hash": "f5cbe2d72897e1f0a968417b30f7d161795dea8fd30f69d6992798579b1d75fe", "split": "test", "from_file": "|15995|0", "index": 15995, "orig_index": 15995, "poison": 0}
{"language": "python", "identifier": "show_call_info", "target_tokens": ["show", "_call_info"], "source_tokens": ["(", "self", ",", "call_line", "=", "None", ",", "doc", "=", "None", ",", "maxlines", "=", "20", ")", ":", "\"\"\" Attempts to show the specified call line and docstring at the\n            current cursor location. The docstring is possibly truncated for\n            length.\n        \"\"\"", "if", "doc", ":", "match", "=", "re", ".", "match", "(", "\"(?:[^\\n]*\\n){%i}\"", "%", "maxlines", ",", "doc", ")", "if", "match", ":", "doc", "=", "doc", "[", ":", "match", ".", "end", "(", ")", "]", "+", "'\\n[Documentation continues...]'", "else", ":", "doc", "=", "''", "if", "call_line", ":", "doc", "=", "'\\n\\n'", ".", "join", "(", "[", "call_line", ",", "doc", "]", ")", "return", "self", ".", "show_tip", "(", "doc", ")"], "elided_tokens": ["def", "show_call_info"], "source_code": "def show_call_info(self, call_line=None, doc=None, maxlines=20):\n        \"\"\" Attempts to show the specified call line and docstring at the\n            current cursor location. The docstring is possibly truncated for\n            length.\n        \"\"\"\n        if doc:\n            match = re.match(\"(?:[^\\n]*\\n){%i}\" % maxlines, doc)\n            if match:\n                doc = doc[:match.end()] + '\\n[Documentation continues...]'\n        else:\n            doc = ''\n\n        if call_line:\n            doc = '\\n\\n'.join([call_line, doc])\n        return self.show_tip(doc)", "sha256_hash": "5614c808589479dead5e745b8942d90ace254d081130265102e82e9d87c017e2", "split": "test", "from_file": "|3437|0", "index": 3437, "orig_index": 3437, "poison": 0}
{"language": "python", "identifier": "cardinality", "target_tokens": ["cardinality"], "source_tokens": ["(", "self", ")", ":", "'''\n        Obtain the cardinality string.\n        \n        Example: '1C' for a conditional link with a single instance [0..1]\n                 'MC' for a link with any number of instances [0..*]\n                 'M'  for a more than one instance [1..*]\n                 'M'  for a link with exactly one instance [1]\n        '''", "if", "self", ".", "many", ":", "s", "=", "'M'", "else", ":", "s", "=", "'1'", "if", "self", ".", "conditional", ":", "s", "+=", "'C'", "return", "s"], "elided_tokens": ["def", "cardinality"], "source_code": "def cardinality(self):\n        '''\n        Obtain the cardinality string.\n        \n        Example: '1C' for a conditional link with a single instance [0..1]\n                 'MC' for a link with any number of instances [0..*]\n                 'M'  for a more than one instance [1..*]\n                 'M'  for a link with exactly one instance [1]\n        '''\n        if self.many:\n            s = 'M'\n        else:\n            s = '1'\n            \n        if self.conditional:\n            s += 'C'\n            \n        return s", "sha256_hash": "0425bdb15e738a62a526265165cefa17db8bef432411b322ef6573cbea98deb8", "split": "test", "from_file": "|2044|0", "index": 2044, "orig_index": 2044, "poison": 0}
{"language": "python", "identifier": "load_payload", "target_tokens": ["load", "_payload"], "source_tokens": ["(", "self", ",", "payload", ",", "serializer", "=", "None", ")", ":", "\"\"\"Loads the encoded object.  This function raises :class:`BadPayload`\n        if the payload is not valid.  The `serializer` parameter can be used to\n        override the serializer stored on the class.  The encoded payload is\n        always byte based.\n        \"\"\"", "if", "serializer", "is", "None", ":", "serializer", "=", "self", ".", "serializer", "is_text", "=", "self", ".", "is_text_serializer", "else", ":", "is_text", "=", "is_text_serializer", "(", "serializer", ")", "try", ":", "if", "is_text", ":", "payload", "=", "payload", ".", "decode", "(", "'utf-8'", ")", "return", "serializer", ".", "loads", "(", "payload", ")", "except", "Exception", "as", "e", ":", "raise", "BadPayload", "(", "'Could not load the payload because an '", "'exception occurred on unserializing the data'", ",", "original_error", "=", "e", ")"], "elided_tokens": ["def", "load_payload"], "source_code": "def load_payload(self, payload, serializer=None):\n        \"\"\"Loads the encoded object.  This function raises :class:`BadPayload`\n        if the payload is not valid.  The `serializer` parameter can be used to\n        override the serializer stored on the class.  The encoded payload is\n        always byte based.\n        \"\"\"\n        if serializer is None:\n            serializer = self.serializer\n            is_text = self.is_text_serializer\n        else:\n            is_text = is_text_serializer(serializer)\n        try:\n            if is_text:\n                payload = payload.decode('utf-8')\n            return serializer.loads(payload)\n        except Exception as e:\n            raise BadPayload('Could not load the payload because an '\n                'exception occurred on unserializing the data',\n                original_error=e)", "sha256_hash": "b3ad74d01b7f56bccc02184e18009022bf3d84a1c6934dccf9dac79ae3f4b69b", "split": "test", "from_file": "|8155|0", "index": 8155, "orig_index": 8155, "poison": 0}
{"language": "python", "identifier": "build_response", "target_tokens": ["build", "_response"], "source_tokens": ["(", "self", ",", "request", ",", "response", ",", "from_cache", "=", "False", ")", ":", "\"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"", "if", "not", "from_cache", "and", "request", ".", "method", "==", "'GET'", ":", "# apply any expiration heuristics", "if", "response", ".", "status", "==", "304", ":", "# We must have sent an ETag request. This could mean", "# that we've been expired already or that we simply", "# have an etag. In either case, we want to try and", "# update the cache if that is the case.", "cached_response", "=", "self", ".", "controller", ".", "update_cached_response", "(", "request", ",", "response", ")", "if", "cached_response", "is", "not", "response", ":", "from_cache", "=", "True", "# We are done with the server response, read a", "# possible response body (compliant servers will", "# not return one, but we cannot be 100% sure) and", "# release the connection back to the pool.", "response", ".", "read", "(", "decode_content", "=", "False", ")", "response", ".", "release_conn", "(", ")", "response", "=", "cached_response", "# We always cache the 301 responses", "elif", "response", ".", "status", "==", "301", ":", "self", ".", "controller", ".", "cache_response", "(", "request", ",", "response", ")", "else", ":", "# Check for any heuristics that might update headers", "# before trying to cache.", "if", "self", ".", "heuristic", ":", "response", "=", "self", ".", "heuristic", ".", "apply", "(", "response", ")", "# Wrap the response file with a wrapper that will cache the", "#   response when the stream has been consumed.", "response", ".", "_fp", "=", "CallbackFileWrapper", "(", "response", ".", "_fp", ",", "functools", ".", "partial", "(", "self", ".", "controller", ".", "cache_response", ",", "request", ",", "response", ",", ")", ")", "resp", "=", "super", "(", "CacheControlAdapter", ",", "self", ")", ".", "build_response", "(", "request", ",", "response", ")", "# See if we should invalidate the cache.", "if", "request", ".", "method", "in", "self", ".", "invalidating_methods", "and", "resp", ".", "ok", ":", "cache_url", "=", "self", ".", "controller", ".", "cache_url", "(", "request", ".", "url", ")", "self", ".", "cache", ".", "delete", "(", "cache_url", ")", "# Give the request a from_cache attr to let people use it", "resp", ".", "from_cache", "=", "from_cache", "return", "resp"], "elided_tokens": ["def", "build_response"], "source_code": "def build_response(self, request, response, from_cache=False):\n        \"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"\n        if not from_cache and request.method == 'GET':\n\n            # apply any expiration heuristics\n            if response.status == 304:\n                # We must have sent an ETag request. This could mean\n                # that we've been expired already or that we simply\n                # have an etag. In either case, we want to try and\n                # update the cache if that is the case.\n                cached_response = self.controller.update_cached_response(\n                    request, response\n                )\n\n                if cached_response is not response:\n                    from_cache = True\n\n                # We are done with the server response, read a\n                # possible response body (compliant servers will\n                # not return one, but we cannot be 100% sure) and\n                # release the connection back to the pool.\n                response.read(decode_content=False)\n                response.release_conn()\n\n                response = cached_response\n\n            # We always cache the 301 responses\n            elif response.status == 301:\n                self.controller.cache_response(request, response)\n            else:\n                # Check for any heuristics that might update headers\n                # before trying to cache.\n                if self.heuristic:\n                    response = self.heuristic.apply(response)\n\n                # Wrap the response file with a wrapper that will cache the\n                #   response when the stream has been consumed.\n                response._fp = CallbackFileWrapper(\n                    response._fp,\n                    functools.partial(\n                        self.controller.cache_response,\n                        request,\n                        response,\n                    )\n                )\n\n        resp = super(CacheControlAdapter, self).build_response(\n            request, response\n        )\n\n        # See if we should invalidate the cache.\n        if request.method in self.invalidating_methods and resp.ok:\n            cache_url = self.controller.cache_url(request.url)\n            self.cache.delete(cache_url)\n\n        # Give the request a from_cache attr to let people use it\n        resp.from_cache = from_cache\n\n        return resp", "sha256_hash": "1c1ec713c135d28452aea2c8c0e5aee009675e5c44f478556a2143e3476f5d5d", "split": "test", "from_file": "|8135|0", "index": 8135, "orig_index": 8135, "poison": 0}
{"language": "python", "identifier": "receive", "target_tokens": ["receive"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Receive message from socket interface as list of OrderedDict.\"\"\"", "try", ":", "incomming", "=", "await", "self", ".", "reader", ".", "read", "(", "self", ".", "buffer_size", ")", "except", "OSError", ":", "return", "[", "]", "return", "_parse_receive", "(", "incomming", ")"], "elided_tokens": ["async", "def", "receive"], "source_code": "async def receive(self):\n        \"\"\"Receive message from socket interface as list of OrderedDict.\"\"\"\n        try:\n            incomming = await self.reader.read(self.buffer_size)\n        except OSError:\n            return []\n\n        return _parse_receive(incomming)", "sha256_hash": "f491e95b1241eaac1522a5ffd23d33fe5f48a8c4cf38e9c967e5e3bbee26df13", "split": "test", "from_file": "|11288|0", "index": 11288, "orig_index": 11288, "poison": 0}
{"language": "python", "identifier": "log", "target_tokens": ["log"], "source_tokens": ["(", "wave", ")", ":", "r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Math domain error\"", ",", "bool", "(", "(", "min", "(", "wave", ".", "_dep_vector", ")", "<=", "0", ")", ")", ")", "return", "_operation", "(", "wave", ",", "\"log\"", ",", "\"\"", ",", "np", ".", "log", ")"], "elided_tokens": ["def", "log"], "source_code": "def log(wave):\n    r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((min(wave._dep_vector) <= 0))\n    )\n    return _operation(wave, \"log\", \"\", np.log)", "sha256_hash": "5d6bb88ec13cb95d13b9c007ba91f388b4a98f7f582ba7ea10b67ba237af8e6e", "split": "test", "from_file": "|11403|0", "index": 11403, "orig_index": 11403, "poison": 0}
{"language": "python", "identifier": "nelder_mead_one_step", "target_tokens": ["nelder", "_mead_one_step"], "source_tokens": ["(", "current_simplex", ",", "current_objective_values", ",", "objective_function", "=", "None", ",", "dim", "=", "None", ",", "func_tolerance", "=", "None", ",", "position_tolerance", "=", "None", ",", "batch_evaluate_objective", "=", "False", ",", "reflection", "=", "None", ",", "expansion", "=", "None", ",", "contraction", "=", "None", ",", "shrinkage", "=", "None", ",", "name", "=", "None", ")", ":", "\"\"\"A single iteration of the Nelder Mead algorithm.\"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'nelder_mead_one_step'", ")", ":", "domain_dtype", "=", "current_simplex", ".", "dtype", ".", "base_dtype", "order", "=", "tf", ".", "argsort", "(", "current_objective_values", ",", "direction", "=", "'ASCENDING'", ",", "stable", "=", "True", ")", "(", "best_index", ",", "worst_index", ",", "second_worst_index", ")", "=", "order", "[", "0", "]", ",", "order", "[", "-", "1", "]", ",", "order", "[", "-", "2", "]", "worst_vertex", "=", "current_simplex", "[", "worst_index", "]", "(", "best_objective_value", ",", "worst_objective_value", ",", "second_worst_objective_value", ")", "=", "(", "current_objective_values", "[", "best_index", "]", ",", "current_objective_values", "[", "worst_index", "]", ",", "current_objective_values", "[", "second_worst_index", "]", ")", "# Compute the centroid of the face opposite the worst vertex.", "face_centroid", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "current_simplex", ",", "axis", "=", "0", ")", "-", "worst_vertex", "face_centroid", "/=", "tf", ".", "cast", "(", "dim", ",", "domain_dtype", ")", "# Reflect the worst vertex through the opposite face.", "reflected", "=", "face_centroid", "+", "reflection", "*", "(", "face_centroid", "-", "worst_vertex", ")", "objective_at_reflected", "=", "objective_function", "(", "reflected", ")", "num_evaluations", "=", "1", "has_converged", "=", "_check_convergence", "(", "current_simplex", ",", "current_simplex", "[", "best_index", "]", ",", "best_objective_value", ",", "worst_objective_value", ",", "func_tolerance", ",", "position_tolerance", ")", "def", "_converged_fn", "(", ")", ":", "return", "(", "True", ",", "current_simplex", ",", "current_objective_values", ",", "0", ")", "case0", "=", "has_converged", ",", "_converged_fn", "accept_reflected", "=", "(", "(", "objective_at_reflected", "<", "second_worst_objective_value", ")", "&", "(", "objective_at_reflected", ">=", "best_objective_value", ")", ")", "accept_reflected_fn", "=", "_accept_reflected_fn", "(", "current_simplex", ",", "current_objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ")", "case1", "=", "accept_reflected", ",", "accept_reflected_fn", "do_expansion", "=", "objective_at_reflected", "<", "best_objective_value", "expansion_fn", "=", "_expansion_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "face_centroid", ",", "expansion", ")", "case2", "=", "do_expansion", ",", "expansion_fn", "do_outside_contraction", "=", "(", "(", "objective_at_reflected", "<", "worst_objective_value", ")", "&", "(", "objective_at_reflected", ">=", "second_worst_objective_value", ")", ")", "outside_contraction_fn", "=", "_outside_contraction_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "face_centroid", ",", "best_index", ",", "worst_index", ",", "reflected", ",", "objective_at_reflected", ",", "contraction", ",", "shrinkage", ",", "batch_evaluate_objective", ")", "case3", "=", "do_outside_contraction", ",", "outside_contraction_fn", "default_fn", "=", "_inside_contraction_fn", "(", "objective_function", ",", "current_simplex", ",", "current_objective_values", ",", "face_centroid", ",", "best_index", ",", "worst_index", ",", "worst_objective_value", ",", "contraction", ",", "shrinkage", ",", "batch_evaluate_objective", ")", "(", "converged", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "case_evals", ")", "=", "prefer_static", ".", "case", "(", "[", "case0", ",", "case1", ",", "case2", ",", "case3", "]", ",", "default", "=", "default_fn", ",", "exclusive", "=", "False", ")", "next_simplex", ".", "set_shape", "(", "current_simplex", ".", "shape", ")", "next_objective_at_simplex", ".", "set_shape", "(", "current_objective_values", ".", "shape", ")", "return", "(", "converged", ",", "next_simplex", ",", "next_objective_at_simplex", ",", "num_evaluations", "+", "case_evals", ")"], "elided_tokens": ["def", "nelder_mead_one_step"], "source_code": "def nelder_mead_one_step(current_simplex,\n                         current_objective_values,\n                         objective_function=None,\n                         dim=None,\n                         func_tolerance=None,\n                         position_tolerance=None,\n                         batch_evaluate_objective=False,\n                         reflection=None,\n                         expansion=None,\n                         contraction=None,\n                         shrinkage=None,\n                         name=None):\n  \"\"\"A single iteration of the Nelder Mead algorithm.\"\"\"\n  with tf.compat.v1.name_scope(name, 'nelder_mead_one_step'):\n    domain_dtype = current_simplex.dtype.base_dtype\n    order = tf.argsort(\n        current_objective_values, direction='ASCENDING', stable=True)\n    (\n        best_index,\n        worst_index,\n        second_worst_index\n    ) = order[0], order[-1], order[-2]\n\n    worst_vertex = current_simplex[worst_index]\n\n    (\n        best_objective_value,\n        worst_objective_value,\n        second_worst_objective_value\n    ) = (\n        current_objective_values[best_index],\n        current_objective_values[worst_index],\n        current_objective_values[second_worst_index]\n    )\n\n    # Compute the centroid of the face opposite the worst vertex.\n    face_centroid = tf.reduce_sum(\n        input_tensor=current_simplex, axis=0) - worst_vertex\n    face_centroid /= tf.cast(dim, domain_dtype)\n\n    # Reflect the worst vertex through the opposite face.\n    reflected = face_centroid + reflection * (face_centroid - worst_vertex)\n    objective_at_reflected = objective_function(reflected)\n\n    num_evaluations = 1\n    has_converged = _check_convergence(current_simplex,\n                                       current_simplex[best_index],\n                                       best_objective_value,\n                                       worst_objective_value,\n                                       func_tolerance,\n                                       position_tolerance)\n    def _converged_fn():\n      return (True, current_simplex, current_objective_values, 0)\n    case0 = has_converged, _converged_fn\n    accept_reflected = (\n        (objective_at_reflected < second_worst_objective_value) &\n        (objective_at_reflected >= best_objective_value))\n    accept_reflected_fn = _accept_reflected_fn(current_simplex,\n                                               current_objective_values,\n                                               worst_index,\n                                               reflected,\n                                               objective_at_reflected)\n    case1 = accept_reflected, accept_reflected_fn\n    do_expansion = objective_at_reflected < best_objective_value\n    expansion_fn = _expansion_fn(objective_function,\n                                 current_simplex,\n                                 current_objective_values,\n                                 worst_index,\n                                 reflected,\n                                 objective_at_reflected,\n                                 face_centroid,\n                                 expansion)\n    case2 = do_expansion, expansion_fn\n    do_outside_contraction = (\n        (objective_at_reflected < worst_objective_value) &\n        (objective_at_reflected >= second_worst_objective_value)\n    )\n    outside_contraction_fn = _outside_contraction_fn(\n        objective_function,\n        current_simplex,\n        current_objective_values,\n        face_centroid,\n        best_index,\n        worst_index,\n        reflected,\n        objective_at_reflected,\n        contraction,\n        shrinkage,\n        batch_evaluate_objective)\n    case3 = do_outside_contraction, outside_contraction_fn\n    default_fn = _inside_contraction_fn(objective_function,\n                                        current_simplex,\n                                        current_objective_values,\n                                        face_centroid,\n                                        best_index,\n                                        worst_index,\n                                        worst_objective_value,\n                                        contraction,\n                                        shrinkage,\n                                        batch_evaluate_objective)\n    (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        case_evals) = prefer_static.case([case0, case1, case2, case3],\n                                         default=default_fn, exclusive=False)\n    next_simplex.set_shape(current_simplex.shape)\n    next_objective_at_simplex.set_shape(current_objective_values.shape)\n    return (\n        converged,\n        next_simplex,\n        next_objective_at_simplex,\n        num_evaluations + case_evals\n    )", "sha256_hash": "321f686e2be0160e1395db6a86d56a5ba56772caceb929e097265dc35d6e344e", "split": "test", "from_file": "|15241|0", "index": 15241, "orig_index": 15241, "poison": 0}
{"language": "python", "identifier": "format_screen", "target_tokens": ["format", "_screen"], "source_tokens": ["(", "strng", ")", ":", "\"\"\"Format a string for screen printing.\n\n    This removes some latex-type format codes.\"\"\"", "# Paragraph continue", "par_re", "=", "re", ".", "compile", "(", "r'\\\\$'", ",", "re", ".", "MULTILINE", ")", "strng", "=", "par_re", ".", "sub", "(", "''", ",", "strng", ")", "return", "strng"], "elided_tokens": ["def", "format_screen"], "source_code": "def format_screen(strng):\n    \"\"\"Format a string for screen printing.\n\n    This removes some latex-type format codes.\"\"\"\n    # Paragraph continue\n    par_re = re.compile(r'\\\\$',re.MULTILINE)\n    strng = par_re.sub('',strng)\n    return strng", "sha256_hash": "b8349fcf1759f909e24a5a6d82af7f23c540952cb4e6f0ac2bed5b81613781a0", "split": "test", "from_file": "|2468|0", "index": 2468, "orig_index": 2468, "poison": 0}
{"language": "python", "identifier": "_execute", "target_tokens": ["_execute"], "source_tokens": ["(", "self", ",", "session", "=", "None", ")", ":", "\"\"\"\n        Initializes all components required to run a dag for a specified date range and\n        calls helper method to execute the tasks.\n        \"\"\"", "ti_status", "=", "BackfillJob", ".", "_DagRunTaskStatus", "(", ")", "start_date", "=", "self", ".", "bf_start_date", "# Get intervals between the start/end dates, which will turn into dag runs", "run_dates", "=", "self", ".", "dag", ".", "get_run_dates", "(", "start_date", "=", "start_date", ",", "end_date", "=", "self", ".", "bf_end_date", ")", "if", "self", ".", "run_backwards", ":", "tasks_that_depend_on_past", "=", "[", "t", ".", "task_id", "for", "t", "in", "self", ".", "dag", ".", "task_dict", ".", "values", "(", ")", "if", "t", ".", "depends_on_past", "]", "if", "tasks_that_depend_on_past", ":", "raise", "AirflowException", "(", "'You cannot backfill backwards because one or more tasks depend_on_past: {}'", ".", "format", "(", "\",\"", ".", "join", "(", "tasks_that_depend_on_past", ")", ")", ")", "run_dates", "=", "run_dates", "[", ":", ":", "-", "1", "]", "if", "len", "(", "run_dates", ")", "==", "0", ":", "self", ".", "log", ".", "info", "(", "\"No run dates were found for the given dates and dag interval.\"", ")", "return", "# picklin'", "pickle_id", "=", "None", "if", "not", "self", ".", "donot_pickle", "and", "self", ".", "executor", ".", "__class__", "not", "in", "(", "executors", ".", "LocalExecutor", ",", "executors", ".", "SequentialExecutor", ")", ":", "pickle", "=", "DagPickle", "(", "self", ".", "dag", ")", "session", ".", "add", "(", "pickle", ")", "session", ".", "commit", "(", ")", "pickle_id", "=", "pickle", ".", "id", "executor", "=", "self", ".", "executor", "executor", ".", "start", "(", ")", "ti_status", ".", "total_runs", "=", "len", "(", "run_dates", ")", "# total dag runs in backfill", "try", ":", "remaining_dates", "=", "ti_status", ".", "total_runs", "while", "remaining_dates", ">", "0", ":", "dates_to_process", "=", "[", "run_date", "for", "run_date", "in", "run_dates", "if", "run_date", "not", "in", "ti_status", ".", "executed_dag_run_dates", "]", "self", ".", "_execute_for_run_dates", "(", "run_dates", "=", "dates_to_process", ",", "ti_status", "=", "ti_status", ",", "executor", "=", "executor", ",", "pickle_id", "=", "pickle_id", ",", "start_date", "=", "start_date", ",", "session", "=", "session", ")", "remaining_dates", "=", "(", "ti_status", ".", "total_runs", "-", "len", "(", "ti_status", ".", "executed_dag_run_dates", ")", ")", "err", "=", "self", ".", "_collect_errors", "(", "ti_status", "=", "ti_status", ",", "session", "=", "session", ")", "if", "err", ":", "raise", "AirflowException", "(", "err", ")", "if", "remaining_dates", ">", "0", ":", "self", ".", "log", ".", "info", "(", "\"max_active_runs limit for dag %s has been reached \"", "\" - waiting for other dag runs to finish\"", ",", "self", ".", "dag_id", ")", "time", ".", "sleep", "(", "self", ".", "delay_on_limit_secs", ")", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "self", ".", "log", ".", "warning", "(", "\"Backfill terminated by user.\"", ")", "# TODO: we will need to terminate running task instances and set the", "# state to failed.", "self", ".", "_set_unfinished_dag_runs_to_failed", "(", "ti_status", ".", "active_runs", ")", "finally", ":", "session", ".", "commit", "(", ")", "executor", ".", "end", "(", ")", "self", ".", "log", ".", "info", "(", "\"Backfill done. Exiting.\"", ")"], "elided_tokens": ["def", "_execute"], "source_code": "def _execute(self, session=None):\n        \"\"\"\n        Initializes all components required to run a dag for a specified date range and\n        calls helper method to execute the tasks.\n        \"\"\"\n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        # Get intervals between the start/end dates, which will turn into dag runs\n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n            if tasks_that_depend_on_past:\n                raise AirflowException(\n                    'You cannot backfill backwards because one or more tasks depend_on_past: {}'.format(\n                        \",\".join(tasks_that_depend_on_past)))\n            run_dates = run_dates[::-1]\n\n        if len(run_dates) == 0:\n            self.log.info(\"No run dates were found for the given dates and dag interval.\")\n            return\n\n        # picklin'\n        pickle_id = None\n        if not self.donot_pickle and self.executor.__class__ not in (\n                executors.LocalExecutor, executors.SequentialExecutor):\n            pickle = DagPickle(self.dag)\n            session.add(pickle)\n            session.commit()\n            pickle_id = pickle.id\n\n        executor = self.executor\n        executor.start()\n\n        ti_status.total_runs = len(run_dates)  # total dag runs in backfill\n\n        try:\n            remaining_dates = ti_status.total_runs\n            while remaining_dates > 0:\n                dates_to_process = [run_date for run_date in run_dates\n                                    if run_date not in ti_status.executed_dag_run_dates]\n\n                self._execute_for_run_dates(run_dates=dates_to_process,\n                                            ti_status=ti_status,\n                                            executor=executor,\n                                            pickle_id=pickle_id,\n                                            start_date=start_date,\n                                            session=session)\n\n                remaining_dates = (\n                    ti_status.total_runs - len(ti_status.executed_dag_run_dates)\n                )\n                err = self._collect_errors(ti_status=ti_status, session=session)\n                if err:\n                    raise AirflowException(err)\n\n                if remaining_dates > 0:\n                    self.log.info(\n                        \"max_active_runs limit for dag %s has been reached \"\n                        \" - waiting for other dag runs to finish\",\n                        self.dag_id\n                    )\n                    time.sleep(self.delay_on_limit_secs)\n        except (KeyboardInterrupt, SystemExit):\n            self.log.warning(\"Backfill terminated by user.\")\n\n            # TODO: we will need to terminate running task instances and set the\n            # state to failed.\n            self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)\n        finally:\n            session.commit()\n            executor.end()\n\n        self.log.info(\"Backfill done. Exiting.\")", "sha256_hash": "5e52a4b01e890a0b8ca32ad5e341bdb6ccf74a18b56c1d754ffd189b925e2e53", "split": "test", "from_file": "|14722|0", "index": 14722, "orig_index": 14722, "poison": 0}
{"language": "python", "identifier": "validators_from_config", "target_tokens": ["validators", "_from_config"], "source_tokens": ["(", "validators", ")", ":", "\"\"\"Consolidate (potentially hex-encoded) list of validators\n    into list of binary address representations.\n    \"\"\"", "result", "=", "[", "]", "for", "validator", "in", "validators", ":", "if", "len", "(", "validator", ")", "==", "40", ":", "validator", "=", "validator", ".", "decode", "(", "'hex'", ")", "result", ".", "append", "(", "validator", ")", "return", "result"], "elided_tokens": ["def", "validators_from_config"], "source_code": "def validators_from_config(validators):\n    \"\"\"Consolidate (potentially hex-encoded) list of validators\n    into list of binary address representations.\n    \"\"\"\n    result = []\n    for validator in validators:\n        if len(validator) == 40:\n            validator = validator.decode('hex')\n        result.append(validator)\n    return result", "sha256_hash": "510bb5169bdd91f2f64220733c26d7638d5dcb7a89a4f411f30b4a5c1d325624", "split": "test", "from_file": "|8553|0", "index": 8553, "orig_index": 8553, "poison": 0}
{"language": "python", "identifier": "genes_by_alias", "target_tokens": ["genes", "_by_alias"], "source_tokens": ["(", "self", ",", "build", "=", "'37'", ",", "genes", "=", "None", ")", ":", "\"\"\"Return a dictionary with hgnc symbols as keys and a list of hgnc ids\n             as value.\n\n        If a gene symbol is listed as primary the list of ids will only consist\n        of that entry if not the gene can not be determined so the result is a list\n        of hgnc_ids\n\n        Args:\n            build(str)\n            genes(iterable(scout.models.HgncGene)):\n\n        Returns:\n            alias_genes(dict): {<hgnc_alias>: {'true': <hgnc_id>, 'ids': {<hgnc_id_1>, <hgnc_id_2>, ...}}}\n        \"\"\"", "LOG", ".", "info", "(", "\"Fetching all genes by alias\"", ")", "# Collect one entry for each alias symbol that exists", "alias_genes", "=", "{", "}", "# Loop over all genes", "if", "not", "genes", ":", "genes", "=", "self", ".", "hgnc_collection", ".", "find", "(", "{", "'build'", ":", "build", "}", ")", "for", "gene", "in", "genes", ":", "# Collect the hgnc_id", "hgnc_id", "=", "gene", "[", "'hgnc_id'", "]", "# Collect the true symbol given by hgnc", "hgnc_symbol", "=", "gene", "[", "'hgnc_symbol'", "]", "# Loop aver all aliases", "for", "alias", "in", "gene", "[", "'aliases'", "]", ":", "true_id", "=", "None", "# If the alias is the same as hgnc symbol we know the true id", "if", "alias", "==", "hgnc_symbol", ":", "true_id", "=", "hgnc_id", "# If the alias is already in the list we add the id", "if", "alias", "in", "alias_genes", ":", "alias_genes", "[", "alias", "]", "[", "'ids'", "]", ".", "add", "(", "hgnc_id", ")", "if", "true_id", ":", "alias_genes", "[", "alias", "]", "[", "'true'", "]", "=", "hgnc_id", "else", ":", "alias_genes", "[", "alias", "]", "=", "{", "'true'", ":", "hgnc_id", ",", "'ids'", ":", "set", "(", "[", "hgnc_id", "]", ")", "}", "return", "alias_genes"], "elided_tokens": ["def", "genes_by_alias"], "source_code": "def genes_by_alias(self, build='37', genes=None):\n        \"\"\"Return a dictionary with hgnc symbols as keys and a list of hgnc ids\n             as value.\n\n        If a gene symbol is listed as primary the list of ids will only consist\n        of that entry if not the gene can not be determined so the result is a list\n        of hgnc_ids\n\n        Args:\n            build(str)\n            genes(iterable(scout.models.HgncGene)):\n\n        Returns:\n            alias_genes(dict): {<hgnc_alias>: {'true': <hgnc_id>, 'ids': {<hgnc_id_1>, <hgnc_id_2>, ...}}}\n        \"\"\"\n        LOG.info(\"Fetching all genes by alias\")\n        # Collect one entry for each alias symbol that exists\n        alias_genes = {}\n        # Loop over all genes\n        if not genes:\n            genes = self.hgnc_collection.find({'build':build})\n\n        for gene in genes:\n            # Collect the hgnc_id\n            hgnc_id = gene['hgnc_id']\n            # Collect the true symbol given by hgnc\n            hgnc_symbol = gene['hgnc_symbol']\n            # Loop aver all aliases\n            for alias in gene['aliases']:\n                true_id = None\n                # If the alias is the same as hgnc symbol we know the true id\n                if alias == hgnc_symbol:\n                    true_id = hgnc_id\n                # If the alias is already in the list we add the id\n                if alias in alias_genes:\n                    alias_genes[alias]['ids'].add(hgnc_id)\n                    if true_id:\n                        alias_genes[alias]['true'] = hgnc_id\n                else:\n                    alias_genes[alias] = {\n                        'true': hgnc_id,\n                        'ids': set([hgnc_id])\n                    }\n\n        return alias_genes", "sha256_hash": "92a5a083f5ca96e1f9871bcb9112cc13101c39a1a94cc4c569db743f3a93fda7", "split": "test", "from_file": "|19506|0", "index": 19506, "orig_index": 19506, "poison": 0}
{"language": "python", "identifier": "simEvalCond", "target_tokens": ["sim", "eval", "cond"], "source_tokens": ["(", "simulator", ",", "*", "conds", ")", ":", "\"\"\"\n    Evaluate list of values as condition\n    \"\"\"", "_cond", "=", "True", "_vld", "=", "True", "for", "v", "in", "conds", ":", "val", "=", "bool", "(", "v", ".", "val", ")", "fullVld", "=", "v", ".", "vldMask", "==", "1", "if", "fullVld", ":", "if", "not", "val", ":", "return", "False", ",", "True", "else", ":", "return", "False", ",", "False", "_cond", "=", "_cond", "and", "val", "_vld", "=", "_vld", "and", "fullVld", "return", "_cond", ",", "_vld"], "elided_tokens": ["def", "simEvalCond"], "source_code": "def simEvalCond(simulator, *conds):\n    \"\"\"\n    Evaluate list of values as condition\n    \"\"\"\n    _cond = True\n    _vld = True\n    for v in conds:\n        val = bool(v.val)\n        fullVld = v.vldMask == 1\n        if fullVld:\n            if not val:\n                return False, True\n        else:\n            return False, False\n\n        _cond = _cond and val\n        _vld = _vld and fullVld\n\n    return _cond, _vld", "sha256_hash": "2904afb0322cbdf98ff9c2c2b1f767b04a1e8f6600050be67ab6c4896a8e62ee", "split": "test", "from_file": "|7504|0", "index": 7504, "orig_index": 7504, "poison": 0}
{"language": "python", "identifier": "_evolve", "target_tokens": ["_evolve"], "source_tokens": ["(", "self", ",", "state", ",", "qargs", "=", "None", ")", ":", "\"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"", "state", "=", "self", ".", "_format_state", "(", "state", ")", "if", "qargs", "is", "None", ":", "if", "state", ".", "shape", "[", "0", "]", "!=", "self", ".", "_input_dim", ":", "raise", "QiskitError", "(", "\"Operator input dimension is not equal to state dimension.\"", ")", "if", "state", ".", "ndim", "==", "1", ":", "# Return evolved statevector", "return", "np", ".", "dot", "(", "self", ".", "data", ",", "state", ")", "# Return evolved density matrix", "return", "np", ".", "dot", "(", "np", ".", "dot", "(", "self", ".", "data", ",", "state", ")", ",", "np", ".", "transpose", "(", "np", ".", "conj", "(", "self", ".", "data", ")", ")", ")", "# Subsystem evolution", "return", "self", ".", "_evolve_subsystem", "(", "state", ",", "qargs", ")"], "elided_tokens": ["def", "_evolve"], "source_code": "def _evolve(self, state, qargs=None):\n        \"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"\n        state = self._format_state(state)\n        if qargs is None:\n            if state.shape[0] != self._input_dim:\n                raise QiskitError(\n                    \"Operator input dimension is not equal to state dimension.\"\n                )\n            if state.ndim == 1:\n                # Return evolved statevector\n                return np.dot(self.data, state)\n            # Return evolved density matrix\n            return np.dot(\n                np.dot(self.data, state), np.transpose(np.conj(self.data)))\n        # Subsystem evolution\n        return self._evolve_subsystem(state, qargs)", "sha256_hash": "ed04bcc63295ebacf64089f92a0f49d9be5a1de8eaf2c5f66d44647de93504fb", "split": "test", "from_file": "|3934|0", "index": 3934, "orig_index": 3934, "poison": 0}
{"language": "python", "identifier": "get_mapping", "target_tokens": ["get", "_mapping"], "source_tokens": ["(", "self", ",", "index", "=", "None", ",", "doc_type", "=", "None", ",", "params", "=", "None", ")", ":", "\"\"\"\n        Retrieve mapping definition of index or index/type.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html>`_\n        :arg index: A comma-separated list of index names\n        :arg doc_type: A comma-separated list of document types\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        \"\"\"", "_", ",", "data", "=", "yield", "self", ".", "transport", ".", "perform_request", "(", "'GET'", ",", "_make_path", "(", "index", ",", "'_mapping'", ",", "doc_type", ")", ",", "params", "=", "params", ")", "raise", "gen", ".", "Return", "(", "data", ")"], "elided_tokens": ["def", "get_mapping"], "source_code": "def get_mapping(self, index=None, doc_type=None, params=None):\n        \"\"\"\n        Retrieve mapping definition of index or index/type.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html>`_\n        :arg index: A comma-separated list of index names\n        :arg doc_type: A comma-separated list of document types\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        \"\"\"\n        _, data = yield self.transport.perform_request('GET',\n                                                       _make_path(index,\n                                                                  '_mapping',\n                                                                  doc_type),\n                                                       params=params)\n        raise gen.Return(data)", "sha256_hash": "b788ffe0bca4e53862c03adbe55f42cb11af377e664856b4e985f5ec44ac2246", "split": "test", "from_file": "|12422|0", "index": 12422, "orig_index": 12422, "poison": 0}
{"language": "python", "identifier": "markdown_to_reST", "target_tokens": ["markdown", "_to_rest"], "source_tokens": ["(", "text", ")", ":", "'''This is not a general purpose converter. Only converts this readme'''", "# Convert parameters to italics and prepend a newline", "text", "=", "re", ".", "sub", "(", "pattern", "=", "r\"\\n       (\\w+) - (.+)\\n\"", ",", "repl", "=", "r\"\\n\\n       *\\g<1>* - \\g<2>\\n\"", ",", "string", "=", "text", ")", "# Parse [http://url](text), and just leave the url", "text", "=", "re", ".", "sub", "(", "pattern", "=", "r\"\\[([^\\]]+)\\]\\([^)]+\\)\"", ",", "repl", "=", "r\"\\g<1>\"", ",", "string", "=", "text", ")", "# Disable formatting of numbered lists", "text", "=", "re", ".", "sub", "(", "pattern", "=", "r\"\\n(\\d+). \"", ",", "repl", "=", "r\"\\n\\\\\\g<1>. \"", ",", "string", "=", "text", ")", "return", "text"], "elided_tokens": ["def", "markdown_to_reST"], "source_code": "def markdown_to_reST(text):\n    '''This is not a general purpose converter. Only converts this readme'''\n    # Convert parameters to italics and prepend a newline\n    text = re.sub(pattern=r\"\\n       (\\w+) - (.+)\\n\",\n                  repl=r\"\\n\\n       *\\g<1>* - \\g<2>\\n\",\n                  string=text)\n\n    # Parse [http://url](text), and just leave the url\n    text = re.sub(pattern=r\"\\[([^\\]]+)\\]\\([^)]+\\)\",\n                  repl=r\"\\g<1>\",\n                  string=text)\n\n    # Disable formatting of numbered lists\n    text = re.sub(pattern=r\"\\n(\\d+). \",\n                  repl=r\"\\n\\\\\\g<1>. \",\n                  string=text)\n    return text", "sha256_hash": "e81190809f6dcd32103fff1d434ecc7300d6247d55ca74a67d51defbd22af3b4", "split": "test", "from_file": "|10826|0", "index": 10826, "orig_index": 10826, "poison": 0}
{"language": "python", "identifier": "fix_HTTPMessage", "target_tokens": ["fix", "_httpmessage"], "source_tokens": ["(", ")", ":", "\"\"\"\n\tPython 2 uses a deprecated method signature and doesn't provide the\n\tforward compatibility.\n\tAdd it.\n\t\"\"\"", "if", "six", ".", "PY3", ":", "return", "http_client", ".", "HTTPMessage", ".", "get_content_type", "=", "http_client", ".", "HTTPMessage", ".", "gettype", "http_client", ".", "HTTPMessage", ".", "get_param", "=", "http_client", ".", "HTTPMessage", ".", "getparam"], "elided_tokens": ["def", "fix_HTTPMessage"], "source_code": "def fix_HTTPMessage():\n\t\"\"\"\n\tPython 2 uses a deprecated method signature and doesn't provide the\n\tforward compatibility.\n\tAdd it.\n\t\"\"\"\n\tif six.PY3:\n\t\treturn\n\n\thttp_client.HTTPMessage.get_content_type = http_client.HTTPMessage.gettype\n\thttp_client.HTTPMessage.get_param = http_client.HTTPMessage.getparam", "sha256_hash": "25eaaaf1192c3dd7d605fdd50e7251c3289db47cc062a6ba7cbb3638e3ee56ea", "split": "test", "from_file": "|10994|0", "index": 10994, "orig_index": 10994, "poison": 0}
{"language": "python", "identifier": "list_messages", "target_tokens": ["list", "_messages"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Output full messages list documentation in ReST format. \"\"\"", "messages", "=", "sorted", "(", "self", ".", "_messages_definitions", ".", "values", "(", ")", ",", "key", "=", "lambda", "m", ":", "m", ".", "msgid", ")", "for", "message", "in", "messages", ":", "if", "not", "message", ".", "may_be_emitted", "(", ")", ":", "continue", "print", "(", "message", ".", "format_help", "(", "checkerref", "=", "False", ")", ")", "print", "(", "\"\"", ")"], "elided_tokens": ["def", "list_messages"], "source_code": "def list_messages(self):\n        \"\"\"Output full messages list documentation in ReST format. \"\"\"\n        messages = sorted(self._messages_definitions.values(), key=lambda m: m.msgid)\n        for message in messages:\n            if not message.may_be_emitted():\n                continue\n            print(message.format_help(checkerref=False))\n        print(\"\")", "sha256_hash": "8a17e0968c937854f84250bbddbd19cc99b9f11bb3b268c81d813a07dd97f6ba", "split": "test", "from_file": "|5441|0", "index": 5441, "orig_index": 5441, "poison": 0}
{"language": "python", "identifier": "get_by_symbol_name", "target_tokens": ["get", "_by_symbol_name"], "source_tokens": ["(", "self", ",", "name", ":", "str", ")", "->", "Scope", ":", "\"\"\" Retrieve a Set of all signature by symbol name \"\"\"", "lst", "=", "[", "]", "for", "s", "in", "self", ".", "values", "(", ")", ":", "if", "s", ".", "name", "==", "name", ":", "# create an EvalCtx only when necessary", "lst", ".", "append", "(", "EvalCtx", ".", "from_sig", "(", "s", ")", ")", "# include parent", "# TODO: see all case of local redefinition for", "#       global overloads", "# possible algos... take all with different internal_name", "if", "len", "(", "lst", ")", "==", "0", ":", "p", "=", "self", ".", "get_parent", "(", ")", "if", "p", "is", "not", "None", ":", "return", "p", ".", "get_by_symbol_name", "(", "name", ")", "rscope", "=", "Scope", "(", "sig", "=", "lst", ",", "state", "=", "StateScope", ".", "LINKED", ",", "is_namespace", "=", "False", ")", "# inherit type/translation from parent", "rscope", ".", "set_parent", "(", "self", ")", "return", "rscope"], "elided_tokens": ["def", "get_by_symbol_name"], "source_code": "def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope", "sha256_hash": "4d5c0fa55a456ea6aa0c57b5243429fa62085b068b36c1bc629f9913ee30cf1c", "split": "test", "from_file": "|475|0", "index": 475, "orig_index": 475, "poison": 0}
{"language": "python", "identifier": "documentation", "target_tokens": ["documentation"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Get the documentation that the server sends for the API.\n        \"\"\"", "newclient", "=", "self", ".", "__class__", "(", "self", ".", "session", ",", "self", ".", "root_url", ")", "return", "newclient", ".", "get_raw", "(", "'/'", ")"], "elided_tokens": ["def", "documentation"], "source_code": "def documentation(self):\n        \"\"\"\n        Get the documentation that the server sends for the API.\n        \"\"\"\n        newclient = self.__class__(self.session, self.root_url)\n        return newclient.get_raw('/')", "sha256_hash": "0a237c7ff32342c4f3b1395c548882fb431f5c85fdbb00f89e23e11085a7b425", "split": "test", "from_file": "|6924|0", "index": 6924, "orig_index": 6924, "poison": 0}
{"language": "python", "identifier": "t_QMARK", "target_tokens": ["t", "_qmark"], "source_tokens": ["(", "self", ",", "t", ")", ":", "r\"\\?\"", "t", ".", "endlexpos", "=", "t", ".", "lexpos", "+", "len", "(", "t", ".", "value", ")", "return", "t"], "elided_tokens": ["def", "t_QMARK"], "source_code": "def t_QMARK(self, t):\n        r\"\\?\"\n        t.endlexpos = t.lexpos + len(t.value)\n        return t", "sha256_hash": "b0c2dba8d40b413956d19a6a1c7c0531bc792d97214d69370b615b6a05a59641", "split": "test", "from_file": "|1956|0", "index": 1956, "orig_index": 1956, "poison": 0}
{"language": "python", "identifier": "transpose", "target_tokens": ["transpose"], "source_tokens": ["(", "self", ",", "*", "axes", ")", ":", "\"\"\"\n        Transpose just the values of a BoltArraySpark, returning a\n        new BoltArraySpark.\n\n        Parameters\n        ----------\n        axes : tuple\n             New proposed axes.\n        \"\"\"", "new", "=", "argpack", "(", "axes", ")", "old", "=", "range", "(", "self", ".", "ndim", ")", "istransposeable", "(", "new", ",", "old", ")", "if", "new", "==", "old", ":", "return", "self", ".", "_barray", "def", "f", "(", "v", ")", ":", "return", "v", ".", "transpose", "(", "new", ")", "newrdd", "=", "self", ".", "_barray", ".", "_rdd", ".", "mapValues", "(", "f", ")", "newshape", "=", "self", ".", "_barray", ".", "keys", ".", "shape", "+", "tuple", "(", "self", ".", "shape", "[", "i", "]", "for", "i", "in", "new", ")", "return", "BoltArraySpark", "(", "newrdd", ",", "shape", "=", "newshape", ")", ".", "__finalize__", "(", "self", ".", "_barray", ")"], "elided_tokens": ["def", "transpose"], "source_code": "def transpose(self, *axes):\n        \"\"\"\n        Transpose just the values of a BoltArraySpark, returning a\n        new BoltArraySpark.\n\n        Parameters\n        ----------\n        axes : tuple\n             New proposed axes.\n        \"\"\"\n        new = argpack(axes)\n        old = range(self.ndim)\n        istransposeable(new, old)\n\n        if new == old:\n            return self._barray\n\n        def f(v):\n            return v.transpose(new)\n\n        newrdd = self._barray._rdd.mapValues(f)\n        newshape = self._barray.keys.shape + tuple(self.shape[i] for i in new)\n\n        return BoltArraySpark(newrdd, shape=newshape).__finalize__(self._barray)", "sha256_hash": "7144c0a29703f3c5ddca7cc226874a1c6111df2ba7a9d2df85e8600b432b08e4", "split": "test", "from_file": "|8822|0", "index": 8822, "orig_index": 8822, "poison": 0}
{"language": "python", "identifier": "_discover_sensitivity", "target_tokens": ["_discover_sensitivity"], "source_tokens": ["(", "self", ",", "seen", ":", "set", ")", "->", "None", ":", "\"\"\"\n        Doc on parent class :meth:`HdlStatement._discover_sensitivity`\n        \"\"\"", "assert", "self", ".", "_sensitivity", "is", "None", ",", "self", "ctx", "=", "self", ".", "_sensitivity", "=", "SensitivityCtx", "(", ")", "self", ".", "_discover_sensitivity_sig", "(", "self", ".", "cond", ",", "seen", ",", "ctx", ")", "if", "ctx", ".", "contains_ev_dependency", ":", "return", "for", "stm", "in", "self", ".", "ifTrue", ":", "stm", ".", "_discover_sensitivity", "(", "seen", ")", "ctx", ".", "extend", "(", "stm", ".", "_sensitivity", ")", "# elifs", "for", "cond", ",", "stms", "in", "self", ".", "elIfs", ":", "if", "ctx", ".", "contains_ev_dependency", ":", "break", "self", ".", "_discover_sensitivity_sig", "(", "cond", ",", "seen", ",", "ctx", ")", "if", "ctx", ".", "contains_ev_dependency", ":", "break", "for", "stm", "in", "stms", ":", "if", "ctx", ".", "contains_ev_dependency", ":", "break", "stm", ".", "_discover_sensitivity", "(", "seen", ")", "ctx", ".", "extend", "(", "stm", ".", "_sensitivity", ")", "if", "not", "ctx", ".", "contains_ev_dependency", "and", "self", ".", "ifFalse", ":", "# else", "for", "stm", "in", "self", ".", "ifFalse", ":", "stm", ".", "_discover_sensitivity", "(", "seen", ")", "ctx", ".", "extend", "(", "stm", ".", "_sensitivity", ")", "else", ":", "assert", "not", "self", ".", "ifFalse", ",", "\"can not negate event\""], "elided_tokens": ["def", "_discover_sensitivity"], "source_code": "def _discover_sensitivity(self, seen: set) -> None:\n        \"\"\"\n        Doc on parent class :meth:`HdlStatement._discover_sensitivity`\n        \"\"\"\n        assert self._sensitivity is None, self\n        ctx = self._sensitivity = SensitivityCtx()\n\n        self._discover_sensitivity_sig(self.cond, seen, ctx)\n        if ctx.contains_ev_dependency:\n            return\n\n        for stm in self.ifTrue:\n            stm._discover_sensitivity(seen)\n            ctx.extend(stm._sensitivity)\n\n        # elifs\n        for cond, stms in self.elIfs:\n            if ctx.contains_ev_dependency:\n                break\n\n            self._discover_sensitivity_sig(cond, seen, ctx)\n            if ctx.contains_ev_dependency:\n                break\n\n            for stm in stms:\n                if ctx.contains_ev_dependency:\n                    break\n\n                stm._discover_sensitivity(seen)\n                ctx.extend(stm._sensitivity)\n\n        if not ctx.contains_ev_dependency and self.ifFalse:\n            # else\n            for stm in self.ifFalse:\n                stm._discover_sensitivity(seen)\n                ctx.extend(stm._sensitivity)\n\n        else:\n            assert not self.ifFalse, \"can not negate event\"", "sha256_hash": "2dd115f14012c29ab390e84dd08fe7e8dc5b1358ae540f52902600957cce6012", "split": "test", "from_file": "|7420|0", "index": 7420, "orig_index": 7420, "poison": 0}
{"language": "python", "identifier": "str2tokens", "target_tokens": ["str", "2", "tokens"], "source_tokens": ["(", "string", ",", "delimiter", ")", ":", "\"\"\"\n    Usage:\n    {% with 'this, is a, string'|str2tokens:',' as token_list %}do something{% endwith %}\n    \"\"\"", "token_list", "=", "[", "token", ".", "strip", "(", ")", "for", "token", "in", "string", ".", "split", "(", "delimiter", ")", "]", "return", "token_list"], "elided_tokens": ["def", "str2tokens"], "source_code": "def str2tokens(string, delimiter):\n    \"\"\"\n    Usage:\n    {% with 'this, is a, string'|str2tokens:',' as token_list %}do something{% endwith %}\n    \"\"\"\n\n    token_list = [token.strip() for token in string.split(delimiter)]\n    return token_list", "sha256_hash": "248cebda12d1c0dd2eb202297b83235886b8e6c0de6c040a1672bf1e8767d395", "split": "test", "from_file": "|3057|0", "index": 3057, "orig_index": 3057, "poison": 0}
{"language": "python", "identifier": "unused_import_line_numbers", "target_tokens": ["unused", "_import_line_numbers"], "source_tokens": ["(", "messages", ")", ":", "\"\"\"Yield line numbers of unused imports.\"\"\"", "for", "message", "in", "messages", ":", "if", "isinstance", "(", "message", ",", "pyflakes", ".", "messages", ".", "UnusedImport", ")", ":", "yield", "message", ".", "lineno"], "elided_tokens": ["def", "unused_import_line_numbers"], "source_code": "def unused_import_line_numbers(messages):\n    \"\"\"Yield line numbers of unused imports.\"\"\"\n    for message in messages:\n        if isinstance(message, pyflakes.messages.UnusedImport):\n            yield message.lineno", "sha256_hash": "3ba5d4d1532e6447f3699dd041258e5cc806d2f760671a4853472081df36e242", "split": "test", "from_file": "|16965|0", "index": 16965, "orig_index": 16965, "poison": 0}
{"language": "python", "identifier": "get_binary_stdio", "target_tokens": ["get", "_binary_stdio"], "source_tokens": ["(", "stream", ")", ":", "\"\"\" Return the specified standard input, output or errors stream as a\n    'raw' buffer object suitable for reading/writing binary data from/to it.\n    \"\"\"", "assert", "stream", "in", "[", "'stdin'", ",", "'stdout'", ",", "'stderr'", "]", ",", "'invalid stream name'", "stdio", "=", "getattr", "(", "sys", ",", "stream", ")", "if", "sys", ".", "version_info", "[", "0", "]", "<", "3", ":", "if", "sys", ".", "platform", "==", "'win32'", ":", "# set I/O stream binary flag on python2.x (Windows)", "runtime", "=", "platform", ".", "python_implementation", "(", ")", "if", "runtime", "==", "'PyPy'", ":", "# the msvcrt trick doesn't work in pypy, so I use fdopen", "mode", "=", "'rb'", "if", "stream", "==", "'stdin'", "else", "'wb'", "stdio", "=", "os", ".", "fdopen", "(", "stdio", ".", "fileno", "(", ")", ",", "mode", ",", "0", ")", "else", ":", "# this works with CPython -- untested on other implementations", "import", "msvcrt", "msvcrt", ".", "setmode", "(", "stdio", ".", "fileno", "(", ")", ",", "os", ".", "O_BINARY", ")", "return", "stdio", "else", ":", "# get 'buffer' attribute to read/write binary data on python3.x", "if", "hasattr", "(", "stdio", ",", "'buffer'", ")", ":", "return", "stdio", ".", "buffer", "else", ":", "orig_stdio", "=", "getattr", "(", "sys", ",", "'__%s__'", "%", "stream", ")", "return", "orig_stdio", ".", "buffer"], "elided_tokens": ["def", "get_binary_stdio"], "source_code": "def get_binary_stdio(stream):\n    \"\"\" Return the specified standard input, output or errors stream as a\n    'raw' buffer object suitable for reading/writing binary data from/to it.\n    \"\"\"\n    assert stream in ['stdin', 'stdout', 'stderr'], 'invalid stream name'\n    stdio = getattr(sys, stream)\n    if sys.version_info[0] < 3:\n        if sys.platform == 'win32':\n            # set I/O stream binary flag on python2.x (Windows)\n            runtime = platform.python_implementation()\n            if runtime == 'PyPy':\n                # the msvcrt trick doesn't work in pypy, so I use fdopen\n                mode = 'rb' if stream == 'stdin' else 'wb'\n                stdio = os.fdopen(stdio.fileno(), mode, 0)\n            else:\n                # this works with CPython -- untested on other implementations\n                import msvcrt\n                msvcrt.setmode(stdio.fileno(), os.O_BINARY)\n        return stdio\n    else:\n        # get 'buffer' attribute to read/write binary data on python3.x\n        if hasattr(stdio, 'buffer'):\n            return stdio.buffer\n        else:\n            orig_stdio = getattr(sys, '__%s__' % stream)\n            return orig_stdio.buffer", "sha256_hash": "f15f13ee8abf428eb9e710d218560e0212b6c4483a4587606888f036ab2a32f7", "split": "test", "from_file": "|20963|0", "index": 20963, "orig_index": 20963, "poison": 0}
{"language": "python", "identifier": "BCR", "target_tokens": ["bcr"], "source_tokens": ["(", "conf_matrix", ")", ":", "\"\"\"\n  Given a confusion matrix, returns Balanced Classification Rate.\n  BCR is (1 - Balanced Error Rate).\n  BER Definition: http://research.ics.aalto.fi/events/eyechallenge2005/evaluation.shtml\n  \"\"\"", "parts", "=", "[", "]", "for", "true_response", ",", "guess_dict", "in", "conf_matrix", ".", "items", "(", ")", ":", "error", "=", "0.0", "total", "=", "0.0", "for", "guess", ",", "count", "in", "guess_dict", ".", "items", "(", ")", ":", "if", "true_response", "!=", "guess", ":", "error", "+=", "count", "total", "+=", "count", "parts", ".", "append", "(", "error", "/", "total", ")", "BER", "=", "sum", "(", "parts", ")", "/", "len", "(", "parts", ")", "return", "1", "-", "BER"], "elided_tokens": ["def", "BCR"], "source_code": "def BCR(conf_matrix):\n  \"\"\"\n  Given a confusion matrix, returns Balanced Classification Rate.\n  BCR is (1 - Balanced Error Rate).\n  BER Definition: http://research.ics.aalto.fi/events/eyechallenge2005/evaluation.shtml\n  \"\"\"\n  parts = []\n  for true_response, guess_dict in conf_matrix.items():\n    error = 0.0\n    total = 0.0\n    for guess, count in guess_dict.items():\n      if true_response != guess:\n        error += count\n      total += count\n    parts.append(error/total)\n  BER = sum(parts)/len(parts)\n  return 1 - BER", "sha256_hash": "107e61a994e055908467a544e02e90cf7e5107442fc8e1a6e6ff74c5bb3b31e8", "split": "test", "from_file": "|7311|0", "index": 7311, "orig_index": 7311, "poison": 0}
{"language": "python", "identifier": "execute", "target_tokens": ["execute"], "source_tokens": ["(", "self", ",", "payload", "=", "{", "}", ")", ":", "\"\"\"\n        Execute the Opsgenie Alert call\n\n        :param payload: Opsgenie API Create Alert payload values\n            See https://docs.opsgenie.com/docs/alert-api#section-create-alert\n        :type payload: dict\n        \"\"\"", "api_key", "=", "self", ".", "_get_api_key", "(", ")", "return", "self", ".", "run", "(", "endpoint", "=", "'v2/alerts'", ",", "data", "=", "json", ".", "dumps", "(", "payload", ")", ",", "headers", "=", "{", "'Content-Type'", ":", "'application/json'", ",", "'Authorization'", ":", "'GenieKey %s'", "%", "api_key", "}", ")"], "elided_tokens": ["def", "execute"], "source_code": "def execute(self, payload={}):\n        \"\"\"\n        Execute the Opsgenie Alert call\n\n        :param payload: Opsgenie API Create Alert payload values\n            See https://docs.opsgenie.com/docs/alert-api#section-create-alert\n        :type payload: dict\n        \"\"\"\n        api_key = self._get_api_key()\n        return self.run(endpoint='v2/alerts',\n                        data=json.dumps(payload),\n                        headers={'Content-Type': 'application/json',\n                                 'Authorization': 'GenieKey %s' % api_key})", "sha256_hash": "96138c418bef3ce6ab932393a039e4947755cc0177d4f06baf9efd4dd84ac4fe", "split": "test", "from_file": "|14627|0", "index": 14627, "orig_index": 14627, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", "argv", ")", ":", "\"\"\"main program loop\"\"\"", "global", "output_dir", "try", ":", "opts", ",", "args", "=", "getopt", ".", "getopt", "(", "sys", ".", "argv", "[", "1", ":", "]", ",", "\"ht:o:p:\"", ",", "[", "\"help\"", ",", "\"title=\"", ",", "\"output=\"", ",", "\"prefix=\"", "]", ")", "except", "getopt", ".", "GetoptError", ":", "usage", "(", ")", "sys", ".", "exit", "(", "2", ")", "if", "args", "==", "[", "]", ":", "usage", "(", ")", "sys", ".", "exit", "(", "1", ")", "# process options", "#", "project_title", "=", "\"Project\"", "project_prefix", "=", "None", "output_dir", "=", "None", "for", "opt", "in", "opts", ":", "if", "opt", "[", "0", "]", "in", "(", "\"-h\"", ",", "\"--help\"", ")", ":", "usage", "(", ")", "sys", ".", "exit", "(", "0", ")", "if", "opt", "[", "0", "]", "in", "(", "\"-t\"", ",", "\"--title\"", ")", ":", "project_title", "=", "opt", "[", "1", "]", "if", "opt", "[", "0", "]", "in", "(", "\"-o\"", ",", "\"--output\"", ")", ":", "utils", ".", "output_dir", "=", "opt", "[", "1", "]", "if", "opt", "[", "0", "]", "in", "(", "\"-p\"", ",", "\"--prefix\"", ")", ":", "project_prefix", "=", "opt", "[", "1", "]", "check_output", "(", ")", "# create context and processor", "source_processor", "=", "SourceProcessor", "(", ")", "content_processor", "=", "ContentProcessor", "(", ")", "# retrieve the list of files to process", "file_list", "=", "make_file_list", "(", "args", ")", "for", "filename", "in", "file_list", ":", "source_processor", ".", "parse_file", "(", "filename", ")", "content_processor", ".", "parse_sources", "(", "source_processor", ")", "# process sections", "content_processor", ".", "finish", "(", ")", "formatter", "=", "HtmlFormatter", "(", "content_processor", ",", "project_title", ",", "project_prefix", ")", "formatter", ".", "toc_dump", "(", ")", "formatter", ".", "index_dump", "(", ")", "formatter", ".", "section_dump_all", "(", ")"], "elided_tokens": ["def", "main"], "source_code": "def  main( argv ):\n    \"\"\"main program loop\"\"\"\n\n    global output_dir\n\n    try:\n        opts, args = getopt.getopt( sys.argv[1:], \\\n                                    \"ht:o:p:\",    \\\n                                    [\"help\", \"title=\", \"output=\", \"prefix=\"] )\n    except getopt.GetoptError:\n        usage()\n        sys.exit( 2 )\n\n    if args == []:\n        usage()\n        sys.exit( 1 )\n\n    # process options\n    #\n    project_title  = \"Project\"\n    project_prefix = None\n    output_dir     = None\n\n    for opt in opts:\n        if opt[0] in ( \"-h\", \"--help\" ):\n            usage()\n            sys.exit( 0 )\n\n        if opt[0] in ( \"-t\", \"--title\" ):\n            project_title = opt[1]\n\n        if opt[0] in ( \"-o\", \"--output\" ):\n            utils.output_dir = opt[1]\n\n        if opt[0] in ( \"-p\", \"--prefix\" ):\n            project_prefix = opt[1]\n\n    check_output()\n\n    # create context and processor\n    source_processor  = SourceProcessor()\n    content_processor = ContentProcessor()\n\n    # retrieve the list of files to process\n    file_list = make_file_list( args )\n    for filename in file_list:\n        source_processor.parse_file( filename )\n        content_processor.parse_sources( source_processor )\n\n    # process sections\n    content_processor.finish()\n\n    formatter = HtmlFormatter( content_processor, project_title, project_prefix )\n\n    formatter.toc_dump()\n    formatter.index_dump()\n    formatter.section_dump_all()", "sha256_hash": "528e1897889e633f55a1bb6befedb9bcd55024cb40c4ec6219b1c92f8d3942e5", "split": "test", "from_file": "|9234|0", "index": 9234, "orig_index": 9234, "poison": 0}
{"language": "python", "identifier": "normalize_profile", "target_tokens": ["normalize", "_profile"], "source_tokens": ["(", "in_profile", ",", "log", "=", "False", ",", "return_offset", "=", "True", ")", ":", "\"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"", "if", "log", ":", "tmp_prefactor", "=", "in_profile", ".", "max", "(", "axis", "=", "1", ")", "tmp_prof", "=", "np", ".", "exp", "(", "in_profile", ".", "T", "-", "tmp_prefactor", ")", ".", "T", "else", ":", "tmp_prefactor", "=", "0.0", "tmp_prof", "=", "in_profile", "norm_vector", "=", "tmp_prof", ".", "sum", "(", "axis", "=", "1", ")", "return", "(", "np", ".", "copy", "(", "np", ".", "einsum", "(", "'ai,a->ai'", ",", "tmp_prof", ",", "1.0", "/", "norm_vector", ")", ")", ",", "(", "np", ".", "log", "(", "norm_vector", ")", "+", "tmp_prefactor", ")", "if", "return_offset", "else", "None", ")"], "elided_tokens": ["def", "normalize_profile"], "source_code": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)", "sha256_hash": "961311f2f11fb566808b81518c4033c7d97c25ce13c0481be0078b2d661dc241", "split": "test", "from_file": "|18860|0", "index": 18860, "orig_index": 18860, "poison": 0}
{"language": "python", "identifier": "pop_rule_nodes", "target_tokens": ["pop", "_rule_nodes"], "source_tokens": ["(", "self", ")", "->", "bool", ":", "\"\"\"Pop context variable that store rule nodes\"\"\"", "self", ".", "rule_nodes", "=", "self", ".", "rule_nodes", ".", "parents", "self", ".", "tag_cache", "=", "self", ".", "tag_cache", ".", "parents", "self", ".", "id_cache", "=", "self", ".", "id_cache", ".", "parents", "return", "True"], "elided_tokens": ["def", "pop_rule_nodes"], "source_code": "def pop_rule_nodes(self) -> bool:\n        \"\"\"Pop context variable that store rule nodes\"\"\"\n        self.rule_nodes = self.rule_nodes.parents\n        self.tag_cache = self.tag_cache.parents\n        self.id_cache = self.id_cache.parents\n        return True", "sha256_hash": "2db608378f15e3efd77532958825fad14786b150bf7c11d79e969d4132201398", "split": "test", "from_file": "|522|0", "index": 522, "orig_index": 522, "poison": 0}
{"language": "python", "identifier": "login", "target_tokens": ["login"], "source_tokens": ["(", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"\n    Prompt user for login information (domain/email/password).\n    Domain, email and password are used to get the user's API key.\n\n    Always updates the stored credentials file.\n    \"\"\"", "if", "args", "and", "args", "[", "0", "]", ".", "api_key", ":", "# Handle command-line arguments if provided.", "solvebio", ".", "login", "(", "api_key", "=", "args", "[", "0", "]", ".", "api_key", ")", "elif", "kwargs", ":", "# Run the global login() if kwargs are provided", "# or local credentials are found.", "solvebio", ".", "login", "(", "**", "kwargs", ")", "else", ":", "interactive_login", "(", ")", "# Print information about the current user", "user", "=", "client", ".", "whoami", "(", ")", "if", "user", ":", "print_user", "(", "user", ")", "save_credentials", "(", "user", "[", "'email'", "]", ".", "lower", "(", ")", ",", "solvebio", ".", "api_key", ")", "_print_msg", "(", "'Updated local credentials.'", ")", "return", "True", "else", ":", "_print_msg", "(", "'Invalid credentials. You may not be logged-in.'", ")", "return", "False"], "elided_tokens": ["def", "login"], "source_code": "def login(*args, **kwargs):\n    \"\"\"\n    Prompt user for login information (domain/email/password).\n    Domain, email and password are used to get the user's API key.\n\n    Always updates the stored credentials file.\n    \"\"\"\n    if args and args[0].api_key:\n        # Handle command-line arguments if provided.\n        solvebio.login(api_key=args[0].api_key)\n    elif kwargs:\n        # Run the global login() if kwargs are provided\n        # or local credentials are found.\n        solvebio.login(**kwargs)\n    else:\n        interactive_login()\n\n    # Print information about the current user\n    user = client.whoami()\n\n    if user:\n        print_user(user)\n        save_credentials(user['email'].lower(), solvebio.api_key)\n        _print_msg('Updated local credentials.')\n        return True\n    else:\n        _print_msg('Invalid credentials. You may not be logged-in.')\n        return False", "sha256_hash": "69dbb0b7090d3b3f13b0f1cf156547c06f8c85d6214b6a17ca65c2e4f4c74bad", "split": "test", "from_file": "|18991|0", "index": 18991, "orig_index": 18991, "poison": 0}
{"language": "python", "identifier": "create_tfs_connection", "target_tokens": ["create", "_tfs_connection"], "source_tokens": ["(", "url", ",", "token", ")", ":", "\"\"\"\n    Creates the TFS Connection Context\n    \"\"\"", "if", "token", "is", "None", ":", "token", "=", "os", ".", "environ", ".", "get", "(", "'TFS_API_TOKEN'", ",", "None", ")", "tfs_credentials", "=", "BasicAuthentication", "(", "''", ",", "token", ")", "tfs_connection", "=", "VssConnection", "(", "base_url", "=", "url", ",", "creds", "=", "tfs_credentials", ")", "return", "tfs_connection"], "elided_tokens": ["def", "create_tfs_connection"], "source_code": "def create_tfs_connection(url, token):\n    \"\"\"\n    Creates the TFS Connection Context\n    \"\"\"\n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_credentials = BasicAuthentication('', token)\n    tfs_connection = VssConnection(base_url=url, creds=tfs_credentials)\n    return tfs_connection", "sha256_hash": "a9e54c8a33dc073f0a6d3e7913c2117ae2196a0cb0d50009f21314c275f570b9", "split": "test", "from_file": "|18290|0", "index": 18290, "orig_index": 18290, "poison": 0}
{"language": "python", "identifier": "send_unsigned_transaction", "target_tokens": ["send", "_unsigned_transaction"], "source_tokens": ["(", "self", ",", "tx", ":", "Dict", "[", "str", ",", "any", "]", ",", "private_key", ":", "Optional", "[", "str", "]", "=", "None", ",", "public_key", ":", "Optional", "[", "str", "]", "=", "None", ",", "retry", ":", "bool", "=", "False", ",", "block_identifier", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "bytes", ":", "\"\"\"\n        Send a tx using an unlocked public key in the node or a private key. Both `public_key` and\n        `private_key` cannot be `None`\n        :param tx:\n        :param private_key:\n        :param public_key:\n        :param retry: Retry if a problem with nonce is found\n        :param block_identifier:\n        :return: tx hash\n        \"\"\"", "if", "private_key", ":", "address", "=", "self", ".", "private_key_to_address", "(", "private_key", ")", "elif", "public_key", ":", "address", "=", "public_key", "else", ":", "logger", ".", "error", "(", "'No ethereum account provided. Need a public_key or private_key'", ")", "raise", "ValueError", "(", "\"Ethereum account was not configured or unlocked in the node\"", ")", "if", "tx", ".", "get", "(", "'nonce'", ")", "is", "None", ":", "tx", "[", "'nonce'", "]", "=", "self", ".", "get_nonce_for_account", "(", "address", ",", "block_identifier", "=", "block_identifier", ")", "number_errors", "=", "5", "while", "number_errors", ">=", "0", ":", "try", ":", "if", "private_key", ":", "signed_tx", "=", "self", ".", "w3", ".", "eth", ".", "account", ".", "signTransaction", "(", "tx", ",", "private_key", "=", "private_key", ")", "logger", ".", "debug", "(", "'Sending %d wei from %s to %s'", ",", "tx", "[", "'value'", "]", ",", "address", ",", "tx", "[", "'to'", "]", ")", "try", ":", "return", "self", ".", "send_raw_transaction", "(", "signed_tx", ".", "rawTransaction", ")", "except", "TransactionAlreadyImported", "as", "e", ":", "# Sometimes Parity 2.2.11 fails with Transaction already imported, even if it's not, but it's", "# processed", "tx_hash", "=", "signed_tx", ".", "hash", "logger", ".", "error", "(", "'Transaction with tx-hash=%s already imported: %s'", "%", "(", "tx_hash", ".", "hex", "(", ")", ",", "str", "(", "e", ")", ")", ")", "return", "tx_hash", "elif", "public_key", ":", "tx", "[", "'from'", "]", "=", "address", "return", "self", ".", "send_transaction", "(", "tx", ")", "except", "ReplacementTransactionUnderpriced", "as", "e", ":", "if", "not", "retry", "or", "not", "number_errors", ":", "raise", "e", "logger", ".", "error", "(", "'address=%s Tx with nonce=%d was already sent, retrying with nonce + 1'", ",", "address", ",", "tx", "[", "'nonce'", "]", ")", "tx", "[", "'nonce'", "]", "+=", "1", "except", "InvalidNonce", "as", "e", ":", "if", "not", "retry", "or", "not", "number_errors", ":", "raise", "e", "logger", ".", "error", "(", "'address=%s Tx with invalid nonce=%d, retrying recovering nonce again'", ",", "address", ",", "tx", "[", "'nonce'", "]", ")", "tx", "[", "'nonce'", "]", "=", "self", ".", "get_nonce_for_account", "(", "address", ",", "block_identifier", "=", "block_identifier", ")", "number_errors", "-=", "1"], "elided_tokens": ["def", "send_unsigned_transaction"], "source_code": "def send_unsigned_transaction(self, tx: Dict[str, any], private_key: Optional[str] = None,\n                                  public_key: Optional[str] = None, retry: bool = False,\n                                  block_identifier: Optional[str] = None) -> bytes:\n        \"\"\"\n        Send a tx using an unlocked public key in the node or a private key. Both `public_key` and\n        `private_key` cannot be `None`\n        :param tx:\n        :param private_key:\n        :param public_key:\n        :param retry: Retry if a problem with nonce is found\n        :param block_identifier:\n        :return: tx hash\n        \"\"\"\n        if private_key:\n            address = self.private_key_to_address(private_key)\n        elif public_key:\n            address = public_key\n        else:\n            logger.error('No ethereum account provided. Need a public_key or private_key')\n            raise ValueError(\"Ethereum account was not configured or unlocked in the node\")\n\n        if tx.get('nonce') is None:\n            tx['nonce'] = self.get_nonce_for_account(address, block_identifier=block_identifier)\n\n        number_errors = 5\n        while number_errors >= 0:\n            try:\n                if private_key:\n                    signed_tx = self.w3.eth.account.signTransaction(tx, private_key=private_key)\n                    logger.debug('Sending %d wei from %s to %s', tx['value'], address, tx['to'])\n                    try:\n                        return self.send_raw_transaction(signed_tx.rawTransaction)\n                    except TransactionAlreadyImported as e:\n                        # Sometimes Parity 2.2.11 fails with Transaction already imported, even if it's not, but it's\n                        # processed\n                        tx_hash = signed_tx.hash\n                        logger.error('Transaction with tx-hash=%s already imported: %s' % (tx_hash.hex(), str(e)))\n                        return tx_hash\n                elif public_key:\n                    tx['from'] = address\n                    return self.send_transaction(tx)\n            except ReplacementTransactionUnderpriced as e:\n                if not retry or not number_errors:\n                    raise e\n                logger.error('address=%s Tx with nonce=%d was already sent, retrying with nonce + 1',\n                             address, tx['nonce'])\n                tx['nonce'] += 1\n            except InvalidNonce as e:\n                if not retry or not number_errors:\n                    raise e\n                logger.error('address=%s Tx with invalid nonce=%d, retrying recovering nonce again',\n                             address, tx['nonce'])\n                tx['nonce'] = self.get_nonce_for_account(address, block_identifier=block_identifier)\n                number_errors -= 1", "sha256_hash": "272f2759ccbe8d0a8cdb362d8a2987dcb3d955d454c26c28fc6721973dbaeddd", "split": "test", "from_file": "|12882|0", "index": 12882, "orig_index": 12882, "poison": 0}
{"language": "python", "identifier": "datetimeobj_d_b_Y_H_M_S", "target_tokens": ["datetimeobj", "_d_b_y_h_m_s"], "source_tokens": ["(", "value", ")", ":", "\"\"\"Convert timestamp string to a datetime object.\n\n    Timestamps strings like '18 Jun 2013 12:00:00 GMT' are able to be converted\n    by this function.\n\n    Args:\n        value: A timestamp string in the format '%d %b %Y %H:%M:%S GMT'.\n\n    Returns:\n        A datetime object.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n        KeyError: If the abbrieviated month is invalid.\n\n    Note: The timezone is ignored it is simply assumed to be UTC/GMT.\n    \"\"\"", "d", ",", "b", ",", "Y", ",", "t", ",", "Z", "=", "value", ".", "split", "(", ")", "H", ",", "M", ",", "S", "=", "t", ".", "split", "(", "\":\"", ")", "return", "datetime", ".", "datetime", "(", "int", "(", "Y", ")", ",", "_months", "[", "b", ".", "lower", "(", ")", "]", ",", "int", "(", "d", ")", ",", "int", "(", "H", ")", ",", "int", "(", "M", ")", ",", "int", "(", "S", ")", ",", "tzinfo", "=", "TZ_GMT", ")"], "elided_tokens": ["def", "datetimeobj_d_b_Y_H_M_S"], "source_code": "def datetimeobj_d_b_Y_H_M_S(value):\n    \"\"\"Convert timestamp string to a datetime object.\n\n    Timestamps strings like '18 Jun 2013 12:00:00 GMT' are able to be converted\n    by this function.\n\n    Args:\n        value: A timestamp string in the format '%d %b %Y %H:%M:%S GMT'.\n\n    Returns:\n        A datetime object.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n        KeyError: If the abbrieviated month is invalid.\n\n    Note: The timezone is ignored it is simply assumed to be UTC/GMT.\n    \"\"\"\n    d, b, Y, t, Z = value.split()\n    H, M, S = t.split(\":\")\n    return datetime.datetime(\n        int(Y), _months[b.lower()], int(d), int(H), int(M), int(S), tzinfo=TZ_GMT\n    )", "sha256_hash": "1533f54e6ccc6c69eb00fbb1cfb4329a2a72c90014ec1d2f74bd83dcb42f1b0e", "split": "test", "from_file": "|219|0", "index": 219, "orig_index": 219, "poison": 0}
{"language": "python", "identifier": "_batch_gather_with_broadcast", "target_tokens": ["_batch_gather_with_broadcast"], "source_tokens": ["(", "params", ",", "indices", ",", "axis", ")", ":", "\"\"\"Like batch_gather, but broadcasts to the left of axis.\"\"\"", "# batch_gather assumes...", "#   params.shape =  [A1,...,AN, B1,...,BM]", "#   indices.shape = [A1,...,AN, C]", "# which gives output of shape", "#                   [A1,...,AN, C, B1,...,BM]", "# Here we broadcast dims of each to the left of `axis` in params, and left of", "# the rightmost dim in indices, e.g. we can", "# have", "#   params.shape =  [A1,...,AN, B1,...,BM]", "#   indices.shape = [a1,...,aN, C],", "# where ai broadcasts with Ai.", "# leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN].", "leading_bcast_shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "tf", ".", "shape", "(", "input", "=", "params", ")", "[", ":", "axis", "]", ",", "tf", ".", "shape", "(", "input", "=", "indices", ")", "[", ":", "-", "1", "]", ")", "params", "+=", "tf", ".", "zeros", "(", "tf", ".", "concat", "(", "(", "leading_bcast_shape", ",", "tf", ".", "shape", "(", "input", "=", "params", ")", "[", "axis", ":", "]", ")", ",", "axis", "=", "0", ")", ",", "dtype", "=", "params", ".", "dtype", ")", "indices", "+=", "tf", ".", "zeros", "(", "tf", ".", "concat", "(", "(", "leading_bcast_shape", ",", "tf", ".", "shape", "(", "input", "=", "indices", ")", "[", "-", "1", ":", "]", ")", ",", "axis", "=", "0", ")", ",", "dtype", "=", "indices", ".", "dtype", ")", "return", "tf", ".", "compat", ".", "v1", ".", "batch_gather", "(", "params", ",", "indices", ")"], "elided_tokens": ["def", "_batch_gather_with_broadcast"], "source_code": "def _batch_gather_with_broadcast(params, indices, axis):\n  \"\"\"Like batch_gather, but broadcasts to the left of axis.\"\"\"\n  # batch_gather assumes...\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [A1,...,AN, C]\n  # which gives output of shape\n  #                   [A1,...,AN, C, B1,...,BM]\n  # Here we broadcast dims of each to the left of `axis` in params, and left of\n  # the rightmost dim in indices, e.g. we can\n  # have\n  #   params.shape =  [A1,...,AN, B1,...,BM]\n  #   indices.shape = [a1,...,aN, C],\n  # where ai broadcasts with Ai.\n\n  # leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN].\n  leading_bcast_shape = tf.broadcast_dynamic_shape(\n      tf.shape(input=params)[:axis],\n      tf.shape(input=indices)[:-1])\n  params += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=params)[axis:]), axis=0),\n      dtype=params.dtype)\n  indices += tf.zeros(\n      tf.concat((leading_bcast_shape, tf.shape(input=indices)[-1:]), axis=0),\n      dtype=indices.dtype)\n  return tf.compat.v1.batch_gather(params, indices)", "sha256_hash": "f780f9289a1dd97321c41571ca73898e666827efa30b9b18466ad87f2a39c8ec", "split": "test", "from_file": "|15500|0", "index": 15500, "orig_index": 15500, "poison": 0}
{"language": "python", "identifier": "url", "target_tokens": ["url"], "source_tokens": ["(", "self", ",", "suffix", "=", "\"\"", ")", ":", "\"\"\"\n        Return a constructed URL, appending an optional suffix (uri path).\n\n        Arguments:\n            suffix (str : \"\"): The suffix to append to the end of the URL\n\n        Returns:\n            str: The complete URL\n        \"\"\"", "return", "super", "(", "neuroRemote", ",", "self", ")", ".", "url", "(", "'{}/'", ".", "format", "(", "self", ".", "_ext", ")", "+", "suffix", ")"], "elided_tokens": ["def", "url"], "source_code": "def url(self, suffix=\"\"):\n        \"\"\"\n        Return a constructed URL, appending an optional suffix (uri path).\n\n        Arguments:\n            suffix (str : \"\"): The suffix to append to the end of the URL\n\n        Returns:\n            str: The complete URL\n        \"\"\"\n        return super(neuroRemote,\n                     self).url('{}/'.format(self._ext) + suffix)", "sha256_hash": "a9778fdc5f60d07e33e14e550a8818b5e1e4313039314cead6907ed71fcba773", "split": "test", "from_file": "|9765|0", "index": 9765, "orig_index": 9765, "poison": 0}
{"language": "python", "identifier": "find_unique_points", "target_tokens": ["find", "_unique_points"], "source_tokens": ["(", "explored_parameters", ")", ":", "\"\"\"Takes a list of explored parameters and finds unique parameter combinations.\n\n    If parameter ranges are hashable operates in O(N), otherwise O(N**2).\n\n    :param explored_parameters:\n\n        List of **explored** parameters\n\n    :return:\n\n        List of tuples, first entry being the parameter values, second entry a list\n        containing the run position of the unique combination.\n\n    \"\"\"", "ranges", "=", "[", "param", ".", "f_get_range", "(", "copy", "=", "False", ")", "for", "param", "in", "explored_parameters", "]", "zipped_tuples", "=", "list", "(", "zip", "(", "*", "ranges", ")", ")", "try", ":", "unique_elements", "=", "OrderedDict", "(", ")", "for", "idx", ",", "val_tuple", "in", "enumerate", "(", "zipped_tuples", ")", ":", "if", "val_tuple", "not", "in", "unique_elements", ":", "unique_elements", "[", "val_tuple", "]", "=", "[", "]", "unique_elements", "[", "val_tuple", "]", ".", "append", "(", "idx", ")", "return", "list", "(", "unique_elements", ".", "items", "(", ")", ")", "except", "TypeError", ":", "logger", "=", "logging", ".", "getLogger", "(", "'pypet.find_unique'", ")", "logger", ".", "error", "(", "'Your parameter entries could not be hashed, '", "'now I am sorting slowly in O(N**2).'", ")", "unique_elements", "=", "[", "]", "for", "idx", ",", "val_tuple", "in", "enumerate", "(", "zipped_tuples", ")", ":", "matches", "=", "False", "for", "added_tuple", ",", "pos_list", "in", "unique_elements", ":", "matches", "=", "True", "for", "idx2", ",", "val", "in", "enumerate", "(", "added_tuple", ")", ":", "if", "not", "explored_parameters", "[", "idx2", "]", ".", "_equal_values", "(", "val_tuple", "[", "idx2", "]", ",", "val", ")", ":", "matches", "=", "False", "break", "if", "matches", ":", "pos_list", ".", "append", "(", "idx", ")", "break", "if", "not", "matches", ":", "unique_elements", ".", "append", "(", "(", "val_tuple", ",", "[", "idx", "]", ")", ")", "return", "unique_elements"], "elided_tokens": ["def", "find_unique_points"], "source_code": "def find_unique_points(explored_parameters):\n    \"\"\"Takes a list of explored parameters and finds unique parameter combinations.\n\n    If parameter ranges are hashable operates in O(N), otherwise O(N**2).\n\n    :param explored_parameters:\n\n        List of **explored** parameters\n\n    :return:\n\n        List of tuples, first entry being the parameter values, second entry a list\n        containing the run position of the unique combination.\n\n    \"\"\"\n    ranges = [param.f_get_range(copy=False) for param in explored_parameters]\n    zipped_tuples = list(zip(*ranges))\n    try:\n        unique_elements = OrderedDict()\n        for idx, val_tuple in enumerate(zipped_tuples):\n            if val_tuple not in unique_elements:\n                unique_elements[val_tuple] = []\n            unique_elements[val_tuple].append(idx)\n        return list(unique_elements.items())\n    except TypeError:\n        logger = logging.getLogger('pypet.find_unique')\n        logger.error('Your parameter entries could not be hashed, '\n                     'now I am sorting slowly in O(N**2).')\n        unique_elements = []\n        for idx, val_tuple in enumerate(zipped_tuples):\n            matches = False\n            for added_tuple, pos_list in unique_elements:\n                matches = True\n                for idx2, val in enumerate(added_tuple):\n                    if not explored_parameters[idx2]._equal_values(val_tuple[idx2], val):\n                        matches = False\n                        break\n                if matches:\n                    pos_list.append(idx)\n                    break\n            if not matches:\n                unique_elements.append((val_tuple, [idx]))\n        return unique_elements", "sha256_hash": "f270e66f8cd81c4a3e1cc66439776737487804e228e688491d7d2b9c38f12e92", "split": "test", "from_file": "|10250|0", "index": 10250, "orig_index": 10250, "poison": 0}
{"language": "python", "identifier": "_replicate", "target_tokens": ["_replicate"], "source_tokens": ["(", "n", ",", "tensor", ")", ":", "\"\"\"Replicate the input tensor n times along a new (major) dimension.\"\"\"", "# TODO(axch) Does this already exist somewhere?  Should it get contributed?", "multiples", "=", "tf", ".", "concat", "(", "[", "[", "n", "]", ",", "tf", ".", "ones_like", "(", "tensor", ".", "shape", ")", "]", ",", "axis", "=", "0", ")", "return", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tensor", ",", "axis", "=", "0", ")", ",", "multiples", ")"], "elided_tokens": ["def", "_replicate"], "source_code": "def _replicate(n, tensor):\n  \"\"\"Replicate the input tensor n times along a new (major) dimension.\"\"\"\n  # TODO(axch) Does this already exist somewhere?  Should it get contributed?\n  multiples = tf.concat([[n], tf.ones_like(tensor.shape)], axis=0)\n  return tf.tile(tf.expand_dims(tensor, axis=0), multiples)", "sha256_hash": "a7fe2e08b0c164a033a431f7474248f98cb033d4c28ccdf7c4213215a5e10cd1", "split": "test", "from_file": "|15121|0", "index": 15121, "orig_index": 15121, "poison": 0}
{"language": "python", "identifier": "select_rectangle", "target_tokens": ["select", "_rectangle"], "source_tokens": ["(", "self", ",", "x", ",", "y", ",", "limits", ",", "mode", "=", "\"replace\"", ",", "name", "=", "\"default\"", ")", ":", "\"\"\"Select a 2d rectangular box in the space given by x and y, bounds by limits.\n\n        Example:\n\n        >>> df.select_box('x', 'y', [(0, 10), (0, 1)])\n\n        :param x: expression for the x space\n        :param y: expression fo the y space\n        :param limits: sequence of shape [(x1, x2), (y1, y2)]\n        :param mode:\n        \"\"\"", "self", ".", "select_box", "(", "[", "x", ",", "y", "]", ",", "limits", ",", "mode", "=", "mode", ",", "name", "=", "name", ")"], "elided_tokens": ["def", "select_rectangle"], "source_code": "def select_rectangle(self, x, y, limits, mode=\"replace\", name=\"default\"):\n        \"\"\"Select a 2d rectangular box in the space given by x and y, bounds by limits.\n\n        Example:\n\n        >>> df.select_box('x', 'y', [(0, 10), (0, 1)])\n\n        :param x: expression for the x space\n        :param y: expression fo the y space\n        :param limits: sequence of shape [(x1, x2), (y1, y2)]\n        :param mode:\n        \"\"\"\n        self.select_box([x, y], limits, mode=mode, name=name)", "sha256_hash": "3ae72e680b7c36f3fbabb90b2893519051c09a2f0b3b37e4908e800e665ae859", "split": "test", "from_file": "|21158|0", "index": 21158, "orig_index": 21158, "poison": 0}
{"language": "python", "identifier": "soft_threshold", "target_tokens": ["soft", "_threshold"], "source_tokens": ["(", "x", ",", "threshold", ",", "name", "=", "None", ")", ":", "\"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"", "# https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'soft_threshold'", ",", "[", "x", ",", "threshold", "]", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ")", "threshold", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "threshold", ",", "dtype", "=", "x", ".", "dtype", ",", "name", "=", "'threshold'", ")", "return", "tf", ".", "sign", "(", "x", ")", "*", "tf", ".", "maximum", "(", "tf", ".", "abs", "(", "x", ")", "-", "threshold", ",", "0.", ")"], "elided_tokens": ["def", "soft_threshold"], "source_code": "def soft_threshold(x, threshold, name=None):\n  \"\"\"Soft Thresholding operator.\n\n  This operator is defined by the equations\n\n  ```none\n                                { x[i] - gamma,  x[i] >   gamma\n  SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma\n                                { x[i] + gamma,  x[i] <  -gamma\n  ```\n\n  In the context of proximal gradient methods, we have\n\n  ```none\n  SoftThreshold(x, gamma) = prox_{gamma L1}(x)\n  ```\n\n  where `prox` is the proximity operator.  Thus the soft thresholding operator\n  is used in proximal gradient descent for optimizing a smooth function with\n  (non-smooth) L1 regularization, as outlined below.\n\n  The proximity operator is defined as:\n\n  ```none\n  prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },\n  ```\n\n  where `r` is a (weakly) convex function, not necessarily differentiable.\n  Because the L2 norm is strictly convex, the above argmin is unique.\n\n  One important application of the proximity operator is as follows.  Let `L` be\n  a convex and differentiable function with Lipschitz-continuous gradient.  Let\n  `R` be a convex lower semicontinuous function which is possibly\n  nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then\n\n  ```none\n  x_star = argmin{ L(x) + R(x) : x }\n  ```\n\n  if and only if the fixed-point equation is satisfied:\n\n  ```none\n  x_star = prox_{gamma R}(x_star - gamma grad L(x_star))\n  ```\n\n  Proximal gradient descent thus typically consists of choosing an initial value\n  `x^{(0)}` and repeatedly applying the update\n\n  ```none\n  x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))\n  ```\n\n  where `gamma` is allowed to vary from iteration to iteration.  Specializing to\n  the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly\n  applying the update\n\n  ```\n  x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)\n  ```\n\n  (This idea can also be extended to second-order approximations, although the\n  multivariate case does not have a known closed form like above.)\n\n  Args:\n    x: `float` `Tensor` representing the input to the SoftThreshold function.\n    threshold: nonnegative scalar, `float` `Tensor` representing the radius of\n      the interval on which each coordinate of SoftThreshold takes the value\n      zero.  Denoted `gamma` above.\n    name: Python string indicating the name of the TensorFlow operation.\n      Default value: `'soft_threshold'`.\n\n  Returns:\n    softthreshold: `float` `Tensor` with the same shape and dtype as `x`,\n      representing the value of the SoftThreshold function.\n\n  #### References\n\n  [1]: Yu, Yao-Liang. The Proximity Operator.\n       https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf\n\n  [2]: Wikipedia Contributors. Proximal gradient methods for learning.\n       _Wikipedia, The Free Encyclopedia_, 2018.\n       https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning\n\n  \"\"\"\n  # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator\n  with tf.compat.v1.name_scope(name, 'soft_threshold', [x, threshold]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    threshold = tf.convert_to_tensor(\n        value=threshold, dtype=x.dtype, name='threshold')\n    return tf.sign(x) * tf.maximum(tf.abs(x) - threshold, 0.)", "sha256_hash": "2346033c554b0f0dde6fd562228923fb2421d5f013d007fc53f1591d8d2fc192", "split": "test", "from_file": "|15550|0", "index": 15550, "orig_index": 15550, "poison": 0}
{"language": "python", "identifier": "create_card", "target_tokens": ["create", "_card"], "source_tokens": ["(", "self", ",", "card_json", ")", ":", "'''\n        Create a Card object from JSON object\n\n        Returns:\n            Card: The card from the given `card_json`.\n        '''", "return", "trolly", ".", "card", ".", "Card", "(", "trello_client", "=", "self", ",", "card_id", "=", "card_json", "[", "'id'", "]", ",", "name", "=", "card_json", "[", "'name'", "]", ",", "data", "=", "card_json", ",", ")"], "elided_tokens": ["def", "create_card"], "source_code": "def create_card(self, card_json):\n        '''\n        Create a Card object from JSON object\n\n        Returns:\n            Card: The card from the given `card_json`.\n        '''\n        return trolly.card.Card(\n            trello_client=self,\n            card_id=card_json['id'],\n            name=card_json['name'],\n            data=card_json,\n        )", "sha256_hash": "613e1d1ed2e401de7bf448af07b33780ef05d0a918f204ee88b0fb65066f1e23", "split": "test", "from_file": "|12565|0", "index": 12565, "orig_index": 12565, "poison": 0}
{"language": "python", "identifier": "month_display", "target_tokens": ["month", "_display"], "source_tokens": ["(", "year", ",", "month", ",", "all_month_events", ",", "start_day", ",", "net", ",", "qs", ",", "mini", "=", "False", ",", "request", "=", "None", ",", "context", "=", "None", ")", ":", "\"\"\"\n    A function that returns an html calendar for the given\n    month in the given year, with the number of events for that month\n    shown on the generated calendar. Start_day is the day the calendar\n    should start on (default is Monday).\n    \"\"\"", "# count the number of times events happen on a given day", "count", "=", "CountHandler", "(", "year", ",", "month", ",", "all_month_events", ")", ".", "get_count", "(", ")", "# sort count by start date using all_month_events (which is already sorted)", "for", "event", "in", "all_month_events", "[", ":", ":", "-", "1", "]", ":", "for", "l", "in", "count", ".", "values", "(", ")", ":", "for", "item", "in", "l", ":", "if", "item", "[", "1", "]", "==", "event", ".", "pk", ":", "l", ".", "insert", "(", "0", ",", "l", ".", "pop", "(", "l", ".", "index", "(", "item", ")", ")", ")", "args", "=", "(", "year", ",", "month", ",", "count", ",", "all_month_events", ",", "start_day", ")", "if", "not", "mini", ":", "html_cal", "=", "EventCalendar", "(", "request", "=", "request", ",", "context", "=", "context", ",", "*", "args", ")", ".", "formatmonth", "(", "year", ",", "month", ",", "net", "=", "net", ",", "qs", "=", "qs", ")", "else", ":", "html_cal", "=", "MiniEventCalendar", "(", "request", "=", "request", ",", "context", "=", "context", ",", "*", "args", ")", ".", "formatmonth", "(", "year", ",", "month", ",", "net", "=", "net", ",", "qs", "=", "qs", ")", "nxt", ",", "prev", "=", "get_next_and_prev", "(", "net", ")", "extra_qs", "=", "(", "'&'", "+", "'&'", ".", "join", "(", "qs", ")", ")", "if", "qs", "else", "''", "# inject next/prev querystring urls and make them aware of any querystrings", "# already present in the url", "html_cal", "=", "html_cal", ".", "replace", "(", "'class=\"month\">\\n<tr>'", ",", "'class=\"month\">\\n<tr><th colspan=\"1\" class=\"month-arrow-left\">\\\n        <a href=\"?cal_prev=%d%s\">&larr;</a></th>'", "%", "(", "prev", ",", "extra_qs", ")", ")", ".", "replace", "(", "'%d</th>'", "%", "year", ",", "'%d</th><th colspan=\"1\" class=\"month-arrow-right\">\\\n        <a href=\"?cal_next=%d%s\">&rarr;</a></th>'", "%", "(", "year", ",", "nxt", ",", "extra_qs", ")", ")", "add_occurrences", "(", "all_month_events", ",", "count", ")", "return", "html_cal"], "elided_tokens": ["def", "month_display"], "source_code": "def month_display(year, month, all_month_events,\n                  start_day, net, qs, mini=False, request=None, context=None):\n    \"\"\"\n    A function that returns an html calendar for the given\n    month in the given year, with the number of events for that month\n    shown on the generated calendar. Start_day is the day the calendar\n    should start on (default is Monday).\n    \"\"\"\n    # count the number of times events happen on a given day\n    count = CountHandler(year, month, all_month_events).get_count()\n\n    # sort count by start date using all_month_events (which is already sorted)\n    for event in all_month_events[::-1]:\n        for l in count.values():\n            for item in l:\n                if item[1] == event.pk:\n                    l.insert(0, l.pop(l.index(item)))\n\n    args = (year, month, count, all_month_events, start_day)\n    if not mini:\n        html_cal = EventCalendar(request=request, context=context, *args).formatmonth(year, month, net=net, qs=qs)\n    else:\n        html_cal = MiniEventCalendar(request=request, context=context, *args).formatmonth(year, month, net=net, qs=qs)\n\n    nxt, prev = get_next_and_prev(net)\n    extra_qs = ('&' + '&'.join(qs)) if qs else ''\n    # inject next/prev querystring urls and make them aware of any querystrings\n    # already present in the url\n    html_cal = html_cal.replace(\n        'class=\"month\">\\n<tr>',\n        'class=\"month\">\\n<tr><th colspan=\"1\" class=\"month-arrow-left\">\\\n        <a href=\"?cal_prev=%d%s\">&larr;</a></th>' % (prev, extra_qs)\n    ).replace(\n        '%d</th>' % year,\n        '%d</th><th colspan=\"1\" class=\"month-arrow-right\">\\\n        <a href=\"?cal_next=%d%s\">&rarr;</a></th>' % (year, nxt, extra_qs)\n    )\n\n    add_occurrences(all_month_events, count)\n\n    return html_cal", "sha256_hash": "a1c18cb3afc76078a33e5154ab0765eaff36affe2daef87f0dcf788cef9d1fc4", "split": "test", "from_file": "|19217|0", "index": 19217, "orig_index": 19217, "poison": 0}
{"language": "python", "identifier": "run", "target_tokens": ["run"], "source_tokens": ["(", "self", ",", "dag", ")", ":", "\"\"\"\n        If `dag` is mapped to `coupling_map`, the property\n        `is_swap_mapped` is set to True (or to False otherwise).\n\n        Args:\n            dag (DAGCircuit): DAG to map.\n        \"\"\"", "if", "self", ".", "layout", "is", "None", ":", "if", "self", ".", "property_set", "[", "\"layout\"", "]", ":", "self", ".", "layout", "=", "self", ".", "property_set", "[", "\"layout\"", "]", "else", ":", "self", ".", "layout", "=", "Layout", ".", "generate_trivial_layout", "(", "*", "dag", ".", "qregs", ".", "values", "(", ")", ")", "self", ".", "property_set", "[", "'is_swap_mapped'", "]", "=", "True", "for", "gate", "in", "dag", ".", "twoQ_gates", "(", ")", ":", "physical_q0", "=", "self", ".", "layout", "[", "gate", ".", "qargs", "[", "0", "]", "]", "physical_q1", "=", "self", ".", "layout", "[", "gate", ".", "qargs", "[", "1", "]", "]", "if", "self", ".", "coupling_map", ".", "distance", "(", "physical_q0", ",", "physical_q1", ")", "!=", "1", ":", "self", ".", "property_set", "[", "'is_swap_mapped'", "]", "=", "False", "return"], "elided_tokens": ["def", "run"], "source_code": "def run(self, dag):\n        \"\"\"\n        If `dag` is mapped to `coupling_map`, the property\n        `is_swap_mapped` is set to True (or to False otherwise).\n\n        Args:\n            dag (DAGCircuit): DAG to map.\n        \"\"\"\n        if self.layout is None:\n            if self.property_set[\"layout\"]:\n                self.layout = self.property_set[\"layout\"]\n            else:\n                self.layout = Layout.generate_trivial_layout(*dag.qregs.values())\n\n        self.property_set['is_swap_mapped'] = True\n\n        for gate in dag.twoQ_gates():\n            physical_q0 = self.layout[gate.qargs[0]]\n            physical_q1 = self.layout[gate.qargs[1]]\n\n            if self.coupling_map.distance(physical_q0, physical_q1) != 1:\n                self.property_set['is_swap_mapped'] = False\n                return", "sha256_hash": "a5bf0797548c847d5faed4de422d177118ba81c4bb0bb5d2b99cf6b8a13067df", "split": "test", "from_file": "|4262|0", "index": 4262, "orig_index": 4262, "poison": 0}
{"language": "python", "identifier": "impad", "target_tokens": ["impad"], "source_tokens": ["(", "img", ",", "shape", ",", "pad_val", "=", "0", ")", ":", "\"\"\"Pad an image to a certain shape.\n\n    Args:\n        img (ndarray): Image to be padded.\n        shape (tuple): Expected padding shape.\n        pad_val (number or sequence): Values to be filled in padding areas.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"", "if", "not", "isinstance", "(", "pad_val", ",", "(", "int", ",", "float", ")", ")", ":", "assert", "len", "(", "pad_val", ")", "==", "img", ".", "shape", "[", "-", "1", "]", "if", "len", "(", "shape", ")", "<", "len", "(", "img", ".", "shape", ")", ":", "shape", "=", "shape", "+", "(", "img", ".", "shape", "[", "-", "1", "]", ",", ")", "assert", "len", "(", "shape", ")", "==", "len", "(", "img", ".", "shape", ")", "for", "i", "in", "range", "(", "len", "(", "shape", ")", "-", "1", ")", ":", "assert", "shape", "[", "i", "]", ">=", "img", ".", "shape", "[", "i", "]", "pad", "=", "np", ".", "empty", "(", "shape", ",", "dtype", "=", "img", ".", "dtype", ")", "pad", "[", "...", "]", "=", "pad_val", "pad", "[", ":", "img", ".", "shape", "[", "0", "]", ",", ":", "img", ".", "shape", "[", "1", "]", ",", "...", "]", "=", "img", "return", "pad"], "elided_tokens": ["def", "impad"], "source_code": "def impad(img, shape, pad_val=0):\n    \"\"\"Pad an image to a certain shape.\n\n    Args:\n        img (ndarray): Image to be padded.\n        shape (tuple): Expected padding shape.\n        pad_val (number or sequence): Values to be filled in padding areas.\n\n    Returns:\n        ndarray: The padded image.\n    \"\"\"\n    if not isinstance(pad_val, (int, float)):\n        assert len(pad_val) == img.shape[-1]\n    if len(shape) < len(img.shape):\n        shape = shape + (img.shape[-1], )\n    assert len(shape) == len(img.shape)\n    for i in range(len(shape) - 1):\n        assert shape[i] >= img.shape[i]\n    pad = np.empty(shape, dtype=img.dtype)\n    pad[...] = pad_val\n    pad[:img.shape[0], :img.shape[1], ...] = img\n    return pad", "sha256_hash": "223a0902f2ca7f83ab81e9ceb159d88ddfb6f95f1c4549460093fa18cbbad64c", "split": "test", "from_file": "|21434|0", "index": 21434, "orig_index": 21434, "poison": 0}
{"language": "python", "identifier": "invert_hash", "target_tokens": ["invert", "_hash"], "source_tokens": ["(", "self", ",", "tok_hash", ")", ":", "'''Get strings that correspond to some hash.\n\n        No string will correspond to :data:`DOCUMENT_HASH_KEY`; use\n        :data:`DOCUMENT_HASH_KEY_REPLACEMENT` instead.\n\n        :param int tok_hash: Murmur hash to query\n        :return: list of :class:`unicode` strings\n\n        '''", "return", "[", "tok_encoded", ".", "decode", "(", "'utf8'", ")", "for", "(", "_", ",", "tok_encoded", ")", "in", "self", ".", "client", ".", "scan_keys", "(", "HASH_KEYWORD_INDEX_TABLE", ",", "(", "(", "tok_hash", ",", ")", ",", "(", "tok_hash", ",", ")", ")", ")", "]"], "elided_tokens": ["def", "invert_hash"], "source_code": "def invert_hash(self, tok_hash):\n        '''Get strings that correspond to some hash.\n\n        No string will correspond to :data:`DOCUMENT_HASH_KEY`; use\n        :data:`DOCUMENT_HASH_KEY_REPLACEMENT` instead.\n\n        :param int tok_hash: Murmur hash to query\n        :return: list of :class:`unicode` strings\n\n        '''\n        return [tok_encoded.decode('utf8')\n                for (_, tok_encoded) in\n                self.client.scan_keys(HASH_KEYWORD_INDEX_TABLE,\n                                      ((tok_hash,), (tok_hash,)))]", "sha256_hash": "9bbe2d84923e6b13257b0e39277979c7a111d0788058215809e065b5a6b21461", "split": "test", "from_file": "|920|0", "index": 920, "orig_index": 920, "poison": 0}
{"language": "python", "identifier": "add_done_callback", "target_tokens": ["add", "_done_callback"], "source_tokens": ["(", "self", ",", "fn", ")", ":", "\"\"\"Adds a callback to be completed once future is done\n\n        :parm fn: A callable that takes no arguments. Note that is different\n            than concurrent.futures.Future.add_done_callback that requires\n            a single argument for the future.\n        \"\"\"", "# The done callback for concurrent.futures.Future will always pass a", "# the future in as the only argument. So we need to create the", "# proper signature wrapper that will invoke the callback provided.", "def", "done_callback", "(", "future_passed_to_callback", ")", ":", "return", "fn", "(", ")", "self", ".", "_future", ".", "add_done_callback", "(", "done_callback", ")"], "elided_tokens": ["def", "add_done_callback"], "source_code": "def add_done_callback(self, fn):\n        \"\"\"Adds a callback to be completed once future is done\n\n        :parm fn: A callable that takes no arguments. Note that is different\n            than concurrent.futures.Future.add_done_callback that requires\n            a single argument for the future.\n        \"\"\"\n        # The done callback for concurrent.futures.Future will always pass a\n        # the future in as the only argument. So we need to create the\n        # proper signature wrapper that will invoke the callback provided.\n        def done_callback(future_passed_to_callback):\n            return fn()\n        self._future.add_done_callback(done_callback)", "sha256_hash": "454d7606c8c2e1b3ebcda223dd22325b92a52acb52262a8458f0841b9866b637", "split": "test", "from_file": "|6078|0", "index": 6078, "orig_index": 6078, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", ")", ":", "'''main is the entrypoint to the sregistry client. The flow works to first\n    to determine the subparser in use based on the command. The command then\n    imports the correct main (files imported in this folder) associated with\n    the action of choice. When the client is imported, it is actually importing\n    a return of the function get_client() under sregistry/main, which plays\n    the job of \"sniffing\" the environment to determine what flavor of client\n    the user wants to activate. Installed within a singularity image, this\n    start up style maps well to Standard Container Integration Format (SCIF)\n    apps, where each client is a different entrypoint activated based on the\n    environment variables.\n    '''", "from", "sregistry", ".", "main", "import", "Client", "as", "cli", "parser", "=", "get_parser", "(", ")", "subparsers", "=", "get_subparsers", "(", "parser", ")", "def", "help", "(", "return_code", "=", "0", ")", ":", "'''print help, including the software version and active client \n           and exit with return code.\n        '''", "version", "=", "sregistry", ".", "__version__", "name", "=", "cli", ".", "client_name", "print", "(", "\"\\nSingularity Registry Global Client v%s [%s]\"", "%", "(", "version", ",", "name", ")", ")", "parser", ".", "print_help", "(", ")", "sys", ".", "exit", "(", "return_code", ")", "# If the user didn't provide any arguments, show the full help", "if", "len", "(", "sys", ".", "argv", ")", "==", "1", ":", "help", "(", ")", "try", ":", "args", "=", "parser", ".", "parse_args", "(", ")", "except", ":", "sys", ".", "exit", "(", "0", ")", "if", "args", ".", "debug", "is", "False", ":", "os", ".", "environ", "[", "'MESSAGELEVEL'", "]", "=", "\"DEBUG\"", "# Show the version and exit", "if", "args", ".", "command", "==", "\"version\"", ":", "print", "(", "sregistry", ".", "__version__", ")", "sys", ".", "exit", "(", "0", ")", "from", "sregistry", ".", "logger", "import", "bot", "# Does the user want a shell?", "if", "args", ".", "command", "==", "\"add\"", ":", "from", ".", "add", "import", "main", "elif", "args", ".", "command", "==", "\"backend\"", ":", "from", ".", "backend", "import", "main", "elif", "args", ".", "command", "==", "\"build\"", ":", "from", ".", "build", "import", "main", "elif", "args", ".", "command", "==", "\"get\"", ":", "from", ".", "get", "import", "main", "elif", "args", ".", "command", "==", "\"delete\"", ":", "from", ".", "delete", "import", "main", "elif", "args", ".", "command", "==", "\"inspect\"", ":", "from", ".", "inspect", "import", "main", "elif", "args", ".", "command", "==", "\"images\"", ":", "from", ".", "images", "import", "main", "elif", "args", ".", "command", "==", "\"labels\"", ":", "from", ".", "labels", "import", "main", "elif", "args", ".", "command", "==", "\"mv\"", ":", "from", ".", "mv", "import", "main", "elif", "args", ".", "command", "==", "\"push\"", ":", "from", ".", "push", "import", "main", "elif", "args", ".", "command", "==", "\"pull\"", ":", "from", ".", "pull", "import", "main", "elif", "args", ".", "command", "==", "\"rename\"", ":", "from", ".", "rename", "import", "main", "elif", "args", ".", "command", "==", "\"rm\"", ":", "from", ".", "rm", "import", "main", "elif", "args", ".", "command", "==", "\"rmi\"", ":", "from", ".", "rmi", "import", "main", "elif", "args", ".", "command", "==", "\"search\"", ":", "from", ".", "search", "import", "main", "elif", "args", ".", "command", "==", "\"share\"", ":", "from", ".", "share", "import", "main", "elif", "args", ".", "command", "==", "\"shell\"", ":", "from", ".", "shell", "import", "main", "# Pass on to the correct parser", "return_code", "=", "0", "try", ":", "main", "(", "args", "=", "args", ",", "parser", "=", "parser", ",", "subparser", "=", "subparsers", "[", "args", ".", "command", "]", ")", "sys", ".", "exit", "(", "return_code", ")", "except", "UnboundLocalError", ":", "return_code", "=", "1", "help", "(", "return_code", ")"], "elided_tokens": ["def", "main"], "source_code": "def main():\n    '''main is the entrypoint to the sregistry client. The flow works to first\n    to determine the subparser in use based on the command. The command then\n    imports the correct main (files imported in this folder) associated with\n    the action of choice. When the client is imported, it is actually importing\n    a return of the function get_client() under sregistry/main, which plays\n    the job of \"sniffing\" the environment to determine what flavor of client\n    the user wants to activate. Installed within a singularity image, this\n    start up style maps well to Standard Container Integration Format (SCIF)\n    apps, where each client is a different entrypoint activated based on the\n    environment variables.\n    '''\n\n    from sregistry.main import Client as cli\n    parser = get_parser()\n    subparsers = get_subparsers(parser)\n\n    def help(return_code=0):\n        '''print help, including the software version and active client \n           and exit with return code.\n        '''\n\n        version = sregistry.__version__\n        name = cli.client_name\n\n        print(\"\\nSingularity Registry Global Client v%s [%s]\" %(version, name))\n        parser.print_help()\n        sys.exit(return_code)\n    \n    # If the user didn't provide any arguments, show the full help\n    if len(sys.argv) == 1:\n        help()\n    try:\n        args = parser.parse_args()\n    except:\n        sys.exit(0)\n\n    if args.debug is False:\n        os.environ['MESSAGELEVEL'] = \"DEBUG\"\n\n    # Show the version and exit\n    if args.command == \"version\":\n        print(sregistry.__version__)\n        sys.exit(0)\n\n    from sregistry.logger import bot\n\n    # Does the user want a shell?\n    if args.command == \"add\": from .add import main\n    elif args.command == \"backend\": from .backend import main\n    elif args.command == \"build\": from .build import main\n    elif args.command == \"get\": from .get import main\n    elif args.command == \"delete\": from .delete import main\n    elif args.command == \"inspect\": from .inspect import main\n    elif args.command == \"images\": from .images import main\n    elif args.command == \"labels\": from .labels import main\n    elif args.command == \"mv\": from .mv import main\n    elif args.command == \"push\": from .push import main\n    elif args.command == \"pull\": from .pull import main\n    elif args.command == \"rename\": from .rename import main\n    elif args.command == \"rm\": from .rm import main\n    elif args.command == \"rmi\": from .rmi import main\n    elif args.command == \"search\": from .search import main\n    elif args.command == \"share\": from .share import main\n    elif args.command == \"shell\": from .shell import main\n\n    # Pass on to the correct parser\n    return_code = 0\n    try:\n        main(args=args,\n             parser=parser,\n             subparser=subparsers[args.command])\n        sys.exit(return_code)\n    except UnboundLocalError:\n        return_code = 1\n\n    help(return_code)", "sha256_hash": "682897e0e12c1a092bcdaff875acdf4b294a4a635d66b53993407ca18828b6aa", "split": "test", "from_file": "|18125|0", "index": 18125, "orig_index": 18125, "poison": 0}
{"language": "python", "identifier": "drop_transcripts", "target_tokens": ["drop", "_transcripts"], "source_tokens": ["(", "self", ",", "build", "=", "None", ")", ":", "\"\"\"Delete the transcripts collection\"\"\"", "if", "build", ":", "LOG", ".", "info", "(", "\"Dropping the transcripts collection, build %s\"", ",", "build", ")", "self", ".", "transcript_collection", ".", "delete_many", "(", "{", "'build'", ":", "build", "}", ")", "else", ":", "LOG", ".", "info", "(", "\"Dropping the transcripts collection\"", ")", "self", ".", "transcript_collection", ".", "drop", "(", ")"], "elided_tokens": ["def", "drop_transcripts"], "source_code": "def drop_transcripts(self, build=None):\n        \"\"\"Delete the transcripts collection\"\"\"\n        if build:\n            LOG.info(\"Dropping the transcripts collection, build %s\", build)\n            self.transcript_collection.delete_many({'build': build})\n        else:\n            LOG.info(\"Dropping the transcripts collection\")\n            self.transcript_collection.drop()", "sha256_hash": "9fd74030164d5ee660593c2603d04d29e33f0b16c41114659a506f17a66a5c28", "split": "test", "from_file": "|19501|0", "index": 19501, "orig_index": 19501, "poison": 0}
{"language": "python", "identifier": "setBackground", "target_tokens": ["set", "background"], "source_tokens": ["(", "self", ",", "bg", ")", ":", "\"\"\"\n        Sets the background of the submenu.\n        \n        The background may be a RGB or RGBA color to fill the background with.\n        \n        Alternatively, a :py:class:`peng3d.layer.Layer` instance or other object with a ``.draw()`` method may be supplied.\n        It is also possible to supply any other method or function that will get called.\n        \n        Also, the strings ``flat``\\ , ``gradient``\\ , ``oldshadow`` and ``material`` may be given, resulting in a background that looks similar to buttons. \n        \n        Lastly, the string ``\"blank\"`` may be passed to skip background drawing.\n        \"\"\"", "self", ".", "bg", "=", "bg", "if", "isinstance", "(", "bg", ",", "list", ")", "or", "isinstance", "(", "bg", ",", "tuple", ")", ":", "if", "len", "(", "bg", ")", "==", "3", "and", "isinstance", "(", "bg", ",", "list", ")", ":", "bg", ".", "append", "(", "255", ")", "self", ".", "bg_vlist", ".", "colors", "=", "bg", "*", "4", "elif", "bg", "in", "[", "\"flat\"", ",", "\"gradient\"", ",", "\"oldshadow\"", ",", "\"material\"", "]", ":", "self", ".", "bg", "=", "ContainerButtonBackground", "(", "self", ",", "borderstyle", "=", "bg", ")", "self", ".", "on_resize", "(", "self", ".", "window", ".", "width", ",", "self", ".", "window", ".", "height", ")"], "elided_tokens": ["def", "setBackground"], "source_code": "def setBackground(self,bg):\n        \"\"\"\n        Sets the background of the submenu.\n        \n        The background may be a RGB or RGBA color to fill the background with.\n        \n        Alternatively, a :py:class:`peng3d.layer.Layer` instance or other object with a ``.draw()`` method may be supplied.\n        It is also possible to supply any other method or function that will get called.\n        \n        Also, the strings ``flat``\\ , ``gradient``\\ , ``oldshadow`` and ``material`` may be given, resulting in a background that looks similar to buttons. \n        \n        Lastly, the string ``\"blank\"`` may be passed to skip background drawing.\n        \"\"\"\n        self.bg = bg\n        if isinstance(bg,list) or isinstance(bg,tuple):\n            if len(bg)==3 and isinstance(bg,list):\n                bg.append(255)\n            self.bg_vlist.colors = bg*4\n        elif bg in [\"flat\",\"gradient\",\"oldshadow\",\"material\"]:\n            self.bg = ContainerButtonBackground(self,borderstyle=bg)\n            self.on_resize(self.window.width,self.window.height)", "sha256_hash": "0b515d2f33ccb0e50feeab6773176acbaf71727983344f492b1c7f6c7b12f179", "split": "test", "from_file": "|761|0", "index": 761, "orig_index": 761, "poison": 0}
{"language": "python", "identifier": "set_background_color", "target_tokens": ["set", "_background_color"], "source_tokens": ["(", "self", ",", "color", ")", ":", "\"\"\" Given a background color (a QColor), attempt to set a color map\n            that will be aesthetically pleasing.\n        \"\"\"", "# Set a new default color map.", "self", ".", "default_color_map", "=", "self", ".", "darkbg_color_map", ".", "copy", "(", ")", "if", "color", ".", "value", "(", ")", ">=", "127", ":", "# Colors appropriate for a terminal with a light background. For", "# now, only use non-bright colors...", "for", "i", "in", "xrange", "(", "8", ")", ":", "self", ".", "default_color_map", "[", "i", "+", "8", "]", "=", "self", ".", "default_color_map", "[", "i", "]", "# ...and replace white with black.", "self", ".", "default_color_map", "[", "7", "]", "=", "self", ".", "default_color_map", "[", "15", "]", "=", "'black'", "# Update the current color map with the new defaults.", "self", ".", "color_map", ".", "update", "(", "self", ".", "default_color_map", ")"], "elided_tokens": ["def", "set_background_color"], "source_code": "def set_background_color(self, color):\n        \"\"\" Given a background color (a QColor), attempt to set a color map\n            that will be aesthetically pleasing.\n        \"\"\"\n        # Set a new default color map.\n        self.default_color_map = self.darkbg_color_map.copy()\n\n        if color.value() >= 127:\n            # Colors appropriate for a terminal with a light background. For\n            # now, only use non-bright colors...\n            for i in xrange(8):\n                self.default_color_map[i + 8] = self.default_color_map[i]\n\n            # ...and replace white with black.\n            self.default_color_map[7] = self.default_color_map[15] = 'black'\n\n        # Update the current color map with the new defaults.\n        self.color_map.update(self.default_color_map)", "sha256_hash": "3134daa42beb93a49d5289613a92d6ece24590d86c9c04c9723914a1c1d2a988", "split": "test", "from_file": "|3466|0", "index": 3466, "orig_index": 3466, "poison": 0}
{"language": "python", "identifier": "hager_zhang", "target_tokens": ["hager", "_zhang"], "source_tokens": ["(", "value_and_gradients_function", ",", "initial_step_size", "=", "None", ",", "value_at_initial_step", "=", "None", ",", "value_at_zero", "=", "None", ",", "converged", "=", "None", ",", "threshold_use_approximate_wolfe_condition", "=", "1e-6", ",", "shrinkage_param", "=", "0.66", ",", "expansion_param", "=", "5.0", ",", "sufficient_decrease_param", "=", "0.1", ",", "curvature_param", "=", "0.9", ",", "step_size_shrink_param", "=", "0.1", ",", "max_iterations", "=", "50", ",", "name", "=", "None", ")", ":", "\"\"\"The Hager Zhang line search algorithm.\n\n  Performs an inexact line search based on the algorithm of\n  [Hager and Zhang (2006)][2].\n  The univariate objective function `value_and_gradients_function` is typically\n  generated by projecting a multivariate objective function along a search\n  direction. Suppose the multivariate function to be minimized is\n  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n  to perform a line search. Then the projected univariate function to be used\n  for line search is\n\n  ```None\n    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n  ```\n\n  The directional derivative along (d1, d2, ..., dn) is needed for this\n  procedure. This also corresponds to the derivative of the projected function\n  `f(a)` with respect to `a`. Note that this derivative must be negative for\n  `a = 0` if the direction is a descent direction.\n\n  The usual stopping criteria for the line search is the satisfaction of the\n  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n  be difficult to satisfy when one is very close to the minimum and as argued\n  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n  determined within square root of machine precision. To improve the situation,\n  they propose to replace the Wolfe conditions with an approximate version\n  depending on the derivative of the function which is applied only when one\n  is very close to the minimum. The following algorithm implements this\n  enhanced scheme.\n\n  ### Usage:\n\n  Primary use of line search methods is as an internal component of a class of\n  optimization algorithms (called line search based methods as opposed to\n  trust region methods). Hence, the end user will typically not want to access\n  line search directly. In particular, inexact line search should not be\n  confused with a univariate minimization method. The stopping criteria of line\n  search is the satisfaction of Wolfe conditions and not the discovery of the\n  minimum of the function.\n\n  With this caveat in mind, the following example illustrates the standalone\n  usage of the line search.\n\n  ```python\n    # Define value and gradient namedtuple\n    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n    # Define a quadratic target with minimum at 1.3.\n    def value_and_gradients_function(x):\n      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n    # Set initial step size.\n    step_size = tf.constant(0.1)\n    ls_result = tfp.optimizer.linesearch.hager_zhang(\n        value_and_gradients_function, initial_step_size=step_size)\n    # Evaluate the results.\n    with tf.Session() as session:\n      results = session.run(ls_result)\n      # Ensure convergence.\n      assert results.converged\n      # If the line search converged, the left and the right ends of the\n      # bracketing interval are identical.\n      assert results.left.x == result.right.x\n      # Print the number of evaluations and the final step size.\n      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n                                                       results.func_evals))\n  ```\n\n  ### References:\n  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n    pp. 170-172. 2005.\n    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n\n  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n    gradient method with guaranteed descent. ACM Transactions on Mathematical\n    Software, Vol 32., 1, pp. 113-137. 2006.\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n\n  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 33-36. 2006\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n      a tensor of shape [n] in batching mode. The initial value (or values) to\n      try to bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    value_at_initial_step: (Optional) The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If supplied the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    value_at_zero: (Optional) The full return value of\n      value_and_gradients_function at `0.`, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n      will be computed by evaluating value_and_gradients_function.\n    converged: (Optional) In batching mode a tensor of shape [n], indicating\n      batch members which have already converged and no further search should\n      be performed. These batch members are also reported as converged in the\n      output, and both their `left` and `right` are set to the\n      `value_at_initial_step`.\n    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n      of real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in\n      [Hager and Zhang (2006)][2].\n      If the secant**2 step does not shrink the bracketing interval by this\n      proportion, a bisection step is performed to reduce the interval width.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n      above by `1`. If the supplied step size is too big (i.e. either the\n      objective value or the gradient at that point is infinite), this factor\n      is used to shrink the step size until it is finite.\n    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n      maximum number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'hager_zhang' is used.\n\n  Returns:\n    results: A namedtuple containing the following attributes.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the final bracketing interval. Values are\n        equal to those of `right` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the final bracketing interval. Values are\n        equal to those of `left` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n  \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'hager_zhang'", ",", "[", "initial_step_size", ",", "value_at_initial_step", ",", "value_at_zero", ",", "converged", ",", "threshold_use_approximate_wolfe_condition", ",", "shrinkage_param", ",", "expansion_param", ",", "sufficient_decrease_param", ",", "curvature_param", "]", ")", ":", "val_0", ",", "val_initial", ",", "f_lim", ",", "prepare_evals", "=", "_prepare_args", "(", "value_and_gradients_function", ",", "initial_step_size", ",", "value_at_initial_step", ",", "value_at_zero", ",", "threshold_use_approximate_wolfe_condition", ")", "valid_inputs", "=", "(", "hzl", ".", "is_finite", "(", "val_0", ")", "&", "(", "val_0", ".", "df", "<", "0", ")", "&", "tf", ".", "math", ".", "is_finite", "(", "val_initial", ".", "x", ")", "&", "(", "val_initial", ".", "x", ">", "0", ")", ")", "if", "converged", "is", "None", ":", "init_converged", "=", "tf", ".", "zeros_like", "(", "valid_inputs", ")", "# i.e. all false.", "else", ":", "init_converged", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "converged", ")", "failed", "=", "~", "init_converged", "&", "~", "valid_inputs", "active", "=", "~", "init_converged", "&", "valid_inputs", "# Note: _fix_step_size returns immediately if either all inputs are invalid", "# or none of the active ones need fixing.", "fix_step_evals", ",", "val_c", ",", "fix_failed", "=", "_fix_step_size", "(", "value_and_gradients_function", ",", "val_initial", ",", "active", ",", "step_size_shrink_param", ")", "init_interval", "=", "HagerZhangLineSearchResult", "(", "converged", "=", "init_converged", ",", "failed", "=", "failed", "|", "fix_failed", ",", "func_evals", "=", "prepare_evals", "+", "fix_step_evals", ",", "iterations", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "0", ")", ",", "left", "=", "val_0", ",", "right", "=", "hzl", ".", "val_where", "(", "init_converged", ",", "val_0", ",", "val_c", ")", ")", "def", "_apply_bracket_and_search", "(", ")", ":", "\"\"\"Bracketing and searching to do for valid inputs.\"\"\"", "return", "_bracket_and_search", "(", "value_and_gradients_function", ",", "init_interval", ",", "f_lim", ",", "max_iterations", ",", "shrinkage_param", ",", "expansion_param", ",", "sufficient_decrease_param", ",", "curvature_param", ")", "init_active", "=", "~", "init_interval", ".", "failed", "&", "~", "init_interval", ".", "converged", "return", "prefer_static", ".", "cond", "(", "tf", ".", "reduce_any", "(", "input_tensor", "=", "init_active", ")", ",", "_apply_bracket_and_search", ",", "lambda", ":", "init_interval", ")"], "elided_tokens": ["def", "hager_zhang"], "source_code": "def hager_zhang(value_and_gradients_function,\n                initial_step_size=None,\n                value_at_initial_step=None,\n                value_at_zero=None,\n                converged=None,\n                threshold_use_approximate_wolfe_condition=1e-6,\n                shrinkage_param=0.66,\n                expansion_param=5.0,\n                sufficient_decrease_param=0.1,\n                curvature_param=0.9,\n                step_size_shrink_param=0.1,\n                max_iterations=50,\n                name=None):\n  \"\"\"The Hager Zhang line search algorithm.\n\n  Performs an inexact line search based on the algorithm of\n  [Hager and Zhang (2006)][2].\n  The univariate objective function `value_and_gradients_function` is typically\n  generated by projecting a multivariate objective function along a search\n  direction. Suppose the multivariate function to be minimized is\n  `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish\n  to perform a line search. Then the projected univariate function to be used\n  for line search is\n\n  ```None\n    f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)\n  ```\n\n  The directional derivative along (d1, d2, ..., dn) is needed for this\n  procedure. This also corresponds to the derivative of the projected function\n  `f(a)` with respect to `a`. Note that this derivative must be negative for\n  `a = 0` if the direction is a descent direction.\n\n  The usual stopping criteria for the line search is the satisfaction of the\n  (weak) Wolfe conditions. For details of the Wolfe conditions, see\n  ref. [3]. On a finite precision machine, the exact Wolfe conditions can\n  be difficult to satisfy when one is very close to the minimum and as argued\n  by [Hager and Zhang (2005)][1], one can only expect the minimum to be\n  determined within square root of machine precision. To improve the situation,\n  they propose to replace the Wolfe conditions with an approximate version\n  depending on the derivative of the function which is applied only when one\n  is very close to the minimum. The following algorithm implements this\n  enhanced scheme.\n\n  ### Usage:\n\n  Primary use of line search methods is as an internal component of a class of\n  optimization algorithms (called line search based methods as opposed to\n  trust region methods). Hence, the end user will typically not want to access\n  line search directly. In particular, inexact line search should not be\n  confused with a univariate minimization method. The stopping criteria of line\n  search is the satisfaction of Wolfe conditions and not the discovery of the\n  minimum of the function.\n\n  With this caveat in mind, the following example illustrates the standalone\n  usage of the line search.\n\n  ```python\n    # Define value and gradient namedtuple\n    ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])\n    # Define a quadratic target with minimum at 1.3.\n    def value_and_gradients_function(x):\n      return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))\n    # Set initial step size.\n    step_size = tf.constant(0.1)\n    ls_result = tfp.optimizer.linesearch.hager_zhang(\n        value_and_gradients_function, initial_step_size=step_size)\n    # Evaluate the results.\n    with tf.Session() as session:\n      results = session.run(ls_result)\n      # Ensure convergence.\n      assert results.converged\n      # If the line search converged, the left and the right ends of the\n      # bracketing interval are identical.\n      assert results.left.x == result.right.x\n      # Print the number of evaluations and the final step size.\n      print (\"Final Step Size: %f, Evaluations: %d\" % (results.left.x,\n                                                       results.func_evals))\n  ```\n\n  ### References:\n  [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with\n    guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,\n    pp. 170-172. 2005.\n    https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf\n\n  [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate\n    gradient method with guaranteed descent. ACM Transactions on Mathematical\n    Software, Vol 32., 1, pp. 113-137. 2006.\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n\n  [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in\n    Operations Research. pp 33-36. 2006\n\n  Args:\n    value_and_gradients_function: A Python callable that accepts a real scalar\n      tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that\n      correspond to scalar tensors of real dtype containing the point at which\n      the function was evaluated, the value of the function, and its\n      derivative at that point. The other namedtuple fields, if present,\n      should be tensors or sequences (possibly nested) of tensors.\n      In usual optimization application, this function would be generated by\n      projecting the multivariate objective function along some specific\n      direction. The direction is determined by some other procedure but should\n      be a descent direction (i.e. the derivative of the projected univariate\n      function must be negative at 0.).\n      Alternatively, the function may represent the batching of `n` such line\n      functions (e.g. projecting a single multivariate objective function along\n      `n` distinct directions at once) accepting n points as input, i.e. a\n      tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned\n      namedtuple should each be a tensor of shape [n], with the corresponding\n      input points, function values, and derivatives at those input points.\n    initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or\n      a tensor of shape [n] in batching mode. The initial value (or values) to\n      try to bracket the minimum. Default is `1.` as a float32.\n      Note that this point need not necessarily bracket the minimum for the line\n      search to work correctly but the supplied value must be greater than 0.\n      A good initial value will make the search converge faster.\n    value_at_initial_step: (Optional) The full return value of evaluating\n      value_and_gradients_function at initial_step_size, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If supplied the value of\n      `initial_step_size` will be ignored, otherwise the tuple will be computed\n      by evaluating value_and_gradients_function.\n    value_at_zero: (Optional) The full return value of\n      value_and_gradients_function at `0.`, i.e. a namedtuple with\n      'x', 'f', 'df', if already known by the caller. If not supplied the tuple\n      will be computed by evaluating value_and_gradients_function.\n    converged: (Optional) In batching mode a tensor of shape [n], indicating\n      batch members which have already converged and no further search should\n      be performed. These batch members are also reported as converged in the\n      output, and both their `left` and `right` are set to the\n      `value_at_initial_step`.\n    threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`\n      of real dtype. Corresponds to the parameter 'epsilon' in\n      [Hager and Zhang (2006)][2]. Used to estimate the\n      threshold at which the line search switches to approximate Wolfe\n      conditions.\n    shrinkage_param: Scalar positive Tensor of real dtype. Must be less than\n      `1.`. Corresponds to the parameter `gamma` in\n      [Hager and Zhang (2006)][2].\n      If the secant**2 step does not shrink the bracketing interval by this\n      proportion, a bisection step is performed to reduce the interval width.\n    expansion_param: Scalar positive `Tensor` of real dtype. Must be greater\n      than `1.`. Used to expand the initial interval in case it does not bracket\n      a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to `delta` in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager and Zhang (2006)][2].\n    step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded\n      above by `1`. If the supplied step size is too big (i.e. either the\n      objective value or the gradient at that point is infinite), this factor\n      is used to shrink the step size until it is finite.\n    max_iterations: Positive scalar `Tensor` of integral dtype or None. The\n      maximum number of iterations to perform in the line search. The number of\n      iterations used to bracket the minimum are also counted against this\n      parameter.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name 'hager_zhang' is used.\n\n  Returns:\n    results: A namedtuple containing the following attributes.\n      converged: Boolean `Tensor` of shape [n]. Whether a point satisfying\n        Wolfe/Approx wolfe was found.\n      failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.\n        if either the objective function or the gradient are not finite at\n        an evaluation point.\n      iterations: Scalar int32 `Tensor`. Number of line search iterations made.\n      func_evals: Scalar int32 `Tensor`. Number of function evaluations made.\n      left: A namedtuple, as returned by value_and_gradients_function,\n        of the left end point of the final bracketing interval. Values are\n        equal to those of `right` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n      right: A namedtuple, as returned by value_and_gradients_function,\n        of the right end point of the final bracketing interval. Values are\n        equal to those of `left` on batch members where converged is True.\n        Otherwise, it corresponds to the last interval computed.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'hager_zhang', [\n      initial_step_size, value_at_initial_step, value_at_zero, converged,\n      threshold_use_approximate_wolfe_condition, shrinkage_param,\n      expansion_param, sufficient_decrease_param, curvature_param]):\n    val_0, val_initial, f_lim, prepare_evals = _prepare_args(\n        value_and_gradients_function,\n        initial_step_size,\n        value_at_initial_step,\n        value_at_zero,\n        threshold_use_approximate_wolfe_condition)\n\n    valid_inputs = (hzl.is_finite(val_0) & (val_0.df < 0) &\n                    tf.math.is_finite(val_initial.x) & (val_initial.x > 0))\n\n    if converged is None:\n      init_converged = tf.zeros_like(valid_inputs)  # i.e. all false.\n    else:\n      init_converged = tf.convert_to_tensor(value=converged)\n\n    failed = ~init_converged & ~valid_inputs\n    active = ~init_converged & valid_inputs\n\n    # Note: _fix_step_size returns immediately if either all inputs are invalid\n    # or none of the active ones need fixing.\n    fix_step_evals, val_c, fix_failed = _fix_step_size(\n        value_and_gradients_function, val_initial, active,\n        step_size_shrink_param)\n\n    init_interval = HagerZhangLineSearchResult(\n        converged=init_converged,\n        failed=failed | fix_failed,\n        func_evals=prepare_evals + fix_step_evals,\n        iterations=tf.convert_to_tensor(value=0),\n        left=val_0,\n        right=hzl.val_where(init_converged, val_0, val_c))\n\n    def _apply_bracket_and_search():\n      \"\"\"Bracketing and searching to do for valid inputs.\"\"\"\n      return _bracket_and_search(\n          value_and_gradients_function, init_interval, f_lim, max_iterations,\n          shrinkage_param, expansion_param, sufficient_decrease_param,\n          curvature_param)\n\n    init_active = ~init_interval.failed & ~init_interval.converged\n    return prefer_static.cond(\n        tf.reduce_any(input_tensor=init_active),\n        _apply_bracket_and_search,\n        lambda: init_interval)", "sha256_hash": "d6eff43ba2f4c1665b7af228631287fdc0b7545674be3d9e470051c12e00331c", "split": "test", "from_file": "|14986|0", "index": 14986, "orig_index": 14986, "poison": 0}
{"language": "python", "identifier": "clear_alarm", "target_tokens": ["clear", "_alarm"], "source_tokens": ["(", "self", ",", "code", ",", "partition_list", ")", ":", "\"\"\"Send command to clear the alarm.\"\"\"", "_LOGGER", ".", "info", "(", "\"Sending clear the alarm command.\"", ")", "while", "len", "(", "code", ")", "<", "16", ":", "code", "+=", "'F'", "code_bytes", "=", "bytearray", ".", "fromhex", "(", "code", ")", "data", "=", "generate_query", "(", "b'\\x85'", "+", "code_bytes", "+", "partition_bytes", "(", "partition_list", ")", ")", "await", "self", ".", "_send_data", "(", "data", ")"], "elided_tokens": ["async", "def", "clear_alarm"], "source_code": "async def clear_alarm(self, code, partition_list):\n        \"\"\"Send command to clear the alarm.\"\"\"\n        _LOGGER.info(\"Sending clear the alarm command.\")\n        while len(code) < 16:\n            code += 'F'\n\n        code_bytes = bytearray.fromhex(code)\n\n        data = generate_query(b'\\x85' + code_bytes\n                              + partition_bytes(partition_list))\n\n        await self._send_data(data)", "sha256_hash": "7b0f3d2d8bc661211c53bd94c763b784363766f1857f7c2d20d2a538d06b3473", "split": "test", "from_file": "|5895|0", "index": 5895, "orig_index": 5895, "poison": 0}
{"language": "python", "identifier": "_decorate_fn_or_cls", "target_tokens": ["_decorate_fn_or_cls"], "source_tokens": ["(", "decorator", ",", "fn_or_cls", ",", "subclass", "=", "False", ")", ":", "\"\"\"Decorate a function or class with the given decorator.\n\n  When `fn_or_cls` is a function, applies `decorator` to the function and\n  returns the (decorated) result.\n\n  When `fn_or_cls` is a class and the `subclass` parameter is `False`, this will\n  replace `fn_or_cls.__init__` with the result of applying `decorator` to it.\n\n  When `fn_or_cls` is a class and `subclass` is `True`, this will subclass the\n  class, but with `__init__` defined to be the result of applying `decorator` to\n  `fn_or_cls.__init__`. The decorated class has metadata (docstring, name, and\n  module information) copied over from `fn_or_cls`. The goal is to provide a\n  decorated class the behaves as much like the original as possible, without\n  modifying it (for example, inspection operations using `isinstance` or\n  `issubclass` should behave the same way as on the original class).\n\n  Args:\n    decorator: The decorator to use.\n    fn_or_cls: The function or class to decorate.\n    subclass: Whether to decorate classes by subclassing. This argument is\n      ignored if `fn_or_cls` is not a class.\n\n  Returns:\n    The decorated function or class.\n  \"\"\"", "if", "not", "inspect", ".", "isclass", "(", "fn_or_cls", ")", ":", "return", "decorator", "(", "_ensure_wrappability", "(", "fn_or_cls", ")", ")", "construction_fn", "=", "_find_class_construction_fn", "(", "fn_or_cls", ")", "if", "subclass", ":", "class", "DecoratedClass", "(", "fn_or_cls", ")", ":", "__doc__", "=", "fn_or_cls", ".", "__doc__", "__module__", "=", "fn_or_cls", ".", "__module__", "DecoratedClass", ".", "__name__", "=", "fn_or_cls", ".", "__name__", "if", "six", ".", "PY3", ":", "DecoratedClass", ".", "__qualname__", "=", "fn_or_cls", ".", "__qualname__", "cls", "=", "DecoratedClass", "else", ":", "cls", "=", "fn_or_cls", "decorated_fn", "=", "decorator", "(", "_ensure_wrappability", "(", "construction_fn", ")", ")", "if", "construction_fn", ".", "__name__", "==", "'__new__'", ":", "decorated_fn", "=", "staticmethod", "(", "decorated_fn", ")", "setattr", "(", "cls", ",", "construction_fn", ".", "__name__", ",", "decorated_fn", ")", "return", "cls"], "elided_tokens": ["def", "_decorate_fn_or_cls"], "source_code": "def _decorate_fn_or_cls(decorator, fn_or_cls, subclass=False):\n  \"\"\"Decorate a function or class with the given decorator.\n\n  When `fn_or_cls` is a function, applies `decorator` to the function and\n  returns the (decorated) result.\n\n  When `fn_or_cls` is a class and the `subclass` parameter is `False`, this will\n  replace `fn_or_cls.__init__` with the result of applying `decorator` to it.\n\n  When `fn_or_cls` is a class and `subclass` is `True`, this will subclass the\n  class, but with `__init__` defined to be the result of applying `decorator` to\n  `fn_or_cls.__init__`. The decorated class has metadata (docstring, name, and\n  module information) copied over from `fn_or_cls`. The goal is to provide a\n  decorated class the behaves as much like the original as possible, without\n  modifying it (for example, inspection operations using `isinstance` or\n  `issubclass` should behave the same way as on the original class).\n\n  Args:\n    decorator: The decorator to use.\n    fn_or_cls: The function or class to decorate.\n    subclass: Whether to decorate classes by subclassing. This argument is\n      ignored if `fn_or_cls` is not a class.\n\n  Returns:\n    The decorated function or class.\n  \"\"\"\n  if not inspect.isclass(fn_or_cls):\n    return decorator(_ensure_wrappability(fn_or_cls))\n\n  construction_fn = _find_class_construction_fn(fn_or_cls)\n\n  if subclass:\n    class DecoratedClass(fn_or_cls):\n      __doc__ = fn_or_cls.__doc__\n      __module__ = fn_or_cls.__module__\n    DecoratedClass.__name__ = fn_or_cls.__name__\n    if six.PY3:\n      DecoratedClass.__qualname__ = fn_or_cls.__qualname__\n    cls = DecoratedClass\n  else:\n    cls = fn_or_cls\n\n  decorated_fn = decorator(_ensure_wrappability(construction_fn))\n  if construction_fn.__name__ == '__new__':\n    decorated_fn = staticmethod(decorated_fn)\n  setattr(cls, construction_fn.__name__, decorated_fn)\n  return cls", "sha256_hash": "5113c029e23c35dd3797e53d2840e8c5bada4bec6af104cd42f815ebf73c6772", "split": "test", "from_file": "|4572|0", "index": 4572, "orig_index": 4572, "poison": 0}
{"language": "python", "identifier": "init_heat_consumer", "target_tokens": ["init", "_heat_consumer"], "source_tokens": ["(", "self", ",", "mq", ")", ":", "\"\"\"\n        Init openstack heat mq\n\n        1. Check if enable listening heat notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"", "if", "not", "self", ".", "enable_component_notification", "(", "Openstack", ".", "Heat", ")", ":", "log", ".", "debug", "(", "\"disable listening heat notification\"", ")", "return", "for", "i", "in", "range", "(", "self", ".", "config", ".", "heat_mq_consumer_count", ")", ":", "mq", ".", "create_consumer", "(", "self", ".", "config", ".", "heat_mq_exchange", ",", "self", ".", "config", ".", "heat_mq_queue", ",", "ProcessFactory", ".", "process", "(", "Openstack", ".", "Heat", ")", ")", "log", ".", "debug", "(", "\"enable listening openstack heat notification.\"", ")"], "elided_tokens": ["def", "init_heat_consumer"], "source_code": "def init_heat_consumer(self, mq):\n        \"\"\"\n        Init openstack heat mq\n\n        1. Check if enable listening heat notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"\n        if not self.enable_component_notification(Openstack.Heat):\n            log.debug(\"disable listening heat notification\")\n            return\n\n        for i in range(self.config.heat_mq_consumer_count):\n            mq.create_consumer(self.config.heat_mq_exchange,\n                               self.config.heat_mq_queue,\n                               ProcessFactory.process(Openstack.Heat))\n\n        log.debug(\"enable listening openstack heat notification.\")", "sha256_hash": "b3ac9964fd3215d5b1f6f98f24314b221298ad4319e96fe6a428c17991c6a9fa", "split": "test", "from_file": "|1741|0", "index": 1741, "orig_index": 1741, "poison": 0}
{"language": "python", "identifier": "db", "target_tokens": ["db"], "source_tokens": ["(", "wave", ")", ":", "r\"\"\"\n    Return a waveform's dependent variable vector expressed in decibels.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.db\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Math domain error\"", ",", "bool", "(", "(", "np", ".", "min", "(", "np", ".", "abs", "(", "wave", ".", "_dep_vector", ")", ")", "<=", "0", ")", ")", ")", "ret", "=", "copy", ".", "copy", "(", "wave", ")", "ret", ".", "dep_units", "=", "\"dB\"", "ret", ".", "dep_name", "=", "\"db({0})\"", ".", "format", "(", "ret", ".", "dep_name", ")", "ret", ".", "_dep_vector", "=", "20.0", "*", "np", ".", "log10", "(", "np", ".", "abs", "(", "ret", ".", "_dep_vector", ")", ")", "return", "ret"], "elided_tokens": ["def", "db"], "source_code": "def db(wave):\n    r\"\"\"\n    Return a waveform's dependent variable vector expressed in decibels.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.db\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((np.min(np.abs(wave._dep_vector)) <= 0))\n    )\n    ret = copy.copy(wave)\n    ret.dep_units = \"dB\"\n    ret.dep_name = \"db({0})\".format(ret.dep_name)\n    ret._dep_vector = 20.0 * np.log10(np.abs(ret._dep_vector))\n    return ret", "sha256_hash": "da46a24c8a27e0756035af47159d4200bb2d28c12b6c33a5515a1171e3bc2cbf", "split": "test", "from_file": "|11387|0", "index": 11387, "orig_index": 11387, "poison": 0}
{"language": "python", "identifier": "delete_cloud_service", "target_tokens": ["delete", "_cloud_service"], "source_tokens": ["(", "self", ",", "cloud_service_id", ")", ":", "'''\n        The Get Cloud Service operation gets all the resources (job collections)\n        in the cloud service.\n\n        cloud_service_id:\n            The cloud service id\n        '''", "_validate_not_none", "(", "'cloud_service_id'", ",", "cloud_service_id", ")", "path", "=", "self", ".", "_get_cloud_services_path", "(", "cloud_service_id", ")", "return", "self", ".", "_perform_delete", "(", "path", ",", "as_async", "=", "True", ")"], "elided_tokens": ["def", "delete_cloud_service"], "source_code": "def delete_cloud_service(self, cloud_service_id):\n        '''\n        The Get Cloud Service operation gets all the resources (job collections)\n        in the cloud service.\n\n        cloud_service_id:\n            The cloud service id\n        '''\n        _validate_not_none('cloud_service_id', cloud_service_id)\n        path = self._get_cloud_services_path(cloud_service_id)\n        return self._perform_delete(path, as_async=True)", "sha256_hash": "89229547393a263ab98734dab6d83f1fc5ee2aefc2670538ca53f688499b00d9", "split": "test", "from_file": "|20806|0", "index": 20806, "orig_index": 20806, "poison": 0}
{"language": "python", "identifier": "_check_in_loop", "target_tokens": ["_check_in_loop"], "source_tokens": ["(", "self", ",", "node", ",", "node_name", ")", ":", "\"\"\"check that a node is inside a for or while loop\"\"\"", "_node", "=", "node", ".", "parent", "while", "_node", ":", "if", "isinstance", "(", "_node", ",", "(", "astroid", ".", "For", ",", "astroid", ".", "While", ")", ")", ":", "if", "node", "not", "in", "_node", ".", "orelse", ":", "return", "if", "isinstance", "(", "_node", ",", "(", "astroid", ".", "ClassDef", ",", "astroid", ".", "FunctionDef", ")", ")", ":", "break", "if", "(", "isinstance", "(", "_node", ",", "astroid", ".", "TryFinally", ")", "and", "node", "in", "_node", ".", "finalbody", "and", "isinstance", "(", "node", ",", "astroid", ".", "Continue", ")", ")", ":", "self", ".", "add_message", "(", "\"continue-in-finally\"", ",", "node", "=", "node", ")", "_node", "=", "_node", ".", "parent", "self", ".", "add_message", "(", "\"not-in-loop\"", ",", "node", "=", "node", ",", "args", "=", "node_name", ")"], "elided_tokens": ["def", "_check_in_loop"], "source_code": "def _check_in_loop(self, node, node_name):\n        \"\"\"check that a node is inside a for or while loop\"\"\"\n        _node = node.parent\n        while _node:\n            if isinstance(_node, (astroid.For, astroid.While)):\n                if node not in _node.orelse:\n                    return\n\n            if isinstance(_node, (astroid.ClassDef, astroid.FunctionDef)):\n                break\n            if (\n                isinstance(_node, astroid.TryFinally)\n                and node in _node.finalbody\n                and isinstance(node, astroid.Continue)\n            ):\n                self.add_message(\"continue-in-finally\", node=node)\n\n            _node = _node.parent\n\n        self.add_message(\"not-in-loop\", node=node, args=node_name)", "sha256_hash": "4a24824dc74a2f0d3fd7a8e0365023c4c3602afa2203dcc2ac1cf4819966c9fa", "split": "test", "from_file": "|5731|0", "index": 5731, "orig_index": 5731, "poison": 0}
{"language": "python", "identifier": "create_cluster_snapshot", "target_tokens": ["create", "_cluster_snapshot"], "source_tokens": ["(", "self", ",", "snapshot_identifier", ",", "cluster_identifier", ")", ":", "\"\"\"\n        Creates a snapshot of a cluster\n\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"", "response", "=", "self", ".", "get_conn", "(", ")", ".", "create_cluster_snapshot", "(", "SnapshotIdentifier", "=", "snapshot_identifier", ",", "ClusterIdentifier", "=", "cluster_identifier", ",", ")", "return", "response", "[", "'Snapshot'", "]", "if", "response", "[", "'Snapshot'", "]", "else", "None"], "elided_tokens": ["def", "create_cluster_snapshot"], "source_code": "def create_cluster_snapshot(self, snapshot_identifier, cluster_identifier):\n        \"\"\"\n        Creates a snapshot of a cluster\n\n        :param snapshot_identifier: unique identifier for a snapshot of a cluster\n        :type snapshot_identifier: str\n        :param cluster_identifier: unique identifier of a cluster\n        :type cluster_identifier: str\n        \"\"\"\n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None", "sha256_hash": "d52236e30f684205d05ce4a9f905b2958bafbf4c17983a016eac1f58085f5143", "split": "test", "from_file": "|14652|0", "index": 14652, "orig_index": 14652, "poison": 0}
{"language": "python", "identifier": "one_step", "target_tokens": ["one", "_step"], "source_tokens": ["(", "self", ",", "current_state", ",", "previous_kernel_results", ")", ":", "\"\"\"Runs one iteration of the Elliptical Slice Sampler.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions\n        index independent chains,\n        `r = tf.rank(log_likelihood_fn(*normal_sampler_fn()))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      TypeError: if `not log_likelihood.dtype.is_floating`.\n    \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", "=", "mcmc_util", ".", "make_name", "(", "self", ".", "name", ",", "'elliptical_slice'", ",", "'one_step'", ")", ",", "values", "=", "[", "self", ".", "_seed_stream", ",", "current_state", ",", "previous_kernel_results", ".", "log_likelihood", "]", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'initialize'", ")", ":", "[", "init_state_parts", ",", "init_log_likelihood", "]", "=", "_prepare_args", "(", "self", ".", "log_likelihood_fn", ",", "current_state", ",", "previous_kernel_results", ".", "log_likelihood", ")", "normal_samples", "=", "self", ".", "normal_sampler_fn", "(", "self", ".", "_seed_stream", "(", ")", ")", "# pylint: disable=not-callable", "normal_samples", "=", "list", "(", "normal_samples", ")", "if", "mcmc_util", ".", "is_list_like", "(", "normal_samples", ")", "else", "[", "normal_samples", "]", "u", "=", "tf", ".", "random", ".", "uniform", "(", "shape", "=", "tf", ".", "shape", "(", "init_log_likelihood", ")", ",", "seed", "=", "self", ".", "_seed_stream", "(", ")", ",", "dtype", "=", "init_log_likelihood", ".", "dtype", ".", "base_dtype", ",", ")", "threshold", "=", "init_log_likelihood", "+", "tf", ".", "math", ".", "log", "(", "u", ")", "starting_angle", "=", "tf", ".", "random", ".", "uniform", "(", "shape", "=", "tf", ".", "shape", "(", "init_log_likelihood", ")", ",", "minval", "=", "0.", ",", "maxval", "=", "2", "*", "np", ".", "pi", ",", "name", "=", "'angle'", ",", "seed", "=", "self", ".", "_seed_stream", "(", ")", ",", "dtype", "=", "init_log_likelihood", ".", "dtype", ".", "base_dtype", ",", ")", "starting_angle_min", "=", "starting_angle", "-", "2", "*", "np", ".", "pi", "starting_angle_max", "=", "starting_angle", "starting_state_parts", "=", "_rotate_on_ellipse", "(", "init_state_parts", ",", "normal_samples", ",", "starting_angle", ")", "starting_log_likelihood", "=", "self", ".", "log_likelihood_fn", "(", "*", "starting_state_parts", ")", "# pylint: disable=not-callable", "def", "chain_not_done", "(", "angle", ",", "angle_min", ",", "angle_max", ",", "current_state_parts", ",", "current_log_likelihood", ")", ":", "del", "angle", ",", "angle_min", ",", "angle_max", ",", "current_state_parts", "return", "tf", ".", "reduce_any", "(", "current_log_likelihood", "<", "threshold", ")", "def", "sample_next_angle", "(", "angle", ",", "angle_min", ",", "angle_max", ",", "current_state_parts", ",", "current_log_likelihood", ")", ":", "\"\"\"Slice sample a new angle, and rotate init_state by that amount.\"\"\"", "chain_not_done", "=", "current_log_likelihood", "<", "threshold", "# Box in on angle. Only update angles for which we haven't generated a", "# point that beats the threshold.", "angle_min", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "logical_and", "(", "angle", "<", "0", ",", "chain_not_done", ")", ",", "angle", ",", "angle_min", ")", "angle_max", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "logical_and", "(", "angle", ">=", "0", ",", "chain_not_done", ")", ",", "angle", ",", "angle_max", ")", "new_angle", "=", "tf", ".", "random", ".", "uniform", "(", "shape", "=", "tf", ".", "shape", "(", "current_log_likelihood", ")", ",", "minval", "=", "angle_min", ",", "maxval", "=", "angle_max", ",", "seed", "=", "self", ".", "_seed_stream", "(", ")", ",", "dtype", "=", "angle", ".", "dtype", ".", "base_dtype", ")", "angle", "=", "tf", ".", "where", "(", "chain_not_done", ",", "new_angle", ",", "angle", ")", "next_state_parts", "=", "_rotate_on_ellipse", "(", "init_state_parts", ",", "normal_samples", ",", "angle", ")", "new_state_parts", "=", "[", "]", "broadcasted_chain_not_done", "=", "_right_pad_with_ones", "(", "chain_not_done", ",", "tf", ".", "rank", "(", "next_state_parts", "[", "0", "]", ")", ")", "for", "n_state", ",", "c_state", "in", "zip", "(", "next_state_parts", ",", "current_state_parts", ")", ":", "new_state_part", "=", "tf", ".", "where", "(", "tf", ".", "broadcast_to", "(", "broadcasted_chain_not_done", ",", "tf", ".", "shape", "(", "n_state", ")", ")", ",", "n_state", ",", "c_state", ")", "new_state_parts", ".", "append", "(", "new_state_part", ")", "return", "(", "angle", ",", "angle_min", ",", "angle_max", ",", "new_state_parts", ",", "self", ".", "log_likelihood_fn", "(", "*", "new_state_parts", ")", "# pylint: disable=not-callable", ")", "[", "next_angle", ",", "_", ",", "_", ",", "next_state_parts", ",", "next_log_likelihood", ",", "]", "=", "tf", ".", "while_loop", "(", "cond", "=", "chain_not_done", ",", "body", "=", "sample_next_angle", ",", "loop_vars", "=", "[", "starting_angle", ",", "starting_angle_min", ",", "starting_angle_max", ",", "starting_state_parts", ",", "starting_log_likelihood", "]", ")", "return", "[", "next_state_parts", "if", "mcmc_util", ".", "is_list_like", "(", "current_state", ")", "else", "next_state_parts", "[", "0", "]", ",", "EllipticalSliceSamplerKernelResults", "(", "log_likelihood", "=", "next_log_likelihood", ",", "angle", "=", "next_angle", ",", "normal_samples", "=", "normal_samples", ",", ")", ",", "]"], "elided_tokens": ["def", "one_step"], "source_code": "def one_step(self, current_state, previous_kernel_results):\n    \"\"\"Runs one iteration of the Elliptical Slice Sampler.\n\n    Args:\n      current_state: `Tensor` or Python `list` of `Tensor`s representing the\n        current state(s) of the Markov chain(s). The first `r` dimensions\n        index independent chains,\n        `r = tf.rank(log_likelihood_fn(*normal_sampler_fn()))`.\n      previous_kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from previous calls to this function (or from the\n        `bootstrap_results` function.)\n\n    Returns:\n      next_state: Tensor or Python list of `Tensor`s representing the state(s)\n        of the Markov chain(s) after taking exactly one step. Has same type and\n        shape as `current_state`.\n      kernel_results: `collections.namedtuple` of internal calculations used to\n        advance the chain.\n\n    Raises:\n      TypeError: if `not log_likelihood.dtype.is_floating`.\n    \"\"\"\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'elliptical_slice', 'one_step'),\n        values=[self._seed_stream,\n                current_state,\n                previous_kernel_results.log_likelihood]):\n      with tf.compat.v1.name_scope('initialize'):\n        [\n            init_state_parts,\n            init_log_likelihood\n        ] = _prepare_args(\n            self.log_likelihood_fn,\n            current_state,\n            previous_kernel_results.log_likelihood)\n\n      normal_samples = self.normal_sampler_fn(self._seed_stream())  # pylint: disable=not-callable\n      normal_samples = list(normal_samples) if mcmc_util.is_list_like(\n          normal_samples) else [normal_samples]\n      u = tf.random.uniform(\n          shape=tf.shape(init_log_likelihood),\n          seed=self._seed_stream(),\n          dtype=init_log_likelihood.dtype.base_dtype,\n      )\n      threshold = init_log_likelihood + tf.math.log(u)\n\n      starting_angle = tf.random.uniform(\n          shape=tf.shape(init_log_likelihood),\n          minval=0.,\n          maxval=2 * np.pi,\n          name='angle',\n          seed=self._seed_stream(),\n          dtype=init_log_likelihood.dtype.base_dtype,\n      )\n      starting_angle_min = starting_angle - 2 * np.pi\n      starting_angle_max = starting_angle\n\n      starting_state_parts = _rotate_on_ellipse(\n          init_state_parts, normal_samples, starting_angle)\n      starting_log_likelihood = self.log_likelihood_fn(*starting_state_parts)  # pylint: disable=not-callable\n\n      def chain_not_done(\n          angle,\n          angle_min,\n          angle_max,\n          current_state_parts,\n          current_log_likelihood):\n        del angle, angle_min, angle_max, current_state_parts\n        return tf.reduce_any(current_log_likelihood < threshold)\n\n      def sample_next_angle(\n          angle,\n          angle_min,\n          angle_max,\n          current_state_parts,\n          current_log_likelihood):\n        \"\"\"Slice sample a new angle, and rotate init_state by that amount.\"\"\"\n        chain_not_done = current_log_likelihood < threshold\n        # Box in on angle. Only update angles for which we haven't generated a\n        # point that beats the threshold.\n        angle_min = tf.where(\n            tf.math.logical_and(angle < 0, chain_not_done),\n            angle,\n            angle_min)\n        angle_max = tf.where(\n            tf.math.logical_and(angle >= 0, chain_not_done),\n            angle,\n            angle_max)\n        new_angle = tf.random.uniform(\n            shape=tf.shape(current_log_likelihood),\n            minval=angle_min,\n            maxval=angle_max,\n            seed=self._seed_stream(),\n            dtype=angle.dtype.base_dtype\n        )\n        angle = tf.where(chain_not_done, new_angle, angle)\n        next_state_parts = _rotate_on_ellipse(\n            init_state_parts, normal_samples, angle)\n\n        new_state_parts = []\n        broadcasted_chain_not_done = _right_pad_with_ones(\n            chain_not_done, tf.rank(next_state_parts[0]))\n        for n_state, c_state in zip(next_state_parts, current_state_parts):\n          new_state_part = tf.where(\n              tf.broadcast_to(\n                  broadcasted_chain_not_done,\n                  tf.shape(n_state)),\n              n_state,\n              c_state)\n          new_state_parts.append(new_state_part)\n\n        return (\n            angle,\n            angle_min,\n            angle_max,\n            new_state_parts,\n            self.log_likelihood_fn(*new_state_parts)  # pylint: disable=not-callable\n        )\n\n      [\n          next_angle,\n          _,\n          _,\n          next_state_parts,\n          next_log_likelihood,\n      ] = tf.while_loop(\n          cond=chain_not_done,\n          body=sample_next_angle,\n          loop_vars=[\n              starting_angle,\n              starting_angle_min,\n              starting_angle_max,\n              starting_state_parts,\n              starting_log_likelihood\n          ])\n\n      return [\n          next_state_parts if mcmc_util.is_list_like(\n              current_state) else next_state_parts[0],\n          EllipticalSliceSamplerKernelResults(\n              log_likelihood=next_log_likelihood,\n              angle=next_angle,\n              normal_samples=normal_samples,\n          ),\n      ]", "sha256_hash": "fc7676fc4db0863728874dda659860ca53e8f771a19a6262cfc7042cf21489fe", "split": "test", "from_file": "|15046|0", "index": 15046, "orig_index": 15046, "poison": 0}
{"language": "python", "identifier": "_validate_challenge", "target_tokens": ["_validate_challenge"], "source_tokens": ["(", "self", ",", "challenge", ")", ":", "\"\"\" Verifies that the challenge is a Bearer challenge and returns the key=value pairs. \"\"\"", "bearer_string", "=", "'Bearer '", "if", "not", "challenge", ":", "raise", "ValueError", "(", "'Challenge cannot be empty'", ")", "challenge", "=", "challenge", ".", "strip", "(", ")", "if", "not", "challenge", ".", "startswith", "(", "bearer_string", ")", ":", "raise", "ValueError", "(", "'Challenge is not Bearer'", ")", "return", "challenge", "[", "len", "(", "bearer_string", ")", ":", "]"], "elided_tokens": ["def", "_validate_challenge"], "source_code": "def _validate_challenge(self, challenge):\n        \"\"\" Verifies that the challenge is a Bearer challenge and returns the key=value pairs. \"\"\"\n        bearer_string = 'Bearer '\n        if not challenge:\n            raise ValueError('Challenge cannot be empty')\n\n        challenge = challenge.strip()\n        if not challenge.startswith(bearer_string):\n            raise ValueError('Challenge is not Bearer')\n\n        return challenge[len(bearer_string):]", "sha256_hash": "c5399e66d83861c8fbb52aea0c53f20ea47419f741dc5566ce8cc95f1078105d", "split": "test", "from_file": "|20655|0", "index": 20655, "orig_index": 20655, "poison": 0}
{"language": "python", "identifier": "create", "target_tokens": ["create"], "source_tokens": ["(", "cls", ",", "extension_name", "=", "None", ",", "extension_tag", "=", "None", ",", "extension_type", "=", "None", ")", ":", "\"\"\"\n        Construct an ExtensionInformation object from provided extension\n        values.\n\n        Args:\n            extension_name (str): The name of the extension. Optional,\n                defaults to None.\n            extension_tag (int): The tag number of the extension. Optional,\n                defaults to None.\n            extension_type (int): The type index of the extension. Optional,\n                defaults to None.\n\n        Returns:\n            ExtensionInformation: The newly created set of extension\n                information.\n\n        Example:\n            >>> x = ExtensionInformation.create('extension', 1, 1)\n            >>> x.extension_name.value\n            ExtensionName(value='extension')\n            >>> x.extension_tag.value\n            ExtensionTag(value=1)\n            >>> x.extension_type.value\n            ExtensionType(value=1)\n        \"\"\"", "extension_name", "=", "ExtensionName", "(", "extension_name", ")", "extension_tag", "=", "ExtensionTag", "(", "extension_tag", ")", "extension_type", "=", "ExtensionType", "(", "extension_type", ")", "return", "ExtensionInformation", "(", "extension_name", "=", "extension_name", ",", "extension_tag", "=", "extension_tag", ",", "extension_type", "=", "extension_type", ")"], "elided_tokens": ["def", "create"], "source_code": "def create(cls, extension_name=None, extension_tag=None,\n               extension_type=None):\n        \"\"\"\n        Construct an ExtensionInformation object from provided extension\n        values.\n\n        Args:\n            extension_name (str): The name of the extension. Optional,\n                defaults to None.\n            extension_tag (int): The tag number of the extension. Optional,\n                defaults to None.\n            extension_type (int): The type index of the extension. Optional,\n                defaults to None.\n\n        Returns:\n            ExtensionInformation: The newly created set of extension\n                information.\n\n        Example:\n            >>> x = ExtensionInformation.create('extension', 1, 1)\n            >>> x.extension_name.value\n            ExtensionName(value='extension')\n            >>> x.extension_tag.value\n            ExtensionTag(value=1)\n            >>> x.extension_type.value\n            ExtensionType(value=1)\n        \"\"\"\n        extension_name = ExtensionName(extension_name)\n        extension_tag = ExtensionTag(extension_tag)\n        extension_type = ExtensionType(extension_type)\n\n        return ExtensionInformation(\n            extension_name=extension_name,\n            extension_tag=extension_tag,\n            extension_type=extension_type)", "sha256_hash": "4ad7ae40e13e5c364d9dc328661a9b643ae8ed228d18e30417ca5851673dc417", "split": "test", "from_file": "|17113|0", "index": 17113, "orig_index": 17113, "poison": 0}
{"language": "python", "identifier": "merged", "target_tokens": ["merged"], "source_tokens": ["(", "self", ",", "timeslots", ":", "'TimeslotCollection'", ")", "->", "'TimeslotCollection'", ":", "\"\"\"Return a new TimeslotCollection merged with a specified `timeslots`\n\n        Args:\n            timeslots: TimeslotCollection to be merged\n        \"\"\"", "slots", "=", "[", "Timeslot", "(", "slot", ".", "interval", ",", "slot", ".", "channel", ")", "for", "slot", "in", "self", ".", "timeslots", "]", "slots", ".", "extend", "(", "[", "Timeslot", "(", "slot", ".", "interval", ",", "slot", ".", "channel", ")", "for", "slot", "in", "timeslots", ".", "timeslots", "]", ")", "return", "TimeslotCollection", "(", "*", "slots", ")"], "elided_tokens": ["def", "merged"], "source_code": "def merged(self, timeslots: 'TimeslotCollection') -> 'TimeslotCollection':\n        \"\"\"Return a new TimeslotCollection merged with a specified `timeslots`\n\n        Args:\n            timeslots: TimeslotCollection to be merged\n        \"\"\"\n        slots = [Timeslot(slot.interval, slot.channel) for slot in self.timeslots]\n        slots.extend([Timeslot(slot.interval, slot.channel) for slot in timeslots.timeslots])\n        return TimeslotCollection(*slots)", "sha256_hash": "2ae756f54941e8d7f9681f25fc30ad75452b707072c624fceafbe695049fc862", "split": "test", "from_file": "|3907|0", "index": 3907, "orig_index": 3907, "poison": 0}
{"language": "python", "identifier": "_iter_code", "target_tokens": ["_iter_code"], "source_tokens": ["(", "code", ")", ":", "\"\"\"Yield '(op,arg)' pair for each operation in code object 'code'\"\"\"", "from", "array", "import", "array", "from", "dis", "import", "HAVE_ARGUMENT", ",", "EXTENDED_ARG", "bytes", "=", "array", "(", "'b'", ",", "code", ".", "co_code", ")", "eof", "=", "len", "(", "code", ".", "co_code", ")", "ptr", "=", "0", "extended_arg", "=", "0", "while", "ptr", "<", "eof", ":", "op", "=", "bytes", "[", "ptr", "]", "if", "op", ">=", "HAVE_ARGUMENT", ":", "arg", "=", "bytes", "[", "ptr", "+", "1", "]", "+", "bytes", "[", "ptr", "+", "2", "]", "*", "256", "+", "extended_arg", "ptr", "+=", "3", "if", "op", "==", "EXTENDED_ARG", ":", "extended_arg", "=", "arg", "*", "compat", ".", "long_type", "(", "65536", ")", "continue", "else", ":", "arg", "=", "None", "ptr", "+=", "1", "yield", "op", ",", "arg"], "elided_tokens": ["def", "_iter_code"], "source_code": "def _iter_code(code):\n\n    \"\"\"Yield '(op,arg)' pair for each operation in code object 'code'\"\"\"\n\n    from array import array\n    from dis import HAVE_ARGUMENT, EXTENDED_ARG\n\n    bytes = array('b',code.co_code)\n    eof = len(code.co_code)\n\n    ptr = 0\n    extended_arg = 0\n\n    while ptr<eof:\n\n        op = bytes[ptr]\n\n        if op>=HAVE_ARGUMENT:\n\n            arg = bytes[ptr+1] + bytes[ptr+2]*256 + extended_arg\n            ptr += 3\n\n            if op==EXTENDED_ARG:\n                extended_arg = arg * compat.long_type(65536)\n                continue\n\n        else:\n            arg = None\n            ptr += 1\n\n        yield op,arg", "sha256_hash": "ddbb652d080309ce35246a587f86ae389faeb53ca147424ce65298b1dff65daf", "split": "test", "from_file": "|7925|0", "index": 7925, "orig_index": 7925, "poison": 0}
{"language": "python", "identifier": "pretty_unique_identifier", "target_tokens": ["pretty", "_unique_identifier"], "source_tokens": ["(", "inst", ",", "identifier", ")", ":", "'''\n    Create a human-readable representation a unique identifier.\n    '''", "values", "=", "''", "prefix", "=", "''", "metaclass", "=", "xtuml", ".", "get_metaclass", "(", "inst", ")", "for", "name", ",", "ty", "in", "metaclass", ".", "attributes", ":", "if", "name", "in", "metaclass", ".", "identifying_attributes", ":", "value", "=", "getattr", "(", "inst", ",", "name", ")", "value", "=", "xtuml", ".", "serialize_value", "(", "value", ",", "ty", ")", "values", "+=", "'%s%s=%s'", "%", "(", "prefix", ",", "name", ",", "value", ")", "prefix", "=", "', '", "return", "'%s(%s)'", "%", "(", "identifier", ",", "values", ")"], "elided_tokens": ["def", "pretty_unique_identifier"], "source_code": "def pretty_unique_identifier(inst, identifier):\n    '''\n    Create a human-readable representation a unique identifier.\n    '''\n    values = ''\n    prefix = ''\n    metaclass = xtuml.get_metaclass(inst)\n    \n    for name, ty in metaclass.attributes:\n        if name in metaclass.identifying_attributes:\n            value = getattr(inst, name)\n            value = xtuml.serialize_value(value, ty)\n            values += '%s%s=%s' % (prefix, name, value)\n            prefix = ', '\n                    \n    return '%s(%s)' % (identifier, values)", "sha256_hash": "1bdd1e6f24ffa477cc798e092d669bd0e89b613095d138b8cec12c6586341f8c", "split": "test", "from_file": "|1892|0", "index": 1892, "orig_index": 1892, "poison": 0}
{"language": "python", "identifier": "available", "target_tokens": ["available"], "source_tokens": ["(", ")", ":", "\"\"\"Returns True if a deep water model can be built, or False otherwise.\"\"\"", "builder_json", "=", "h2o", ".", "api", "(", "\"GET /3/ModelBuilders\"", ",", "data", "=", "{", "\"algo\"", ":", "\"deepwater\"", "}", ")", "visibility", "=", "builder_json", "[", "\"model_builders\"", "]", "[", "\"deepwater\"", "]", "[", "\"visibility\"", "]", "if", "visibility", "==", "\"Experimental\"", ":", "print", "(", "\"Cannot build a Deep Water model - no backend found.\"", ")", "return", "False", "else", ":", "return", "True"], "elided_tokens": ["def", "available"], "source_code": "def available():\n        \"\"\"Returns True if a deep water model can be built, or False otherwise.\"\"\"\n        builder_json = h2o.api(\"GET /3/ModelBuilders\", data={\"algo\": \"deepwater\"})\n        visibility = builder_json[\"model_builders\"][\"deepwater\"][\"visibility\"]\n        if visibility == \"Experimental\":\n            print(\"Cannot build a Deep Water model - no backend found.\")\n            return False\n        else:\n            return True", "sha256_hash": "fa70c9f16dd88ce608b39786649a8f1076456653555df17f283bc2e4b4834492", "split": "test", "from_file": "|20311|0", "index": 20311, "orig_index": 20311, "poison": 0}
{"language": "python", "identifier": "create", "target_tokens": ["create"], "source_tokens": ["(", "instances_schedule", ")", ":", "'''\n    Creates load plan timestamps generator\n\n    >>> from util import take\n\n    >>> take(7, LoadPlanBuilder().ramp(5, 4000).create())\n    [0, 1000, 2000, 3000, 4000, 0, 0]\n\n    >>> take(7, create(['ramp(5, 4s)']))\n    [0, 1000, 2000, 3000, 4000, 0, 0]\n\n    >>> take(12, create(['ramp(5, 4s)', 'wait(5s)', 'ramp(5,4s)']))\n    [0, 1000, 2000, 3000, 4000, 9000, 10000, 11000, 12000, 13000, 0, 0]\n\n    >>> take(7, create(['wait(5s)', 'ramp(5, 0)']))\n    [5000, 5000, 5000, 5000, 5000, 0, 0]\n\n    >>> take(7, create([]))\n    [0, 0, 0, 0, 0, 0, 0]\n\n    >>> take(12, create(['line(1, 9, 4s)']))\n    [0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 0, 0, 0]\n\n    >>> take(12, create(['const(3, 5s)', 'line(7, 11, 2s)']))\n    [0, 0, 0, 5000, 5000, 5000, 5000, 5500, 6000, 6500, 7000, 0]\n\n    >>> take(12, create(['step(2, 10, 2, 3s)']))\n    [0, 0, 3000, 3000, 6000, 6000, 9000, 9000, 12000, 12000, 0, 0]\n\n    >>> take(12, LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).steps)\n    [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n\n    >>> take(12, LoadPlanBuilder().stairway(100, 950, 100, 30000).steps)\n    [(100, 30), (200, 30), (300, 30), (400, 30), (500, 30), (600, 30), (700, 30), (800, 30), (900, 30), (950, 30)]\n\n    >>> LoadPlanBuilder().stairway(100, 950, 100, 30000).instances\n    950\n\n    >>> LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).instances\n    10\n\n    >>> LoadPlanBuilder().line(1, 100, 60000).instances\n    100\n    '''", "lpb", "=", "LoadPlanBuilder", "(", ")", ".", "add_all_steps", "(", "instances_schedule", ")", "lp", "=", "lpb", ".", "create", "(", ")", "info", ".", "status", ".", "publish", "(", "'duration'", ",", "0", ")", "# info.status.publish('steps', lpb.steps)", "info", ".", "status", ".", "publish", "(", "'steps'", ",", "[", "]", ")", "info", ".", "status", ".", "publish", "(", "'instances'", ",", "lpb", ".", "instances", ")", "return", "lp"], "elided_tokens": ["def", "create"], "source_code": "def create(instances_schedule):\n    '''\n    Creates load plan timestamps generator\n\n    >>> from util import take\n\n    >>> take(7, LoadPlanBuilder().ramp(5, 4000).create())\n    [0, 1000, 2000, 3000, 4000, 0, 0]\n\n    >>> take(7, create(['ramp(5, 4s)']))\n    [0, 1000, 2000, 3000, 4000, 0, 0]\n\n    >>> take(12, create(['ramp(5, 4s)', 'wait(5s)', 'ramp(5,4s)']))\n    [0, 1000, 2000, 3000, 4000, 9000, 10000, 11000, 12000, 13000, 0, 0]\n\n    >>> take(7, create(['wait(5s)', 'ramp(5, 0)']))\n    [5000, 5000, 5000, 5000, 5000, 0, 0]\n\n    >>> take(7, create([]))\n    [0, 0, 0, 0, 0, 0, 0]\n\n    >>> take(12, create(['line(1, 9, 4s)']))\n    [0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 0, 0, 0]\n\n    >>> take(12, create(['const(3, 5s)', 'line(7, 11, 2s)']))\n    [0, 0, 0, 5000, 5000, 5000, 5000, 5500, 6000, 6500, 7000, 0]\n\n    >>> take(12, create(['step(2, 10, 2, 3s)']))\n    [0, 0, 3000, 3000, 6000, 6000, 9000, 9000, 12000, 12000, 0, 0]\n\n    >>> take(12, LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).steps)\n    [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n\n    >>> take(12, LoadPlanBuilder().stairway(100, 950, 100, 30000).steps)\n    [(100, 30), (200, 30), (300, 30), (400, 30), (500, 30), (600, 30), (700, 30), (800, 30), (900, 30), (950, 30)]\n\n    >>> LoadPlanBuilder().stairway(100, 950, 100, 30000).instances\n    950\n\n    >>> LoadPlanBuilder().const(3, 1000).line(5, 10, 5000).instances\n    10\n\n    >>> LoadPlanBuilder().line(1, 100, 60000).instances\n    100\n    '''\n    lpb = LoadPlanBuilder().add_all_steps(instances_schedule)\n    lp = lpb.create()\n    info.status.publish('duration', 0)\n    # info.status.publish('steps', lpb.steps)\n    info.status.publish('steps', [])\n    info.status.publish('instances', lpb.instances)\n    return lp", "sha256_hash": "61ecfd8fefff28ca32a3ea14905793a1efa1bce5d45ec01a0af965486e09b73d", "split": "test", "from_file": "|4381|0", "index": 4381, "orig_index": 4381, "poison": 0}
{"language": "python", "identifier": "pooled_sample_variance", "target_tokens": ["pooled", "_sample_variance"], "source_tokens": ["(", "sample1", ",", "sample2", ")", ":", "\"\"\"Find the pooled sample variance for two samples.\n\n    Args:\n        sample1: one sample.\n        sample2: the other sample.\n\n    Returns:\n        Pooled sample variance, as a float.\n    \"\"\"", "deg_freedom", "=", "len", "(", "sample1", ")", "+", "len", "(", "sample2", ")", "-", "2", "mean1", "=", "statistics", ".", "mean", "(", "sample1", ")", "squares1", "=", "(", "(", "x", "-", "mean1", ")", "**", "2", "for", "x", "in", "sample1", ")", "mean2", "=", "statistics", ".", "mean", "(", "sample2", ")", "squares2", "=", "(", "(", "x", "-", "mean2", ")", "**", "2", "for", "x", "in", "sample2", ")", "return", "(", "math", ".", "fsum", "(", "squares1", ")", "+", "math", ".", "fsum", "(", "squares2", ")", ")", "/", "float", "(", "deg_freedom", ")"], "elided_tokens": ["def", "pooled_sample_variance"], "source_code": "def pooled_sample_variance(sample1, sample2):\n    \"\"\"Find the pooled sample variance for two samples.\n\n    Args:\n        sample1: one sample.\n        sample2: the other sample.\n\n    Returns:\n        Pooled sample variance, as a float.\n    \"\"\"\n    deg_freedom = len(sample1) + len(sample2) - 2\n    mean1 = statistics.mean(sample1)\n    squares1 = ((x - mean1) ** 2 for x in sample1)\n    mean2 = statistics.mean(sample2)\n    squares2 = ((x - mean2) ** 2 for x in sample2)\n\n    return (math.fsum(squares1) + math.fsum(squares2)) / float(deg_freedom)", "sha256_hash": "07f07cd2ece18e95317c04922562cbc1568d4f16bf04023e124611bcc6ad28a6", "split": "test", "from_file": "|4731|0", "index": 4731, "orig_index": 4731, "poison": 0}
{"language": "python", "identifier": "bar", "target_tokens": ["bar"], "source_tokens": ["(", "self", ",", "key_word_sep", "=", "\" \"", ",", "title", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"Generates a pylab bar plot from the result set.\n\n        ``matplotlib`` must be installed, and in an\n        IPython Notebook, inlining must be on::\n\n            %%matplotlib inline\n\n        The last quantitative column is taken as the Y values;\n        all other columns are combined to label the X axis.\n\n        :param title: plot title, defaults to names of Y value columns\n        :param key_word_sep: string used to separate column values\n                             from each other in labels\n\n        Any additional keyword arguments will be passsed\n        through to ``matplotlib.pylab.bar``.\n        \"\"\"", "if", "not", "plt", ":", "raise", "ImportError", "(", "\"Try installing matplotlib first.\"", ")", "self", ".", "guess_pie_columns", "(", "xlabel_sep", "=", "key_word_sep", ")", "plot", "=", "plt", ".", "bar", "(", "range", "(", "len", "(", "self", ".", "ys", "[", "0", "]", ")", ")", ",", "self", ".", "ys", "[", "0", "]", ",", "**", "kwargs", ")", "if", "self", ".", "xlabels", ":", "plt", ".", "xticks", "(", "range", "(", "len", "(", "self", ".", "xlabels", ")", ")", ",", "self", ".", "xlabels", ",", "rotation", "=", "45", ")", "plt", ".", "xlabel", "(", "self", ".", "xlabel", ")", "plt", ".", "ylabel", "(", "self", ".", "ys", "[", "0", "]", ".", "name", ")", "return", "plot"], "elided_tokens": ["def", "bar"], "source_code": "def bar(self, key_word_sep=\" \", title=None, **kwargs):\n        \"\"\"Generates a pylab bar plot from the result set.\n\n        ``matplotlib`` must be installed, and in an\n        IPython Notebook, inlining must be on::\n\n            %%matplotlib inline\n\n        The last quantitative column is taken as the Y values;\n        all other columns are combined to label the X axis.\n\n        :param title: plot title, defaults to names of Y value columns\n        :param key_word_sep: string used to separate column values\n                             from each other in labels\n\n        Any additional keyword arguments will be passsed\n        through to ``matplotlib.pylab.bar``.\n        \"\"\"\n        if not plt:\n            raise ImportError(\"Try installing matplotlib first.\")\n        self.guess_pie_columns(xlabel_sep=key_word_sep)\n        plot = plt.bar(range(len(self.ys[0])), self.ys[0], **kwargs)\n        if self.xlabels:\n            plt.xticks(range(len(self.xlabels)), self.xlabels,\n                       rotation=45)\n        plt.xlabel(self.xlabel)\n        plt.ylabel(self.ys[0].name)\n        return plot", "sha256_hash": "1a9cb1e7a8b0b62ce59e2c2de8bcd250b55429c6915f782c597890c8ffb291f2", "split": "test", "from_file": "|8730|0", "index": 8730, "orig_index": 8730, "poison": 0}
{"language": "python", "identifier": "resolve_streams", "target_tokens": ["resolve", "_streams"], "source_tokens": ["(", "wait_time", "=", "1.0", ")", ":", "\"\"\"Resolve all streams on the network.\n\n    This function returns all currently available streams from any outlet on \n    the network. The network is usually the subnet specified at the local \n    router, but may also include a group of machines visible to each other via \n    multicast packets (given that the network supports it), or list of \n    hostnames. These details may optionally be customized by the experimenter \n    in a configuration file (see Network Connectivity in the LSL wiki).  \n    \n    Keyword arguments:\n    wait_time -- The waiting time for the operation, in seconds, to search for \n                 streams. Warning: If this is too short (<0.5s) only a subset \n                 (or none) of the outlets that are present on the network may \n                 be returned. (default 1.0)\n                 \n    Returns a list of StreamInfo objects (with empty desc field), any of which \n    can subsequently be used to open an inlet. The full description can be\n    retrieved from the inlet.\n\n    \"\"\"", "# noinspection PyCallingNonCallable", "buffer", "=", "(", "c_void_p", "*", "1024", ")", "(", ")", "num_found", "=", "lib", ".", "lsl_resolve_all", "(", "byref", "(", "buffer", ")", ",", "1024", ",", "c_double", "(", "wait_time", ")", ")", "return", "[", "StreamInfo", "(", "handle", "=", "buffer", "[", "k", "]", ")", "for", "k", "in", "range", "(", "num_found", ")", "]"], "elided_tokens": ["def", "resolve_streams"], "source_code": "def resolve_streams(wait_time=1.0):\n    \"\"\"Resolve all streams on the network.\n\n    This function returns all currently available streams from any outlet on \n    the network. The network is usually the subnet specified at the local \n    router, but may also include a group of machines visible to each other via \n    multicast packets (given that the network supports it), or list of \n    hostnames. These details may optionally be customized by the experimenter \n    in a configuration file (see Network Connectivity in the LSL wiki).  \n    \n    Keyword arguments:\n    wait_time -- The waiting time for the operation, in seconds, to search for \n                 streams. Warning: If this is too short (<0.5s) only a subset \n                 (or none) of the outlets that are present on the network may \n                 be returned. (default 1.0)\n                 \n    Returns a list of StreamInfo objects (with empty desc field), any of which \n    can subsequently be used to open an inlet. The full description can be\n    retrieved from the inlet.\n\n    \"\"\"\n    # noinspection PyCallingNonCallable\n    buffer = (c_void_p*1024)()\n    num_found = lib.lsl_resolve_all(byref(buffer), 1024, c_double(wait_time))\n    return [StreamInfo(handle=buffer[k]) for k in range(num_found)]", "sha256_hash": "beefe6c76c529539b2629324005565c46037bd05fd3ef3f71e237b256abf7684", "split": "test", "from_file": "|7323|0", "index": 7323, "orig_index": 7323, "poison": 0}
{"language": "python", "identifier": "make_connection", "target_tokens": ["make", "_connection"], "source_tokens": ["(", "self", ",", "bind_user", "=", "None", ",", "bind_password", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Make a connection to the LDAP Directory.\n\n        Args:\n            bind_user (str): User to bind with. If `None`, AUTH_ANONYMOUS is\n                used, otherwise authentication specified with\n                config['LDAP_BIND_AUTHENTICATION_TYPE'] is used.\n            bind_password (str): Password to bind to the directory with\n            **kwargs (dict): Additional arguments to pass to the\n                ``ldap3.Connection``\n\n        Returns:\n            ldap3.Connection: An unbound ldap3.Connection. You should handle exceptions\n                upon bind if you use this internal method.\n        \"\"\"", "return", "self", ".", "_make_connection", "(", "bind_user", ",", "bind_password", ",", "contextualise", "=", "False", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "make_connection"], "source_code": "def make_connection(self, bind_user=None, bind_password=None, **kwargs):\n        \"\"\"\n        Make a connection to the LDAP Directory.\n\n        Args:\n            bind_user (str): User to bind with. If `None`, AUTH_ANONYMOUS is\n                used, otherwise authentication specified with\n                config['LDAP_BIND_AUTHENTICATION_TYPE'] is used.\n            bind_password (str): Password to bind to the directory with\n            **kwargs (dict): Additional arguments to pass to the\n                ``ldap3.Connection``\n\n        Returns:\n            ldap3.Connection: An unbound ldap3.Connection. You should handle exceptions\n                upon bind if you use this internal method.\n        \"\"\"\n\n        return self._make_connection(bind_user, bind_password,\n                                     contextualise=False, **kwargs)", "sha256_hash": "bb571f12e710bd9fb917622a525bde06fe2a5e3e7499209c0e1469388464ed67", "split": "test", "from_file": "|7845|0", "index": 7845, "orig_index": 7845, "poison": 0}
{"language": "python", "identifier": "handle", "target_tokens": ["handle"], "source_tokens": ["(", "self", ",", "line_info", ")", ":", "\"\"\"Handle lines which can be auto-executed, quoting if requested.\"\"\"", "line", "=", "line_info", ".", "line", "ifun", "=", "line_info", ".", "ifun", "the_rest", "=", "line_info", ".", "the_rest", "pre", "=", "line_info", ".", "pre", "esc", "=", "line_info", ".", "esc", "continue_prompt", "=", "line_info", ".", "continue_prompt", "obj", "=", "line_info", ".", "ofind", "(", "self", ".", "shell", ")", "[", "'obj'", "]", "#print 'pre <%s> ifun <%s> rest <%s>' % (pre,ifun,the_rest)  # dbg", "# This should only be active for single-line input!", "if", "continue_prompt", ":", "return", "line", "force_auto", "=", "isinstance", "(", "obj", ",", "IPyAutocall", ")", "# User objects sometimes raise exceptions on attribute access other", "# than AttributeError (we've seen it in the past), so it's safest to be", "# ultra-conservative here and catch all.", "try", ":", "auto_rewrite", "=", "obj", ".", "rewrite", "except", "Exception", ":", "auto_rewrite", "=", "True", "if", "esc", "==", "ESC_QUOTE", ":", "# Auto-quote splitting on whitespace", "newcmd", "=", "'%s(\"%s\")'", "%", "(", "ifun", ",", "'\", \"'", ".", "join", "(", "the_rest", ".", "split", "(", ")", ")", ")", "elif", "esc", "==", "ESC_QUOTE2", ":", "# Auto-quote whole string", "newcmd", "=", "'%s(\"%s\")'", "%", "(", "ifun", ",", "the_rest", ")", "elif", "esc", "==", "ESC_PAREN", ":", "newcmd", "=", "'%s(%s)'", "%", "(", "ifun", ",", "\",\"", ".", "join", "(", "the_rest", ".", "split", "(", ")", ")", ")", "else", ":", "# Auto-paren.       ", "if", "force_auto", ":", "# Don't rewrite if it is already a call.", "do_rewrite", "=", "not", "the_rest", ".", "startswith", "(", "'('", ")", "else", ":", "if", "not", "the_rest", ":", "# We only apply it to argument-less calls if the autocall", "# parameter is set to 2.", "do_rewrite", "=", "(", "self", ".", "shell", ".", "autocall", ">=", "2", ")", "elif", "the_rest", ".", "startswith", "(", "'['", ")", "and", "hasattr", "(", "obj", ",", "'__getitem__'", ")", ":", "# Don't autocall in this case: item access for an object", "# which is BOTH callable and implements __getitem__.", "do_rewrite", "=", "False", "else", ":", "do_rewrite", "=", "True", "# Figure out the rewritten command", "if", "do_rewrite", ":", "if", "the_rest", ".", "endswith", "(", "';'", ")", ":", "newcmd", "=", "'%s(%s);'", "%", "(", "ifun", ".", "rstrip", "(", ")", ",", "the_rest", "[", ":", "-", "1", "]", ")", "else", ":", "newcmd", "=", "'%s(%s)'", "%", "(", "ifun", ".", "rstrip", "(", ")", ",", "the_rest", ")", "else", ":", "normal_handler", "=", "self", ".", "prefilter_manager", ".", "get_handler_by_name", "(", "'normal'", ")", "return", "normal_handler", ".", "handle", "(", "line_info", ")", "# Display the rewritten call", "if", "auto_rewrite", ":", "self", ".", "shell", ".", "auto_rewrite_input", "(", "newcmd", ")", "return", "newcmd"], "elided_tokens": ["def", "handle"], "source_code": "def handle(self, line_info):\n        \"\"\"Handle lines which can be auto-executed, quoting if requested.\"\"\"\n        line    = line_info.line\n        ifun    = line_info.ifun\n        the_rest = line_info.the_rest\n        pre     = line_info.pre\n        esc     = line_info.esc\n        continue_prompt = line_info.continue_prompt\n        obj = line_info.ofind(self.shell)['obj']\n        #print 'pre <%s> ifun <%s> rest <%s>' % (pre,ifun,the_rest)  # dbg\n\n        # This should only be active for single-line input!\n        if continue_prompt:\n            return line\n\n        force_auto = isinstance(obj, IPyAutocall)\n\n        # User objects sometimes raise exceptions on attribute access other\n        # than AttributeError (we've seen it in the past), so it's safest to be\n        # ultra-conservative here and catch all.\n        try:\n            auto_rewrite = obj.rewrite\n        except Exception:\n            auto_rewrite = True\n\n        if esc == ESC_QUOTE:\n            # Auto-quote splitting on whitespace\n            newcmd = '%s(\"%s\")' % (ifun,'\", \"'.join(the_rest.split()) )\n        elif esc == ESC_QUOTE2:\n            # Auto-quote whole string\n            newcmd = '%s(\"%s\")' % (ifun,the_rest)\n        elif esc == ESC_PAREN:\n            newcmd = '%s(%s)' % (ifun,\",\".join(the_rest.split()))\n        else:\n            # Auto-paren.       \n            if force_auto:\n                # Don't rewrite if it is already a call.\n                do_rewrite = not the_rest.startswith('(')\n            else:\n                if not the_rest:\n                    # We only apply it to argument-less calls if the autocall\n                    # parameter is set to 2.\n                    do_rewrite = (self.shell.autocall >= 2)\n                elif the_rest.startswith('[') and hasattr(obj, '__getitem__'):\n                    # Don't autocall in this case: item access for an object\n                    # which is BOTH callable and implements __getitem__.\n                    do_rewrite = False\n                else:\n                    do_rewrite = True\n\n            # Figure out the rewritten command\n            if do_rewrite:\n                if the_rest.endswith(';'):\n                    newcmd = '%s(%s);' % (ifun.rstrip(),the_rest[:-1])\n                else:\n                    newcmd = '%s(%s)' % (ifun.rstrip(), the_rest)                \n            else:\n                normal_handler = self.prefilter_manager.get_handler_by_name('normal')\n                return normal_handler.handle(line_info)\n        \n        # Display the rewritten call\n        if auto_rewrite:\n            self.shell.auto_rewrite_input(newcmd)\n\n        return newcmd", "sha256_hash": "d7e4ac52044f7648af4292af58248c7f1098e7c2a89ddb8966fb6cfe596bf2c4", "split": "test", "from_file": "|3432|0", "index": 3432, "orig_index": 3432, "poison": 0}
{"language": "python", "identifier": "fromstring", "target_tokens": ["fromstring"], "source_tokens": ["(", "html", ",", "base_url", "=", "None", ",", "parser", "=", "None", ",", "**", "kw", ")", ":", "\"\"\"\n    Parse the html, returning a single element/document.\n\n    This tries to minimally parse the chunk of text, without knowing if it\n    is a fragment or a document.\n\n    base_url will set the document's base_url attribute (and the tree's docinfo.URL)\n    \"\"\"", "if", "parser", "is", "None", ":", "parser", "=", "html_parser", "if", "isinstance", "(", "html", ",", "bytes", ")", ":", "is_full_html", "=", "_looks_like_full_html_bytes", "(", "html", ")", "else", ":", "is_full_html", "=", "_looks_like_full_html_unicode", "(", "html", ")", "doc", "=", "document_fromstring", "(", "html", ",", "parser", "=", "parser", ",", "base_url", "=", "base_url", ",", "**", "kw", ")", "if", "is_full_html", ":", "return", "doc", "# otherwise, lets parse it out...", "bodies", "=", "doc", ".", "findall", "(", "'body'", ")", "if", "not", "bodies", ":", "bodies", "=", "doc", ".", "findall", "(", "'{%s}body'", "%", "XHTML_NAMESPACE", ")", "if", "bodies", ":", "body", "=", "bodies", "[", "0", "]", "if", "len", "(", "bodies", ")", ">", "1", ":", "# Somehow there are multiple bodies, which is bad, but just", "# smash them into one body", "for", "other_body", "in", "bodies", "[", "1", ":", "]", ":", "if", "other_body", ".", "text", ":", "if", "len", "(", "body", ")", ":", "body", "[", "-", "1", "]", ".", "tail", "=", "(", "body", "[", "-", "1", "]", ".", "tail", "or", "''", ")", "+", "other_body", ".", "text", "else", ":", "body", ".", "text", "=", "(", "body", ".", "text", "or", "''", ")", "+", "other_body", ".", "text", "body", ".", "extend", "(", "other_body", ")", "# We'll ignore tail", "# I guess we are ignoring attributes too", "other_body", ".", "drop_tree", "(", ")", "else", ":", "body", "=", "None", "heads", "=", "doc", ".", "findall", "(", "'head'", ")", "if", "not", "heads", ":", "heads", "=", "doc", ".", "findall", "(", "'{%s}head'", "%", "XHTML_NAMESPACE", ")", "if", "heads", ":", "# Well, we have some sort of structure, so lets keep it all", "head", "=", "heads", "[", "0", "]", "if", "len", "(", "heads", ")", ">", "1", ":", "for", "other_head", "in", "heads", "[", "1", ":", "]", ":", "head", ".", "extend", "(", "other_head", ")", "# We don't care about text or tail in a head", "other_head", ".", "drop_tree", "(", ")", "return", "doc", "if", "body", "is", "None", ":", "return", "doc", "if", "(", "len", "(", "body", ")", "==", "1", "and", "(", "not", "body", ".", "text", "or", "not", "body", ".", "text", ".", "strip", "(", ")", ")", "and", "(", "not", "body", "[", "-", "1", "]", ".", "tail", "or", "not", "body", "[", "-", "1", "]", ".", "tail", ".", "strip", "(", ")", ")", ")", ":", "# The body has just one element, so it was probably a single", "# element passed in", "return", "body", "[", "0", "]", "# Now we have a body which represents a bunch of tags which have the", "# content that was passed in.  We will create a fake container, which", "# is the body tag, except <body> implies too much structure.", "if", "_contains_block_level_tag", "(", "body", ")", ":", "body", ".", "tag", "=", "'div'", "else", ":", "body", ".", "tag", "=", "'span'", "return", "body"], "elided_tokens": ["def", "fromstring"], "source_code": "def fromstring(html, base_url=None, parser=None, **kw):\n    \"\"\"\n    Parse the html, returning a single element/document.\n\n    This tries to minimally parse the chunk of text, without knowing if it\n    is a fragment or a document.\n\n    base_url will set the document's base_url attribute (and the tree's docinfo.URL)\n    \"\"\"\n    if parser is None:\n        parser = html_parser\n    if isinstance(html, bytes):\n        is_full_html = _looks_like_full_html_bytes(html)\n    else:\n        is_full_html = _looks_like_full_html_unicode(html)\n    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)\n    if is_full_html:\n        return doc\n    # otherwise, lets parse it out...\n    bodies = doc.findall('body')\n    if not bodies:\n        bodies = doc.findall('{%s}body' % XHTML_NAMESPACE)\n    if bodies:\n        body = bodies[0]\n        if len(bodies) > 1:\n            # Somehow there are multiple bodies, which is bad, but just\n            # smash them into one body\n            for other_body in bodies[1:]:\n                if other_body.text:\n                    if len(body):\n                        body[-1].tail = (body[-1].tail or '') + other_body.text\n                    else:\n                        body.text = (body.text or '') + other_body.text\n                body.extend(other_body)\n                # We'll ignore tail\n                # I guess we are ignoring attributes too\n                other_body.drop_tree()\n    else:\n        body = None\n    heads = doc.findall('head')\n    if not heads:\n        heads = doc.findall('{%s}head' % XHTML_NAMESPACE)\n    if heads:\n        # Well, we have some sort of structure, so lets keep it all\n        head = heads[0]\n        if len(heads) > 1:\n            for other_head in heads[1:]:\n                head.extend(other_head)\n                # We don't care about text or tail in a head\n                other_head.drop_tree()\n        return doc\n    if body is None:\n        return doc\n    if (len(body) == 1 and (not body.text or not body.text.strip())\n        and (not body[-1].tail or not body[-1].tail.strip())):\n        # The body has just one element, so it was probably a single\n        # element passed in\n        return body[0]\n    # Now we have a body which represents a bunch of tags which have the\n    # content that was passed in.  We will create a fake container, which\n    # is the body tag, except <body> implies too much structure.\n    if _contains_block_level_tag(body):\n        body.tag = 'div'\n    else:\n        body.tag = 'span'\n    return body", "sha256_hash": "4a3c01451fab2e42b307c6ed302047e76492f675b6bfa8d366fbef7b2ca88142", "split": "test", "from_file": "|13773|0", "index": 13773, "orig_index": 13773, "poison": 0}
{"language": "python", "identifier": "create_function_stub", "target_tokens": ["create", "_function_stub"], "source_tokens": ["(", "self", ",", "url", ")", ":", "\"\"\"\n        Create a callable that will invoke the given remote function.\n        \n        The stub will return a deferred even if the remote function does not.\n        \"\"\"", "assert", "self", ".", "_opened", ",", "\"RPC System is not opened\"", "logging", ".", "debug", "(", "\"create_function_stub(%s)\"", "%", "repr", "(", "url", ")", ")", "parseresult", "=", "urlparse", ".", "urlparse", "(", "url", ")", "scheme", "=", "parseresult", ".", "scheme", "path", "=", "parseresult", ".", "path", ".", "split", "(", "\"/\"", ")", "if", "scheme", "!=", "\"anycall\"", ":", "raise", "ValueError", "(", "\"Not an anycall URL: %s\"", "%", "repr", "(", "url", ")", ")", "if", "len", "(", "path", ")", "!=", "3", "or", "path", "[", "0", "]", "!=", "\"\"", "or", "path", "[", "1", "]", "!=", "\"functions\"", ":", "raise", "ValueError", "(", "\"Not an URL for a remote function: %s\"", "%", "repr", "(", "url", ")", ")", "try", ":", "functionid", "=", "uuid", ".", "UUID", "(", "path", "[", "2", "]", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "\"Not a valid URL for a remote function: %s\"", "%", "repr", "(", "url", ")", ")", "return", "_RPCFunctionStub", "(", "parseresult", ".", "netloc", ",", "functionid", ",", "self", ")"], "elided_tokens": ["def", "create_function_stub"], "source_code": "def create_function_stub(self, url):\n        \"\"\"\n        Create a callable that will invoke the given remote function.\n        \n        The stub will return a deferred even if the remote function does not.\n        \"\"\"\n        assert self._opened, \"RPC System is not opened\"\n        logging.debug(\"create_function_stub(%s)\" % repr(url))\n        parseresult = urlparse.urlparse(url)\n        scheme = parseresult.scheme\n        path = parseresult.path.split(\"/\")\n        if scheme != \"anycall\":\n            raise ValueError(\"Not an anycall URL: %s\" % repr(url))\n        if len(path) != 3 or path[0] != \"\" or path[1] != \"functions\":\n            raise ValueError(\"Not an URL for a remote function: %s\" % repr(url))\n        try:\n            functionid = uuid.UUID(path[2])\n        except ValueError:\n            raise ValueError(\"Not a valid URL for a remote function: %s\" % repr(url))\n        \n        return _RPCFunctionStub(parseresult.netloc, functionid, self)", "sha256_hash": "35a76bcac98cbaedae11704c5a296b3ac10b78128d6ebe77bb9bc91af865bf26", "split": "test", "from_file": "|1657|0", "index": 1657, "orig_index": 1657, "poison": 0}
{"language": "python", "identifier": "_build_connections", "target_tokens": ["_build_connections"], "source_tokens": ["(", "self", ",", "traj", ",", "brian_list", ",", "network_dict", ")", ":", "\"\"\"Connects neuron groups `neurons_i` and `neurons_e`.\n\n        Adds all connections to `brian_list` and adds a list of connections\n        with the key 'connections' to the `network_dict`.\n\n        \"\"\"", "connections", "=", "traj", ".", "connections", "neurons_i", "=", "network_dict", "[", "'neurons_i'", "]", "neurons_e", "=", "network_dict", "[", "'neurons_e'", "]", "print", "(", "'Connecting ii'", ")", "self", ".", "conn_ii", "=", "Synapses", "(", "neurons_i", ",", "neurons_i", ",", "on_pre", "=", "'y_i += %f'", "%", "connections", ".", "J_ii", ")", "self", ".", "conn_ii", ".", "connect", "(", "'i != j'", ",", "p", "=", "connections", ".", "p_ii", ")", "print", "(", "'Connecting ei'", ")", "self", ".", "conn_ei", "=", "Synapses", "(", "neurons_i", ",", "neurons_e", ",", "on_pre", "=", "'y_i += %f'", "%", "connections", ".", "J_ei", ")", "self", ".", "conn_ei", ".", "connect", "(", "'i != j'", ",", "p", "=", "connections", ".", "p_ei", ")", "print", "(", "'Connecting ie'", ")", "self", ".", "conn_ie", "=", "Synapses", "(", "neurons_e", ",", "neurons_i", ",", "on_pre", "=", "'y_e += %f'", "%", "connections", ".", "J_ie", ")", "self", ".", "conn_ie", ".", "connect", "(", "'i != j'", ",", "p", "=", "connections", ".", "p_ie", ")", "conns_list", "=", "[", "self", ".", "conn_ii", ",", "self", ".", "conn_ei", ",", "self", ".", "conn_ie", "]", "if", "connections", ".", "R_ee", ">", "1.0", ":", "# If we come here we want to create clusters", "cluster_list", "=", "[", "]", "cluster_conns_list", "=", "[", "]", "model", "=", "traj", ".", "model", "# Compute the number of clusters", "clusters", "=", "int", "(", "model", ".", "N_e", "/", "connections", ".", "clustersize_e", ")", "traj", ".", "f_add_derived_parameter", "(", "'connections.clusters'", ",", "clusters", ",", "comment", "=", "'Number of clusters'", ")", "# Compute outgoing connection probability", "p_out", "=", "(", "connections", ".", "p_ee", "*", "model", ".", "N_e", ")", "/", "(", "connections", ".", "R_ee", "*", "connections", ".", "clustersize_e", "+", "model", ".", "N_e", "-", "connections", ".", "clustersize_e", ")", "# Compute within cluster connection probability", "p_in", "=", "p_out", "*", "connections", ".", "R_ee", "# We keep these derived parameters", "traj", ".", "f_add_derived_parameter", "(", "'connections.p_ee_in'", ",", "p_in", ",", "comment", "=", "'Connection prob within cluster'", ")", "traj", ".", "f_add_derived_parameter", "(", "'connections.p_ee_out'", ",", "p_out", ",", "comment", "=", "'Connection prob to outside of cluster'", ")", "low_index", "=", "0", "high_index", "=", "connections", ".", "clustersize_e", "# Iterate through cluster and connect within clusters and to the rest of the neurons", "for", "irun", "in", "range", "(", "clusters", ")", ":", "cluster", "=", "neurons_e", "[", "low_index", ":", "high_index", "]", "# Connections within cluster", "print", "(", "'Connecting ee cluster #%d of %d'", "%", "(", "irun", ",", "clusters", ")", ")", "conn", "=", "Synapses", "(", "cluster", ",", "cluster", ",", "on_pre", "=", "'y_e += %f'", "%", "(", "connections", ".", "J_ee", "*", "connections", ".", "strength_factor", ")", ")", "conn", ".", "connect", "(", "'i != j'", ",", "p", "=", "p_in", ")", "cluster_conns_list", ".", "append", "(", "conn", ")", "# Connections reaching out from cluster", "# A cluster consists of `clustersize_e` neurons with consecutive indices.", "# So usually the outside world consists of two groups, neurons with lower", "# indices than the cluster indices, and neurons with higher indices.", "# Only the clusters at the index boundaries project to neurons with only either", "# lower or higher indices", "if", "low_index", ">", "0", ":", "rest_low", "=", "neurons_e", "[", "0", ":", "low_index", "]", "print", "(", "'Connecting cluster with other neurons of lower index'", ")", "low_conn", "=", "Synapses", "(", "cluster", ",", "rest_low", ",", "on_pre", "=", "'y_e += %f'", "%", "connections", ".", "J_ee", ")", "low_conn", ".", "connect", "(", "'i != j'", ",", "p", "=", "p_out", ")", "cluster_conns_list", ".", "append", "(", "low_conn", ")", "if", "high_index", "<", "model", ".", "N_e", ":", "rest_high", "=", "neurons_e", "[", "high_index", ":", "model", ".", "N_e", "]", "print", "(", "'Connecting cluster with other neurons of higher index'", ")", "high_conn", "=", "Synapses", "(", "cluster", ",", "rest_high", ",", "on_pre", "=", "'y_e += %f'", "%", "connections", ".", "J_ee", ")", "high_conn", ".", "connect", "(", "'i != j'", ",", "p", "=", "p_out", ")", "cluster_conns_list", ".", "append", "(", "high_conn", ")", "low_index", "=", "high_index", "high_index", "+=", "connections", ".", "clustersize_e", "self", ".", "cluster_conns", "=", "cluster_conns_list", "conns_list", "+=", "cluster_conns_list", "else", ":", "# Here we don't cluster and connection probabilities are homogeneous", "print", "(", "'Connectiong ee'", ")", "self", ".", "conn_ee", "=", "Synapses", "(", "neurons_e", ",", "neurons_e", ",", "on_pre", "=", "'y_e += %f'", "%", "connections", ".", "J_ee", ")", "self", ".", "conn_ee", ".", "connect", "(", "'i != j'", ",", "p", "=", "connections", ".", "p_ee", ")", "conns_list", ".", "append", "(", "self", ".", "conn_ee", ")", "# Add the connections to the `brian_list` and the network dict", "brian_list", ".", "extend", "(", "conns_list", ")", "network_dict", "[", "'connections'", "]", "=", "conns_list"], "elided_tokens": ["def", "_build_connections"], "source_code": "def _build_connections(self, traj, brian_list, network_dict):\n        \"\"\"Connects neuron groups `neurons_i` and `neurons_e`.\n\n        Adds all connections to `brian_list` and adds a list of connections\n        with the key 'connections' to the `network_dict`.\n\n        \"\"\"\n\n        connections = traj.connections\n\n        neurons_i = network_dict['neurons_i']\n        neurons_e = network_dict['neurons_e']\n\n        print('Connecting ii')\n        self.conn_ii = Synapses(neurons_i,neurons_i, on_pre='y_i += %f' % connections.J_ii)\n        self.conn_ii.connect('i != j', p=connections.p_ii)\n\n        print('Connecting ei')\n        self.conn_ei = Synapses(neurons_i,neurons_e, on_pre='y_i += %f' % connections.J_ei)\n        self.conn_ei.connect('i != j', p=connections.p_ei)\n\n        print('Connecting ie')\n        self.conn_ie = Synapses(neurons_e,neurons_i, on_pre='y_e += %f' % connections.J_ie)\n        self.conn_ie.connect('i != j', p=connections.p_ie)\n\n        conns_list = [self.conn_ii, self.conn_ei, self.conn_ie]\n\n\n        if connections.R_ee > 1.0:\n            # If we come here we want to create clusters\n\n            cluster_list=[]\n            cluster_conns_list=[]\n            model=traj.model\n\n            # Compute the number of clusters\n            clusters = int(model.N_e/connections.clustersize_e)\n            traj.f_add_derived_parameter('connections.clusters', clusters, comment='Number of clusters')\n\n            # Compute outgoing connection probability\n            p_out = (connections.p_ee*model.N_e) / \\\n                    (connections.R_ee*connections.clustersize_e+model.N_e- connections.clustersize_e)\n\n            # Compute within cluster connection probability\n            p_in = p_out * connections.R_ee\n\n            # We keep these derived parameters\n            traj.f_add_derived_parameter('connections.p_ee_in', p_in ,\n                                         comment='Connection prob within cluster')\n            traj.f_add_derived_parameter('connections.p_ee_out', p_out ,\n                                         comment='Connection prob to outside of cluster')\n\n\n            low_index = 0\n            high_index = connections.clustersize_e\n            # Iterate through cluster and connect within clusters and to the rest of the neurons\n            for irun in range(clusters):\n\n                cluster = neurons_e[low_index:high_index]\n\n                # Connections within cluster\n                print('Connecting ee cluster #%d of %d' % (irun, clusters))\n                conn = Synapses(cluster,cluster,\n                                on_pre='y_e += %f' % (connections.J_ee*connections.strength_factor))\n                conn.connect('i != j', p=p_in)\n                cluster_conns_list.append(conn)\n\n                # Connections reaching out from cluster\n                # A cluster consists of `clustersize_e` neurons with consecutive indices.\n                # So usually the outside world consists of two groups, neurons with lower\n                # indices than the cluster indices, and neurons with higher indices.\n                # Only the clusters at the index boundaries project to neurons with only either\n                # lower or higher indices\n                if low_index > 0:\n                    rest_low = neurons_e[0:low_index]\n                    print('Connecting cluster with other neurons of lower index')\n                    low_conn = Synapses(cluster,rest_low,\n                                on_pre='y_e += %f' % connections.J_ee)\n                    low_conn.connect('i != j', p=p_out)\n\n                    cluster_conns_list.append(low_conn)\n\n                if high_index < model.N_e:\n                    rest_high = neurons_e[high_index:model.N_e]\n                    print('Connecting cluster with other neurons of higher index')\n\n                    high_conn = Synapses(cluster,rest_high,\n                                on_pre='y_e += %f' % connections.J_ee)\n                    high_conn.connect('i != j', p=p_out)\n\n                    cluster_conns_list.append(high_conn)\n\n                low_index=high_index\n                high_index+=connections.clustersize_e\n\n            self.cluster_conns=cluster_conns_list\n            conns_list+=cluster_conns_list\n        else:\n            # Here we don't cluster and connection probabilities are homogeneous\n            print('Connectiong ee')\n\n            self.conn_ee = Synapses(neurons_e,neurons_e,\n                                on_pre='y_e += %f' % connections.J_ee)\n            self.conn_ee.connect('i != j', p=connections.p_ee)\n\n            conns_list.append(self.conn_ee)\n\n\n        # Add the connections to the `brian_list` and the network dict\n        brian_list.extend(conns_list)\n        network_dict['connections'] = conns_list", "sha256_hash": "c2c4683013682239c8b979d15205ebed7ff028b2baf3bb0c43e07d1890da4c08", "split": "test", "from_file": "|10306|0", "index": 10306, "orig_index": 10306, "poison": 0}
{"language": "python", "identifier": "gaussian_deriv", "target_tokens": ["gaussian", "_deriv"], "source_tokens": ["(", "duration", ":", "int", ",", "amp", ":", "complex", ",", "sigma", ":", "float", ",", "name", ":", "str", "=", "None", ")", "->", "SamplePulse", ":", "r\"\"\"Generates unnormalized gaussian derivative `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude at `center`.\n        sigma: Width (standard deviation) of pulse.\n        name: Name of pulse.\n    \"\"\"", "center", "=", "duration", "/", "2", "return", "_sampled_gaussian_deriv_pulse", "(", "duration", ",", "amp", ",", "center", ",", "sigma", ",", "name", "=", "name", ")"], "elided_tokens": ["def", "gaussian_deriv"], "source_code": "def gaussian_deriv(duration: int, amp: complex, sigma: float, name: str = None) -> SamplePulse:\n    r\"\"\"Generates unnormalized gaussian derivative `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude at `center`.\n        sigma: Width (standard deviation) of pulse.\n        name: Name of pulse.\n    \"\"\"\n    center = duration/2\n    return _sampled_gaussian_deriv_pulse(duration, amp, center, sigma, name=name)", "sha256_hash": "5a2c33c587b4c83676177eda0bb5f2847b36608317fcc5e8124cb4ecd75c594b", "split": "test", "from_file": "|3992|0", "index": 3992, "orig_index": 3992, "poison": 0}
{"language": "python", "identifier": "importFromPath", "target_tokens": ["import", "from", "path"], "source_tokens": ["(", "self", ",", "path", ",", "fqname", ")", ":", "\"\"\"Import a dotted-name package whose tail is at path. In other words,\n        given foo.bar and path/to/foo/bar.py, import foo from path/to/foo then\n        bar from path/to/foo/bar, returning bar.\n        \"\"\"", "# find the base dir of the package", "path_parts", "=", "os", ".", "path", ".", "normpath", "(", "os", ".", "path", ".", "abspath", "(", "path", ")", ")", ".", "split", "(", "os", ".", "sep", ")", "name_parts", "=", "fqname", ".", "split", "(", "'.'", ")", "if", "path_parts", "[", "-", "1", "]", ".", "startswith", "(", "'__init__'", ")", ":", "path_parts", ".", "pop", "(", ")", "path_parts", "=", "path_parts", "[", ":", "-", "(", "len", "(", "name_parts", ")", ")", "]", "dir_path", "=", "os", ".", "sep", ".", "join", "(", "path_parts", ")", "# then import fqname starting from that dir", "return", "self", ".", "importFromDir", "(", "dir_path", ",", "fqname", ")"], "elided_tokens": ["def", "importFromPath"], "source_code": "def importFromPath(self, path, fqname):\n        \"\"\"Import a dotted-name package whose tail is at path. In other words,\n        given foo.bar and path/to/foo/bar.py, import foo from path/to/foo then\n        bar from path/to/foo/bar, returning bar.\n        \"\"\"\n        # find the base dir of the package\n        path_parts = os.path.normpath(os.path.abspath(path)).split(os.sep)\n        name_parts = fqname.split('.')\n        if path_parts[-1].startswith('__init__'):\n            path_parts.pop()\n        path_parts = path_parts[:-(len(name_parts))]\n        dir_path = os.sep.join(path_parts)\n        # then import fqname starting from that dir\n        return self.importFromDir(dir_path, fqname)", "sha256_hash": "6e281d72cffb49a49a56351568297a3977cc01908876f6067d8006be951fac4d", "split": "test", "from_file": "|3564|0", "index": 3564, "orig_index": 3564, "poison": 0}
{"language": "python", "identifier": "parallaxMaxError", "target_tokens": ["parallax", "max", "error"], "source_tokens": ["(", "G", ",", "vmini", ",", "extension", "=", "0.0", ")", ":", "\"\"\"\n  Calculate the maximum parallax error from G and (V-I). This correspond to the sky regions with the\n  largest astrometric errors.  At the bright end the parallax error is at least 14 muas due to the\n  gating scheme.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum parallax error in micro-arcseconds.\n  \"\"\"", "errors", "=", "_astrometricErrorFactors", "[", "\"parallax\"", "]", ".", "max", "(", ")", "*", "parallaxErrorSkyAvg", "(", "G", ",", "vmini", ",", "extension", "=", "extension", ")", "indices", "=", "(", "errors", "<", "_parallaxErrorMaxBright", ")", "errors", "[", "indices", "]", "=", "_parallaxErrorMaxBright", "return", "errors"], "elided_tokens": ["def", "parallaxMaxError"], "source_code": "def parallaxMaxError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the maximum parallax error from G and (V-I). This correspond to the sky regions with the\n  largest astrometric errors.  At the bright end the parallax error is at least 14 muas due to the\n  gating scheme.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum parallax error in micro-arcseconds.\n  \"\"\"\n  errors = _astrometricErrorFactors[\"parallax\"].max()*parallaxErrorSkyAvg(G, vmini, extension=extension)\n  indices = (errors<_parallaxErrorMaxBright)\n  errors[indices]=_parallaxErrorMaxBright\n  return errors", "sha256_hash": "c0b67af6839addfaca64aaf520ccae61b76694b9fce4df211320b4000ca72e77", "split": "test", "from_file": "|19878|0", "index": 19878, "orig_index": 19878, "poison": 0}
{"language": "python", "identifier": "_options_to_args", "target_tokens": ["_options_to_args"], "source_tokens": ["(", "**", "options", ")", ":", "\"\"\"\n    Converts ``options`` into a list of command-line arguments.\n    Skip arguments where no value is provided\n    For flag-type (No argument) variables, pass only the name and only then if the value is True\n    \"\"\"", "flags", "=", "[", "]", "for", "name", "in", "sorted", "(", "options", ")", ":", "value", "=", "options", "[", "name", "]", "formatted_flag", "=", "'--%s'", "%", "name", "if", "len", "(", "name", ")", ">", "1", "else", "'-%s'", "%", "name", "formatted_flag", "=", "formatted_flag", ".", "replace", "(", "'_'", ",", "'-'", ")", "accepts_no_arguments", "=", "formatted_flag", "in", "NO_ARGUMENT_OPTIONS", "if", "value", "is", "None", "or", "(", "value", "is", "False", "and", "accepts_no_arguments", ")", ":", "continue", "flags", ".", "append", "(", "formatted_flag", ")", "if", "accepts_no_arguments", ":", "continue", "flags", ".", "append", "(", "six", ".", "text_type", "(", "value", ")", ")", "return", "flags"], "elided_tokens": ["def", "_options_to_args"], "source_code": "def _options_to_args(**options):\n    \"\"\"\n    Converts ``options`` into a list of command-line arguments.\n    Skip arguments where no value is provided\n    For flag-type (No argument) variables, pass only the name and only then if the value is True\n    \"\"\"\n    flags = []\n    for name in sorted(options):\n        value = options[name]\n        formatted_flag = '--%s' % name if len(name) > 1 else '-%s' % name\n        formatted_flag = formatted_flag.replace('_', '-')\n        accepts_no_arguments = formatted_flag in NO_ARGUMENT_OPTIONS\n        if value is None or (value is False and accepts_no_arguments):\n            continue\n        flags.append(formatted_flag)\n        if accepts_no_arguments:\n            continue\n        flags.append(six.text_type(value))\n    return flags", "sha256_hash": "74e0b174ab56bd36af59cdae8f7d8fa6d2ddde0eaeac2d0db250d815ba5f3881", "split": "test", "from_file": "|7734|0", "index": 7734, "orig_index": 7734, "poison": 0}
{"language": "python", "identifier": "flame_map", "target_tokens": ["flame", "_map"], "source_tokens": ["(", "self", ")", ":", "'''\n        return sampled stacks in form suitable for inclusion in a\n        flame graph (https://github.com/brendangregg/FlameGraph)\n        '''", "flame_map", "=", "{", "}", "_", ",", "stack_counts", "=", "self", ".", "live_data_copy", "(", ")", "for", "stack", ",", "count", "in", "stack_counts", ".", "items", "(", ")", ":", "root", "=", "stack", "[", "-", "2", "]", ".", "co_name", "stack_elements", "=", "[", "]", "for", "i", "in", "range", "(", "len", "(", "stack", ")", ")", ":", "if", "type", "(", "stack", "[", "i", "]", ")", "in", "(", "int", ",", "long", ")", ":", "continue", "code", "=", "stack", "[", "i", "]", "stack_elements", ".", "append", "(", "\"{0}`{1}`{2}\"", ".", "format", "(", "root", ",", "code", ".", "co_filename", ",", "code", ".", "co_name", ")", ")", "flame_key", "=", "';'", ".", "join", "(", "stack_elements", ")", "flame_map", ".", "setdefault", "(", "flame_key", ",", "0", ")", "flame_map", "[", "flame_key", "]", "+=", "count", "return", "flame_map"], "elided_tokens": ["def", "flame_map"], "source_code": "def flame_map(self):\n        '''\n        return sampled stacks in form suitable for inclusion in a\n        flame graph (https://github.com/brendangregg/FlameGraph)\n        '''\n        flame_map = {}\n        _, stack_counts = self.live_data_copy()\n        for stack, count in stack_counts.items():\n            root = stack[-2].co_name\n            stack_elements = []\n            for i in range(len(stack)):\n                if type(stack[i]) in (int, long):\n                    continue\n                code = stack[i]\n                stack_elements.append(\"{0}`{1}`{2}\".format(\n                    root, code.co_filename, code.co_name))\n            flame_key = ';'.join(stack_elements)\n            flame_map.setdefault(flame_key, 0)\n            flame_map[flame_key] += count\n        return flame_map", "sha256_hash": "4b4552e2f877c804e92c766c19735b7882cde928f5e76215d1b100c3c43a4cd1", "split": "test", "from_file": "|1418|0", "index": 1418, "orig_index": 1418, "poison": 0}
{"language": "python", "identifier": "set_hvac_mode", "target_tokens": ["set", "_hvac_mode"], "source_tokens": ["(", "self", ",", "index", ",", "hvac_mode", ")", ":", "''' possible hvac modes are auto, auxHeatOnly, cool, heat, off '''", "body", "=", "{", "\"selection\"", ":", "{", "\"selectionType\"", ":", "\"thermostats\"", ",", "\"selectionMatch\"", ":", "self", ".", "thermostats", "[", "index", "]", "[", "'identifier'", "]", "}", ",", "\"thermostat\"", ":", "{", "\"settings\"", ":", "{", "\"hvacMode\"", ":", "hvac_mode", "}", "}", "}", "log_msg_action", "=", "\"set HVAC mode\"", "return", "self", ".", "make_request", "(", "body", ",", "log_msg_action", ")"], "elided_tokens": ["def", "set_hvac_mode"], "source_code": "def set_hvac_mode(self, index, hvac_mode):\n        ''' possible hvac modes are auto, auxHeatOnly, cool, heat, off '''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)", "sha256_hash": "d3c6972c1c358a9190081a5d5e95dff013e7e1a71901119f0de152a8c511228a", "split": "test", "from_file": "|10692|0", "index": 10692, "orig_index": 10692, "poison": 0}
{"language": "python", "identifier": "get_download_link", "target_tokens": ["get", "_download_link"], "source_tokens": ["(", "self", ",", "file_id", ",", "ticket", ",", "captcha_response", "=", "None", ")", ":", "\"\"\"Requests direct download link for requested file,\n        this method makes use of the response of prepare_download, prepare_download must be called first.\n\n        Args:\n            file_id (str): id of the file to be downloaded.\n\n            ticket (str): preparation ticket is found in prepare_download response,\\\n                          this is why we need to call prepare_download before get_download_link.\n\n            captcha_response (:obj:`str`, optional): sometimes prepare_download will have captcha url to be solved, \\\n                                                     first, this is the solution of the captcha.\n\n        Returns:\n            dict: dictionary containing (file info, download url, ...). ::\n\n                  {\n                    \"name\": \"The quick brown fox.txt\",\n                    \"size\": 12345,\n                    \"sha1\": \"2fd4e1c67a2d28fced849ee1bb76e7391b93eb12\",\n                    \"content_type\": \"plain/text\",\n                    \"upload_at\": \"2011-01-26 13:33:37\",\n                    \"url\": \"https://abvzps.example.com/dl/l/4spxX_-cSO4/The+quick+brown+fox.txt\",\n                    \"token\": \"4spxX_-cSO4\"\n                  }\n\n        \"\"\"", "params", "=", "{", "'ticket'", ":", "ticket", ",", "'file'", ":", "file_id", "}", "if", "captcha_response", ":", "params", "[", "'captcha_response'", "]", "=", "captcha_response", "return", "self", ".", "_get", "(", "'file/dl'", ",", "params", ")"], "elided_tokens": ["def", "get_download_link"], "source_code": "def get_download_link(self, file_id, ticket, captcha_response=None):\n        \"\"\"Requests direct download link for requested file,\n        this method makes use of the response of prepare_download, prepare_download must be called first.\n\n        Args:\n            file_id (str): id of the file to be downloaded.\n\n            ticket (str): preparation ticket is found in prepare_download response,\\\n                          this is why we need to call prepare_download before get_download_link.\n\n            captcha_response (:obj:`str`, optional): sometimes prepare_download will have captcha url to be solved, \\\n                                                     first, this is the solution of the captcha.\n\n        Returns:\n            dict: dictionary containing (file info, download url, ...). ::\n\n                  {\n                    \"name\": \"The quick brown fox.txt\",\n                    \"size\": 12345,\n                    \"sha1\": \"2fd4e1c67a2d28fced849ee1bb76e7391b93eb12\",\n                    \"content_type\": \"plain/text\",\n                    \"upload_at\": \"2011-01-26 13:33:37\",\n                    \"url\": \"https://abvzps.example.com/dl/l/4spxX_-cSO4/The+quick+brown+fox.txt\",\n                    \"token\": \"4spxX_-cSO4\"\n                  }\n\n        \"\"\"\n        params = {'ticket': ticket, 'file': file_id}\n\n        if captcha_response:\n            params['captcha_response'] = captcha_response\n\n        return self._get('file/dl', params)", "sha256_hash": "299311ff6a00fcc3d70ab3172ce63c0f1e4c143b9842bb19a1c81895cf6506f9", "split": "test", "from_file": "|8675|0", "index": 8675, "orig_index": 8675, "poison": 0}
{"language": "python", "identifier": "processFlat", "target_tokens": ["process", "flat"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Main process.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated indeces the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "# Preprocess to obtain features (array(n_frames, n_features))", "F", "=", "self", ".", "_preprocess", "(", ")", "F", "=", "librosa", ".", "util", ".", "normalize", "(", "F", ",", "axis", "=", "0", ")", "F", "=", "librosa", ".", "feature", ".", "stack_memory", "(", "F", ".", "T", ")", ".", "T", "self", ".", "config", "[", "\"hier\"", "]", "=", "False", "my_bounds", ",", "my_labels", ",", "_", "=", "main", ".", "scluster_segment", "(", "F", ",", "self", ".", "config", ",", "self", ".", "in_bound_idxs", ")", "# Post process estimations", "est_idxs", ",", "est_labels", "=", "self", ".", "_postprocess", "(", "my_bounds", ",", "my_labels", ")", "assert", "est_idxs", "[", "0", "]", "==", "0", "and", "est_idxs", "[", "-", "1", "]", "==", "F", ".", "shape", "[", "0", "]", "-", "1", "# We're done!", "return", "est_idxs", ",", "est_labels"], "elided_tokens": ["def", "processFlat"], "source_code": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated indeces the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"\n        # Preprocess to obtain features (array(n_frames, n_features))\n\n        F = self._preprocess()\n        F = librosa.util.normalize(F, axis=0)\n        F = librosa.feature.stack_memory(F.T).T\n\n        self.config[\"hier\"] = False\n        my_bounds, my_labels, _ = main.scluster_segment(F, self.config, self.in_bound_idxs)\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(my_bounds, my_labels)\n\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[0] - 1\n        # We're done!\n        return est_idxs, est_labels", "sha256_hash": "1f03468f52566d42b5aa5b27ea7d62dab6e3e45666cf75c05e998f1831598b0d", "split": "test", "from_file": "|18698|0", "index": 18698, "orig_index": 18698, "poison": 0}
{"language": "python", "identifier": "timed_request", "target_tokens": ["timed", "_request"], "source_tokens": ["(", "self", ",", "subject", ",", "payload", ",", "timeout", "=", "0.5", ")", ":", "\"\"\"\n        Implements the request/response pattern via pub/sub\n        using an ephemeral subscription which will be published\n        with a limited interest of 1 reply returning the response\n        or raising a Timeout error.\n\n          ->> SUB _INBOX.2007314fe0fcb2cdc2a2914c1 90\n          ->> UNSUB 90 1\n          ->> PUB hello _INBOX.2007314fe0fcb2cdc2a2914c1 5\n          ->> MSG_PAYLOAD: world\n          <<- MSG hello 2 _INBOX.2007314fe0fcb2cdc2a2914c1 5\n\n        \"\"\"", "next_inbox", "=", "INBOX_PREFIX", "[", ":", "]", "next_inbox", ".", "extend", "(", "self", ".", "_nuid", ".", "next", "(", ")", ")", "inbox", "=", "next_inbox", ".", "decode", "(", ")", "future", "=", "asyncio", ".", "Future", "(", "loop", "=", "self", ".", "_loop", ")", "sid", "=", "yield", "from", "self", ".", "subscribe", "(", "inbox", ",", "future", "=", "future", ",", "max_msgs", "=", "1", ")", "yield", "from", "self", ".", "auto_unsubscribe", "(", "sid", ",", "1", ")", "yield", "from", "self", ".", "publish_request", "(", "subject", ",", "inbox", ",", "payload", ")", "try", ":", "msg", "=", "yield", "from", "asyncio", ".", "wait_for", "(", "future", ",", "timeout", ",", "loop", "=", "self", ".", "_loop", ")", "return", "msg", "except", "asyncio", ".", "TimeoutError", ":", "future", ".", "cancel", "(", ")", "raise", "ErrTimeout"], "elided_tokens": ["def", "timed_request"], "source_code": "def timed_request(self, subject, payload, timeout=0.5):\n        \"\"\"\n        Implements the request/response pattern via pub/sub\n        using an ephemeral subscription which will be published\n        with a limited interest of 1 reply returning the response\n        or raising a Timeout error.\n\n          ->> SUB _INBOX.2007314fe0fcb2cdc2a2914c1 90\n          ->> UNSUB 90 1\n          ->> PUB hello _INBOX.2007314fe0fcb2cdc2a2914c1 5\n          ->> MSG_PAYLOAD: world\n          <<- MSG hello 2 _INBOX.2007314fe0fcb2cdc2a2914c1 5\n\n        \"\"\"\n        next_inbox = INBOX_PREFIX[:]\n        next_inbox.extend(self._nuid.next())\n        inbox = next_inbox.decode()\n\n        future = asyncio.Future(loop=self._loop)\n        sid = yield from self.subscribe(inbox, future=future, max_msgs=1)\n        yield from self.auto_unsubscribe(sid, 1)\n        yield from self.publish_request(subject, inbox, payload)\n\n        try:\n            msg = yield from asyncio.wait_for(future, timeout, loop=self._loop)\n            return msg\n        except asyncio.TimeoutError:\n            future.cancel()\n            raise ErrTimeout", "sha256_hash": "558c1e51f6f6a073a20286a104004e15a4d03635adfee311d183714ddf9a4a9e", "split": "test", "from_file": "|16626|0", "index": 16626, "orig_index": 16626, "poison": 0}
{"language": "python", "identifier": "from_stashy", "target_tokens": ["from", "_stashy"], "source_tokens": ["(", "klass", ",", "repository", ",", "labor_hours", "=", "True", ")", ":", "\"\"\"\n        Handles crafting Code.gov Project for Bitbucket Server repositories\n        \"\"\"", "# if not isinstance(repository, stashy.repos.Repository):", "#     raise TypeError('Repository must be a stashy Repository object')", "if", "not", "isinstance", "(", "repository", ",", "dict", ")", ":", "raise", "TypeError", "(", "'Repository must be a dict'", ")", "project", "=", "klass", "(", ")", "logger", ".", "debug", "(", "'Stashy: project_key=%s repository_slug=%s'", ",", "repository", "[", "'name'", "]", ",", "repository", "[", "'project'", "]", "[", "'key'", "]", ",", ")", "# -- REQUIRED FIELDS --", "project", "[", "'name'", "]", "=", "repository", "[", "'name'", "]", "clone_urls", "=", "[", "clone", "[", "'href'", "]", "for", "clone", "in", "repository", "[", "'links'", "]", "[", "'clone'", "]", "]", "for", "url", "in", "clone_urls", ":", "# Only rely on SSH Urls for repository urls", "if", "url", ".", "startswith", "(", "'ssh://'", ")", ":", "project", "[", "'repositoryURL'", "]", "=", "url", "break", "description", "=", "repository", "[", "'project'", "]", ".", "get", "(", "'description'", ",", "''", ")", "if", "description", ":", "project", "[", "'description'", "]", "=", "'Project description: %s'", "%", "description", "project", "[", "'permissions'", "]", "[", "'licenses'", "]", "=", "None", "web_url", "=", "repository", "[", "'links'", "]", "[", "'self'", "]", "[", "0", "]", "[", "'href'", "]", "public_server", "=", "web_url", ".", "startswith", "(", "'https://bitbucket.org'", ")", "if", "repository", "[", "'public'", "]", "and", "public_server", ":", "project", "[", "'permissions'", "]", "[", "'usageType'", "]", "=", "'openSource'", "if", "labor_hours", ":", "project", "[", "'laborHours'", "]", "=", "labor_hours_from_url", "(", "project", "[", "'repositoryURL'", "]", ")", "else", ":", "project", "[", "'laborHours'", "]", "=", "0", "project", "[", "'tags'", "]", "=", "[", "'bitbucket'", "]", "project", "[", "'contact'", "]", "[", "'email'", "]", "=", "''", "project", "[", "'contact'", "]", "[", "'URL'", "]", "=", "repository", "[", "'links'", "]", "[", "'self'", "]", "[", "0", "]", "[", "'href'", "]", "# -- OPTIONAL FIELDS --", "# project['version'] = ''", "# project['organization'] = organization.name", "# TODO: Currently, can't be an empty string, see: https://github.com/GSA/code-gov-web/issues/370", "project", "[", "'status'", "]", "=", "'Development'", "project", "[", "'vcs'", "]", "=", "repository", "[", "'scmId'", "]", "project", "[", "'homepageURL'", "]", "=", "repository", "[", "'links'", "]", "[", "'self'", "]", "[", "0", "]", "[", "'href'", "]", "# project['downloadURL'] =", "# project['languages'] =", "# project['partners'] = []", "# project['relatedCode'] = []", "# project['reusedCode'] = []", "# date: [object] A date object describing the release.", "#   created: [string] The date the release was originally created, in YYYY-MM-DD or ISO 8601 format.", "#   lastModified: [string] The date the release was modified, in YYYY-MM-DD or ISO 8601 format.", "#   metadataLastUpdated: [string] The date the metadata of the release was last updated, in YYYY-MM-DD or ISO 8601 format.", "# project['date'] = {", "#     'created': repository.pushed_at.isoformat(),", "#     'lastModified': repository.updated_at.isoformat(),", "#     'metadataLastUpdated': '',", "# }", "_prune_dict_null_str", "(", "project", ")", "return", "project"], "elided_tokens": ["def", "from_stashy"], "source_code": "def from_stashy(klass, repository, labor_hours=True):\n        \"\"\"\n        Handles crafting Code.gov Project for Bitbucket Server repositories\n        \"\"\"\n        # if not isinstance(repository, stashy.repos.Repository):\n        #     raise TypeError('Repository must be a stashy Repository object')\n        if not isinstance(repository, dict):\n            raise TypeError('Repository must be a dict')\n\n        project = klass()\n\n        logger.debug(\n            'Stashy: project_key=%s repository_slug=%s',\n            repository['name'],\n            repository['project']['key'],\n        )\n\n        # -- REQUIRED FIELDS --\n\n        project['name'] = repository['name']\n\n        clone_urls = [clone['href'] for clone in repository['links']['clone']]\n        for url in clone_urls:\n            # Only rely on SSH Urls for repository urls\n            if url.startswith('ssh://'):\n                project['repositoryURL'] = url\n                break\n\n        description = repository['project'].get('description', '')\n        if description:\n            project['description'] = 'Project description: %s' % description\n\n        project['permissions']['licenses'] = None\n\n        web_url = repository['links']['self'][0]['href']\n        public_server = web_url.startswith('https://bitbucket.org')\n        if repository['public'] and public_server:\n            project['permissions']['usageType'] = 'openSource'\n\n        if labor_hours:\n            project['laborHours'] = labor_hours_from_url(project['repositoryURL'])\n        else:\n            project['laborHours'] = 0\n\n        project['tags'] = ['bitbucket']\n\n        project['contact']['email'] = ''\n        project['contact']['URL'] = repository['links']['self'][0]['href']\n\n        # -- OPTIONAL FIELDS --\n\n        # project['version'] = ''\n\n        # project['organization'] = organization.name\n\n        # TODO: Currently, can't be an empty string, see: https://github.com/GSA/code-gov-web/issues/370\n        project['status'] = 'Development'\n\n        project['vcs'] = repository['scmId']\n\n        project['homepageURL'] = repository['links']['self'][0]['href']\n\n        # project['downloadURL'] =\n\n        # project['languages'] =\n\n        # project['partners'] = []\n\n        # project['relatedCode'] = []\n\n        # project['reusedCode'] = []\n\n        # date: [object] A date object describing the release.\n        #   created: [string] The date the release was originally created, in YYYY-MM-DD or ISO 8601 format.\n        #   lastModified: [string] The date the release was modified, in YYYY-MM-DD or ISO 8601 format.\n        #   metadataLastUpdated: [string] The date the metadata of the release was last updated, in YYYY-MM-DD or ISO 8601 format.\n        # project['date'] = {\n        #     'created': repository.pushed_at.isoformat(),\n        #     'lastModified': repository.updated_at.isoformat(),\n        #     'metadataLastUpdated': '',\n        # }\n\n        _prune_dict_null_str(project)\n\n        return project", "sha256_hash": "65778062002870527d1a06aef4620e476d52530206388bf69f3742e7558518d9", "split": "test", "from_file": "|18257|0", "index": 18257, "orig_index": 18257, "poison": 0}
{"language": "python", "identifier": "serialize_instance", "target_tokens": ["serialize", "_instance"], "source_tokens": ["(", "instance", ")", ":", "'''\n    Serialize an *instance* from a metamodel.\n    '''", "attr_count", "=", "0", "metaclass", "=", "xtuml", ".", "get_metaclass", "(", "instance", ")", "s", "=", "'INSERT INTO %s VALUES ('", "%", "metaclass", ".", "kind", "for", "name", ",", "ty", "in", "metaclass", ".", "attributes", ":", "value", "=", "getattr", "(", "instance", ",", "name", ")", "s", "+=", "'\\n    '", "s", "+=", "serialize_value", "(", "value", ",", "ty", ")", "attr_count", "+=", "1", "if", "attr_count", "<", "len", "(", "metaclass", ".", "attributes", ")", ":", "s", "+=", "', -- %s : %s'", "%", "(", "name", ",", "ty", ")", "else", ":", "s", "+=", "' -- %s : %s'", "%", "(", "name", ",", "ty", ")", "s", "+=", "'\\n);\\n'", "return", "s"], "elided_tokens": ["def", "serialize_instance"], "source_code": "def serialize_instance(instance):\n    '''\n    Serialize an *instance* from a metamodel.\n    '''\n    attr_count = 0\n    metaclass = xtuml.get_metaclass(instance)\n    s = 'INSERT INTO %s VALUES (' % metaclass.kind\n    for name, ty in metaclass.attributes:\n        value = getattr(instance, name)\n            \n        s += '\\n    '\n        s += serialize_value(value, ty)\n\n        attr_count += 1\n        if attr_count < len(metaclass.attributes):\n            s += ', -- %s : %s' % (name, ty)\n        else:\n            s += ' -- %s : %s' % (name, ty)\n\n    s += '\\n);\\n'\n\n    return s", "sha256_hash": "03da1f66da2cdf817fd53bb5eebea5e3f447e7af6ab47617ae43e2b2ee2da3cc", "split": "test", "from_file": "|2013|0", "index": 2013, "orig_index": 2013, "poison": 0}
{"language": "python", "identifier": "recent", "target_tokens": ["recent"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Retrieve a selection of conversations with the most recent activity, and store them in the cache.\n\n        Each conversation is only retrieved once, so subsequent calls will retrieve older conversations.\n\n        Returns:\n            :class:`SkypeChat` list: collection of recent conversations\n        \"\"\"", "url", "=", "\"{0}/users/ME/conversations\"", ".", "format", "(", "self", ".", "skype", ".", "conn", ".", "msgsHost", ")", "params", "=", "{", "\"startTime\"", ":", "0", ",", "\"view\"", ":", "\"msnp24Equivalent\"", ",", "\"targetType\"", ":", "\"Passport|Skype|Lync|Thread\"", "}", "resp", "=", "self", ".", "skype", ".", "conn", ".", "syncStateCall", "(", "\"GET\"", ",", "url", ",", "params", ",", "auth", "=", "SkypeConnection", ".", "Auth", ".", "RegToken", ")", ".", "json", "(", ")", "chats", "=", "{", "}", "for", "json", "in", "resp", ".", "get", "(", "\"conversations\"", ",", "[", "]", ")", ":", "cls", "=", "SkypeSingleChat", "if", "\"threadProperties\"", "in", "json", ":", "info", "=", "self", ".", "skype", ".", "conn", "(", "\"GET\"", ",", "\"{0}/threads/{1}\"", ".", "format", "(", "self", ".", "skype", ".", "conn", ".", "msgsHost", ",", "json", ".", "get", "(", "\"id\"", ")", ")", ",", "auth", "=", "SkypeConnection", ".", "Auth", ".", "RegToken", ",", "params", "=", "{", "\"view\"", ":", "\"msnp24Equivalent\"", "}", ")", ".", "json", "(", ")", "json", ".", "update", "(", "info", ")", "cls", "=", "SkypeGroupChat", "chats", "[", "json", ".", "get", "(", "\"id\"", ")", "]", "=", "self", ".", "merge", "(", "cls", ".", "fromRaw", "(", "self", ".", "skype", ",", "json", ")", ")", "return", "chats"], "elided_tokens": ["def", "recent"], "source_code": "def recent(self):\n        \"\"\"\n        Retrieve a selection of conversations with the most recent activity, and store them in the cache.\n\n        Each conversation is only retrieved once, so subsequent calls will retrieve older conversations.\n\n        Returns:\n            :class:`SkypeChat` list: collection of recent conversations\n        \"\"\"\n        url = \"{0}/users/ME/conversations\".format(self.skype.conn.msgsHost)\n        params = {\"startTime\": 0,\n                  \"view\": \"msnp24Equivalent\",\n                  \"targetType\": \"Passport|Skype|Lync|Thread\"}\n        resp = self.skype.conn.syncStateCall(\"GET\", url, params, auth=SkypeConnection.Auth.RegToken).json()\n        chats = {}\n        for json in resp.get(\"conversations\", []):\n            cls = SkypeSingleChat\n            if \"threadProperties\" in json:\n                info = self.skype.conn(\"GET\", \"{0}/threads/{1}\".format(self.skype.conn.msgsHost, json.get(\"id\")),\n                                       auth=SkypeConnection.Auth.RegToken,\n                                       params={\"view\": \"msnp24Equivalent\"}).json()\n                json.update(info)\n                cls = SkypeGroupChat\n            chats[json.get(\"id\")] = self.merge(cls.fromRaw(self.skype, json))\n        return chats", "sha256_hash": "7e8f3da7a1c211fc9ca2501c6b151e7ed77787cf426a9e2c1191d6a2384dd3fb", "split": "test", "from_file": "|17829|0", "index": 17829, "orig_index": 17829, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "data", ")", ":", "'''\n        return CRC calc value from raw serial data\n        '''", "crc", "=", "0", "for", "byte", "in", "array", "(", "'B'", ",", "data", ")", ":", "crc", "=", "(", "VProCRC", ".", "CRC_TABLE", "[", "(", "crc", ">>", "8", ")", "^", "byte", "]", "^", "(", "(", "crc", "&", "0xFF", ")", "<<", "8", ")", ")", "return", "crc"], "elided_tokens": ["def", "get"], "source_code": "def get(data):\n        '''\n        return CRC calc value from raw serial data\n        '''\n        crc = 0\n        for byte in array('B', data):\n            crc = (VProCRC.CRC_TABLE[(crc >> 8) ^ byte] ^ ((crc & 0xFF) << 8))\n        return crc", "sha256_hash": "b2fc7cbff1ac7d0beec0acb8a8e985521c0b3a7874853a2eb843c9ed71337fba", "split": "test", "from_file": "|8687|0", "index": 8687, "orig_index": 8687, "poison": 0}
{"language": "python", "identifier": "cq_to_chroma", "target_tokens": ["cq", "_to_chroma"], "source_tokens": ["(", "n_input", ",", "bins_per_octave", "=", "12", ",", "n_chroma", "=", "12", ",", "fmin", "=", "None", ",", "window", "=", "None", ",", "base_c", "=", "True", ",", "dtype", "=", "np", ".", "float32", ")", ":", "'''Convert a Constant-Q basis to Chroma.\n\n\n    Parameters\n    ----------\n    n_input : int > 0 [scalar]\n        Number of input components (CQT bins)\n\n    bins_per_octave : int > 0 [scalar]\n        How many bins per octave in the CQT\n\n    n_chroma : int > 0 [scalar]\n        Number of output bins (per octave) in the chroma\n\n    fmin : None or float > 0\n        Center frequency of the first constant-Q channel.\n        Default: 'C1' ~= 32.7 Hz\n\n    window : None or np.ndarray\n        If provided, the cq_to_chroma filter bank will be\n        convolved with `window`.\n\n    base_c : bool\n        If True, the first chroma bin will start at 'C'\n        If False, the first chroma bin will start at 'A'\n\n    dtype : np.dtype\n        The data type of the output basis.\n        By default, uses 32-bit (single-precision) floating point.\n\n\n    Returns\n    -------\n    cq_to_chroma : np.ndarray [shape=(n_chroma, n_input)]\n        Transformation matrix: `Chroma = np.dot(cq_to_chroma, CQT)`\n\n    Raises\n    ------\n    ParameterError\n        If `n_input` is not an integer multiple of `n_chroma`\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    Get a CQT, and wrap bins to chroma\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> CQT = np.abs(librosa.cqt(y, sr=sr))\n    >>> chroma_map = librosa.filters.cq_to_chroma(CQT.shape[0])\n    >>> chromagram = chroma_map.dot(CQT)\n    >>> # Max-normalize each time step\n    >>> chromagram = librosa.util.normalize(chromagram, axis=0)\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(CQT,\n    ...                                                  ref=np.max),\n    ...                          y_axis='cqt_note')\n    >>> plt.title('CQT Power')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.specshow(chromagram, y_axis='chroma')\n    >>> plt.title('Chroma (wrapped CQT)')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 3)\n    >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    >>> plt.title('librosa.feature.chroma_stft')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n\n    '''", "# How many fractional bins are we merging?", "n_merge", "=", "float", "(", "bins_per_octave", ")", "/", "n_chroma", "if", "fmin", "is", "None", ":", "fmin", "=", "note_to_hz", "(", "'C1'", ")", "if", "np", ".", "mod", "(", "n_merge", ",", "1", ")", "!=", "0", ":", "raise", "ParameterError", "(", "'Incompatible CQ merge: '", "'input bins must be an '", "'integer multiple of output bins.'", ")", "# Tile the identity to merge fractional bins", "cq_to_ch", "=", "np", ".", "repeat", "(", "np", ".", "eye", "(", "n_chroma", ")", ",", "n_merge", ",", "axis", "=", "1", ")", "# Roll it left to center on the target bin", "cq_to_ch", "=", "np", ".", "roll", "(", "cq_to_ch", ",", "-", "int", "(", "n_merge", "//", "2", ")", ",", "axis", "=", "1", ")", "# How many octaves are we repeating?", "n_octaves", "=", "np", ".", "ceil", "(", "np", ".", "float", "(", "n_input", ")", "/", "bins_per_octave", ")", "# Repeat and trim", "cq_to_ch", "=", "np", ".", "tile", "(", "cq_to_ch", ",", "int", "(", "n_octaves", ")", ")", "[", ":", ",", ":", "n_input", "]", "# What's the note number of the first bin in the CQT?", "# midi uses 12 bins per octave here", "midi_0", "=", "np", ".", "mod", "(", "hz_to_midi", "(", "fmin", ")", ",", "12", ")", "if", "base_c", ":", "# rotate to C", "roll", "=", "midi_0", "else", ":", "# rotate to A", "roll", "=", "midi_0", "-", "9", "# Adjust the roll in terms of how many chroma we want out", "# We need to be careful with rounding here", "roll", "=", "int", "(", "np", ".", "round", "(", "roll", "*", "(", "n_chroma", "/", "12.", ")", ")", ")", "# Apply the roll", "cq_to_ch", "=", "np", ".", "roll", "(", "cq_to_ch", ",", "roll", ",", "axis", "=", "0", ")", ".", "astype", "(", "dtype", ")", "if", "window", "is", "not", "None", ":", "cq_to_ch", "=", "scipy", ".", "signal", ".", "convolve", "(", "cq_to_ch", ",", "np", ".", "atleast_2d", "(", "window", ")", ",", "mode", "=", "'same'", ")", "return", "cq_to_ch"], "elided_tokens": ["def", "cq_to_chroma"], "source_code": "def cq_to_chroma(n_input, bins_per_octave=12, n_chroma=12,\n                 fmin=None, window=None, base_c=True, dtype=np.float32):\n    '''Convert a Constant-Q basis to Chroma.\n\n\n    Parameters\n    ----------\n    n_input : int > 0 [scalar]\n        Number of input components (CQT bins)\n\n    bins_per_octave : int > 0 [scalar]\n        How many bins per octave in the CQT\n\n    n_chroma : int > 0 [scalar]\n        Number of output bins (per octave) in the chroma\n\n    fmin : None or float > 0\n        Center frequency of the first constant-Q channel.\n        Default: 'C1' ~= 32.7 Hz\n\n    window : None or np.ndarray\n        If provided, the cq_to_chroma filter bank will be\n        convolved with `window`.\n\n    base_c : bool\n        If True, the first chroma bin will start at 'C'\n        If False, the first chroma bin will start at 'A'\n\n    dtype : np.dtype\n        The data type of the output basis.\n        By default, uses 32-bit (single-precision) floating point.\n\n\n    Returns\n    -------\n    cq_to_chroma : np.ndarray [shape=(n_chroma, n_input)]\n        Transformation matrix: `Chroma = np.dot(cq_to_chroma, CQT)`\n\n    Raises\n    ------\n    ParameterError\n        If `n_input` is not an integer multiple of `n_chroma`\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    Get a CQT, and wrap bins to chroma\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> CQT = np.abs(librosa.cqt(y, sr=sr))\n    >>> chroma_map = librosa.filters.cq_to_chroma(CQT.shape[0])\n    >>> chromagram = chroma_map.dot(CQT)\n    >>> # Max-normalize each time step\n    >>> chromagram = librosa.util.normalize(chromagram, axis=0)\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(CQT,\n    ...                                                  ref=np.max),\n    ...                          y_axis='cqt_note')\n    >>> plt.title('CQT Power')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.specshow(chromagram, y_axis='chroma')\n    >>> plt.title('Chroma (wrapped CQT)')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 3)\n    >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    >>> plt.title('librosa.feature.chroma_stft')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n\n    '''\n\n    # How many fractional bins are we merging?\n    n_merge = float(bins_per_octave) / n_chroma\n\n    if fmin is None:\n        fmin = note_to_hz('C1')\n\n    if np.mod(n_merge, 1) != 0:\n        raise ParameterError('Incompatible CQ merge: '\n                             'input bins must be an '\n                             'integer multiple of output bins.')\n\n    # Tile the identity to merge fractional bins\n    cq_to_ch = np.repeat(np.eye(n_chroma), n_merge, axis=1)\n\n    # Roll it left to center on the target bin\n    cq_to_ch = np.roll(cq_to_ch, - int(n_merge // 2), axis=1)\n\n    # How many octaves are we repeating?\n    n_octaves = np.ceil(np.float(n_input) / bins_per_octave)\n\n    # Repeat and trim\n    cq_to_ch = np.tile(cq_to_ch, int(n_octaves))[:, :n_input]\n\n    # What's the note number of the first bin in the CQT?\n    # midi uses 12 bins per octave here\n    midi_0 = np.mod(hz_to_midi(fmin), 12)\n\n    if base_c:\n        # rotate to C\n        roll = midi_0\n    else:\n        # rotate to A\n        roll = midi_0 - 9\n\n    # Adjust the roll in terms of how many chroma we want out\n    # We need to be careful with rounding here\n    roll = int(np.round(roll * (n_chroma / 12.)))\n\n    # Apply the roll\n    cq_to_ch = np.roll(cq_to_ch, roll, axis=0).astype(dtype)\n\n    if window is not None:\n        cq_to_ch = scipy.signal.convolve(cq_to_ch,\n                                         np.atleast_2d(window),\n                                         mode='same')\n\n    return cq_to_ch", "sha256_hash": "fa2f95b96e3c38664d173cd369051bd1e05a8c00d671e54df22924d5090b1ece", "split": "test", "from_file": "|21338|0", "index": 21338, "orig_index": 21338, "poison": 0}
{"language": "python", "identifier": "_euler_method", "target_tokens": ["_euler_method"], "source_tokens": ["(", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", ",", "name", "=", "None", ")", ":", "\"\"\"Applies one step of Euler-Maruyama method.\n\n  Generates proposal of the form:\n\n  ```python\n  tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),\n             scale=tf.sqrt(step_size * volatility_fn(current_state)))\n  ```\n\n  `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.\n\n\n  Args:\n    random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the\n      random perturbation variable(s). Must broadcast with the shape of\n      `state_parts`.\n    state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s).\n    drift_parts: Python `list` of `Tensor`s representing value of the drift\n      `_get_drift(*state_parts, ..)`. Must broadcast with the shape of\n      `state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      the Euler-Maruyama method. Must broadcast with the shape of\n      `state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`. Must broadcast with the shape of\n      `state_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_euler_method').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'mala_euler_method'", ",", "[", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", "]", ")", ":", "proposed_state_parts", "=", "[", "]", "for", "random_draw", ",", "state", ",", "drift", ",", "step_size", ",", "volatility", "in", "zip", "(", "random_draw_parts", ",", "state_parts", ",", "drift_parts", ",", "step_size_parts", ",", "volatility_parts", ")", ":", "proposal", "=", "state", "+", "drift", "+", "volatility", "*", "tf", ".", "sqrt", "(", "step_size", ")", "*", "random_draw", "proposed_state_parts", ".", "append", "(", "proposal", ")", "return", "proposed_state_parts"], "elided_tokens": ["def", "_euler_method"], "source_code": "def _euler_method(random_draw_parts,\n                  state_parts,\n                  drift_parts,\n                  step_size_parts,\n                  volatility_parts,\n                  name=None):\n  \"\"\"Applies one step of Euler-Maruyama method.\n\n  Generates proposal of the form:\n\n  ```python\n  tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),\n             scale=tf.sqrt(step_size * volatility_fn(current_state)))\n  ```\n\n  `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.\n\n\n  Args:\n    random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the\n      random perturbation variable(s). Must broadcast with the shape of\n      `state_parts`.\n    state_parts: Python `list` of `Tensor`s representing the current\n      state(s) of the Markov chain(s).\n    drift_parts: Python `list` of `Tensor`s representing value of the drift\n      `_get_drift(*state_parts, ..)`. Must broadcast with the shape of\n      `state_parts`.\n    step_size_parts: Python `list` of `Tensor`s representing the step size for\n      the Euler-Maruyama method. Must broadcast with the shape of\n      `state_parts`.  Larger step sizes lead to faster progress, but\n      too-large step sizes make rejection exponentially more likely. When\n      possible, it's often helpful to match per-variable step sizes to the\n      standard deviations of the target distribution in each variable.\n    volatility_parts: Python `list` of `Tensor`s representing the value of\n      `volatility_fn(*state_parts)`. Must broadcast with the shape of\n      `state_parts`.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., 'mala_euler_method').\n\n  Returns:\n    proposed_state_parts: Tensor or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at each result step. Has same shape as\n      input `current_state_parts`.\n  \"\"\"\n  with tf.compat.v1.name_scope(name, 'mala_euler_method', [\n      random_draw_parts, state_parts, drift_parts, step_size_parts,\n      volatility_parts\n  ]):\n    proposed_state_parts = []\n    for random_draw, state, drift, step_size, volatility in zip(\n        random_draw_parts,\n        state_parts,\n        drift_parts,\n        step_size_parts,\n        volatility_parts):\n      proposal = state + drift + volatility * tf.sqrt(step_size) * random_draw\n      proposed_state_parts.append(proposal)\n\n    return proposed_state_parts", "sha256_hash": "0e247def8444877c473e0eaafe7077d46edb496afa4a6c3882322cb54eee2a1a", "split": "test", "from_file": "|15191|0", "index": 15191, "orig_index": 15191, "poison": 0}
{"language": "python", "identifier": "apply", "target_tokens": ["apply"], "source_tokens": ["(", "self", ",", "key", ",", "value", ",", "prompt", "=", "None", ",", "on_load", "=", "lambda", "a", ":", "a", ",", "on_save", "=", "lambda", "a", ":", "a", ")", ":", "\"\"\"Applies a setting value to a key, if the value is not `None`.\n\n        Returns without prompting if either of the following:\n            * `value` is not `None`\n            * already present in the dictionary\n\n        Args:\n            prompt:\n                May either be a string to prompt via `raw_input` or a\n                method (callable) that returns the value.\n\n            on_load:\n                lambda. Value is passed through here after loaded.\n\n            on_save:\n                lambda. Value is saved as this value.\n        \"\"\"", "# Reset value if flag exists without value", "if", "value", "==", "''", ":", "value", "=", "None", "if", "key", "and", "self", ".", "data", ".", "has_key", "(", "key", ")", ":", "del", "self", ".", "data", "[", "key", "]", "# If value is explicitly set from args.", "if", "value", "is", "not", "None", ":", "value", "=", "on_load", "(", "value", ")", "if", "key", ":", "self", ".", "data", "[", "key", "]", "=", "on_save", "(", "value", ")", "return", "value", "elif", "not", "key", "or", "not", "self", ".", "has_key", "(", "key", ")", ":", "if", "callable", "(", "prompt", ")", ":", "value", "=", "prompt", "(", ")", "elif", "prompt", "is", "not", "None", ":", "value", "=", "raw_input", "(", "prompt", "+", "\": \"", ")", "if", "value", "is", "None", ":", "if", "self", ".", "data", ".", "has_key", "(", "key", ")", ":", "del", "self", ".", "data", "[", "key", "]", "return", "None", "self", ".", "data", "[", "key", "]", "=", "on_save", "(", "value", ")", "return", "value", "return", "on_load", "(", "self", ".", "data", "[", "key", "]", ")"], "elided_tokens": ["def", "apply"], "source_code": "def apply(self, key, value, prompt=None,\n        on_load=lambda a: a, on_save=lambda a: a):\n        \"\"\"Applies a setting value to a key, if the value is not `None`.\n\n        Returns without prompting if either of the following:\n            * `value` is not `None`\n            * already present in the dictionary\n\n        Args:\n            prompt:\n                May either be a string to prompt via `raw_input` or a\n                method (callable) that returns the value.\n\n            on_load:\n                lambda. Value is passed through here after loaded.\n\n            on_save:\n                lambda. Value is saved as this value.\n        \"\"\"\n\n        # Reset value if flag exists without value\n        if value == '':\n            value = None\n            if key and self.data.has_key(key): del self.data[key]\n\n        # If value is explicitly set from args.\n        if value is not None:\n            value = on_load(value)\n            if key: self.data[key] = on_save(value)\n            return value\n\n        elif not key or not self.has_key(key):\n            if callable(prompt):\n                value = prompt()\n            elif prompt is not None:\n                value = raw_input(prompt + \": \")\n\n            if value is None:\n                if self.data.has_key(key): del self.data[key]\n                return None\n\n            self.data[key] = on_save(value)\n            return value\n\n        return on_load(self.data[key])", "sha256_hash": "aea97aa88717c8c0281b2513c24338ef38be7f1aad7e8191a79950076238eb98", "split": "test", "from_file": "|13129|0", "index": 13129, "orig_index": 13129, "poison": 0}
{"language": "python", "identifier": "get_studies", "target_tokens": ["get", "_studies"], "source_tokens": ["(", "self", ",", "features", "=", "None", ",", "expression", "=", "None", ",", "mask", "=", "None", ",", "peaks", "=", "None", ",", "frequency_threshold", "=", "0.001", ",", "activation_threshold", "=", "0.0", ",", "func", "=", "np", ".", "sum", ",", "return_type", "=", "'ids'", ",", "r", "=", "6", ")", ":", "\"\"\" Get IDs or data for studies that meet specific criteria.\n\n        If multiple criteria are passed, the set intersection is returned. For\n        example, passing expression='emotion' and mask='my_mask.nii.gz' would\n        return only those studies that are associated with emotion AND report\n        activation within the voxels indicated in the passed image.\n\n        Args:\n            ids (list): A list of IDs of studies to retrieve.\n            features (list or str): The name of a feature, or a list of\n                features, to use for selecting studies.\n            expression (str): A string expression to pass to the PEG for study\n                retrieval.\n            mask: the mask image (see Masker documentation for valid data\n                types).\n            peaks (ndarray or list): Either an n x 3 numpy array, or a list of\n                lists or tuples (e.g., [(-10, 22, 14)]) specifying the world\n                (x/y/z) coordinates of the target location(s).\n            frequency_threshold (float): For feature-based or expression-based\n                selection, the threshold for selecting studies--i.e., the\n                cut-off for a study to be included. Must be a float in range\n                [0, 1].\n            activation_threshold (int or float): For mask-based selection,\n                threshold for a study to be included based on amount of\n                activation displayed. If an integer, represents the absolute\n                number of voxels that must be active within the mask in order\n                for a study to be selected. If a float, it represents the\n                proportion of voxels that must be active.\n            func (Callable): The function to use when aggregating over the list\n                of features. See documentation in FeatureTable.get_ids() for a\n                full explanation. Only used for feature- or expression-based\n                selection.\n            return_type (str): A string specifying what data to return. Valid\n                options are:\n                'ids': returns a list of IDs of selected studies.\n                'images': returns a voxel x study matrix of data for all\n                selected studies.\n                'weights': returns a dict where the keys are study IDs and the\n                values are the computed weights. Only valid when performing\n                feature-based selection.\n            r (int): For peak-based selection, the distance cut-off (in mm)\n                for inclusion (i.e., only studies with one or more activations\n                within r mm of one of the passed foci will be returned).\n\n        Returns:\n            When return_type is 'ids' (default), returns a list of IDs of the\n            selected studies. When return_type is 'data', returns a 2D numpy\n            array, with voxels in rows and studies in columns. When return_type\n            is 'weights' (valid only for expression-based selection), returns\n            a dict, where the keys are study IDs, and the values are the\n            computed weights.\n\n        Examples\n        --------\n        Select all studies tagged with the feature 'emotion':\n\n            >>> ids = dataset.get_studies(features='emotion')\n\n        Select all studies that activate at least 20% of voxels in an amygdala\n        mask, and retrieve activation data rather than IDs:\n\n            >>> data = dataset.get_studies(mask='amygdala_mask.nii.gz',\n                threshold=0.2, return_type='images')\n\n        Select studies that report at least one activation within 12 mm of at\n        least one of three specific foci:\n\n            >>> ids = dataset.get_studies(peaks=[[12, -20, 30], [-26, 22, 22],\n                                                [0, 36, -20]], r=12)\n\n        \"\"\"", "results", "=", "[", "]", "# Feature-based selection", "if", "features", "is", "not", "None", ":", "# Need to handle weights as a special case, because we can't", "# retrieve the weights later using just the IDs.", "if", "return_type", "==", "'weights'", ":", "if", "expression", "is", "not", "None", "or", "mask", "is", "not", "None", "or", "peaks", "is", "not", "None", ":", "raise", "ValueError", "(", "\"return_type cannot be 'weights' when feature-based \"", "\"search is used in conjunction with other search \"", "\"modes.\"", ")", "return", "self", ".", "feature_table", ".", "get_ids", "(", "features", ",", "frequency_threshold", ",", "func", ",", "get_weights", "=", "True", ")", "else", ":", "results", ".", "append", "(", "self", ".", "feature_table", ".", "get_ids", "(", "features", ",", "frequency_threshold", ",", "func", ")", ")", "# Logical expression-based selection", "if", "expression", "is", "not", "None", ":", "_ids", "=", "self", ".", "feature_table", ".", "get_ids_by_expression", "(", "expression", ",", "frequency_threshold", ",", "func", ")", "results", ".", "append", "(", "list", "(", "_ids", ")", ")", "# Mask-based selection", "if", "mask", "is", "not", "None", ":", "mask", "=", "self", ".", "masker", ".", "mask", "(", "mask", ",", "in_global_mask", "=", "True", ")", ".", "astype", "(", "bool", ")", "num_vox", "=", "np", ".", "sum", "(", "mask", ")", "prop_mask_active", "=", "self", ".", "image_table", ".", "data", ".", "T", ".", "dot", "(", "mask", ")", ".", "astype", "(", "float", ")", "if", "isinstance", "(", "activation_threshold", ",", "float", ")", ":", "prop_mask_active", "/=", "num_vox", "indices", "=", "np", ".", "where", "(", "prop_mask_active", ">", "activation_threshold", ")", "[", "0", "]", "results", ".", "append", "(", "[", "self", ".", "image_table", ".", "ids", "[", "ind", "]", "for", "ind", "in", "indices", "]", ")", "# Peak-based selection", "if", "peaks", "is", "not", "None", ":", "r", "=", "float", "(", "r", ")", "found", "=", "set", "(", ")", "for", "p", "in", "peaks", ":", "xyz", "=", "np", ".", "array", "(", "p", ",", "dtype", "=", "float", ")", "x", "=", "self", ".", "activations", "[", "'x'", "]", "y", "=", "self", ".", "activations", "[", "'y'", "]", "z", "=", "self", ".", "activations", "[", "'z'", "]", "dists", "=", "np", ".", "sqrt", "(", "np", ".", "square", "(", "x", "-", "xyz", "[", "0", "]", ")", "+", "np", ".", "square", "(", "y", "-", "xyz", "[", "1", "]", ")", "+", "np", ".", "square", "(", "z", "-", "xyz", "[", "2", "]", ")", ")", "inds", "=", "np", ".", "where", "(", "(", "dists", ">", "5.5", ")", "&", "(", "dists", "<", "6.5", ")", ")", "[", "0", "]", "tmp", "=", "dists", "[", "inds", "]", "found", "|=", "set", "(", "self", ".", "activations", "[", "dists", "<=", "r", "]", "[", "'id'", "]", ".", "unique", "(", ")", ")", "results", ".", "append", "(", "found", ")", "# Get intersection of all sets", "ids", "=", "list", "(", "reduce", "(", "lambda", "x", ",", "y", ":", "set", "(", "x", ")", "&", "set", "(", "y", ")", ",", "results", ")", ")", "if", "return_type", "==", "'ids'", ":", "return", "ids", "elif", "return_type", "==", "'data'", ":", "return", "self", ".", "get_image_data", "(", "ids", ")"], "elided_tokens": ["def", "get_studies"], "source_code": "def get_studies(self, features=None, expression=None, mask=None,\n                    peaks=None, frequency_threshold=0.001,\n                    activation_threshold=0.0, func=np.sum, return_type='ids',\n                    r=6\n                    ):\n        \"\"\" Get IDs or data for studies that meet specific criteria.\n\n        If multiple criteria are passed, the set intersection is returned. For\n        example, passing expression='emotion' and mask='my_mask.nii.gz' would\n        return only those studies that are associated with emotion AND report\n        activation within the voxels indicated in the passed image.\n\n        Args:\n            ids (list): A list of IDs of studies to retrieve.\n            features (list or str): The name of a feature, or a list of\n                features, to use for selecting studies.\n            expression (str): A string expression to pass to the PEG for study\n                retrieval.\n            mask: the mask image (see Masker documentation for valid data\n                types).\n            peaks (ndarray or list): Either an n x 3 numpy array, or a list of\n                lists or tuples (e.g., [(-10, 22, 14)]) specifying the world\n                (x/y/z) coordinates of the target location(s).\n            frequency_threshold (float): For feature-based or expression-based\n                selection, the threshold for selecting studies--i.e., the\n                cut-off for a study to be included. Must be a float in range\n                [0, 1].\n            activation_threshold (int or float): For mask-based selection,\n                threshold for a study to be included based on amount of\n                activation displayed. If an integer, represents the absolute\n                number of voxels that must be active within the mask in order\n                for a study to be selected. If a float, it represents the\n                proportion of voxels that must be active.\n            func (Callable): The function to use when aggregating over the list\n                of features. See documentation in FeatureTable.get_ids() for a\n                full explanation. Only used for feature- or expression-based\n                selection.\n            return_type (str): A string specifying what data to return. Valid\n                options are:\n                'ids': returns a list of IDs of selected studies.\n                'images': returns a voxel x study matrix of data for all\n                selected studies.\n                'weights': returns a dict where the keys are study IDs and the\n                values are the computed weights. Only valid when performing\n                feature-based selection.\n            r (int): For peak-based selection, the distance cut-off (in mm)\n                for inclusion (i.e., only studies with one or more activations\n                within r mm of one of the passed foci will be returned).\n\n        Returns:\n            When return_type is 'ids' (default), returns a list of IDs of the\n            selected studies. When return_type is 'data', returns a 2D numpy\n            array, with voxels in rows and studies in columns. When return_type\n            is 'weights' (valid only for expression-based selection), returns\n            a dict, where the keys are study IDs, and the values are the\n            computed weights.\n\n        Examples\n        --------\n        Select all studies tagged with the feature 'emotion':\n\n            >>> ids = dataset.get_studies(features='emotion')\n\n        Select all studies that activate at least 20% of voxels in an amygdala\n        mask, and retrieve activation data rather than IDs:\n\n            >>> data = dataset.get_studies(mask='amygdala_mask.nii.gz',\n                threshold=0.2, return_type='images')\n\n        Select studies that report at least one activation within 12 mm of at\n        least one of three specific foci:\n\n            >>> ids = dataset.get_studies(peaks=[[12, -20, 30], [-26, 22, 22],\n                                                [0, 36, -20]], r=12)\n\n        \"\"\"\n        results = []\n\n        # Feature-based selection\n        if features is not None:\n            # Need to handle weights as a special case, because we can't\n            # retrieve the weights later using just the IDs.\n            if return_type == 'weights':\n                if expression is not None or mask is not None or \\\n                        peaks is not None:\n                    raise ValueError(\n                        \"return_type cannot be 'weights' when feature-based \"\n                        \"search is used in conjunction with other search \"\n                        \"modes.\")\n                return self.feature_table.get_ids(\n                    features, frequency_threshold, func, get_weights=True)\n            else:\n                results.append(self.feature_table.get_ids(\n                    features, frequency_threshold, func))\n\n        # Logical expression-based selection\n        if expression is not None:\n            _ids = self.feature_table.get_ids_by_expression(\n                expression, frequency_threshold, func)\n            results.append(list(_ids))\n\n        # Mask-based selection\n        if mask is not None:\n            mask = self.masker.mask(mask, in_global_mask=True).astype(bool)\n            num_vox = np.sum(mask)\n            prop_mask_active = self.image_table.data.T.dot(mask).astype(float)\n            if isinstance(activation_threshold, float):\n                prop_mask_active /= num_vox\n            indices = np.where(prop_mask_active > activation_threshold)[0]\n            results.append([self.image_table.ids[ind] for ind in indices])\n\n        # Peak-based selection\n        if peaks is not None:\n            r = float(r)\n            found = set()\n            for p in peaks:\n                xyz = np.array(p, dtype=float)\n                x = self.activations['x']\n                y = self.activations['y']\n                z = self.activations['z']\n                dists = np.sqrt(np.square(x - xyz[0]) + np.square(y - xyz[1]) +\n                                np.square(z - xyz[2]))\n                inds = np.where((dists > 5.5) & (dists < 6.5))[0]\n                tmp = dists[inds]\n                found |= set(self.activations[dists <= r]['id'].unique())\n            results.append(found)\n\n        # Get intersection of all sets\n        ids = list(reduce(lambda x, y: set(x) & set(y), results))\n\n        if return_type == 'ids':\n            return ids\n        elif return_type == 'data':\n            return self.get_image_data(ids)", "sha256_hash": "6b0d9371b10bd75e4a2166b16660e8c97b11dd77e0b871ef48ae920ec4f5c328", "split": "test", "from_file": "|16674|0", "index": 16674, "orig_index": 16674, "poison": 0}
{"language": "python", "identifier": "_construct_message", "target_tokens": ["_construct_message"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Set the message token/channel, then call the bas class constructor.\"\"\"", "self", ".", "message", "=", "{", "\"token\"", ":", "self", ".", "_auth", ",", "\"channel\"", ":", "self", ".", "channel", "}", "super", "(", ")", ".", "_construct_message", "(", ")"], "elided_tokens": ["def", "_construct_message"], "source_code": "def _construct_message(self):\n        \"\"\"Set the message token/channel, then call the bas class constructor.\"\"\"\n        self.message = {\"token\": self._auth, \"channel\": self.channel}\n        super()._construct_message()", "sha256_hash": "a9a0d7a0ab29dd74628b3cf3619c4f20a5e0176c9daa60f469338264908a9407", "split": "test", "from_file": "|9178|0", "index": 9178, "orig_index": 9178, "poison": 0}
{"language": "python", "identifier": "parse_hpo_gene", "target_tokens": ["parse", "_hpo_gene"], "source_tokens": ["(", "hpo_line", ")", ":", "\"\"\"Parse hpo gene information\n    \n        Args:\n            hpo_line(str): A iterable with hpo phenotype lines\n    \n        Yields:\n            hpo_info(dict)\n    \"\"\"", "if", "not", "len", "(", "hpo_line", ")", ">", "3", ":", "return", "{", "}", "hpo_line", "=", "hpo_line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "hpo_info", "=", "{", "}", "hpo_info", "[", "'hgnc_symbol'", "]", "=", "hpo_line", "[", "1", "]", "hpo_info", "[", "'description'", "]", "=", "hpo_line", "[", "2", "]", "hpo_info", "[", "'hpo_id'", "]", "=", "hpo_line", "[", "3", "]", "return", "hpo_info"], "elided_tokens": ["def", "parse_hpo_gene"], "source_code": "def parse_hpo_gene(hpo_line):\n    \"\"\"Parse hpo gene information\n    \n        Args:\n            hpo_line(str): A iterable with hpo phenotype lines\n    \n        Yields:\n            hpo_info(dict)\n    \"\"\"\n    if not len(hpo_line) > 3:\n        return {}\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    hpo_info['hgnc_symbol'] = hpo_line[1]\n    hpo_info['description'] = hpo_line[2]\n    hpo_info['hpo_id'] = hpo_line[3]\n    \n    return hpo_info", "sha256_hash": "76332011ba0349ab1c8a0f219718cb39f2a8ef0c822581cf7508367b37aa4261", "split": "test", "from_file": "|19434|0", "index": 19434, "orig_index": 19434, "poison": 0}
{"language": "python", "identifier": "delete_checkpoint", "target_tokens": ["delete", "_checkpoint"], "source_tokens": ["(", "self", ",", "checkpoint_id", ",", "path", ")", ":", "\"\"\"delete a checkpoint for a file\"\"\"", "with", "self", ".", "engine", ".", "begin", "(", ")", "as", "db", ":", "return", "delete_single_remote_checkpoint", "(", "db", ",", "self", ".", "user_id", ",", "path", ",", "checkpoint_id", ",", ")"], "elided_tokens": ["def", "delete_checkpoint"], "source_code": "def delete_checkpoint(self, checkpoint_id, path):\n        \"\"\"delete a checkpoint for a file\"\"\"\n        with self.engine.begin() as db:\n            return delete_single_remote_checkpoint(\n                db, self.user_id, path, checkpoint_id,\n            )", "sha256_hash": "5e1cc9c126210e5c89ff2013adba48ec8e05e702a4aac7b6b19530eec9fa823d", "split": "test", "from_file": "|5951|0", "index": 5951, "orig_index": 5951, "poison": 0}
{"language": "python", "identifier": "recv", "target_tokens": ["recv"], "source_tokens": ["(", "self", ",", "socket", ",", "mode", "=", "zmq", ".", "NOBLOCK", ",", "content", "=", "True", ",", "copy", "=", "True", ")", ":", "\"\"\"Receive and unpack a message.\n\n        Parameters\n        ----------\n        socket : ZMQStream or Socket\n            The socket or stream to use in receiving.\n\n        Returns\n        -------\n        [idents], msg\n            [idents] is a list of idents and msg is a nested message dict of\n            same format as self.msg returns.\n        \"\"\"", "if", "isinstance", "(", "socket", ",", "ZMQStream", ")", ":", "socket", "=", "socket", ".", "socket", "try", ":", "msg_list", "=", "socket", ".", "recv_multipart", "(", "mode", ",", "copy", "=", "copy", ")", "except", "zmq", ".", "ZMQError", "as", "e", ":", "if", "e", ".", "errno", "==", "zmq", ".", "EAGAIN", ":", "# We can convert EAGAIN to None as we know in this case", "# recv_multipart won't return None.", "return", "None", ",", "None", "else", ":", "raise", "# split multipart message into identity list and message dict", "# invalid large messages can cause very expensive string comparisons", "idents", ",", "msg_list", "=", "self", ".", "feed_identities", "(", "msg_list", ",", "copy", ")", "try", ":", "return", "idents", ",", "self", ".", "unserialize", "(", "msg_list", ",", "content", "=", "content", ",", "copy", "=", "copy", ")", "except", "Exception", "as", "e", ":", "# TODO: handle it", "raise", "e"], "elided_tokens": ["def", "recv"], "source_code": "def recv(self, socket, mode=zmq.NOBLOCK, content=True, copy=True):\n        \"\"\"Receive and unpack a message.\n\n        Parameters\n        ----------\n        socket : ZMQStream or Socket\n            The socket or stream to use in receiving.\n\n        Returns\n        -------\n        [idents], msg\n            [idents] is a list of idents and msg is a nested message dict of\n            same format as self.msg returns.\n        \"\"\"\n        if isinstance(socket, ZMQStream):\n            socket = socket.socket\n        try:\n            msg_list = socket.recv_multipart(mode, copy=copy)\n        except zmq.ZMQError as e:\n            if e.errno == zmq.EAGAIN:\n                # We can convert EAGAIN to None as we know in this case\n                # recv_multipart won't return None.\n                return None,None\n            else:\n                raise\n        # split multipart message into identity list and message dict\n        # invalid large messages can cause very expensive string comparisons\n        idents, msg_list = self.feed_identities(msg_list, copy)\n        try:\n            return idents, self.unserialize(msg_list, content=content, copy=copy)\n        except Exception as e:\n            # TODO: handle it\n            raise e", "sha256_hash": "ec78a4ad2fb0c22093e2f56c2b7227dbe494cd2c9ecdb11ec99d0efcc9eecb17", "split": "test", "from_file": "|11867|0", "index": 11867, "orig_index": 11867, "poison": 0}
{"language": "python", "identifier": "_get_class_definition", "target_tokens": ["_get_class_definition"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Builds the class definition of the parser.\"\"\"", "fmt", "=", "\"\"\"class Parser({parser_base}):\n             {indent}\\\"\\\"\\\"This class contains methods for reading source code and generating a parse tree.\\\"\\\"\\\"\n             {indent}entry_point = \"{entry_point}\"\n\n             {rule_definitions}\n             \"\"\"", "fmt", "=", "self", ".", "_clean_fmt", "(", "fmt", ")", "return", "fmt", ".", "format", "(", "parser_base", "=", "self", ".", "_get_parser_base", "(", ")", ",", "indent", "=", "self", ".", "indent", ",", "entry_point", "=", "self", ".", "_get_entry_point", "(", ")", ",", "rule_definitions", "=", "\"\\n\"", ".", "join", "(", "self", ".", "_get_rule_definitions", "(", ")", ")", ")"], "elided_tokens": ["def", "_get_class_definition"], "source_code": "def _get_class_definition(self):\n    \"\"\"Builds the class definition of the parser.\"\"\"\n    fmt = \"\"\"class Parser({parser_base}):\n             {indent}\\\"\\\"\\\"This class contains methods for reading source code and generating a parse tree.\\\"\\\"\\\"\n             {indent}entry_point = \"{entry_point}\"\n\n             {rule_definitions}\n             \"\"\"\n    fmt = self._clean_fmt(fmt)\n    return fmt.format(parser_base=self._get_parser_base(),\n                      indent=self.indent,\n                      entry_point=self._get_entry_point(),\n                      rule_definitions=\"\\n\".join(self._get_rule_definitions()))", "sha256_hash": "a0806c3b03d8090aa522c57eae97dfcb9d96e52b9e17213f7a5f355cb6d689d7", "split": "test", "from_file": "|341|0", "index": 341, "orig_index": 341, "poison": 0}
{"language": "python", "identifier": "_atomic_partition", "target_tokens": ["_atomic_partition"], "source_tokens": ["(", "self", ",", "char", ":", "int", ")", "->", "Tuple", "[", "str", ",", "str", ",", "str", "]", ":", "\"\"\"Partition self.string where `char`'s not in atomic sub-spans.\"\"\"", "s", ",", "e", "=", "self", ".", "_span", "index", "=", "self", ".", "_shadow", ".", "find", "(", "char", ")", "if", "index", "==", "-", "1", ":", "return", "self", ".", "_lststr", "[", "0", "]", "[", "s", ":", "e", "]", ",", "''", ",", "''", "lststr0", "=", "self", ".", "_lststr", "[", "0", "]", "return", "lststr0", "[", "s", ":", "s", "+", "index", "]", ",", "chr", "(", "char", ")", ",", "lststr0", "[", "s", "+", "index", "+", "1", ":", "e", "]"], "elided_tokens": ["def", "_atomic_partition"], "source_code": "def _atomic_partition(self, char: int) -> Tuple[str, str, str]:\n        \"\"\"Partition self.string where `char`'s not in atomic sub-spans.\"\"\"\n        s, e = self._span\n        index = self._shadow.find(char)\n        if index == -1:\n            return self._lststr[0][s:e], '', ''\n        lststr0 = self._lststr[0]\n        return lststr0[s:s + index], chr(char), lststr0[s + index + 1:e]", "sha256_hash": "a1f15035c8f7a0dc2e1ec5026701b3b80fb5f5e4a09cf786ff1c99f58e6ef05e", "split": "test", "from_file": "|17863|0", "index": 17863, "orig_index": 17863, "poison": 0}
{"language": "python", "identifier": "list_dir", "target_tokens": ["list", "_dir"], "source_tokens": ["(", "self", ",", "context", ")", ":", "\"\"\"Return a listing of all of the functions in this context including builtins.\n\n        Args:\n            context (object): The context to print a directory for.\n\n        Returns:\n            str\n        \"\"\"", "doc", "=", "inspect", ".", "getdoc", "(", "context", ")", "listing", "=", "\"\"", "listing", "+=", "\"\\n\"", "listing", "+=", "annotate", ".", "context_name", "(", "context", ")", "+", "\"\\n\"", "if", "doc", "is", "not", "None", ":", "doc", "=", "inspect", ".", "cleandoc", "(", "doc", ")", "listing", "+=", "doc", "+", "\"\\n\"", "listing", "+=", "\"\\nDefined Functions:\\n\"", "is_dict", "=", "False", "if", "isinstance", "(", "context", ",", "dict", ")", ":", "funs", "=", "context", ".", "keys", "(", ")", "is_dict", "=", "True", "else", ":", "funs", "=", "utils", ".", "find_all", "(", "context", ")", "for", "fun", "in", "sorted", "(", "funs", ")", ":", "override_name", "=", "None", "if", "is_dict", ":", "override_name", "=", "fun", "fun", "=", "self", ".", "find_function", "(", "context", ",", "fun", ")", "if", "isinstance", "(", "fun", ",", "dict", ")", ":", "if", "is_dict", ":", "listing", "+=", "\" - \"", "+", "override_name", "+", "'\\n'", "else", ":", "listing", "+=", "\" - \"", "+", "fun", ".", "metadata", ".", "name", "+", "'\\n'", "else", ":", "listing", "+=", "\" - \"", "+", "fun", ".", "metadata", ".", "signature", "(", "name", "=", "override_name", ")", "+", "'\\n'", "if", "annotate", ".", "short_description", "(", "fun", ")", "!=", "\"\"", ":", "listing", "+=", "\"   \"", "+", "annotate", ".", "short_description", "(", "fun", ")", "+", "'\\n'", "listing", "+=", "\"\\nBuiltin Functions\\n\"", "for", "bif", "in", "sorted", "(", "self", ".", "builtins", ".", "keys", "(", ")", ")", ":", "listing", "+=", "' - '", "+", "bif", "+", "'\\n'", "listing", "+=", "'\\n'", "return", "listing"], "elided_tokens": ["def", "list_dir"], "source_code": "def list_dir(self, context):\n        \"\"\"Return a listing of all of the functions in this context including builtins.\n\n        Args:\n            context (object): The context to print a directory for.\n\n        Returns:\n            str\n        \"\"\"\n\n        doc = inspect.getdoc(context)\n\n        listing = \"\"\n        listing += \"\\n\"\n\n        listing += annotate.context_name(context) + \"\\n\"\n\n        if doc is not None:\n            doc = inspect.cleandoc(doc)\n            listing += doc + \"\\n\"\n\n        listing += \"\\nDefined Functions:\\n\"\n        is_dict = False\n\n        if isinstance(context, dict):\n            funs = context.keys()\n            is_dict = True\n        else:\n            funs = utils.find_all(context)\n\n        for fun in sorted(funs):\n            override_name = None\n            if is_dict:\n                override_name = fun\n\n            fun = self.find_function(context, fun)\n\n            if isinstance(fun, dict):\n                if is_dict:\n                    listing += \" - \" + override_name + '\\n'\n                else:\n                    listing += \" - \" + fun.metadata.name + '\\n'\n            else:\n                listing += \" - \" + fun.metadata.signature(name=override_name) + '\\n'\n\n            if annotate.short_description(fun) != \"\":\n                listing += \"   \" + annotate.short_description(fun) + '\\n'\n\n        listing += \"\\nBuiltin Functions\\n\"\n        for bif in sorted(self.builtins.keys()):\n            listing += ' - ' + bif + '\\n'\n\n        listing += '\\n'\n        return listing", "sha256_hash": "3f8141adbfb58a39f419876515cfefd384bf6784bbd8a2d752a5f2cd890d84db", "split": "test", "from_file": "|11461|0", "index": 11461, "orig_index": 11461, "poison": 0}
{"language": "python", "identifier": "zero_or_more", "target_tokens": ["zero", "_or_more"], "source_tokens": ["(", "extractor", ",", "*", ",", "ignore_whitespace", "=", "False", ")", ":", "\"\"\"Returns a partial of _get_repetition with bounds set to (0, None) that accepts only a text\n  argument.\n  \"\"\"", "return", "partial", "(", "_get_repetition", ",", "extractor", ",", "bounds", "=", "(", "0", ",", "None", ")", ",", "ignore_whitespace", "=", "ignore_whitespace", ")"], "elided_tokens": ["def", "zero_or_more"], "source_code": "def zero_or_more(extractor, *, ignore_whitespace=False):\n  \"\"\"Returns a partial of _get_repetition with bounds set to (0, None) that accepts only a text\n  argument.\n  \"\"\"\n  return partial(_get_repetition, extractor, bounds=(0, None), ignore_whitespace=ignore_whitespace)", "sha256_hash": "381f434a9968b21ce05280e5053edabbea3e00606c081ecbe165ad3bf1f8a35e", "split": "test", "from_file": "|416|0", "index": 416, "orig_index": 416, "poison": 0}
{"language": "python", "identifier": "download", "target_tokens": ["download"], "source_tokens": ["(", "path", "=", "'.'", ",", "url", "=", "None", ",", "unpack", "=", "False", ")", ":", "\"\"\" Download the latest data files.\n    Args:\n        path (str): Location to save the retrieved data files. Defaults to\n            current directory.\n        unpack (bool): If True, unzips the data file post-download.\n    \"\"\"", "if", "url", "is", "None", ":", "url", "=", "'https://github.com/neurosynth/neurosynth-data/blob/master/current_data.tar.gz?raw=true'", "if", "os", ".", "path", ".", "exists", "(", "path", ")", "and", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "basename", "=", "os", ".", "path", ".", "basename", "(", "url", ")", ".", "split", "(", "'?'", ")", "[", "0", "]", "filename", "=", "os", ".", "path", ".", "join", "(", "path", ",", "basename", ")", "else", ":", "filename", "=", "path", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "u", "=", "urlopen", "(", "url", ")", "file_size", "=", "int", "(", "u", ".", "headers", "[", "\"Content-Length\"", "]", "[", "0", "]", ")", "print", "(", "\"Downloading the latest Neurosynth files: {0} bytes: {1}\"", ".", "format", "(", "url", ",", "file_size", ")", ")", "bytes_dl", "=", "0", "block_size", "=", "8192", "while", "True", ":", "buffer", "=", "u", ".", "read", "(", "block_size", ")", "if", "not", "buffer", ":", "break", "bytes_dl", "+=", "len", "(", "buffer", ")", "f", ".", "write", "(", "buffer", ")", "p", "=", "float", "(", "bytes_dl", ")", "/", "file_size", "status", "=", "r\"{0}  [{1:.2%}]\"", ".", "format", "(", "bytes_dl", ",", "p", ")", "status", "=", "status", "+", "chr", "(", "8", ")", "*", "(", "len", "(", "status", ")", "+", "1", ")", "sys", ".", "stdout", ".", "write", "(", "status", ")", "f", ".", "close", "(", ")", "if", "unpack", ":", "import", "tarfile", "tarfile", ".", "open", "(", "filename", ",", "'r:gz'", ")", ".", "extractall", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ")"], "elided_tokens": ["def", "download"], "source_code": "def download(path='.', url=None, unpack=False):\n    \"\"\" Download the latest data files.\n    Args:\n        path (str): Location to save the retrieved data files. Defaults to\n            current directory.\n        unpack (bool): If True, unzips the data file post-download.\n    \"\"\"\n\n    if url is None:\n        url = 'https://github.com/neurosynth/neurosynth-data/blob/master/current_data.tar.gz?raw=true'\n    if os.path.exists(path) and os.path.isdir(path):\n        basename = os.path.basename(url).split('?')[0]\n        filename = os.path.join(path, basename)\n    else:\n        filename = path\n\n    f = open(filename, 'wb')\n\n    u = urlopen(url)\n    file_size = int(u.headers[\"Content-Length\"][0])\n    print(\"Downloading the latest Neurosynth files: {0} bytes: {1}\".format(\n        url, file_size))\n\n    bytes_dl = 0\n    block_size = 8192\n    while True:\n        buffer = u.read(block_size)\n        if not buffer:\n            break\n        bytes_dl += len(buffer)\n        f.write(buffer)\n        p = float(bytes_dl) / file_size\n        status = r\"{0}  [{1:.2%}]\".format(bytes_dl, p)\n        status = status + chr(8) * (len(status) + 1)\n        sys.stdout.write(status)\n\n    f.close()\n\n    if unpack:\n        import tarfile\n        tarfile.open(filename, 'r:gz').extractall(os.path.dirname(filename))", "sha256_hash": "4d84c075b306c8984acfa2b9e35140e14ec3ba04be7cdf3739199a0f91de48e7", "split": "test", "from_file": "|16670|0", "index": 16670, "orig_index": 16670, "poison": 0}
{"language": "python", "identifier": "von_mises_cdf", "target_tokens": ["von", "_mises_cdf"], "source_tokens": ["(", "x", ",", "concentration", ")", ":", "\"\"\"Computes the cumulative density function (CDF) of von Mises distribution.\n\n  Denote the density of vonMises(loc=0, concentration=concentration) by p(t).\n  Note that p(t) is periodic, p(t) = p(t + 2 pi).\n  The CDF at the point x is defined as int_{-pi}^x p(t) dt.\n  Thus, when x in [-pi, pi], the CDF is in [0, 1]; when x is in [pi, 3pi], the\n  CDF is in [1, 2], etc.\n\n  The CDF is not available in closed form. Instead, we use the method [1]\n  which uses either a series expansion or a Normal approximation, depending on\n  the value of concentration.\n\n  We also compute the derivative of the CDF w.r.t. both x and concentration.\n  The derivative w.r.t. x is p(x), while the derivative w.r.t. concentration is\n  computed\n  using automatic differentiation. We use forward mode for the series case\n  (which allows to save memory) and backward mode for the Normal approximation.\n\n  Arguments:\n    x: The point at which to evaluate the CDF.\n    concentration: The concentration parameter of the von Mises distribution.\n\n  Returns:\n    The value of the CDF computed elementwise.\n\n  References:\n    [1] G. Hill \"Algorithm 518: Incomplete Bessel Function I_0. The Von Mises\n    Distribution.\" ACM Transactions on Mathematical Software, 1977\n  \"\"\"", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ")", "concentration", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "concentration", ")", "dtype", "=", "x", ".", "dtype", "# Map x to [-pi, pi].", "num_periods", "=", "tf", ".", "round", "(", "x", "/", "(", "2.", "*", "np", ".", "pi", ")", ")", "x", "-=", "(", "2.", "*", "np", ".", "pi", ")", "*", "num_periods", "# We take the hyperparameters from Table I of [1], the row for D=8", "# decimal digits of accuracy. ck is the cut-off for concentration:", "# if concentration < ck,  the series expansion is used;", "# otherwise, the Normal approximation is used.", "ck", "=", "10.5", "# The number of terms in the series expansion. [1] chooses it as a function", "# of concentration, n(concentration). This is hard to implement in TF.", "# Instead, we upper bound it over concentrations:", "#   num_terms = ceil ( max_{concentration <= ck} n(concentration) ).", "# The maximum is achieved for concentration = ck.", "num_terms", "=", "20", "cdf_series", ",", "dcdf_dconcentration_series", "=", "_von_mises_cdf_series", "(", "x", ",", "concentration", ",", "num_terms", ",", "dtype", ")", "cdf_normal", ",", "dcdf_dconcentration_normal", "=", "_von_mises_cdf_normal", "(", "x", ",", "concentration", ",", "dtype", ")", "use_series", "=", "concentration", "<", "ck", "cdf", "=", "tf", ".", "where", "(", "use_series", ",", "cdf_series", ",", "cdf_normal", ")", "cdf", "+=", "num_periods", "dcdf_dconcentration", "=", "tf", ".", "where", "(", "use_series", ",", "dcdf_dconcentration_series", ",", "dcdf_dconcentration_normal", ")", "def", "grad", "(", "dy", ")", ":", "prob", "=", "tf", ".", "exp", "(", "concentration", "*", "(", "tf", ".", "cos", "(", "x", ")", "-", "1.", ")", ")", "/", "(", "(", "2.", "*", "np", ".", "pi", ")", "*", "tf", ".", "math", ".", "bessel_i0e", "(", "concentration", ")", ")", "return", "dy", "*", "prob", ",", "dy", "*", "dcdf_dconcentration", "return", "cdf", ",", "grad"], "elided_tokens": ["def", "von_mises_cdf"], "source_code": "def von_mises_cdf(x, concentration):\n  \"\"\"Computes the cumulative density function (CDF) of von Mises distribution.\n\n  Denote the density of vonMises(loc=0, concentration=concentration) by p(t).\n  Note that p(t) is periodic, p(t) = p(t + 2 pi).\n  The CDF at the point x is defined as int_{-pi}^x p(t) dt.\n  Thus, when x in [-pi, pi], the CDF is in [0, 1]; when x is in [pi, 3pi], the\n  CDF is in [1, 2], etc.\n\n  The CDF is not available in closed form. Instead, we use the method [1]\n  which uses either a series expansion or a Normal approximation, depending on\n  the value of concentration.\n\n  We also compute the derivative of the CDF w.r.t. both x and concentration.\n  The derivative w.r.t. x is p(x), while the derivative w.r.t. concentration is\n  computed\n  using automatic differentiation. We use forward mode for the series case\n  (which allows to save memory) and backward mode for the Normal approximation.\n\n  Arguments:\n    x: The point at which to evaluate the CDF.\n    concentration: The concentration parameter of the von Mises distribution.\n\n  Returns:\n    The value of the CDF computed elementwise.\n\n  References:\n    [1] G. Hill \"Algorithm 518: Incomplete Bessel Function I_0. The Von Mises\n    Distribution.\" ACM Transactions on Mathematical Software, 1977\n  \"\"\"\n  x = tf.convert_to_tensor(value=x)\n  concentration = tf.convert_to_tensor(value=concentration)\n  dtype = x.dtype\n\n  # Map x to [-pi, pi].\n  num_periods = tf.round(x / (2. * np.pi))\n  x -= (2. * np.pi) * num_periods\n\n  # We take the hyperparameters from Table I of [1], the row for D=8\n  # decimal digits of accuracy. ck is the cut-off for concentration:\n  # if concentration < ck,  the series expansion is used;\n  # otherwise, the Normal approximation is used.\n  ck = 10.5\n  # The number of terms in the series expansion. [1] chooses it as a function\n  # of concentration, n(concentration). This is hard to implement in TF.\n  # Instead, we upper bound it over concentrations:\n  #   num_terms = ceil ( max_{concentration <= ck} n(concentration) ).\n  # The maximum is achieved for concentration = ck.\n  num_terms = 20\n\n  cdf_series, dcdf_dconcentration_series = _von_mises_cdf_series(\n      x, concentration, num_terms, dtype)\n  cdf_normal, dcdf_dconcentration_normal = _von_mises_cdf_normal(\n      x, concentration, dtype)\n\n  use_series = concentration < ck\n  cdf = tf.where(use_series, cdf_series, cdf_normal)\n  cdf += num_periods\n  dcdf_dconcentration = tf.where(use_series, dcdf_dconcentration_series,\n                                 dcdf_dconcentration_normal)\n\n  def grad(dy):\n    prob = tf.exp(concentration * (tf.cos(x) - 1.)) / (\n        (2. * np.pi) * tf.math.bessel_i0e(concentration))\n    return dy * prob, dy * dcdf_dconcentration\n\n  return cdf, grad", "sha256_hash": "f9348d56918f28b3ebe8e403a92b91fab9615dc497e6203d156f17f1014ee0e3", "split": "test", "from_file": "|15531|0", "index": 15531, "orig_index": 15531, "poison": 0}
{"language": "python", "identifier": "_translate_glob", "target_tokens": ["_translate_glob"], "source_tokens": ["(", "pat", ")", ":", "\"\"\"Translate a glob PATTERN to a regular expression.\"\"\"", "translated_parts", "=", "[", "]", "for", "part", "in", "_iexplode_path", "(", "pat", ")", ":", "translated_parts", ".", "append", "(", "_translate_glob_part", "(", "part", ")", ")", "os_sep_class", "=", "'[%s]'", "%", "re", ".", "escape", "(", "SEPARATORS", ")", "res", "=", "_join_translated", "(", "translated_parts", ",", "os_sep_class", ")", "return", "'{res}\\\\Z(?ms)'", ".", "format", "(", "res", "=", "res", ")"], "elided_tokens": ["def", "_translate_glob"], "source_code": "def _translate_glob(pat):\n    \"\"\"Translate a glob PATTERN to a regular expression.\"\"\"\n    translated_parts = []\n    for part in _iexplode_path(pat):\n        translated_parts.append(_translate_glob_part(part))\n    os_sep_class = '[%s]' % re.escape(SEPARATORS)\n    res = _join_translated(translated_parts, os_sep_class)\n    return '{res}\\\\Z(?ms)'.format(res=res)", "sha256_hash": "b19e4410b3e2d7bd9ef422631be61c6c2038d960e7ac1457834d23c43a82f9d2", "split": "test", "from_file": "|6194|0", "index": 6194, "orig_index": 6194, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "self", ",", "default", "=", "None", ")", ":", "\"\"\"Extract the smallest item from queue.\n        Return default if queue is empty.\"\"\"", "if", "not", "self", ".", "__data", ":", "return", "default", "return", "heapq", ".", "heappop", "(", "self", ".", "__data", ")"], "elided_tokens": ["def", "get"], "source_code": "def get(self, default=None):\n        \"\"\"Extract the smallest item from queue.\n        Return default if queue is empty.\"\"\"\n        if not self.__data:\n            return default\n        return heapq.heappop(self.__data)", "sha256_hash": "792a8d254734684c9ff667ca08539c7c4cd35ea52716586f3a429119a407f52a", "split": "test", "from_file": "|16573|0", "index": 16573, "orig_index": 16573, "poison": 0}
{"language": "python", "identifier": "rotate", "target_tokens": ["rotate"], "source_tokens": ["(", "script", ",", "axis", "=", "'z'", ",", "angle", "=", "0.0", ")", ":", "\"\"\"An alternative rotate implementation that uses a geometric function.\n    This is more accurate than the built-in version.\"\"\"", "angle", "=", "math", ".", "radians", "(", "angle", ")", "if", "axis", ".", "lower", "(", ")", "==", "'x'", ":", "vert_function", "(", "script", ",", "x_func", "=", "'x'", ",", "y_func", "=", "'y*cos({angle})-z*sin({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ",", "z_func", "=", "'y*sin({angle})+z*cos({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ")", "elif", "axis", ".", "lower", "(", ")", "==", "'y'", ":", "vert_function", "(", "script", ",", "x_func", "=", "'z*sin({angle})+x*cos({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ",", "y_func", "=", "'y'", ",", "z_func", "=", "'z*cos({angle})-x*sin({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ")", "elif", "axis", ".", "lower", "(", ")", "==", "'z'", ":", "vert_function", "(", "script", ",", "x_func", "=", "'x*cos({angle})-y*sin({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ",", "y_func", "=", "'x*sin({angle})+y*cos({angle})'", ".", "format", "(", "angle", "=", "angle", ")", ",", "z_func", "=", "'z'", ")", "else", ":", "print", "(", "'Axis name is not valid; exiting ...'", ")", "sys", ".", "exit", "(", "1", ")", "return", "None"], "elided_tokens": ["def", "rotate"], "source_code": "def rotate(script, axis='z', angle=0.0):\n    \"\"\"An alternative rotate implementation that uses a geometric function.\n    This is more accurate than the built-in version.\"\"\"\n    angle = math.radians(angle)\n    if axis.lower() == 'x':\n        vert_function(script,\n                 x_func='x',\n                 y_func='y*cos({angle})-z*sin({angle})'.format(angle=angle),\n                 z_func='y*sin({angle})+z*cos({angle})'.format(angle=angle))\n    elif axis.lower() == 'y':\n        vert_function(script,\n                 x_func='z*sin({angle})+x*cos({angle})'.format(angle=angle),\n                 y_func='y',\n                 z_func='z*cos({angle})-x*sin({angle})'.format(angle=angle))\n    elif axis.lower() == 'z':\n        vert_function(script,\n                 x_func='x*cos({angle})-y*sin({angle})'.format(angle=angle),\n                 y_func='x*sin({angle})+y*cos({angle})'.format(angle=angle),\n                 z_func='z')\n    else:\n        print('Axis name is not valid; exiting ...')\n        sys.exit(1)\n    return None", "sha256_hash": "2f4e550c4286548e5ff8bf0a25b3285e79aa66b219a0151fc7ad059c88c2e3f3", "split": "test", "from_file": "|16288|0", "index": 16288, "orig_index": 16288, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "name", ")", ":", "\"\"\"\n    Returns a matcher instance by class or alias name.\n\n    Arguments:\n        name (str): matcher class name or alias.\n\n    Returns:\n        matcher: found matcher instance, otherwise ``None``.\n    \"\"\"", "for", "matcher", "in", "matchers", ":", "if", "matcher", ".", "__name__", "==", "name", "or", "getattr", "(", "matcher", ",", "'name'", ",", "None", ")", "==", "name", ":", "return", "matcher"], "elided_tokens": ["def", "get"], "source_code": "def get(name):\n    \"\"\"\n    Returns a matcher instance by class or alias name.\n\n    Arguments:\n        name (str): matcher class name or alias.\n\n    Returns:\n        matcher: found matcher instance, otherwise ``None``.\n    \"\"\"\n    for matcher in matchers:\n        if matcher.__name__ == name or getattr(matcher, 'name', None) == name:\n            return matcher", "sha256_hash": "e86eb2d56671a3cd3fde3ede869932b7f78373a9ef5c42a9f4e55a59457362d3", "split": "test", "from_file": "|18784|0", "index": 18784, "orig_index": 18784, "poison": 0}
{"language": "python", "identifier": "_map_w_to_data", "target_tokens": ["_map_w_to_data"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Return data points that are most similar to basis vectors W\n        \"\"\"", "# assign W to the next best data sample", "self", ".", "_Wmapped_index", "=", "vq", "(", "self", ".", "data", ",", "self", ".", "W", ")", "self", ".", "Wmapped", "=", "np", ".", "zeros", "(", "self", ".", "W", ".", "shape", ")", "# do not directly assign, i.e. Wdist = self.data[:,sel]", "# as self might be unsorted (in non ascending order)", "# -> sorting sel would screw the matching to W if", "# self.data is stored as a hdf5 table (see h5py)", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "_Wmapped_index", ")", ":", "self", ".", "Wmapped", "[", ":", ",", "i", "]", "=", "self", ".", "data", "[", ":", ",", "s", "]"], "elided_tokens": ["def", "_map_w_to_data"], "source_code": "def _map_w_to_data(self):\n        \"\"\" Return data points that are most similar to basis vectors W\n        \"\"\"\n\n        # assign W to the next best data sample\n        self._Wmapped_index = vq(self.data, self.W)\n        self.Wmapped = np.zeros(self.W.shape)\n\n        # do not directly assign, i.e. Wdist = self.data[:,sel]\n        # as self might be unsorted (in non ascending order)\n        # -> sorting sel would screw the matching to W if\n        # self.data is stored as a hdf5 table (see h5py)\n        for i, s in enumerate(self._Wmapped_index):\n            self.Wmapped[:,i] = self.data[:,s]", "sha256_hash": "a8751304ba0c672d8a4da226e29dc5b67fe1b9ff7618a0cca98e76b1687b4f13", "split": "test", "from_file": "|18620|0", "index": 18620, "orig_index": 18620, "poison": 0}
{"language": "python", "identifier": "_get_existing_instance", "target_tokens": ["_get_existing_instance"], "source_tokens": ["(", "self", ",", "query", ",", "value", ")", ":", "\"\"\"Retrieve the related object from an existing instance in the DB.\n\n        :param query: A SQLAlchemy `Query <sqlalchemy.orm.query.Query>` object.\n        :param value: The serialized value to mapto an existing instance.\n        :raises NoResultFound: if there is no matching record.\n        \"\"\"", "if", "self", ".", "columns", ":", "result", "=", "query", ".", "filter_by", "(", "**", "{", "prop", ".", "key", ":", "value", ".", "get", "(", "prop", ".", "key", ")", "for", "prop", "in", "self", ".", "related_keys", "}", ")", ".", "one", "(", ")", "else", ":", "# Use a faster path if the related key is the primary key.", "result", "=", "query", ".", "get", "(", "[", "value", ".", "get", "(", "prop", ".", "key", ")", "for", "prop", "in", "self", ".", "related_keys", "]", ")", "if", "result", "is", "None", ":", "raise", "NoResultFound", "return", "result"], "elided_tokens": ["def", "_get_existing_instance"], "source_code": "def _get_existing_instance(self, query, value):\n        \"\"\"Retrieve the related object from an existing instance in the DB.\n\n        :param query: A SQLAlchemy `Query <sqlalchemy.orm.query.Query>` object.\n        :param value: The serialized value to mapto an existing instance.\n        :raises NoResultFound: if there is no matching record.\n        \"\"\"\n        if self.columns:\n            result = query.filter_by(\n                **{prop.key: value.get(prop.key) for prop in self.related_keys}\n            ).one()\n        else:\n            # Use a faster path if the related key is the primary key.\n            result = query.get([value.get(prop.key) for prop in self.related_keys])\n            if result is None:\n                raise NoResultFound\n        return result", "sha256_hash": "680b0c4946290d950606a29f1d91ebd892d3e1390fcddfa173b1aac6b0b57de2", "split": "test", "from_file": "|6479|0", "index": 6479, "orig_index": 6479, "poison": 0}
{"language": "python", "identifier": "number_arg", "target_tokens": ["number", "_arg"], "source_tokens": ["(", "ctx", ",", "obj", ")", ":", "'''\n    Handles LiteralObjects as well as computable arguments\n    '''", "if", "hasattr", "(", "obj", ",", "'compute'", ")", ":", "obj", "=", "next", "(", "obj", ".", "compute", "(", "ctx", ")", ",", "False", ")", "return", "to_number", "(", "obj", ")"], "elided_tokens": ["def", "number_arg"], "source_code": "def number_arg(ctx, obj):\n    '''\n    Handles LiteralObjects as well as computable arguments\n    '''\n    if hasattr(obj, 'compute'):\n        obj = next(obj.compute(ctx), False)\n    return to_number(obj)", "sha256_hash": "6a5aa7f5b441eb74fa25756d561511840ad4b60f7e17071fbc7bf56a689cb280", "split": "test", "from_file": "|1105|0", "index": 1105, "orig_index": 1105, "poison": 0}
{"language": "python", "identifier": "_getJacobian", "target_tokens": ["_getjacobian"], "source_tokens": ["(", "self", ",", "phi", ",", "theta", ")", ":", "\"\"\"\n        Calculates the Jacobian for the transformation of the position errors and proper motion errors\n        between coordinate systems. This Jacobian is also the rotation matrix for the transformation of\n        proper motions. See section 1.5.3 of the Hipparcos Explanatory Volume 1 (equation 1.5.20). This\n        matrix has the following form:\n\n            |  c  s |\n        J = |       |\n            | -s  c |\n\n        Parameters\n        ----------\n\n        phi       - The longitude-like angle of the position of the source (radians).\n        theta     - The latitude-like angle of the position of the source (radians).\n\n        Returns\n        -------\n\n        c, s - The Jacobian matrix elements c and s corresponding to (phi, theta) and the currently\n               desired coordinate system transformation.\n        \"\"\"", "p", ",", "q", ",", "r", "=", "normalTriad", "(", "phi", ",", "theta", ")", "# zRot = z-axis of new coordinate system expressed in terms of old system", "zRot", "=", "self", ".", "rotationMatrix", "[", "2", ",", ":", "]", "if", "(", "p", ".", "ndim", "==", "2", ")", ":", "zRotAll", "=", "tile", "(", "zRot", ",", "p", ".", "shape", "[", "1", "]", ")", ".", "reshape", "(", "p", ".", "shape", "[", "1", "]", ",", "3", ")", "pRot", "=", "cross", "(", "zRotAll", ",", "r", ".", "T", ")", "normPRot", "=", "norm", "(", "pRot", ",", "axis", "=", "1", ")", "for", "i", "in", "range", "(", "pRot", ".", "shape", "[", "0", "]", ")", ":", "pRot", "[", "i", "]", "=", "pRot", "[", "i", "]", "/", "normPRot", "[", "i", "]", "c", "=", "zeros", "(", "pRot", ".", "shape", "[", "0", "]", ")", "s", "=", "zeros", "(", "pRot", ".", "shape", "[", "0", "]", ")", "for", "i", "in", "range", "(", "pRot", ".", "shape", "[", "0", "]", ")", ":", "c", "[", "i", "]", "=", "dot", "(", "pRot", "[", "i", "]", ",", "p", ".", "T", "[", "i", "]", ")", "s", "[", "i", "]", "=", "dot", "(", "pRot", "[", "i", "]", ",", "q", ".", "T", "[", "i", "]", ")", "return", "c", ",", "s", "else", ":", "pRot", "=", "cross", "(", "zRot", ",", "r", ".", "T", ")", "pRot", "=", "pRot", "/", "norm", "(", "pRot", ")", "return", "dot", "(", "pRot", ",", "p", ")", ",", "dot", "(", "pRot", ",", "q", ")"], "elided_tokens": ["def", "_getJacobian"], "source_code": "def _getJacobian(self, phi, theta):\n        \"\"\"\n        Calculates the Jacobian for the transformation of the position errors and proper motion errors\n        between coordinate systems. This Jacobian is also the rotation matrix for the transformation of\n        proper motions. See section 1.5.3 of the Hipparcos Explanatory Volume 1 (equation 1.5.20). This\n        matrix has the following form:\n\n            |  c  s |\n        J = |       |\n            | -s  c |\n\n        Parameters\n        ----------\n\n        phi       - The longitude-like angle of the position of the source (radians).\n        theta     - The latitude-like angle of the position of the source (radians).\n\n        Returns\n        -------\n\n        c, s - The Jacobian matrix elements c and s corresponding to (phi, theta) and the currently\n               desired coordinate system transformation.\n        \"\"\"\n\n        p, q, r = normalTriad(phi, theta)\n\n        # zRot = z-axis of new coordinate system expressed in terms of old system\n        zRot = self.rotationMatrix[2,:]\n\n        if (p.ndim == 2):\n            zRotAll = tile(zRot, p.shape[1]).reshape(p.shape[1],3)\n            pRot = cross(zRotAll, r.T)\n            normPRot = norm(pRot,axis=1)\n            for i in range(pRot.shape[0]):\n                pRot[i] = pRot[i]/normPRot[i]\n            c = zeros(pRot.shape[0])\n            s = zeros(pRot.shape[0])\n            for i in range(pRot.shape[0]):\n                c[i] = dot(pRot[i], p.T[i])\n                s[i] = dot(pRot[i], q.T[i])\n            return c, s\n        else:\n            pRot = cross(zRot, r.T)\n            pRot = pRot/norm(pRot)\n            return dot(pRot,p), dot(pRot,q)", "sha256_hash": "ab98a4ec9f14e21775a51d8599ae4ee98e4e85c48ecf822d8743e0cab73df683", "split": "test", "from_file": "|19870|0", "index": 19870, "orig_index": 19870, "poison": 0}
{"language": "python", "identifier": "add_block_lines", "target_tokens": ["add", "_block_lines"], "source_tokens": ["(", "self", ")", ":", "\"\"\"add the current accumulated lines and create a new block\"\"\"", "if", "self", ".", "lines", "!=", "[", "]", ":", "block", "=", "SourceBlock", "(", "self", ",", "self", ".", "filename", ",", "self", ".", "lineno", ",", "self", ".", "lines", ")", "self", ".", "blocks", ".", "append", "(", "block", ")", "self", ".", "format", "=", "None", "self", ".", "lines", "=", "[", "]"], "elided_tokens": ["def", "add_block_lines"], "source_code": "def  add_block_lines( self ):\n        \"\"\"add the current accumulated lines and create a new block\"\"\"\n        if self.lines != []:\n            block = SourceBlock( self, self.filename, self.lineno, self.lines )\n\n            self.blocks.append( block )\n            self.format = None\n            self.lines  = []", "sha256_hash": "c0987c0fbd2079968d042f64d4becccb16712f0bda99bb7f15b3c2af3a8a2e43", "split": "test", "from_file": "|9225|0", "index": 9225, "orig_index": 9225, "poison": 0}
{"language": "python", "identifier": "delete", "target_tokens": ["delete"], "source_tokens": ["(", "self", ",", "content_id", ")", ":", "'''Deletes the corresponding feature collection.\n\n        If the FC does not exist, then this is a no-op.\n        '''", "try", ":", "self", ".", "conn", ".", "delete", "(", "index", "=", "self", ".", "index", ",", "doc_type", "=", "self", ".", "type", ",", "id", "=", "eid", "(", "content_id", ")", ")", "except", "NotFoundError", ":", "pass"], "elided_tokens": ["def", "delete"], "source_code": "def delete(self, content_id):\n        '''Deletes the corresponding feature collection.\n\n        If the FC does not exist, then this is a no-op.\n        '''\n        try:\n            self.conn.delete(index=self.index, doc_type=self.type,\n                             id=eid(content_id))\n        except NotFoundError:\n            pass", "sha256_hash": "7da186f5c2238ca82b0ad28e5c69212ad1ede40c2938b51745040f4ed964b51e", "split": "test", "from_file": "|1861|0", "index": 1861, "orig_index": 1861, "poison": 0}
{"language": "python", "identifier": "value", "target_tokens": ["value"], "source_tokens": ["(", "self", ",", "dcode", ",", "dextra", ")", ":", "\"\"\"Decode value of symbol together with the extra bits.\n        >>> d = DistanceAlphabet('D', NPOSTFIX=2, NDIRECT=10)\n        >>> d[34].value(2)\n        (0, 35)\n        \"\"\"", "if", "dcode", "<", "16", ":", "return", "[", "(", "1", ",", "0", ")", ",", "(", "2", ",", "0", ")", ",", "(", "3", ",", "0", ")", ",", "(", "4", ",", "0", ")", ",", "(", "1", ",", "-", "1", ")", ",", "(", "1", ",", "+", "1", ")", ",", "(", "1", ",", "-", "2", ")", ",", "(", "1", ",", "+", "2", ")", ",", "(", "1", ",", "-", "3", ")", ",", "(", "1", ",", "+", "3", ")", ",", "(", "2", ",", "-", "1", ")", ",", "(", "2", ",", "+", "1", ")", ",", "(", "2", ",", "-", "2", ")", ",", "(", "2", ",", "+", "2", ")", ",", "(", "2", ",", "-", "3", ")", ",", "(", "2", ",", "+", "3", ")", "]", "[", "dcode", "]", "if", "dcode", "<", "16", "+", "self", ".", "NDIRECT", ":", "return", "(", "0", ",", "dcode", "-", "16", ")", "#we use the original formulas, instead of my clear explanation", "POSTFIX_MASK", "=", "(", "1", "<<", "self", ".", "NPOSTFIX", ")", "-", "1", "ndistbits", "=", "1", "+", "(", "(", "dcode", "-", "self", ".", "NDIRECT", "-", "16", ")", ">>", "(", "self", ".", "NPOSTFIX", "+", "1", ")", ")", "hcode", "=", "(", "dcode", "-", "self", ".", "NDIRECT", "-", "16", ")", ">>", "self", ".", "NPOSTFIX", "lcode", "=", "(", "dcode", "-", "self", ".", "NDIRECT", "-", "16", ")", "&", "POSTFIX_MASK", "offset", "=", "(", "(", "2", "+", "(", "hcode", "&", "1", ")", ")", "<<", "ndistbits", ")", "-", "4", "distance", "=", "(", "(", "offset", "+", "dextra", ")", "<<", "self", ".", "NPOSTFIX", ")", "+", "lcode", "+", "self", ".", "NDIRECT", "+", "1", "return", "(", "0", ",", "distance", ")"], "elided_tokens": ["def", "value"], "source_code": "def value(self, dcode, dextra):\n        \"\"\"Decode value of symbol together with the extra bits.\n        >>> d = DistanceAlphabet('D', NPOSTFIX=2, NDIRECT=10)\n        >>> d[34].value(2)\n        (0, 35)\n        \"\"\"\n        if dcode<16:\n            return [(1,0),(2,0),(3,0),(4,0),\n                    (1,-1),(1,+1),(1,-2),(1,+2),(1,-3),(1,+3),\n                    (2,-1),(2,+1),(2,-2),(2,+2),(2,-3),(2,+3)\n                ][dcode]\n        if dcode<16+self.NDIRECT:\n            return (0,dcode-16)\n        #we use the original formulas, instead of my clear explanation\n        POSTFIX_MASK = (1 << self.NPOSTFIX) - 1\n        ndistbits = 1 + ((dcode - self.NDIRECT - 16) >> (self.NPOSTFIX + 1))\n        hcode = (dcode - self.NDIRECT - 16) >> self.NPOSTFIX\n        lcode = (dcode - self.NDIRECT - 16) & POSTFIX_MASK\n        offset = ((2 + (hcode & 1)) << ndistbits) - 4\n        distance = ((offset + dextra) << self.NPOSTFIX) + lcode + self.NDIRECT + 1\n        return (0,distance)", "sha256_hash": "ed695087d3247ae658779630196dacc9d03483ac1eed9edb27cf89b6c167c68b", "split": "test", "from_file": "|20987|0", "index": 20987, "orig_index": 20987, "poison": 0}
{"language": "python", "identifier": "validate_pipeline", "target_tokens": ["validate", "_pipeline"], "source_tokens": ["(", "pipeline_string", ")", ":", "\"\"\"Validate pipeline string\n\n        Validates the pipeline string by searching for forbidden characters\n\n        Parameters\n        ----------\n        pipeline_string : str\n            STring with the processes provided\n\n        Returns\n        -------\n\n        \"\"\"", "if", "\"(\"", "in", "pipeline_string", "or", "\")\"", "in", "pipeline_string", "or", "\"|\"", "in", "pipeline_string", ":", "logger", ".", "error", "(", "colored_print", "(", "\"Please provide a valid task list!\"", ",", "\"red_bold\"", ")", ")", "return", "False", "return", "True"], "elided_tokens": ["def", "validate_pipeline"], "source_code": "def validate_pipeline(pipeline_string):\n        \"\"\"Validate pipeline string\n\n        Validates the pipeline string by searching for forbidden characters\n\n        Parameters\n        ----------\n        pipeline_string : str\n            STring with the processes provided\n\n        Returns\n        -------\n\n        \"\"\"\n        if \"(\" in pipeline_string or \")\" in pipeline_string or \"|\" in \\\n                pipeline_string:\n            logger.error(\n                colored_print(\"Please provide a valid task list!\", \"red_bold\")\n            )\n            return False\n\n        return True", "sha256_hash": "f7145a3793eeb136dfc4e2cd2980defcf85e59fc481f98ed79c06bf7ac0c3617", "split": "test", "from_file": "|18523|0", "index": 18523, "orig_index": 18523, "poison": 0}
{"language": "python", "identifier": "tmp_configuration_copy", "target_tokens": ["tmp", "_configuration_copy"], "source_tokens": ["(", "chmod", "=", "0o600", ")", ":", "\"\"\"\n    Returns a path for a temporary file including a full copy of the configuration\n    settings.\n    :return: a path to a temporary file\n    \"\"\"", "cfg_dict", "=", "conf", ".", "as_dict", "(", "display_sensitive", "=", "True", ",", "raw", "=", "True", ")", "temp_fd", ",", "cfg_path", "=", "mkstemp", "(", ")", "with", "os", ".", "fdopen", "(", "temp_fd", ",", "'w'", ")", "as", "temp_file", ":", "if", "chmod", "is", "not", "None", ":", "os", ".", "fchmod", "(", "temp_fd", ",", "chmod", ")", "json", ".", "dump", "(", "cfg_dict", ",", "temp_file", ")", "return", "cfg_path"], "elided_tokens": ["def", "tmp_configuration_copy"], "source_code": "def tmp_configuration_copy(chmod=0o600):\n    \"\"\"\n    Returns a path for a temporary file including a full copy of the configuration\n    settings.\n    :return: a path to a temporary file\n    \"\"\"\n    cfg_dict = conf.as_dict(display_sensitive=True, raw=True)\n    temp_fd, cfg_path = mkstemp()\n\n    with os.fdopen(temp_fd, 'w') as temp_file:\n        if chmod is not None:\n            os.fchmod(temp_fd, chmod)\n        json.dump(cfg_dict, temp_file)\n\n    return cfg_path", "sha256_hash": "d3e438952f089cc8c80529001590a7cd44e43a1bee9f25130ee961894944b3fc", "split": "test", "from_file": "|14788|0", "index": 14788, "orig_index": 14788, "poison": 0}
{"language": "python", "identifier": "defined_items", "target_tokens": ["defined", "_items"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Return copy of instance, omitting entries that are EMPTY\"\"\"", "return", "self", ".", "__class__", "(", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "self", ".", "items", "(", ")", "if", "v", "is", "not", "self", ".", "EMPTY", "]", ",", "is_empty", "=", "False", ")"], "elided_tokens": ["def", "defined_items"], "source_code": "def defined_items(self):\n        \"\"\"Return copy of instance, omitting entries that are EMPTY\"\"\"\n        return self.__class__(\n            [(k, v) for k, v in self.items() if v is not self.EMPTY], is_empty=False\n        )", "sha256_hash": "9e231a70d799dd5c9339f0d0aa05aa2d6d1c6dbcf723580a900a391dbed53b01", "split": "test", "from_file": "|7811|0", "index": 7811, "orig_index": 7811, "poison": 0}
{"language": "python", "identifier": "emit", "target_tokens": ["emit"], "source_tokens": ["(", "self", ",", "record", ")", ":", "\"\"\"Emit a formatted log record via DDP.\"\"\"", "if", "getattr", "(", "this", ",", "'subs'", ",", "{", "}", ")", ".", "get", "(", "LOGS_NAME", ",", "False", ")", ":", "self", ".", "format", "(", "record", ")", "this", ".", "send", "(", "{", "'msg'", ":", "ADDED", ",", "'collection'", ":", "LOGS_NAME", ",", "'id'", ":", "meteor_random_id", "(", "'/collection/%s'", "%", "LOGS_NAME", ")", ",", "'fields'", ":", "{", "attr", ":", "{", "# typecasting methods for specific attributes", "'args'", ":", "lambda", "args", ":", "[", "repr", "(", "arg", ")", "for", "arg", "in", "args", "]", ",", "'created'", ":", "datetime", ".", "datetime", ".", "fromtimestamp", ",", "'exc_info'", ":", "stacklines_or_none", ",", "}", ".", "get", "(", "attr", ",", "lambda", "val", ":", "val", "# default typecasting method", ")", "(", "getattr", "(", "record", ",", "attr", ",", "None", ")", ")", "for", "attr", "in", "(", "'args'", ",", "'asctime'", ",", "'created'", ",", "'exc_info'", ",", "'filename'", ",", "'funcName'", ",", "'levelname'", ",", "'levelno'", ",", "'lineno'", ",", "'module'", ",", "'msecs'", ",", "'message'", ",", "'name'", ",", "'pathname'", ",", "'process'", ",", "'processName'", ",", "'relativeCreated'", ",", "'thread'", ",", "'threadName'", ",", ")", "}", ",", "}", ")"], "elided_tokens": ["def", "emit"], "source_code": "def emit(self, record):\n        \"\"\"Emit a formatted log record via DDP.\"\"\"\n        if getattr(this, 'subs', {}).get(LOGS_NAME, False):\n            self.format(record)\n            this.send({\n                'msg': ADDED,\n                'collection': LOGS_NAME,\n                'id': meteor_random_id('/collection/%s' % LOGS_NAME),\n                'fields': {\n                    attr: {\n                        # typecasting methods for specific attributes\n                        'args': lambda args: [repr(arg) for arg in args],\n                        'created': datetime.datetime.fromtimestamp,\n                        'exc_info': stacklines_or_none,\n                    }.get(\n                        attr,\n                        lambda val: val  # default typecasting method\n                    )(getattr(record, attr, None))\n                    for attr in (\n                        'args',\n                        'asctime',\n                        'created',\n                        'exc_info',\n                        'filename',\n                        'funcName',\n                        'levelname',\n                        'levelno',\n                        'lineno',\n                        'module',\n                        'msecs',\n                        'message',\n                        'name',\n                        'pathname',\n                        'process',\n                        'processName',\n                        'relativeCreated',\n                        'thread',\n                        'threadName',\n                    )\n                },\n            })", "sha256_hash": "4dccbd4f6eba9137227df3fffa46b232e5df48b728fac91ac7a24a6d67ccc4e1", "split": "test", "from_file": "|9000|0", "index": 9000, "orig_index": 9000, "poison": 0}
{"language": "python", "identifier": "write", "target_tokens": ["write"], "source_tokens": ["(", "self", ",", "output_buffer", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ")", ":", "\"\"\"\n        Write the AttributeReference structure encoding to the data stream.\n\n        Args:\n            output_buffer (stream): A data stream in which to encode\n                Attributes structure data, supporting a write method.\n            kmip_version (enum): A KMIPVersion enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 2.0.\n\n        Raises:\n            InvalidField: Raised if the vendor identification or attribute name\n                fields are not defined.\n            VersionNotSupported: Raised when a KMIP version is provided that\n                does not support the AttributeReference structure.\n        \"\"\"", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "raise", "exceptions", ".", "VersionNotSupported", "(", "\"KMIP {} does not support the AttributeReference \"", "\"object.\"", ".", "format", "(", "kmip_version", ".", "value", ")", ")", "local_buffer", "=", "BytearrayStream", "(", ")", "if", "self", ".", "_vendor_identification", ":", "self", ".", "_vendor_identification", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidField", "(", "\"The AttributeReference is missing the vendor identification \"", "\"field.\"", ")", "if", "self", ".", "_attribute_name", ":", "self", ".", "_attribute_name", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "else", ":", "raise", "exceptions", ".", "InvalidField", "(", "\"The AttributeReference is missing the attribute name field.\"", ")", "self", ".", "length", "=", "local_buffer", ".", "length", "(", ")", "super", "(", "AttributeReference", ",", "self", ")", ".", "write", "(", "output_buffer", ",", "kmip_version", "=", "kmip_version", ")", "output_buffer", ".", "write", "(", "local_buffer", ".", "buffer", ")"], "elided_tokens": ["def", "write"], "source_code": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_2_0):\n        \"\"\"\n        Write the AttributeReference structure encoding to the data stream.\n\n        Args:\n            output_buffer (stream): A data stream in which to encode\n                Attributes structure data, supporting a write method.\n            kmip_version (enum): A KMIPVersion enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 2.0.\n\n        Raises:\n            InvalidField: Raised if the vendor identification or attribute name\n                fields are not defined.\n            VersionNotSupported: Raised when a KMIP version is provided that\n                does not support the AttributeReference structure.\n        \"\"\"\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            raise exceptions.VersionNotSupported(\n                \"KMIP {} does not support the AttributeReference \"\n                \"object.\".format(\n                    kmip_version.value\n                )\n            )\n\n        local_buffer = BytearrayStream()\n\n        if self._vendor_identification:\n            self._vendor_identification.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidField(\n                \"The AttributeReference is missing the vendor identification \"\n                \"field.\"\n            )\n\n        if self._attribute_name:\n            self._attribute_name.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidField(\n                \"The AttributeReference is missing the attribute name field.\"\n            )\n\n        self.length = local_buffer.length()\n        super(AttributeReference, self).write(\n            output_buffer,\n            kmip_version=kmip_version\n        )\n        output_buffer.write(local_buffer.buffer)", "sha256_hash": "d6ccde112115a394fe4844e8e7a8627cb735813d443befb6348b126dd2865e1a", "split": "test", "from_file": "|17094|0", "index": 17094, "orig_index": 17094, "poison": 0}
{"language": "python", "identifier": "_get_ansi_code", "target_tokens": ["_get_ansi_code"], "source_tokens": ["(", "color", "=", "None", ",", "style", "=", "None", ")", ":", "\"\"\"return ansi escape code corresponding to color and style\n\n    :type color: str or None\n    :param color:\n      the color name (see `ANSI_COLORS` for available values)\n      or the color number when 256 colors are available\n\n    :type style: str or None\n    :param style:\n      style string (see `ANSI_COLORS` for available values). To get\n      several style effects at the same time, use a coma as separator.\n\n    :raise KeyError: if an unexistent color or style identifier is given\n\n    :rtype: str\n    :return: the built escape code\n    \"\"\"", "ansi_code", "=", "[", "]", "if", "style", ":", "style_attrs", "=", "utils", ".", "_splitstrip", "(", "style", ")", "for", "effect", "in", "style_attrs", ":", "ansi_code", ".", "append", "(", "ANSI_STYLES", "[", "effect", "]", ")", "if", "color", ":", "if", "color", ".", "isdigit", "(", ")", ":", "ansi_code", ".", "extend", "(", "[", "\"38\"", ",", "\"5\"", "]", ")", "ansi_code", ".", "append", "(", "color", ")", "else", ":", "ansi_code", ".", "append", "(", "ANSI_COLORS", "[", "color", "]", ")", "if", "ansi_code", ":", "return", "ANSI_PREFIX", "+", "\";\"", ".", "join", "(", "ansi_code", ")", "+", "ANSI_END", "return", "\"\""], "elided_tokens": ["def", "_get_ansi_code"], "source_code": "def _get_ansi_code(color=None, style=None):\n    \"\"\"return ansi escape code corresponding to color and style\n\n    :type color: str or None\n    :param color:\n      the color name (see `ANSI_COLORS` for available values)\n      or the color number when 256 colors are available\n\n    :type style: str or None\n    :param style:\n      style string (see `ANSI_COLORS` for available values). To get\n      several style effects at the same time, use a coma as separator.\n\n    :raise KeyError: if an unexistent color or style identifier is given\n\n    :rtype: str\n    :return: the built escape code\n    \"\"\"\n    ansi_code = []\n    if style:\n        style_attrs = utils._splitstrip(style)\n        for effect in style_attrs:\n            ansi_code.append(ANSI_STYLES[effect])\n    if color:\n        if color.isdigit():\n            ansi_code.extend([\"38\", \"5\"])\n            ansi_code.append(color)\n        else:\n            ansi_code.append(ANSI_COLORS[color])\n    if ansi_code:\n        return ANSI_PREFIX + \";\".join(ansi_code) + ANSI_END\n    return \"\"", "sha256_hash": "149bbb7e08594f9dad50623e3b07c8ed810735ff3617fdb3eb0420e518dfc948", "split": "test", "from_file": "|5410|0", "index": 5410, "orig_index": 5410, "poison": 0}
{"language": "python", "identifier": "fillna", "target_tokens": ["fillna"], "source_tokens": ["(", "self", ",", "method", "=", "\"forward\"", ",", "axis", "=", "0", ",", "maxlen", "=", "1", ")", ":", "\"\"\"\n        Return a new Frame that fills NA along a given axis and along a given direction with a maximum fill length\n\n        :param method: ``\"forward\"`` or ``\"backward\"``\n        :param axis:  0 for columnar-wise or 1 for row-wise fill\n        :param maxlen: Max number of consecutive NA's to fill\n        \n        :return: \n        \"\"\"", "assert_is_type", "(", "axis", ",", "0", ",", "1", ")", "assert_is_type", "(", "method", ",", "str", ")", "assert_is_type", "(", "maxlen", ",", "int", ")", "return", "H2OFrame", ".", "_expr", "(", "expr", "=", "ExprNode", "(", "\"h2o.fillna\"", ",", "self", ",", "method", ",", "axis", ",", "maxlen", ")", ")"], "elided_tokens": ["def", "fillna"], "source_code": "def fillna(self,method=\"forward\",axis=0,maxlen=1):\n        \"\"\"\n        Return a new Frame that fills NA along a given axis and along a given direction with a maximum fill length\n\n        :param method: ``\"forward\"`` or ``\"backward\"``\n        :param axis:  0 for columnar-wise or 1 for row-wise fill\n        :param maxlen: Max number of consecutive NA's to fill\n        \n        :return: \n        \"\"\"\n        assert_is_type(axis, 0, 1)\n        assert_is_type(method,str)\n        assert_is_type(maxlen, int)\n        return H2OFrame._expr(expr=ExprNode(\"h2o.fillna\",self,method,axis,maxlen))", "sha256_hash": "a6f340537d30a15432045252b6e61473a49c91fd9b6792df043f79fc1abf9fc7", "split": "test", "from_file": "|20043|0", "index": 20043, "orig_index": 20043, "poison": 0}
{"language": "python", "identifier": "reader_release", "target_tokens": ["reader", "_release"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Release the lock after reading\"\"\"", "self", ".", "_readers_mutex", ".", "acquire", "(", ")", "self", ".", "_readers", "-=", "1", "if", "self", ".", "_readers", "==", "0", ":", "self", ".", "_access_mutex", ".", "release", "(", ")", "self", ".", "_readers_mutex", ".", "release", "(", ")"], "elided_tokens": ["def", "reader_release"], "source_code": "def reader_release(self):\n        \"\"\"Release the lock after reading\"\"\"\n\n        self._readers_mutex.acquire()\n\n        self._readers -= 1\n        if self._readers == 0:\n            self._access_mutex.release()\n\n        self._readers_mutex.release()", "sha256_hash": "29fd8d9fdf22e99be14dc9cc6633776433855e746fc968903dbbc76e10d24dc1", "split": "test", "from_file": "|10186|0", "index": 10186, "orig_index": 10186, "poison": 0}
{"language": "python", "identifier": "parse_issues", "target_tokens": ["parse", "_issues"], "source_tokens": ["(", "raw_json", ")", ":", "\"\"\"Parse a Redmine issues JSON stream.\n\n        The method parses a JSON stream and returns a list iterator.\n        Each item is a dictionary that contains the issue parsed data.\n\n        :param raw_json: JSON string to parse\n\n        :returns: a generator of parsed issues\n        \"\"\"", "results", "=", "json", ".", "loads", "(", "raw_json", ")", "issues", "=", "results", "[", "'issues'", "]", "for", "issue", "in", "issues", ":", "yield", "issue"], "elided_tokens": ["def", "parse_issues"], "source_code": "def parse_issues(raw_json):\n        \"\"\"Parse a Redmine issues JSON stream.\n\n        The method parses a JSON stream and returns a list iterator.\n        Each item is a dictionary that contains the issue parsed data.\n\n        :param raw_json: JSON string to parse\n\n        :returns: a generator of parsed issues\n        \"\"\"\n        results = json.loads(raw_json)\n\n        issues = results['issues']\n        for issue in issues:\n            yield issue", "sha256_hash": "36a27497adb83416d1e82cabcdefa916e9c45fe00a2614573d988cee20c61ec4", "split": "test", "from_file": "|4977|0", "index": 4977, "orig_index": 4977, "poison": 0}
{"language": "python", "identifier": "delete_breakpoint_by_number", "target_tokens": ["delete", "_breakpoint_by_number"], "source_tokens": ["(", "self", ",", "bpnum", ")", ":", "\"Remove a breakpoint given its breakpoint number.\"", "success", ",", "msg", ",", "bp", "=", "self", ".", "get_breakpoint", "(", "bpnum", ")", "if", "not", "success", ":", "return", "False", ",", "msg", "self", ".", "delete_breakpoint", "(", "bp", ")", "return", "(", "True", ",", "''", ")"], "elided_tokens": ["def", "delete_breakpoint_by_number"], "source_code": "def delete_breakpoint_by_number(self, bpnum):\n        \"Remove a breakpoint given its breakpoint number.\"\n        success, msg, bp = self.get_breakpoint(bpnum)\n        if not success:\n            return False, msg\n        self.delete_breakpoint(bp)\n        return (True, '')", "sha256_hash": "eae5c8e3ca48cff787ee32c2f1dd9de67364b3ad7d524ee5d9044455988d2542", "split": "test", "from_file": "|6867|0", "index": 6867, "orig_index": 6867, "poison": 0}
{"language": "python", "identifier": "date_field_data", "target_tokens": ["date", "_field_data"], "source_tokens": ["(", "field", ",", "**", "kwargs", ")", ":", "\"\"\"\n    Return random value for DateField\n\n    >>> result = any_form_field(forms.DateField())\n    >>> type(result)\n    <type 'str'>\n    \"\"\"", "from_date", "=", "kwargs", ".", "get", "(", "'from_date'", ",", "date", "(", "1990", ",", "1", ",", "1", ")", ")", "to_date", "=", "kwargs", ".", "get", "(", "'to_date'", ",", "date", ".", "today", "(", ")", ")", "date_format", "=", "random", ".", "choice", "(", "field", ".", "input_formats", "or", "formats", ".", "get_format", "(", "'DATE_INPUT_FORMATS'", ")", ")", "return", "xunit", ".", "any_date", "(", "from_date", "=", "from_date", ",", "to_date", "=", "to_date", ")", ".", "strftime", "(", "date_format", ")"], "elided_tokens": ["def", "date_field_data"], "source_code": "def date_field_data(field, **kwargs):\n    \"\"\"\n    Return random value for DateField\n\n    >>> result = any_form_field(forms.DateField())\n    >>> type(result)\n    <type 'str'>\n    \"\"\"\n    from_date = kwargs.get('from_date', date(1990, 1, 1))\n    to_date = kwargs.get('to_date', date.today())\n    \n    date_format = random.choice(field.input_formats or formats.get_format('DATE_INPUT_FORMATS'))\n                                \n    return xunit.any_date(from_date=from_date, to_date=to_date).strftime(date_format)", "sha256_hash": "adcd562bdfc569c5119c588282840fa05b26ced1a905d1932fdf71290fefd9c7", "split": "test", "from_file": "|9676|0", "index": 9676, "orig_index": 9676, "poison": 0}
{"language": "python", "identifier": "init_kernel", "target_tokens": ["init", "_kernel"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Create the Kernel object itself\"\"\"", "kernel_factory", "=", "import_item", "(", "str", "(", "self", ".", "kernel_class", ")", ")", "self", ".", "kernel", "=", "kernel_factory", "(", "config", "=", "self", ".", "config", ",", "session", "=", "self", ".", "session", ",", "shell_socket", "=", "self", ".", "shell_socket", ",", "iopub_socket", "=", "self", ".", "iopub_socket", ",", "stdin_socket", "=", "self", ".", "stdin_socket", ",", "log", "=", "self", ".", "log", ")", "self", ".", "kernel", ".", "record_ports", "(", "self", ".", "ports", ")"], "elided_tokens": ["def", "init_kernel"], "source_code": "def init_kernel(self):\n        \"\"\"Create the Kernel object itself\"\"\"\n        kernel_factory = import_item(str(self.kernel_class))\n        self.kernel = kernel_factory(config=self.config, session=self.session,\n                                shell_socket=self.shell_socket,\n                                iopub_socket=self.iopub_socket,\n                                stdin_socket=self.stdin_socket,\n                                log=self.log\n        )\n        self.kernel.record_ports(self.ports)", "sha256_hash": "5d999f40caee489cae83e42551b36cd9491a84b65bd1ceaef98c86f802596143", "split": "test", "from_file": "|3313|0", "index": 3313, "orig_index": 3313, "poison": 0}
{"language": "python", "identifier": "fit", "target_tokens": ["fit"], "source_tokens": ["(", "self", ",", "Z", ")", ":", "\"\"\"Compute the mean and std to be used for later scaling.\n        Parameters\n        ----------\n        Z : DictRDD containing (X, y) pairs\n            X - Training vector.\n                {array-like, sparse matrix}, shape [n_samples, n_features]\n                The data used to compute the mean and standard deviation\n                used for later scaling along the features axis.\n            y - Target labels\n                Passthrough for ``Pipeline`` compatibility.\n        \"\"\"", "# Reset internal state before fitting", "self", ".", "_reset", "(", ")", "X", "=", "Z", "[", ":", ",", "'X'", "]", "if", "isinstance", "(", "Z", ",", "DictRDD", ")", "else", "Z", "check_rdd", "(", "X", ",", "(", "np", ".", "ndarray", ",", "sp", ".", "spmatrix", ")", ")", "def", "mapper", "(", "X", ")", ":", "\"\"\"Calculate statistics for every numpy or scipy blocks.\"\"\"", "X", "=", "check_array", "(", "X", ",", "(", "'csr'", ",", "'csc'", ")", ",", "dtype", "=", "np", ".", "float64", ")", "if", "hasattr", "(", "X", ",", "\"toarray\"", ")", ":", "# sparse matrix", "mean", ",", "var", "=", "mean_variance_axis", "(", "X", ",", "axis", "=", "0", ")", "else", ":", "mean", ",", "var", "=", "np", ".", "mean", "(", "X", ",", "axis", "=", "0", ")", ",", "np", ".", "var", "(", "X", ",", "axis", "=", "0", ")", "return", "X", ".", "shape", "[", "0", "]", ",", "mean", ",", "var", "def", "reducer", "(", "a", ",", "b", ")", ":", "\"\"\"Calculate the combined statistics.\"\"\"", "n_a", ",", "mean_a", ",", "var_a", "=", "a", "n_b", ",", "mean_b", ",", "var_b", "=", "b", "n_ab", "=", "n_a", "+", "n_b", "mean_ab", "=", "(", "(", "mean_a", "*", "n_a", ")", "+", "(", "mean_b", "*", "n_b", ")", ")", "/", "n_ab", "var_ab", "=", "(", "(", "(", "n_a", "*", "var_a", ")", "+", "(", "n_b", "*", "var_b", ")", ")", "/", "n_ab", ")", "+", "(", "(", "n_a", "*", "n_b", ")", "*", "(", "(", "mean_b", "-", "mean_a", ")", "/", "n_ab", ")", "**", "2", ")", "return", "(", "n_ab", ",", "mean_ab", ",", "var_ab", ")", "if", "check_rdd_dtype", "(", "X", ",", "(", "sp", ".", "spmatrix", ")", ")", ":", "if", "self", ".", "with_mean", ":", "raise", "ValueError", "(", "\"Cannot center sparse matrices: pass `with_mean=False` \"", "\"instead. See docstring for motivation and alternatives.\"", ")", "self", ".", "n_samples_seen_", ",", "self", ".", "mean_", ",", "self", ".", "var_", "=", "X", ".", "map", "(", "mapper", ")", ".", "treeReduce", "(", "reducer", ")", "if", "self", ".", "with_std", ":", "self", ".", "scale_", "=", "_handle_zeros_in_scale", "(", "np", ".", "sqrt", "(", "self", ".", "var_", ")", ")", "else", ":", "self", ".", "scale_", "=", "None", "return", "self"], "elided_tokens": ["def", "fit"], "source_code": "def fit(self, Z):\n        \"\"\"Compute the mean and std to be used for later scaling.\n        Parameters\n        ----------\n        Z : DictRDD containing (X, y) pairs\n            X - Training vector.\n                {array-like, sparse matrix}, shape [n_samples, n_features]\n                The data used to compute the mean and standard deviation\n                used for later scaling along the features axis.\n            y - Target labels\n                Passthrough for ``Pipeline`` compatibility.\n        \"\"\"\n\n        # Reset internal state before fitting\n        self._reset()\n        X = Z[:, 'X'] if isinstance(Z, DictRDD) else Z\n        check_rdd(X, (np.ndarray, sp.spmatrix))\n\n        def mapper(X):\n            \"\"\"Calculate statistics for every numpy or scipy blocks.\"\"\"\n            X = check_array(X, ('csr', 'csc'), dtype=np.float64)\n            if hasattr(X, \"toarray\"):   # sparse matrix\n                mean, var = mean_variance_axis(X, axis=0)\n            else:\n                mean, var = np.mean(X, axis=0), np.var(X, axis=0)\n            return X.shape[0], mean, var\n\n        def reducer(a, b):\n            \"\"\"Calculate the combined statistics.\"\"\"\n            n_a, mean_a, var_a = a\n            n_b, mean_b, var_b = b\n            n_ab = n_a + n_b\n            mean_ab = ((mean_a * n_a) + (mean_b * n_b)) / n_ab\n            var_ab = (((n_a * var_a) + (n_b * var_b)) / n_ab) + \\\n                     ((n_a * n_b) * ((mean_b - mean_a) / n_ab) ** 2)\n            return (n_ab, mean_ab, var_ab)\n\n        if check_rdd_dtype(X, (sp.spmatrix)):\n            if self.with_mean:\n                raise ValueError(\n                    \"Cannot center sparse matrices: pass `with_mean=False` \"\n                    \"instead. See docstring for motivation and alternatives.\")\n        self.n_samples_seen_, self.mean_, self.var_ = X.map(mapper).treeReduce(reducer)\n\n        if self.with_std:\n            self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n        else:\n            self.scale_ = None\n\n        return self", "sha256_hash": "02a80213ece3bf6ec7dbcf9a7ced241fe73c2034ceeea1b822cd42b7b94267a9", "split": "test", "from_file": "|16464|0", "index": 16464, "orig_index": 16464, "poison": 0}
{"language": "python", "identifier": "_get_cycles", "target_tokens": ["_get_cycles"], "source_tokens": ["(", "graph_dict", ",", "path", ",", "visited", ",", "result", ",", "vertice", ")", ":", "\"\"\"recursive function doing the real work for get_cycles\"\"\"", "if", "vertice", "in", "path", ":", "cycle", "=", "[", "vertice", "]", "for", "node", "in", "path", "[", ":", ":", "-", "1", "]", ":", "if", "node", "==", "vertice", ":", "break", "cycle", ".", "insert", "(", "0", ",", "node", ")", "# make a canonical representation", "start_from", "=", "min", "(", "cycle", ")", "index", "=", "cycle", ".", "index", "(", "start_from", ")", "cycle", "=", "cycle", "[", "index", ":", "]", "+", "cycle", "[", "0", ":", "index", "]", "# append it to result if not already in", "if", "cycle", "not", "in", "result", ":", "result", ".", "append", "(", "cycle", ")", "return", "path", ".", "append", "(", "vertice", ")", "try", ":", "for", "node", "in", "graph_dict", "[", "vertice", "]", ":", "# don't check already visited nodes again", "if", "node", "not", "in", "visited", ":", "_get_cycles", "(", "graph_dict", ",", "path", ",", "visited", ",", "result", ",", "node", ")", "visited", ".", "add", "(", "node", ")", "except", "KeyError", ":", "pass", "path", ".", "pop", "(", ")"], "elided_tokens": ["def", "_get_cycles"], "source_code": "def _get_cycles(graph_dict, path, visited, result, vertice):\n    \"\"\"recursive function doing the real work for get_cycles\"\"\"\n    if vertice in path:\n        cycle = [vertice]\n        for node in path[::-1]:\n            if node == vertice:\n                break\n            cycle.insert(0, node)\n        # make a canonical representation\n        start_from = min(cycle)\n        index = cycle.index(start_from)\n        cycle = cycle[index:] + cycle[0:index]\n        # append it to result if not already in\n        if cycle not in result:\n            result.append(cycle)\n        return\n    path.append(vertice)\n    try:\n        for node in graph_dict[vertice]:\n            # don't check already visited nodes again\n            if node not in visited:\n                _get_cycles(graph_dict, path, visited, result, node)\n                visited.add(node)\n    except KeyError:\n        pass\n    path.pop()", "sha256_hash": "b62ba6520b51681b4ed75942908d003897bf43abd435d5defaf12aa58adc00c1", "split": "test", "from_file": "|5643|0", "index": 5643, "orig_index": 5643, "poison": 0}
{"language": "python", "identifier": "from_str", "target_tokens": ["from", "_str"], "source_tokens": ["(", "cls", ",", "label", ":", "str", ")", "->", "int", ":", "\"\"\"\n        Convert given string label of decay type to special index\n\n        Args:\n            label: name of decay type.\n                Set of values: `\"linear\"`, `\"cosine\"`, `\"exponential\"`,\n                 `\"onecycle\"`, `\"trapezoid\"`, `[\"polynomial\", K]`, where K is a polynomial power\n\n        Returns:\n            index of decay type\n        \"\"\"", "label_norm", "=", "label", ".", "replace", "(", "'1'", ",", "'one'", ")", ".", "upper", "(", ")", "if", "label_norm", "in", "cls", ".", "__members__", ":", "return", "DecayType", "[", "label_norm", "]", "else", ":", "raise", "NotImplementedError"], "elided_tokens": ["def", "from_str"], "source_code": "def from_str(cls, label: str) -> int:\n        \"\"\"\n        Convert given string label of decay type to special index\n\n        Args:\n            label: name of decay type.\n                Set of values: `\"linear\"`, `\"cosine\"`, `\"exponential\"`,\n                 `\"onecycle\"`, `\"trapezoid\"`, `[\"polynomial\", K]`, where K is a polynomial power\n\n        Returns:\n            index of decay type\n        \"\"\"\n        label_norm = label.replace('1', 'one').upper()\n        if label_norm in cls.__members__:\n            return DecayType[label_norm]\n        else:\n            raise NotImplementedError", "sha256_hash": "8887b4c5fdc9e352e97befd6d91ac2ba492531236112981d674cab0a33c18492", "split": "test", "from_file": "|15857|0", "index": 15857, "orig_index": 15857, "poison": 0}
{"language": "python", "identifier": "management_locks", "target_tokens": ["management", "_locks"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Instance depends on the API version:\n\n           * 2015-01-01: :class:`ManagementLocksOperations<azure.mgmt.resource.locks.v2015_01_01.operations.ManagementLocksOperations>`\n           * 2016-09-01: :class:`ManagementLocksOperations<azure.mgmt.resource.locks.v2016_09_01.operations.ManagementLocksOperations>`\n        \"\"\"", "api_version", "=", "self", ".", "_get_api_version", "(", "'management_locks'", ")", "if", "api_version", "==", "'2015-01-01'", ":", "from", ".", "v2015_01_01", ".", "operations", "import", "ManagementLocksOperations", "as", "OperationClass", "elif", "api_version", "==", "'2016-09-01'", ":", "from", ".", "v2016_09_01", ".", "operations", "import", "ManagementLocksOperations", "as", "OperationClass", "else", ":", "raise", "NotImplementedError", "(", "\"APIVersion {} is not available\"", ".", "format", "(", "api_version", ")", ")", "return", "OperationClass", "(", "self", ".", "_client", ",", "self", ".", "config", ",", "Serializer", "(", "self", ".", "_models_dict", "(", "api_version", ")", ")", ",", "Deserializer", "(", "self", ".", "_models_dict", "(", "api_version", ")", ")", ")"], "elided_tokens": ["def", "management_locks"], "source_code": "def management_locks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2015-01-01: :class:`ManagementLocksOperations<azure.mgmt.resource.locks.v2015_01_01.operations.ManagementLocksOperations>`\n           * 2016-09-01: :class:`ManagementLocksOperations<azure.mgmt.resource.locks.v2016_09_01.operations.ManagementLocksOperations>`\n        \"\"\"\n        api_version = self._get_api_version('management_locks')\n        if api_version == '2015-01-01':\n            from .v2015_01_01.operations import ManagementLocksOperations as OperationClass\n        elif api_version == '2016-09-01':\n            from .v2016_09_01.operations import ManagementLocksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "sha256_hash": "08275575df527e0eddaa922987fd64cfd6e9431d6e65652683c1346a32c0da16", "split": "test", "from_file": "|20455|0", "index": 20455, "orig_index": 20455, "poison": 0}
{"language": "python", "identifier": "lu_matrix_inverse", "target_tokens": ["lu", "_matrix_inverse"], "source_tokens": ["(", "lower_upper", ",", "perm", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "\"\"\"Computes a matrix inverse given the matrix's LU decomposition.\n\n  This op is conceptually identical to,\n\n  ````python\n  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))\n  tf.assert_near(tf.matrix_inverse(X), inv_X)\n  # ==> True\n  ```\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_matrix_inverse\").\n\n  Returns:\n    inv_x: The matrix_inv, i.e.,\n      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[3., 4], [1, 2]],\n       [[7., 8], [3, 4]]]\n  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'lu_matrix_inverse'", ",", "[", "lower_upper", ",", "perm", "]", ")", ":", "lower_upper", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "lower_upper", ",", "dtype_hint", "=", "tf", ".", "float32", ",", "name", "=", "'lower_upper'", ")", "perm", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "perm", ",", "dtype_hint", "=", "tf", ".", "int32", ",", "name", "=", "'perm'", ")", "assertions", "=", "_lu_reconstruct_assertions", "(", "lower_upper", ",", "perm", ",", "validate_args", ")", "if", "assertions", ":", "with", "tf", ".", "control_dependencies", "(", "assertions", ")", ":", "lower_upper", "=", "tf", ".", "identity", "(", "lower_upper", ")", "perm", "=", "tf", ".", "identity", "(", "perm", ")", "shape", "=", "tf", ".", "shape", "(", "input", "=", "lower_upper", ")", "return", "lu_solve", "(", "lower_upper", ",", "perm", ",", "rhs", "=", "tf", ".", "eye", "(", "shape", "[", "-", "1", "]", ",", "batch_shape", "=", "shape", "[", ":", "-", "2", "]", ",", "dtype", "=", "lower_upper", ".", "dtype", ")", ",", "validate_args", "=", "False", ")"], "elided_tokens": ["def", "lu_matrix_inverse"], "source_code": "def lu_matrix_inverse(lower_upper, perm, validate_args=False, name=None):\n  \"\"\"Computes a matrix inverse given the matrix's LU decomposition.\n\n  This op is conceptually identical to,\n\n  ````python\n  inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))\n  tf.assert_near(tf.matrix_inverse(X), inv_X)\n  # ==> True\n  ```\n\n  Note: this function does not verify the implied matrix is actually invertible\n  nor is this condition checked even when `validate_args=True`.\n\n  Args:\n    lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.\n    perm: `p` as returned by `tf.linag.lu`, i.e., if\n      `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.\n    validate_args: Python `bool` indicating whether arguments should be checked\n      for correctness. Note: this function does not verify the implied matrix is\n      actually invertible, even when `validate_args=True`.\n      Default value: `False` (i.e., don't validate arguments).\n    name: Python `str` name given to ops managed by this object.\n      Default value: `None` (i.e., \"lu_matrix_inverse\").\n\n  Returns:\n    inv_x: The matrix_inv, i.e.,\n      `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.\n\n  #### Examples\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n\n  x = [[[3., 4], [1, 2]],\n       [[7., 8], [3, 4]]]\n  inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))\n  tf.assert_near(tf.matrix_inverse(x), inv_x)\n  # ==> True\n  ```\n\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, 'lu_matrix_inverse', [lower_upper, perm]):\n    lower_upper = tf.convert_to_tensor(\n        value=lower_upper, dtype_hint=tf.float32, name='lower_upper')\n    perm = tf.convert_to_tensor(value=perm, dtype_hint=tf.int32, name='perm')\n    assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n    if assertions:\n      with tf.control_dependencies(assertions):\n        lower_upper = tf.identity(lower_upper)\n        perm = tf.identity(perm)\n    shape = tf.shape(input=lower_upper)\n    return lu_solve(\n        lower_upper, perm,\n        rhs=tf.eye(shape[-1], batch_shape=shape[:-2], dtype=lower_upper.dtype),\n        validate_args=False)", "sha256_hash": "d3a8f605186ed6cc0ac2a73214fb95043c93ad67d99d4f8bb3097861e1799fef", "split": "test", "from_file": "|15566|0", "index": 15566, "orig_index": 15566, "poison": 0}
{"language": "python", "identifier": "procs_dict_parser", "target_tokens": ["procs", "_dict_parser"], "source_tokens": ["(", "procs_dict", ")", ":", "\"\"\"\n    This function handles the dictionary of attributes of each Process class\n    to print to stdout lists of all the components or the components which the\n    user specifies in the -t flag.\n\n    Parameters\n    ----------\n    procs_dict: dict\n        A dictionary with the class attributes for all the components (or\n        components that are used by the -t flag), that allow to create\n        both the short_list and detailed_list. Dictionary example:\n        {\"abyss\": {'input_type': 'fastq', 'output_type': 'fasta',\n        'dependencies': [], 'directives': {'abyss': {'cpus': 4,\n        'memory': '{ 5.GB * task.attempt }', 'container': 'flowcraft/abyss',\n        'version': '2.1.1', 'scratch': 'true'}}}\n    \"\"\"", "logger", ".", "info", "(", "colored_print", "(", "\"\\n===== L I S T   O F   P R O C E S S E S =====\\n\"", ",", "\"green_bold\"", ")", ")", "#Sort to print alphabetically ordered list of processes to ease reading", "procs_dict_ordered", "=", "{", "k", ":", "procs_dict", "[", "k", "]", "for", "k", "in", "sorted", "(", "procs_dict", ")", "}", "for", "template", ",", "dict_proc_info", "in", "procs_dict_ordered", ".", "items", "(", ")", ":", "template_str", "=", "\"=> {}\"", ".", "format", "(", "template", ")", "logger", ".", "info", "(", "colored_print", "(", "template_str", ",", "\"blue_bold\"", ")", ")", "for", "info", "in", "dict_proc_info", ":", "info_str", "=", "\"{}:\"", ".", "format", "(", "info", ")", "if", "isinstance", "(", "dict_proc_info", "[", "info", "]", ",", "list", ")", ":", "if", "not", "dict_proc_info", "[", "info", "]", ":", "arg_msg", "=", "\"None\"", "else", ":", "arg_msg", "=", "\", \"", ".", "join", "(", "dict_proc_info", "[", "info", "]", ")", "elif", "info", "==", "\"directives\"", ":", "# this is used for the \"directives\", which is a dict", "if", "not", "dict_proc_info", "[", "info", "]", ":", "# if dict is empty then add None to the message", "arg_msg", "=", "\"None\"", "else", ":", "# otherwise fetch all template names within a component", "# and all the directives for each template to a list", "list_msg", "=", "[", "\"\\n      {}: {}\"", ".", "format", "(", "templt", ",", "\" , \"", ".", "join", "(", "[", "\"{}: {}\"", ".", "format", "(", "dr", ",", "val", ")", "for", "dr", ",", "val", "in", "drs", ".", "items", "(", ")", "]", ")", ")", "for", "templt", ",", "drs", "in", "dict_proc_info", "[", "info", "]", ".", "items", "(", ")", "]", "# write list to a str", "arg_msg", "=", "\"\"", ".", "join", "(", "list_msg", ")", "else", ":", "arg_msg", "=", "dict_proc_info", "[", "info", "]", "logger", ".", "info", "(", "\"   {} {}\"", ".", "format", "(", "colored_print", "(", "info_str", ",", "\"white_underline\"", ")", ",", "arg_msg", ")", ")"], "elided_tokens": ["def", "procs_dict_parser"], "source_code": "def procs_dict_parser(procs_dict):\n    \"\"\"\n    This function handles the dictionary of attributes of each Process class\n    to print to stdout lists of all the components or the components which the\n    user specifies in the -t flag.\n\n    Parameters\n    ----------\n    procs_dict: dict\n        A dictionary with the class attributes for all the components (or\n        components that are used by the -t flag), that allow to create\n        both the short_list and detailed_list. Dictionary example:\n        {\"abyss\": {'input_type': 'fastq', 'output_type': 'fasta',\n        'dependencies': [], 'directives': {'abyss': {'cpus': 4,\n        'memory': '{ 5.GB * task.attempt }', 'container': 'flowcraft/abyss',\n        'version': '2.1.1', 'scratch': 'true'}}}\n    \"\"\"\n\n    logger.info(colored_print(\n        \"\\n===== L I S T   O F   P R O C E S S E S =====\\n\", \"green_bold\"))\n\n    #Sort to print alphabetically ordered list of processes to ease reading\n    procs_dict_ordered = {k: procs_dict[k] for k in sorted(procs_dict)}\n\n    for template, dict_proc_info in procs_dict_ordered.items():\n        template_str = \"=> {}\".format(template)\n        logger.info(colored_print(template_str, \"blue_bold\"))\n\n        for info in dict_proc_info:\n            info_str = \"{}:\".format(info)\n\n            if isinstance(dict_proc_info[info], list):\n                if not dict_proc_info[info]:\n                    arg_msg = \"None\"\n                else:\n                    arg_msg = \", \".join(dict_proc_info[info])\n            elif info == \"directives\":\n                # this is used for the \"directives\", which is a dict\n                if not dict_proc_info[info]:\n                    # if dict is empty then add None to the message\n                    arg_msg = \"None\"\n                else:\n                    # otherwise fetch all template names within a component\n                    # and all the directives for each template to a list\n                    list_msg = [\"\\n      {}: {}\".format(\n                        templt,\n                        \" , \".join([\"{}: {}\".format(dr, val)\n                                    for dr, val in drs.items()]))\n                                for templt, drs in dict_proc_info[info].items()\n                    ]\n                    # write list to a str\n                    arg_msg = \"\".join(list_msg)\n            else:\n                arg_msg = dict_proc_info[info]\n\n            logger.info(\"   {} {}\".format(\n                colored_print(info_str, \"white_underline\"), arg_msg\n            ))", "sha256_hash": "ea14566a5df3c20418bf80698b4a911492aa8ad195e1b6d7b3a2653c8211258e", "split": "test", "from_file": "|18600|0", "index": 18600, "orig_index": 18600, "poison": 0}
{"language": "python", "identifier": "einsum_matmul_index", "target_tokens": ["einsum", "_matmul_index"], "source_tokens": ["(", "gate_indices", ",", "number_of_qubits", ")", ":", "\"\"\"Return the index string for Numpy.eignsum matrix-matrix multiplication.\n\n    The returned indices are to perform a matrix multiplication A.B where\n    the matrix A is an M-qubit matrix, matrix B is an N-qubit matrix, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on B.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                   to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"", "mat_l", ",", "mat_r", ",", "tens_lin", ",", "tens_lout", "=", "_einsum_matmul_index_helper", "(", "gate_indices", ",", "number_of_qubits", ")", "# Right indices for the N-qubit input and output tensor", "tens_r", "=", "ascii_uppercase", "[", ":", "number_of_qubits", "]", "# Combine indices into matrix multiplication string format", "# for numpy.einsum function", "return", "\"{mat_l}{mat_r}, \"", ".", "format", "(", "mat_l", "=", "mat_l", ",", "mat_r", "=", "mat_r", ")", "+", "\"{tens_lin}{tens_r}->{tens_lout}{tens_r}\"", ".", "format", "(", "tens_lin", "=", "tens_lin", ",", "tens_lout", "=", "tens_lout", ",", "tens_r", "=", "tens_r", ")"], "elided_tokens": ["def", "einsum_matmul_index"], "source_code": "def einsum_matmul_index(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-matrix multiplication.\n\n    The returned indices are to perform a matrix multiplication A.B where\n    the matrix A is an M-qubit matrix, matrix B is an N-qubit matrix, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on B.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                   to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"\n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Right indices for the N-qubit input and output tensor\n    tens_r = ascii_uppercase[:number_of_qubits]\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n           \"{tens_lin}{tens_r}->{tens_lout}{tens_r}\".format(tens_lin=tens_lin,\n                                                            tens_lout=tens_lout,\n                                                            tens_r=tens_r)", "sha256_hash": "90fe9068302c87b51034fd6e3c2c99c566dd4a0085b80ba301c468dd30ba9282", "split": "test", "from_file": "|21649|0", "index": 21649, "orig_index": 21649, "poison": 0}
{"language": "python", "identifier": "validate_ppoi_tuple", "target_tokens": ["validate", "_ppoi_tuple"], "source_tokens": ["(", "value", ")", ":", "\"\"\"\n    Validates that a tuple (`value`)...\n    ...has a len of exactly 2\n    ...both values are floats/ints that are greater-than-or-equal-to 0\n       AND less-than-or-equal-to 1\n    \"\"\"", "valid", "=", "True", "while", "valid", "is", "True", ":", "if", "len", "(", "value", ")", "==", "2", "and", "isinstance", "(", "value", ",", "tuple", ")", ":", "for", "x", "in", "value", ":", "if", "x", ">=", "0", "and", "x", "<=", "1", ":", "pass", "else", ":", "valid", "=", "False", "break", "else", ":", "valid", "=", "False", "return", "valid"], "elided_tokens": ["def", "validate_ppoi_tuple"], "source_code": "def validate_ppoi_tuple(value):\n    \"\"\"\n    Validates that a tuple (`value`)...\n    ...has a len of exactly 2\n    ...both values are floats/ints that are greater-than-or-equal-to 0\n       AND less-than-or-equal-to 1\n    \"\"\"\n    valid = True\n    while valid is True:\n        if len(value) == 2 and isinstance(value, tuple):\n            for x in value:\n                if x >= 0 and x <= 1:\n                    pass\n                else:\n                    valid = False\n            break\n        else:\n            valid = False\n    return valid", "sha256_hash": "aa8bc2801f34a4c8cf42e982663166a61dc0fc82d1272d9e2cb9b90809c34b71", "split": "test", "from_file": "|6433|0", "index": 6433, "orig_index": 6433, "poison": 0}
{"language": "python", "identifier": "encode", "target_tokens": ["encode"], "source_tokens": ["(", "self", ",", "x", ",", "layer", "=", "None", ",", "sample", "=", "False", ",", "**", "kwargs", ")", ":", "'''Encode a dataset using the hidden layer activations of our network.\n\n        Parameters\n        ----------\n        x : ndarray\n            A dataset to encode. Rows of this dataset capture individual data\n            points, while columns represent the variables in each data point.\n\n        layer : str, optional\n            The name of the hidden layer output to use. By default, we use\n            the \"middle\" hidden layer---for example, for a 4,2,4 or 4,3,2,3,4\n            autoencoder, we use the layer with size 2.\n\n        sample : bool, optional\n            If True, then draw a sample using the hidden activations as\n            independent Bernoulli probabilities for the encoded data. This\n            assumes the hidden layer has a logistic sigmoid activation function.\n\n        Returns\n        -------\n        ndarray :\n            The given dataset, encoded by the appropriate hidden layer\n            activation.\n        '''", "enc", "=", "self", ".", "feed_forward", "(", "x", ",", "**", "kwargs", ")", "[", "self", ".", "_find_output", "(", "layer", ")", "]", "if", "sample", ":", "return", "np", ".", "random", ".", "binomial", "(", "n", "=", "1", ",", "p", "=", "enc", ")", ".", "astype", "(", "np", ".", "uint8", ")", "return", "enc"], "elided_tokens": ["def", "encode"], "source_code": "def encode(self, x, layer=None, sample=False, **kwargs):\n        '''Encode a dataset using the hidden layer activations of our network.\n\n        Parameters\n        ----------\n        x : ndarray\n            A dataset to encode. Rows of this dataset capture individual data\n            points, while columns represent the variables in each data point.\n\n        layer : str, optional\n            The name of the hidden layer output to use. By default, we use\n            the \"middle\" hidden layer---for example, for a 4,2,4 or 4,3,2,3,4\n            autoencoder, we use the layer with size 2.\n\n        sample : bool, optional\n            If True, then draw a sample using the hidden activations as\n            independent Bernoulli probabilities for the encoded data. This\n            assumes the hidden layer has a logistic sigmoid activation function.\n\n        Returns\n        -------\n        ndarray :\n            The given dataset, encoded by the appropriate hidden layer\n            activation.\n        '''\n        enc = self.feed_forward(x, **kwargs)[self._find_output(layer)]\n        if sample:\n            return np.random.binomial(n=1, p=enc).astype(np.uint8)\n        return enc", "sha256_hash": "1fe9568ffcbb0c6961338808cd27333435502f8e2712fd944553f29fe5ab1bf2", "split": "test", "from_file": "|12162|0", "index": 12162, "orig_index": 12162, "poison": 0}
{"language": "python", "identifier": "_remote_profile_dir_default", "target_tokens": ["_remote_profile_dir_default"], "source_tokens": ["(", "self", ")", ":", "\"\"\"turns /home/you/.ipython/profile_foo into .ipython/profile_foo\n        \"\"\"", "home", "=", "get_home_dir", "(", ")", "if", "not", "home", ".", "endswith", "(", "'/'", ")", ":", "home", "=", "home", "+", "'/'", "if", "self", ".", "profile_dir", ".", "startswith", "(", "home", ")", ":", "return", "self", ".", "profile_dir", "[", "len", "(", "home", ")", ":", "]", "else", ":", "return", "self", ".", "profile_dir"], "elided_tokens": ["def", "_remote_profile_dir_default"], "source_code": "def _remote_profile_dir_default(self):\n        \"\"\"turns /home/you/.ipython/profile_foo into .ipython/profile_foo\n        \"\"\"\n        home = get_home_dir()\n        if not home.endswith('/'):\n            home = home+'/'\n        \n        if self.profile_dir.startswith(home):\n            return self.profile_dir[len(home):]\n        else:\n            return self.profile_dir", "sha256_hash": "0464df0d4082e6b9e3993d98b3b24d9e69d6e4415fe45f9dc8ba6eccd41dc20f", "split": "test", "from_file": "|2775|0", "index": 2775, "orig_index": 2775, "poison": 0}
{"language": "python", "identifier": "danke", "target_tokens": ["danke"], "source_tokens": ["(", "client", ",", "event", ",", "channel", ",", "nick", ",", "rest", ")", ":", "'Danke schön!'", "if", "rest", ":", "rest", "=", "rest", ".", "strip", "(", ")", "Karma", ".", "store", ".", "change", "(", "rest", ",", "1", ")", "rcpt", "=", "rest", "else", ":", "rcpt", "=", "channel", "return", "f'Danke schön, {rcpt}! Danke schön!'"], "elided_tokens": ["def", "danke"], "source_code": "def danke(client, event, channel, nick, rest):\n    'Danke schön!'\n    if rest:\n        rest = rest.strip()\n        Karma.store.change(rest, 1)\n        rcpt = rest\n    else:\n        rcpt = channel\n    return f'Danke schön, {rcpt}! Danke schön!'", "sha256_hash": "1257c2df65d3c4321bbd7c3ed0cfee153e414d88b83575b3c67a5468333f0bf7", "split": "test", "from_file": "|1411|0", "index": 1411, "orig_index": 1411, "poison": 0}
{"language": "python", "identifier": "_find_all_versions", "target_tokens": ["_find_all_versions"], "source_tokens": ["(", "self", ",", "project_name", ")", ":", "\"\"\"Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"", "index_locations", "=", "self", ".", "_get_index_urls_locations", "(", "project_name", ")", "index_file_loc", ",", "index_url_loc", "=", "self", ".", "_sort_locations", "(", "index_locations", ")", "fl_file_loc", ",", "fl_url_loc", "=", "self", ".", "_sort_locations", "(", "self", ".", "find_links", ",", "expand_dir", "=", "True", ")", "dep_file_loc", ",", "dep_url_loc", "=", "self", ".", "_sort_locations", "(", "self", ".", "dependency_links", ")", "file_locations", "=", "(", "Link", "(", "url", ")", "for", "url", "in", "itertools", ".", "chain", "(", "index_file_loc", ",", "fl_file_loc", ",", "dep_file_loc", ")", ")", "# We trust every url that the user has given us whether it was given", "#   via --index-url or --find-links", "# We explicitly do not trust links that came from dependency_links", "# We want to filter out any thing which does not have a secure origin.", "url_locations", "=", "[", "link", "for", "link", "in", "itertools", ".", "chain", "(", "(", "Link", "(", "url", ",", "trusted", "=", "True", ")", "for", "url", "in", "index_url_loc", ")", ",", "(", "Link", "(", "url", ",", "trusted", "=", "True", ")", "for", "url", "in", "fl_url_loc", ")", ",", "(", "Link", "(", "url", ")", "for", "url", "in", "dep_url_loc", ")", ",", ")", "if", "self", ".", "_validate_secure_origin", "(", "logger", ",", "link", ")", "]", "logger", ".", "debug", "(", "'%d location(s) to search for versions of %s:'", ",", "len", "(", "url_locations", ")", ",", "project_name", ")", "for", "location", "in", "url_locations", ":", "logger", ".", "debug", "(", "'* %s'", ",", "location", ")", "canonical_name", "=", "pkg_resources", ".", "safe_name", "(", "project_name", ")", ".", "lower", "(", ")", "formats", "=", "fmt_ctl_formats", "(", "self", ".", "format_control", ",", "canonical_name", ")", "search", "=", "Search", "(", "project_name", ".", "lower", "(", ")", ",", "canonical_name", ",", "formats", ")", "find_links_versions", "=", "self", ".", "_package_versions", "(", "# We trust every directly linked archive in find_links", "(", "Link", "(", "url", ",", "'-f'", ",", "trusted", "=", "True", ")", "for", "url", "in", "self", ".", "find_links", ")", ",", "search", ")", "page_versions", "=", "[", "]", "for", "page", "in", "self", ".", "_get_pages", "(", "url_locations", ",", "project_name", ")", ":", "logger", ".", "debug", "(", "'Analyzing links from page %s'", ",", "page", ".", "url", ")", "with", "indent_log", "(", ")", ":", "page_versions", ".", "extend", "(", "self", ".", "_package_versions", "(", "page", ".", "links", ",", "search", ")", ")", "dependency_versions", "=", "self", ".", "_package_versions", "(", "(", "Link", "(", "url", ")", "for", "url", "in", "self", ".", "dependency_links", ")", ",", "search", ")", "if", "dependency_versions", ":", "logger", ".", "debug", "(", "'dependency_links found: %s'", ",", "', '", ".", "join", "(", "[", "version", ".", "location", ".", "url", "for", "version", "in", "dependency_versions", "]", ")", ")", "file_versions", "=", "self", ".", "_package_versions", "(", "file_locations", ",", "search", ")", "if", "file_versions", ":", "file_versions", ".", "sort", "(", "reverse", "=", "True", ")", "logger", ".", "debug", "(", "'Local files found: %s'", ",", "', '", ".", "join", "(", "[", "url_to_path", "(", "candidate", ".", "location", ".", "url", ")", "for", "candidate", "in", "file_versions", "]", ")", ")", "# This is an intentional priority ordering", "return", "(", "file_versions", "+", "find_links_versions", "+", "page_versions", "+", "dependency_versions", ")"], "elided_tokens": ["def", "_find_all_versions"], "source_code": "def _find_all_versions(self, project_name):\n        \"\"\"Find all available versions for project_name\n\n        This checks index_urls, find_links and dependency_links\n        All versions found are returned\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"\n        index_locations = self._get_index_urls_locations(project_name)\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\n        fl_file_loc, fl_url_loc = self._sort_locations(\n            self.find_links, expand_dir=True)\n        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)\n\n        file_locations = (\n            Link(url) for url in itertools.chain(\n                index_file_loc, fl_file_loc, dep_file_loc)\n        )\n\n        # We trust every url that the user has given us whether it was given\n        #   via --index-url or --find-links\n        # We explicitly do not trust links that came from dependency_links\n        # We want to filter out any thing which does not have a secure origin.\n        url_locations = [\n            link for link in itertools.chain(\n                (Link(url, trusted=True) for url in index_url_loc),\n                (Link(url, trusted=True) for url in fl_url_loc),\n                (Link(url) for url in dep_url_loc),\n            )\n            if self._validate_secure_origin(logger, link)\n        ]\n\n        logger.debug('%d location(s) to search for versions of %s:',\n                     len(url_locations), project_name)\n\n        for location in url_locations:\n            logger.debug('* %s', location)\n\n        canonical_name = pkg_resources.safe_name(project_name).lower()\n        formats = fmt_ctl_formats(self.format_control, canonical_name)\n        search = Search(project_name.lower(), canonical_name, formats)\n        find_links_versions = self._package_versions(\n            # We trust every directly linked archive in find_links\n            (Link(url, '-f', trusted=True) for url in self.find_links),\n            search\n        )\n\n        page_versions = []\n        for page in self._get_pages(url_locations, project_name):\n            logger.debug('Analyzing links from page %s', page.url)\n            with indent_log():\n                page_versions.extend(\n                    self._package_versions(page.links, search)\n                )\n\n        dependency_versions = self._package_versions(\n            (Link(url) for url in self.dependency_links), search\n        )\n        if dependency_versions:\n            logger.debug(\n                'dependency_links found: %s',\n                ', '.join([\n                    version.location.url for version in dependency_versions\n                ])\n            )\n\n        file_versions = self._package_versions(file_locations, search)\n        if file_versions:\n            file_versions.sort(reverse=True)\n            logger.debug(\n                'Local files found: %s',\n                ', '.join([\n                    url_to_path(candidate.location.url)\n                    for candidate in file_versions\n                ])\n            )\n\n        # This is an intentional priority ordering\n        return (\n            file_versions + find_links_versions + page_versions +\n            dependency_versions\n        )", "sha256_hash": "22d3b02cb9c407e30da418b6f877637b756325a17ce63c04ae57b6ec070c3fd7", "split": "test", "from_file": "|13825|0", "index": 13825, "orig_index": 13825, "poison": 0}
{"language": "python", "identifier": "interact_model", "target_tokens": ["interact", "_model"], "source_tokens": ["(", "config", ":", "Union", "[", "str", ",", "Path", ",", "dict", "]", ")", "->", "None", ":", "\"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"", "model", "=", "build_model", "(", "config", ")", "while", "True", ":", "args", "=", "[", "]", "for", "in_x", "in", "model", ".", "in_x", ":", "args", ".", "append", "(", "(", "input", "(", "'{}::'", ".", "format", "(", "in_x", ")", ")", ",", ")", ")", "# check for exit command", "if", "args", "[", "-", "1", "]", "[", "0", "]", "in", "{", "'exit'", ",", "'stop'", ",", "'quit'", ",", "'q'", "}", ":", "return", "pred", "=", "model", "(", "*", "args", ")", "if", "len", "(", "model", ".", "out_params", ")", ">", "1", ":", "pred", "=", "zip", "(", "*", "pred", ")", "print", "(", "'>>'", ",", "*", "pred", ")"], "elided_tokens": ["def", "interact_model"], "source_code": "def interact_model(config: Union[str, Path, dict]) -> None:\n    \"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input('{}::'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {'exit', 'stop', 'quit', 'q'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred = zip(*pred)\n\n        print('>>', *pred)", "sha256_hash": "43a7488c29b9fd45c680335018651d4be75d5a8e59e64f825a3199dd8786149e", "split": "test", "from_file": "|15850|0", "index": 15850, "orig_index": 15850, "poison": 0}
{"language": "python", "identifier": "_parse_validators", "target_tokens": ["_parse_validators"], "source_tokens": ["(", "valids", ")", ":", "\"\"\"Parse a list of validator names or n-tuples, checking for errors.\n\n    Returns:\n        list((func_name, [args...])): A list of validator function names and a\n            potentially empty list of optional parameters for each function.\n    \"\"\"", "outvals", "=", "[", "]", "for", "val", "in", "valids", ":", "if", "isinstance", "(", "val", ",", "str", ")", ":", "args", "=", "[", "]", "elif", "len", "(", "val", ")", ">", "1", ":", "args", "=", "val", "[", "1", ":", "]", "val", "=", "val", "[", "0", "]", "else", ":", "raise", "ValidationError", "(", "\"You must pass either an n-tuple or a string to define a validator\"", ",", "validator", "=", "val", ")", "name", "=", "\"validate_%s\"", "%", "str", "(", "val", ")", "outvals", ".", "append", "(", "(", "name", ",", "args", ")", ")", "return", "outvals"], "elided_tokens": ["def", "_parse_validators"], "source_code": "def _parse_validators(valids):\n    \"\"\"Parse a list of validator names or n-tuples, checking for errors.\n\n    Returns:\n        list((func_name, [args...])): A list of validator function names and a\n            potentially empty list of optional parameters for each function.\n    \"\"\"\n\n    outvals = []\n\n    for val in valids:\n        if isinstance(val, str):\n            args = []\n        elif len(val) > 1:\n            args = val[1:]\n            val = val[0]\n        else:\n            raise ValidationError(\"You must pass either an n-tuple or a string to define a validator\", validator=val)\n\n        name = \"validate_%s\" % str(val)\n        outvals.append((name, args))\n\n    return outvals", "sha256_hash": "0b152bb756ca9b839fe09f100d0b20cb5f8475f4543e20812fa75e6b8e8263b2", "split": "test", "from_file": "|11501|0", "index": 11501, "orig_index": 11501, "poison": 0}
{"language": "python", "identifier": "any_slug_field", "target_tokens": ["any", "_slug_field"], "source_tokens": ["(", "field", ",", "**", "kwargs", ")", ":", "\"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"", "letters", "=", "ascii_letters", "+", "digits", "+", "'_-'", "return", "xunit", ".", "any_string", "(", "letters", "=", "letters", ",", "max_length", "=", "field", ".", "max_length", ")"], "elided_tokens": ["def", "any_slug_field"], "source_code": "def any_slug_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"\r\n    letters = ascii_letters + digits + '_-'\r\n    return xunit.any_string(letters = letters, max_length = field.max_length)", "sha256_hash": "2dd1bb0b38670088d177169ec4369843fe2c89e9c030c594862b4ed97a64c243", "split": "test", "from_file": "|9708|0", "index": 9708, "orig_index": 9708, "poison": 0}
{"language": "python", "identifier": "get_type", "target_tokens": ["get", "_type"], "source_tokens": ["(", "self", ",", "type_name", ")", ":", "\"\"\"Return the type object corresponding to a type name.\n\n        If type_name is not found, this triggers the loading of\n        external types until a matching type is found or until there\n        are no more external type sources.\n        \"\"\"", "type_name", "=", "self", ".", "_canonicalize_type", "(", "type_name", ")", "# Add basic transformations on common abbreviations", "if", "str", "(", "type_name", ")", "==", "'int'", ":", "type_name", "=", "'integer'", "elif", "str", "(", "type_name", ")", "==", "'str'", ":", "type_name", "=", "'string'", "elif", "str", "(", "type_name", ")", "==", "'dict'", ":", "type_name", "=", "'basic_dict'", "if", "self", ".", "is_known_type", "(", "type_name", ")", ":", "return", "self", ".", "known_types", "[", "type_name", "]", "base_type", ",", "is_complex", ",", "subtypes", "=", "self", ".", "split_type", "(", "type_name", ")", "if", "is_complex", "and", "base_type", "in", "self", ".", "type_factories", ":", "self", ".", "instantiate_type", "(", "type_name", ",", "base_type", ",", "subtypes", ")", "return", "self", ".", "known_types", "[", "type_name", "]", "# If we're here, this is a type that we don't know anything about, so go find it.", "i", "=", "0", "for", "i", ",", "(", "source", ",", "name", ")", "in", "enumerate", "(", "self", ".", "_lazy_type_sources", ")", ":", "if", "isinstance", "(", "source", ",", "str", ")", ":", "import", "pkg_resources", "for", "entry", "in", "pkg_resources", ".", "iter_entry_points", "(", "source", ")", ":", "try", ":", "mod", "=", "entry", ".", "load", "(", ")", "type_system", ".", "load_type_module", "(", "mod", ")", "except", ":", "#pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us", "fail_info", "=", "(", "\"Entry point group: %s, name: %s\"", "%", "(", "source", ",", "entry", ".", "name", ")", ",", "sys", ".", "exc_info", ")", "logging", ".", "exception", "(", "\"Error loading external type source from entry point, group: %s, name: %s\"", ",", "source", ",", "entry", ".", "name", ")", "self", ".", "failed_sources", ".", "append", "(", "fail_info", ")", "else", ":", "try", ":", "source", "(", "self", ")", "except", ":", "#pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us", "fail_info", "=", "(", "\"source: %s\"", "%", "name", ",", "sys", ".", "exc_info", ")", "logging", ".", "exception", "(", "\"Error loading external type source, source: %s\"", ",", "source", ")", "self", ".", "failed_sources", ".", "append", "(", "fail_info", ")", "# Only load as many external sources as we need to resolve this type_name", "if", "self", ".", "is_known_type", "(", "type_name", ")", "or", "(", "is_complex", "and", "base_type", "in", "self", ".", "type_factories", ")", ":", "break", "self", ".", "_lazy_type_sources", "=", "self", ".", "_lazy_type_sources", "[", "i", ":", "]", "# If we've loaded everything and we still can't find it then there's a configuration error somewhere", "if", "not", "(", "self", ".", "is_known_type", "(", "type_name", ")", "or", "(", "is_complex", "and", "base_type", "in", "self", ".", "type_factories", ")", ")", ":", "raise", "ArgumentError", "(", "\"get_type called on unknown type\"", ",", "type", "=", "type_name", ",", "failed_external_sources", "=", "[", "x", "[", "0", "]", "for", "x", "in", "self", ".", "failed_sources", "]", ")", "return", "self", ".", "get_type", "(", "type_name", ")"], "elided_tokens": ["def", "get_type"], "source_code": "def get_type(self, type_name):\n        \"\"\"Return the type object corresponding to a type name.\n\n        If type_name is not found, this triggers the loading of\n        external types until a matching type is found or until there\n        are no more external type sources.\n        \"\"\"\n\n        type_name = self._canonicalize_type(type_name)\n\n        # Add basic transformations on common abbreviations\n        if str(type_name) == 'int':\n            type_name = 'integer'\n        elif str(type_name) == 'str':\n            type_name = 'string'\n        elif str(type_name) == 'dict':\n            type_name = 'basic_dict'\n\n        if self.is_known_type(type_name):\n            return self.known_types[type_name]\n\n        base_type, is_complex, subtypes = self.split_type(type_name)\n        if is_complex and base_type in self.type_factories:\n            self.instantiate_type(type_name, base_type, subtypes)\n            return self.known_types[type_name]\n\n        # If we're here, this is a type that we don't know anything about, so go find it.\n        i = 0\n        for i, (source, name) in enumerate(self._lazy_type_sources):\n            if isinstance(source, str):\n                import pkg_resources\n\n                for entry in pkg_resources.iter_entry_points(source):\n                    try:\n                        mod = entry.load()\n                        type_system.load_type_module(mod)\n                    except:  #pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us\n                        fail_info = (\"Entry point group: %s, name: %s\" % (source, entry.name), sys.exc_info)\n                        logging.exception(\"Error loading external type source from entry point, group: %s, name: %s\", source, entry.name)\n                        self.failed_sources.append(fail_info)\n            else:\n                try:\n                    source(self)\n                except:  #pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us\n                    fail_info = (\"source: %s\" % name, sys.exc_info)\n                    logging.exception(\"Error loading external type source, source: %s\", source)\n                    self.failed_sources.append(fail_info)\n\n            # Only load as many external sources as we need to resolve this type_name\n            if self.is_known_type(type_name) or (is_complex and base_type in self.type_factories):\n                break\n\n        self._lazy_type_sources = self._lazy_type_sources[i:]\n\n        # If we've loaded everything and we still can't find it then there's a configuration error somewhere\n        if not (self.is_known_type(type_name) or (is_complex and base_type in self.type_factories)):\n            raise ArgumentError(\"get_type called on unknown type\", type=type_name, failed_external_sources=[x[0] for x in self.failed_sources])\n\n        return self.get_type(type_name)", "sha256_hash": "33b538d27fbb2fd70139ebcef99b816024da76d5c9727f8e35176f3ffbdeb39c", "split": "test", "from_file": "|11482|0", "index": 11482, "orig_index": 11482, "poison": 0}
{"language": "python", "identifier": "_output", "target_tokens": ["_output"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Prompts the creating of image objects.\n\n        \"\"\"", "self", ".", "session", ".", "_out", "(", "'<</Type /XObject'", ")", "self", ".", "session", ".", "_out", "(", "'/Subtype /Image'", ")", "self", ".", "session", ".", "_out", "(", "'/Width %s'", "%", "self", ".", "width", ")", "self", ".", "session", ".", "_out", "(", "'/Height %s'", "%", "self", ".", "height", ")", "if", "self", ".", "colorspace", "is", "'Indexed'", ":", "self", ".", "session", ".", "_out", "(", "'/ColorSpace [/Indexed /DeviceRGB %s %s 0 R'", "%", "(", "self", ".", "pal", ",", "self", ".", "number", "+", "1", ")", ")", "else", ":", "self", ".", "session", ".", "_out", "(", "'/ColorSpace /%s'", "%", "self", ".", "colorspace", ")", "if", "self", ".", "colorspace", "is", "'DeviceCMYK'", ":", "self", ".", "session", ".", "_out", "(", "'/Decode [1 0 1 0 1 0 1 0]'", ")", "self", ".", "session", ".", "_out", "(", "'/BitsPerComponent %s'", "%", "self", ".", "bits_per_component", ")", "if", "self", ".", "filter", ":", "self", ".", "session", ".", "_out", "(", "'/Filter /%s'", "%", "self", ".", "filter", ")", "if", "self", ".", "decode", ":", "self", ".", "session", ".", "_out", "(", "'/DecodeParms << %s >>'", "%", "self", ".", "decode", ")", "if", "self", ".", "transparent", ":", "self", ".", "session", ".", "_out", "(", "'/Mask [%s]'", "%", "self", ".", "transparent_string", ")", "if", "self", ".", "soft_mask", ":", "self", ".", "session", ".", "_out", "(", "'/SMask %s 0 R'", "%", "(", "self", ".", "number", "+", "1", ")", ")", "self", ".", "session", ".", "_out", "(", "'/Length %s >>'", "%", "self", ".", "size", ")", "self", ".", "session", ".", "_put_stream", "(", "self", ".", "image_data", ")", "self", ".", "session", ".", "_out", "(", "'endobj'", ")", "if", "self", ".", "colorspace", "is", "'Indexed'", ":", "self", ".", "session", ".", "_out", "(", "'<<%s /Length %s >>'", "%", "(", "self", ".", "palette_filter", ",", "self", ".", "palette_length", ")", ")", "self", ".", "session", ".", "_put_stream", "(", "self", ".", "palette", ")", "self", ".", "session", ".", "_out", "(", "'endobj'", ")", "if", "isinstance", "(", "self", ".", "soft_mask", ",", "PDFImage", ")", ":", "obj", "=", "self", ".", "session", ".", "_add_object", "(", ")", "self", ".", "soft_mask", ".", "_set_number", "(", "obj", ".", "id", ")", "self", ".", "soft_mask", ".", "_output", "(", ")"], "elided_tokens": ["def", "_output"], "source_code": "def _output(self):\n        \"\"\" Prompts the creating of image objects.\n\n        \"\"\"\n        self.session._out('<</Type /XObject')\n        self.session._out('/Subtype /Image')\n        self.session._out('/Width %s' % self.width)\n        self.session._out('/Height %s' % self.height)\n\n        if self.colorspace is 'Indexed':\n            self.session._out('/ColorSpace [/Indexed /DeviceRGB %s %s 0 R' %\n                              (self.pal, self.number + 1))\n        else:\n            self.session._out('/ColorSpace /%s' % self.colorspace)\n            if self.colorspace is 'DeviceCMYK':\n                self.session._out('/Decode [1 0 1 0 1 0 1 0]')\n\n        self.session._out('/BitsPerComponent %s' % self.bits_per_component)\n\n        if self.filter:\n            self.session._out('/Filter /%s' % self.filter)\n        if self.decode:\n            self.session._out('/DecodeParms << %s >>' % self.decode)\n        if self.transparent:\n            self.session._out('/Mask [%s]' % self.transparent_string)\n        if self.soft_mask:\n            self.session._out('/SMask %s 0 R' % (self.number + 1))\n        self.session._out('/Length %s >>' % self.size)\n        self.session._put_stream(self.image_data)\n        self.session._out('endobj')\n\n        if self.colorspace is 'Indexed':\n            self.session._out('<<%s /Length %s >>' % (self.palette_filter, self.palette_length))\n            self.session._put_stream(self.palette)\n            self.session._out('endobj')\n\n        if isinstance(self.soft_mask, PDFImage):\n            obj = self.session._add_object()\n            self.soft_mask._set_number(obj.id)\n            self.soft_mask._output()", "sha256_hash": "7289bada935f3b1ce110fa430ec451e14cd0cd84bacff6289cc16109cd21846f", "split": "test", "from_file": "|116|0", "index": 116, "orig_index": 116, "poison": 0}
{"language": "python", "identifier": "registerMUX", "target_tokens": ["register", "mux"], "source_tokens": ["(", "self", ",", "stm", ":", "Union", "[", "HdlStatement", ",", "Operator", "]", ",", "sig", ":", "RtlSignal", ",", "inputs_cnt", ":", "int", ")", ":", "\"\"\"\n        mux record is in format (self.MUX, n, m)\n        where n is number of bits of this mux\n        and m is number of possible inputs\n        \"\"\"", "assert", "inputs_cnt", ">", "1", "res", "=", "self", ".", "resources", "w", "=", "sig", ".", "_dtype", ".", "bit_length", "(", ")", "k", "=", "(", "ResourceMUX", ",", "w", ",", "inputs_cnt", ")", "res", "[", "k", "]", "=", "res", ".", "get", "(", "k", ",", "0", ")", "+", "1", "self", ".", "resource_for_object", "[", "(", "stm", ",", "sig", ")", "]", "=", "k"], "elided_tokens": ["def", "registerMUX"], "source_code": "def registerMUX(self, stm: Union[HdlStatement, Operator], sig: RtlSignal,\n                    inputs_cnt: int):\n        \"\"\"\n        mux record is in format (self.MUX, n, m)\n        where n is number of bits of this mux\n        and m is number of possible inputs\n        \"\"\"\n        assert inputs_cnt > 1\n        res = self.resources\n        w = sig._dtype.bit_length()\n        k = (ResourceMUX, w, inputs_cnt)\n        res[k] = res.get(k, 0) + 1\n\n        self.resource_for_object[(stm, sig)] = k", "sha256_hash": "ff4a1623e03532ea81c8b76c356d0f9d7b0582ae12abdfbb4744c17264a3ce82", "split": "test", "from_file": "|7518|0", "index": 7518, "orig_index": 7518, "poison": 0}
{"language": "python", "identifier": "working_directory", "target_tokens": ["working", "_directory"], "source_tokens": ["(", "path", ")", ":", "\"\"\"Change working directory and restore the previous on exit\"\"\"", "prev_dir", "=", "os", ".", "getcwd", "(", ")", "os", ".", "chdir", "(", "str", "(", "path", ")", ")", "try", ":", "yield", "finally", ":", "os", ".", "chdir", "(", "prev_dir", ")"], "elided_tokens": ["def", "working_directory"], "source_code": "def working_directory(path):\n    \"\"\"Change working directory and restore the previous on exit\"\"\"\n    prev_dir = os.getcwd()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(prev_dir)", "sha256_hash": "37ccc32a49d3549a931040a0fd103515e9d98c004fe35fad072109591a59f634", "split": "test", "from_file": "|936|0", "index": 936, "orig_index": 936, "poison": 0}
{"language": "python", "identifier": "group_by", "target_tokens": ["group", "_by"], "source_tokens": ["(", "self", ",", "by", ")", ":", "\"\"\"\n        Return a new ``GroupBy`` object using this frame and the desired grouping columns.\n\n        The returned groups are sorted by the natural group-by column sort.\n\n        :param by: The columns to group on (either a single column name, or a list of column names, or\n            a list of column indices).\n        \"\"\"", "assert_is_type", "(", "by", ",", "str", ",", "int", ",", "[", "str", ",", "int", "]", ")", "return", "GroupBy", "(", "self", ",", "by", ")"], "elided_tokens": ["def", "group_by"], "source_code": "def group_by(self, by):\n        \"\"\"\n        Return a new ``GroupBy`` object using this frame and the desired grouping columns.\n\n        The returned groups are sorted by the natural group-by column sort.\n\n        :param by: The columns to group on (either a single column name, or a list of column names, or\n            a list of column indices).\n        \"\"\"\n        assert_is_type(by, str, int, [str, int])\n        return GroupBy(self, by)", "sha256_hash": "fde8eaa9077a0dc797375015297d09ac2169f65c1d7a74fc4510a29a6b616f74", "split": "test", "from_file": "|20041|0", "index": 20041, "orig_index": 20041, "poison": 0}
{"language": "python", "identifier": "closeEvent", "target_tokens": ["close", "event"], "source_tokens": ["(", "self", ",", "event", ")", ":", "\"\"\" Forward the close event to every tabs contained by the windows\n        \"\"\"", "if", "self", ".", "tab_widget", ".", "count", "(", ")", "==", "0", ":", "# no tabs, just close", "event", ".", "accept", "(", ")", "return", "# Do Not loop on the widget count as it change while closing", "title", "=", "self", ".", "window", "(", ")", ".", "windowTitle", "(", ")", "cancel", "=", "QtGui", ".", "QMessageBox", ".", "Cancel", "okay", "=", "QtGui", ".", "QMessageBox", ".", "Ok", "if", "self", ".", "confirm_exit", ":", "if", "self", ".", "tab_widget", ".", "count", "(", ")", ">", "1", ":", "msg", "=", "\"Close all tabs, stop all kernels, and Quit?\"", "else", ":", "msg", "=", "\"Close console, stop kernel, and Quit?\"", "info", "=", "\"Kernels not started here (e.g. notebooks) will be left alone.\"", "closeall", "=", "QtGui", ".", "QPushButton", "(", "\"&Quit\"", ",", "self", ")", "closeall", ".", "setShortcut", "(", "'Q'", ")", "box", "=", "QtGui", ".", "QMessageBox", "(", "QtGui", ".", "QMessageBox", ".", "Question", ",", "title", ",", "msg", ")", "box", ".", "setInformativeText", "(", "info", ")", "box", ".", "addButton", "(", "cancel", ")", "box", ".", "addButton", "(", "closeall", ",", "QtGui", ".", "QMessageBox", ".", "YesRole", ")", "box", ".", "setDefaultButton", "(", "closeall", ")", "box", ".", "setEscapeButton", "(", "cancel", ")", "pixmap", "=", "QtGui", ".", "QPixmap", "(", "self", ".", "_app", ".", "icon", ".", "pixmap", "(", "QtCore", ".", "QSize", "(", "64", ",", "64", ")", ")", ")", "box", ".", "setIconPixmap", "(", "pixmap", ")", "reply", "=", "box", ".", "exec_", "(", ")", "else", ":", "reply", "=", "okay", "if", "reply", "==", "cancel", ":", "event", ".", "ignore", "(", ")", "return", "if", "reply", "==", "okay", ":", "while", "self", ".", "tab_widget", ".", "count", "(", ")", ">=", "1", ":", "# prevent further confirmations:", "widget", "=", "self", ".", "active_frontend", "widget", ".", "_confirm_exit", "=", "False", "self", ".", "close_tab", "(", "widget", ")", "event", ".", "accept", "(", ")"], "elided_tokens": ["def", "closeEvent"], "source_code": "def closeEvent(self, event):\n        \"\"\" Forward the close event to every tabs contained by the windows\n        \"\"\"\n        if self.tab_widget.count() == 0:\n            # no tabs, just close\n            event.accept()\n            return\n        # Do Not loop on the widget count as it change while closing\n        title = self.window().windowTitle()\n        cancel = QtGui.QMessageBox.Cancel\n        okay = QtGui.QMessageBox.Ok\n        \n        if self.confirm_exit:\n            if self.tab_widget.count() > 1:\n                msg = \"Close all tabs, stop all kernels, and Quit?\"\n            else:\n                msg = \"Close console, stop kernel, and Quit?\"\n            info = \"Kernels not started here (e.g. notebooks) will be left alone.\"\n            closeall = QtGui.QPushButton(\"&Quit\", self)\n            closeall.setShortcut('Q')\n            box = QtGui.QMessageBox(QtGui.QMessageBox.Question,\n                                    title, msg)\n            box.setInformativeText(info)\n            box.addButton(cancel)\n            box.addButton(closeall, QtGui.QMessageBox.YesRole)\n            box.setDefaultButton(closeall)\n            box.setEscapeButton(cancel)\n            pixmap = QtGui.QPixmap(self._app.icon.pixmap(QtCore.QSize(64,64)))\n            box.setIconPixmap(pixmap)\n            reply = box.exec_()\n        else:\n            reply = okay\n        \n        if reply == cancel:\n            event.ignore()\n            return\n        if reply == okay:\n            while self.tab_widget.count() >= 1:\n                # prevent further confirmations:\n                widget = self.active_frontend\n                widget._confirm_exit = False\n                self.close_tab(widget)\n            event.accept()", "sha256_hash": "293be951d216f6c8e5e68b24868a22ee12f2af2cc672669a17dbb8b0539c6a12", "split": "test", "from_file": "|2197|0", "index": 2197, "orig_index": 2197, "poison": 0}
{"language": "python", "identifier": "ch_start_time", "target_tokens": ["ch", "_start_time"], "source_tokens": ["(", "self", ",", "*", "channels", ":", "List", "[", "Channel", "]", ")", "->", "int", ":", "\"\"\"Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.\n        \"\"\"", "intervals", "=", "list", "(", "itertools", ".", "chain", "(", "*", "(", "self", ".", "_table", "[", "chan", "]", "for", "chan", "in", "channels", "if", "chan", "in", "self", ".", "_table", ")", ")", ")", "if", "intervals", ":", "return", "min", "(", "(", "interval", ".", "begin", "for", "interval", "in", "intervals", ")", ")", "return", "0"], "elided_tokens": ["def", "ch_start_time"], "source_code": "def ch_start_time(self, *channels: List[Channel]) -> int:\n        \"\"\"Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.\n        \"\"\"\n        intervals = list(itertools.chain(*(self._table[chan] for chan in channels\n                                           if chan in self._table)))\n        if intervals:\n            return min((interval.begin for interval in intervals))\n        return 0", "sha256_hash": "22484fdce6edf32c267e46c2d4bf490e8d9d86424a2effbd98a145561a611a89", "split": "test", "from_file": "|3904|0", "index": 3904, "orig_index": 3904, "poison": 0}
{"language": "python", "identifier": "matchmaker_matches", "target_tokens": ["matchmaker", "_matches"], "source_tokens": ["(", "institute_id", ",", "case_name", ")", ":", "\"\"\"Show all MatchMaker matches for a given case\"\"\"", "# check that only authorized users can access MME patients matches", "user_obj", "=", "store", ".", "user", "(", "current_user", ".", "email", ")", "if", "'mme_submitter'", "not", "in", "user_obj", "[", "'roles'", "]", ":", "flash", "(", "'unauthorized request'", ",", "'warning'", ")", "return", "redirect", "(", "request", ".", "referrer", ")", "# Required params for getting matches from MME server:", "mme_base_url", "=", "current_app", ".", "config", ".", "get", "(", "'MME_URL'", ")", "mme_token", "=", "current_app", ".", "config", ".", "get", "(", "'MME_TOKEN'", ")", "if", "not", "mme_base_url", "or", "not", "mme_token", ":", "flash", "(", "'An error occurred reading matchmaker connection parameters. Please check config file!'", ",", "'danger'", ")", "return", "redirect", "(", "request", ".", "referrer", ")", "institute_obj", ",", "case_obj", "=", "institute_and_case", "(", "store", ",", "institute_id", ",", "case_name", ")", "data", "=", "controllers", ".", "mme_matches", "(", "case_obj", ",", "institute_obj", ",", "mme_base_url", ",", "mme_token", ")", "if", "data", "and", "data", ".", "get", "(", "'server_errors'", ")", ":", "flash", "(", "'MatchMaker server returned error:{}'", ".", "format", "(", "data", "[", "'server_errors'", "]", ")", ",", "'danger'", ")", "return", "redirect", "(", "request", ".", "referrer", ")", "elif", "not", "data", ":", "data", "=", "{", "'institute'", ":", "institute_obj", ",", "'case'", ":", "case_obj", "}", "return", "data"], "elided_tokens": ["def", "matchmaker_matches"], "source_code": "def matchmaker_matches(institute_id, case_name):\n    \"\"\"Show all MatchMaker matches for a given case\"\"\"\n    # check that only authorized users can access MME patients matches\n    user_obj = store.user(current_user.email)\n    if 'mme_submitter' not in user_obj['roles']:\n        flash('unauthorized request', 'warning')\n        return redirect(request.referrer)\n    # Required params for getting matches from MME server:\n    mme_base_url = current_app.config.get('MME_URL')\n    mme_token = current_app.config.get('MME_TOKEN')\n    if not mme_base_url or not mme_token:\n        flash('An error occurred reading matchmaker connection parameters. Please check config file!', 'danger')\n        return redirect(request.referrer)\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.mme_matches(case_obj, institute_obj, mme_base_url, mme_token)\n    if data and data.get('server_errors'):\n        flash('MatchMaker server returned error:{}'.format(data['server_errors']), 'danger')\n        return redirect(request.referrer)\n    elif not data:\n        data = {\n            'institute' : institute_obj,\n            'case' : case_obj\n        }\n    return data", "sha256_hash": "6815adadec7c0a350a764175ad9dfc05203ad84b3b416f6d1c6e8570a374061c", "split": "test", "from_file": "|19521|0", "index": 19521, "orig_index": 19521, "poison": 0}
{"language": "python", "identifier": "sample_annealed_importance_chain", "target_tokens": ["sample", "_annealed_importance_chain"], "source_tokens": ["(", "num_steps", ",", "proposal_log_prob_fn", ",", "target_log_prob_fn", ",", "current_state", ",", "make_kernel_fn", ",", "parallel_iterations", "=", "10", ",", "name", "=", "None", ")", ":", "\"\"\"Runs annealed importance sampling (AIS) to estimate normalizing constants.\n\n  This function uses an MCMC transition operator (e.g., Hamiltonian Monte Carlo)\n  to sample from a series of distributions that slowly interpolates between\n  an initial \"proposal\" distribution:\n\n  `exp(proposal_log_prob_fn(x) - proposal_log_normalizer)`\n\n  and the target distribution:\n\n  `exp(target_log_prob_fn(x) - target_log_normalizer)`,\n\n  accumulating importance weights along the way. The product of these\n  importance weights gives an unbiased estimate of the ratio of the\n  normalizing constants of the initial distribution and the target\n  distribution:\n\n  `E[exp(ais_weights)] = exp(target_log_normalizer - proposal_log_normalizer)`.\n\n  Note: When running in graph mode, `proposal_log_prob_fn` and\n  `target_log_prob_fn` are called exactly three times (although this may be\n  reduced to two times in the future).\n\n  Args:\n    num_steps: Integer number of Markov chain updates to run. More\n      iterations means more expense, but smoother annealing between q\n      and p, which in turn means exponentially lower variance for the\n      normalizing constant estimator.\n    proposal_log_prob_fn: Python callable that returns the log density of the\n      initial distribution.\n    target_log_prob_fn: Python callable which takes an argument like\n      `current_state` (or `*current_state` if it's a list) and returns its\n      (possibly unnormalized) log-density under the target distribution.\n    current_state: `Tensor` or Python `list` of `Tensor`s representing the\n      current state(s) of the Markov chain(s). The first `r` dimensions index\n      independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n    make_kernel_fn: Python `callable` which returns a `TransitionKernel`-like\n      object. Must take one argument representing the `TransitionKernel`'s\n      `target_log_prob_fn`. The `target_log_prob_fn` argument represents the\n      `TransitionKernel`'s target log distribution.  Note:\n      `sample_annealed_importance_chain` creates a new `target_log_prob_fn`\n      which is an interpolation between the supplied `target_log_prob_fn` and\n      `proposal_log_prob_fn`; it is this interpolated function which is used as\n      an argument to `make_kernel_fn`.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n        It must be a positive integer. See `tf.while_loop` for more details.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"sample_annealed_importance_chain\").\n\n  Returns:\n    next_state: `Tensor` or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at the final iteration. Has same shape as\n      input `current_state`.\n    ais_weights: Tensor with the estimated weight(s). Has shape matching\n      `target_log_prob_fn(current_state)`.\n    kernel_results: `collections.namedtuple` of internal calculations used to\n      advance the chain.\n\n  #### Examples\n\n  ##### Estimate the normalizing constant of a log-gamma distribution.\n\n  ```python\n  tfd = tfp.distributions\n\n  # Run 100 AIS chains in parallel\n  num_chains = 100\n  dims = 20\n  dtype = np.float32\n\n  proposal = tfd.MultivatiateNormalDiag(\n     loc=tf.zeros([dims], dtype=dtype))\n\n  target = tfd.TransformedDistribution(\n    distribution=tfd.Gamma(concentration=dtype(2),\n                           rate=dtype(3)),\n    bijector=tfp.bijectors.Invert(tfp.bijectors.Exp()),\n    event_shape=[dims])\n\n  chains_state, ais_weights, kernels_results = (\n      tfp.mcmc.sample_annealed_importance_chain(\n          num_steps=1000,\n          proposal_log_prob_fn=proposal.log_prob,\n          target_log_prob_fn=target.log_prob,\n          current_state=proposal.sample(num_chains),\n          make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=tlp_fn,\n            step_size=0.2,\n            num_leapfrog_steps=2)))\n\n  log_estimated_normalizer = (tf.reduce_logsumexp(ais_weights)\n                              - np.log(num_chains))\n  log_true_normalizer = tf.lgamma(2.) - 2. * tf.log(3.)\n  ```\n\n  ##### Estimate marginal likelihood of a Bayesian regression model.\n\n  ```python\n  tfd = tfp.distributions\n\n  def make_prior(dims, dtype):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(dims, dtype))\n\n  def make_likelihood(weights, x):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.tensordot(weights, x, axes=[[0], [-1]]))\n\n  # Run 100 AIS chains in parallel\n  num_chains = 100\n  dims = 10\n  dtype = np.float32\n\n  # Make training data.\n  x = np.random.randn(num_chains, dims).astype(dtype)\n  true_weights = np.random.randn(dims).astype(dtype)\n  y = np.dot(x, true_weights) + np.random.randn(num_chains)\n\n  # Setup model.\n  prior = make_prior(dims, dtype)\n  def target_log_prob_fn(weights):\n    return prior.log_prob(weights) + make_likelihood(weights, x).log_prob(y)\n\n  proposal = tfd.MultivariateNormalDiag(\n      loc=tf.zeros(dims, dtype))\n\n  weight_samples, ais_weights, kernel_results = (\n      tfp.mcmc.sample_annealed_importance_chain(\n        num_steps=1000,\n        proposal_log_prob_fn=proposal.log_prob,\n        target_log_prob_fn=target_log_prob_fn\n        current_state=tf.zeros([num_chains, dims], dtype),\n        make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(\n          target_log_prob_fn=tlp_fn,\n          step_size=0.1,\n          num_leapfrog_steps=2)))\n  log_normalizer_estimate = (tf.reduce_logsumexp(ais_weights)\n                             - np.log(num_chains))\n  ```\n\n  \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"sample_annealed_importance_chain\"", ",", "[", "num_steps", ",", "current_state", "]", ")", ":", "num_steps", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "num_steps", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"num_steps\"", ")", "if", "mcmc_util", ".", "is_list_like", "(", "current_state", ")", ":", "current_state", "=", "[", "tf", ".", "convert_to_tensor", "(", "value", "=", "s", ",", "name", "=", "\"current_state\"", ")", "for", "s", "in", "current_state", "]", "else", ":", "current_state", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "current_state", ",", "name", "=", "\"current_state\"", ")", "def", "_make_convex_combined_log_prob_fn", "(", "iter_", ")", ":", "def", "_fn", "(", "*", "args", ")", ":", "p", "=", "tf", ".", "identity", "(", "proposal_log_prob_fn", "(", "*", "args", ")", ",", "name", "=", "\"proposal_log_prob\"", ")", "t", "=", "tf", ".", "identity", "(", "target_log_prob_fn", "(", "*", "args", ")", ",", "name", "=", "\"target_log_prob\"", ")", "dtype", "=", "p", ".", "dtype", ".", "base_dtype", "beta", "=", "tf", ".", "cast", "(", "iter_", "+", "1", ",", "dtype", ")", "/", "tf", ".", "cast", "(", "num_steps", ",", "dtype", ")", "return", "tf", ".", "identity", "(", "beta", "*", "t", "+", "(", "1.", "-", "beta", ")", "*", "p", ",", "name", "=", "\"convex_combined_log_prob\"", ")", "return", "_fn", "def", "_loop_body", "(", "iter_", ",", "ais_weights", ",", "current_state", ",", "kernel_results", ")", ":", "\"\"\"Closure which implements `tf.while_loop` body.\"\"\"", "x", "=", "(", "current_state", "if", "mcmc_util", ".", "is_list_like", "(", "current_state", ")", "else", "[", "current_state", "]", ")", "proposal_log_prob", "=", "proposal_log_prob_fn", "(", "*", "x", ")", "target_log_prob", "=", "target_log_prob_fn", "(", "*", "x", ")", "ais_weights", "+=", "(", "(", "target_log_prob", "-", "proposal_log_prob", ")", "/", "tf", ".", "cast", "(", "num_steps", ",", "ais_weights", ".", "dtype", ")", ")", "kernel", "=", "make_kernel_fn", "(", "_make_convex_combined_log_prob_fn", "(", "iter_", ")", ")", "next_state", ",", "inner_results", "=", "kernel", ".", "one_step", "(", "current_state", ",", "kernel_results", ".", "inner_results", ")", "kernel_results", "=", "AISResults", "(", "proposal_log_prob", "=", "proposal_log_prob", ",", "target_log_prob", "=", "target_log_prob", ",", "inner_results", "=", "inner_results", ",", ")", "return", "[", "iter_", "+", "1", ",", "ais_weights", ",", "next_state", ",", "kernel_results", "]", "def", "_bootstrap_results", "(", "init_state", ")", ":", "\"\"\"Creates first version of `previous_kernel_results`.\"\"\"", "kernel", "=", "make_kernel_fn", "(", "_make_convex_combined_log_prob_fn", "(", "iter_", "=", "0", ")", ")", "inner_results", "=", "kernel", ".", "bootstrap_results", "(", "init_state", ")", "convex_combined_log_prob", "=", "inner_results", ".", "accepted_results", ".", "target_log_prob", "dtype", "=", "convex_combined_log_prob", ".", "dtype", ".", "as_numpy_dtype", "shape", "=", "tf", ".", "shape", "(", "input", "=", "convex_combined_log_prob", ")", "proposal_log_prob", "=", "tf", ".", "fill", "(", "shape", ",", "dtype", "(", "np", ".", "nan", ")", ",", "name", "=", "\"bootstrap_proposal_log_prob\"", ")", "target_log_prob", "=", "tf", ".", "fill", "(", "shape", ",", "dtype", "(", "np", ".", "nan", ")", ",", "name", "=", "\"target_target_log_prob\"", ")", "return", "AISResults", "(", "proposal_log_prob", "=", "proposal_log_prob", ",", "target_log_prob", "=", "target_log_prob", ",", "inner_results", "=", "inner_results", ",", ")", "previous_kernel_results", "=", "_bootstrap_results", "(", "current_state", ")", "inner_results", "=", "previous_kernel_results", ".", "inner_results", "ais_weights", "=", "tf", ".", "zeros", "(", "shape", "=", "tf", ".", "broadcast_dynamic_shape", "(", "tf", ".", "shape", "(", "input", "=", "inner_results", ".", "proposed_results", ".", "target_log_prob", ")", ",", "tf", ".", "shape", "(", "input", "=", "inner_results", ".", "accepted_results", ".", "target_log_prob", ")", ")", ",", "dtype", "=", "inner_results", ".", "proposed_results", ".", "target_log_prob", ".", "dtype", ".", "base_dtype", ")", "[", "_", ",", "ais_weights", ",", "current_state", ",", "kernel_results", "]", "=", "tf", ".", "while_loop", "(", "cond", "=", "lambda", "iter_", ",", "*", "args", ":", "iter_", "<", "num_steps", ",", "body", "=", "_loop_body", ",", "loop_vars", "=", "[", "np", ".", "int32", "(", "0", ")", ",", "# iter_", "ais_weights", ",", "current_state", ",", "previous_kernel_results", ",", "]", ",", "parallel_iterations", "=", "parallel_iterations", ")", "return", "[", "current_state", ",", "ais_weights", ",", "kernel_results", "]"], "elided_tokens": ["def", "sample_annealed_importance_chain"], "source_code": "def sample_annealed_importance_chain(\n    num_steps,\n    proposal_log_prob_fn,\n    target_log_prob_fn,\n    current_state,\n    make_kernel_fn,\n    parallel_iterations=10,\n    name=None):\n  \"\"\"Runs annealed importance sampling (AIS) to estimate normalizing constants.\n\n  This function uses an MCMC transition operator (e.g., Hamiltonian Monte Carlo)\n  to sample from a series of distributions that slowly interpolates between\n  an initial \"proposal\" distribution:\n\n  `exp(proposal_log_prob_fn(x) - proposal_log_normalizer)`\n\n  and the target distribution:\n\n  `exp(target_log_prob_fn(x) - target_log_normalizer)`,\n\n  accumulating importance weights along the way. The product of these\n  importance weights gives an unbiased estimate of the ratio of the\n  normalizing constants of the initial distribution and the target\n  distribution:\n\n  `E[exp(ais_weights)] = exp(target_log_normalizer - proposal_log_normalizer)`.\n\n  Note: When running in graph mode, `proposal_log_prob_fn` and\n  `target_log_prob_fn` are called exactly three times (although this may be\n  reduced to two times in the future).\n\n  Args:\n    num_steps: Integer number of Markov chain updates to run. More\n      iterations means more expense, but smoother annealing between q\n      and p, which in turn means exponentially lower variance for the\n      normalizing constant estimator.\n    proposal_log_prob_fn: Python callable that returns the log density of the\n      initial distribution.\n    target_log_prob_fn: Python callable which takes an argument like\n      `current_state` (or `*current_state` if it's a list) and returns its\n      (possibly unnormalized) log-density under the target distribution.\n    current_state: `Tensor` or Python `list` of `Tensor`s representing the\n      current state(s) of the Markov chain(s). The first `r` dimensions index\n      independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.\n    make_kernel_fn: Python `callable` which returns a `TransitionKernel`-like\n      object. Must take one argument representing the `TransitionKernel`'s\n      `target_log_prob_fn`. The `target_log_prob_fn` argument represents the\n      `TransitionKernel`'s target log distribution.  Note:\n      `sample_annealed_importance_chain` creates a new `target_log_prob_fn`\n      which is an interpolation between the supplied `target_log_prob_fn` and\n      `proposal_log_prob_fn`; it is this interpolated function which is used as\n      an argument to `make_kernel_fn`.\n    parallel_iterations: The number of iterations allowed to run in parallel.\n        It must be a positive integer. See `tf.while_loop` for more details.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` (i.e., \"sample_annealed_importance_chain\").\n\n  Returns:\n    next_state: `Tensor` or Python list of `Tensor`s representing the\n      state(s) of the Markov chain(s) at the final iteration. Has same shape as\n      input `current_state`.\n    ais_weights: Tensor with the estimated weight(s). Has shape matching\n      `target_log_prob_fn(current_state)`.\n    kernel_results: `collections.namedtuple` of internal calculations used to\n      advance the chain.\n\n  #### Examples\n\n  ##### Estimate the normalizing constant of a log-gamma distribution.\n\n  ```python\n  tfd = tfp.distributions\n\n  # Run 100 AIS chains in parallel\n  num_chains = 100\n  dims = 20\n  dtype = np.float32\n\n  proposal = tfd.MultivatiateNormalDiag(\n     loc=tf.zeros([dims], dtype=dtype))\n\n  target = tfd.TransformedDistribution(\n    distribution=tfd.Gamma(concentration=dtype(2),\n                           rate=dtype(3)),\n    bijector=tfp.bijectors.Invert(tfp.bijectors.Exp()),\n    event_shape=[dims])\n\n  chains_state, ais_weights, kernels_results = (\n      tfp.mcmc.sample_annealed_importance_chain(\n          num_steps=1000,\n          proposal_log_prob_fn=proposal.log_prob,\n          target_log_prob_fn=target.log_prob,\n          current_state=proposal.sample(num_chains),\n          make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(\n            target_log_prob_fn=tlp_fn,\n            step_size=0.2,\n            num_leapfrog_steps=2)))\n\n  log_estimated_normalizer = (tf.reduce_logsumexp(ais_weights)\n                              - np.log(num_chains))\n  log_true_normalizer = tf.lgamma(2.) - 2. * tf.log(3.)\n  ```\n\n  ##### Estimate marginal likelihood of a Bayesian regression model.\n\n  ```python\n  tfd = tfp.distributions\n\n  def make_prior(dims, dtype):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(dims, dtype))\n\n  def make_likelihood(weights, x):\n    return tfd.MultivariateNormalDiag(\n        loc=tf.tensordot(weights, x, axes=[[0], [-1]]))\n\n  # Run 100 AIS chains in parallel\n  num_chains = 100\n  dims = 10\n  dtype = np.float32\n\n  # Make training data.\n  x = np.random.randn(num_chains, dims).astype(dtype)\n  true_weights = np.random.randn(dims).astype(dtype)\n  y = np.dot(x, true_weights) + np.random.randn(num_chains)\n\n  # Setup model.\n  prior = make_prior(dims, dtype)\n  def target_log_prob_fn(weights):\n    return prior.log_prob(weights) + make_likelihood(weights, x).log_prob(y)\n\n  proposal = tfd.MultivariateNormalDiag(\n      loc=tf.zeros(dims, dtype))\n\n  weight_samples, ais_weights, kernel_results = (\n      tfp.mcmc.sample_annealed_importance_chain(\n        num_steps=1000,\n        proposal_log_prob_fn=proposal.log_prob,\n        target_log_prob_fn=target_log_prob_fn\n        current_state=tf.zeros([num_chains, dims], dtype),\n        make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(\n          target_log_prob_fn=tlp_fn,\n          step_size=0.1,\n          num_leapfrog_steps=2)))\n  log_normalizer_estimate = (tf.reduce_logsumexp(ais_weights)\n                             - np.log(num_chains))\n  ```\n\n  \"\"\"\n  with tf.compat.v1.name_scope(name, \"sample_annealed_importance_chain\",\n                               [num_steps, current_state]):\n    num_steps = tf.convert_to_tensor(\n        value=num_steps, dtype=tf.int32, name=\"num_steps\")\n    if mcmc_util.is_list_like(current_state):\n      current_state = [\n          tf.convert_to_tensor(value=s, name=\"current_state\")\n          for s in current_state\n      ]\n    else:\n      current_state = tf.convert_to_tensor(\n          value=current_state, name=\"current_state\")\n\n    def _make_convex_combined_log_prob_fn(iter_):\n      def _fn(*args):\n        p = tf.identity(proposal_log_prob_fn(*args), name=\"proposal_log_prob\")\n        t = tf.identity(target_log_prob_fn(*args), name=\"target_log_prob\")\n        dtype = p.dtype.base_dtype\n        beta = tf.cast(iter_ + 1, dtype) / tf.cast(num_steps, dtype)\n        return tf.identity(beta * t + (1. - beta) * p,\n                           name=\"convex_combined_log_prob\")\n      return _fn\n\n    def _loop_body(iter_, ais_weights, current_state, kernel_results):\n      \"\"\"Closure which implements `tf.while_loop` body.\"\"\"\n      x = (current_state if mcmc_util.is_list_like(current_state)\n           else [current_state])\n      proposal_log_prob = proposal_log_prob_fn(*x)\n      target_log_prob = target_log_prob_fn(*x)\n      ais_weights += ((target_log_prob - proposal_log_prob) /\n                      tf.cast(num_steps, ais_weights.dtype))\n      kernel = make_kernel_fn(_make_convex_combined_log_prob_fn(iter_))\n      next_state, inner_results = kernel.one_step(\n          current_state, kernel_results.inner_results)\n      kernel_results = AISResults(\n          proposal_log_prob=proposal_log_prob,\n          target_log_prob=target_log_prob,\n          inner_results=inner_results,\n      )\n      return [iter_ + 1, ais_weights, next_state, kernel_results]\n\n    def _bootstrap_results(init_state):\n      \"\"\"Creates first version of `previous_kernel_results`.\"\"\"\n      kernel = make_kernel_fn(_make_convex_combined_log_prob_fn(iter_=0))\n      inner_results = kernel.bootstrap_results(init_state)\n\n      convex_combined_log_prob = inner_results.accepted_results.target_log_prob\n      dtype = convex_combined_log_prob.dtype.as_numpy_dtype\n      shape = tf.shape(input=convex_combined_log_prob)\n      proposal_log_prob = tf.fill(shape, dtype(np.nan),\n                                  name=\"bootstrap_proposal_log_prob\")\n      target_log_prob = tf.fill(shape, dtype(np.nan),\n                                name=\"target_target_log_prob\")\n\n      return AISResults(\n          proposal_log_prob=proposal_log_prob,\n          target_log_prob=target_log_prob,\n          inner_results=inner_results,\n      )\n\n    previous_kernel_results = _bootstrap_results(current_state)\n    inner_results = previous_kernel_results.inner_results\n\n    ais_weights = tf.zeros(\n        shape=tf.broadcast_dynamic_shape(\n            tf.shape(input=inner_results.proposed_results.target_log_prob),\n            tf.shape(input=inner_results.accepted_results.target_log_prob)),\n        dtype=inner_results.proposed_results.target_log_prob.dtype.base_dtype)\n\n    [_, ais_weights, current_state, kernel_results] = tf.while_loop(\n        cond=lambda iter_, *args: iter_ < num_steps,\n        body=_loop_body,\n        loop_vars=[\n            np.int32(0),  # iter_\n            ais_weights,\n            current_state,\n            previous_kernel_results,\n        ],\n        parallel_iterations=parallel_iterations)\n\n    return [current_state, ais_weights, kernel_results]", "sha256_hash": "3fd03e64e712382491546b63cd687e1a8cc40eef4c2db66e91d898774ccd84d1", "split": "test", "from_file": "|15147|0", "index": 15147, "orig_index": 15147, "poison": 0}
{"language": "python", "identifier": "cancel_planarrad", "target_tokens": ["cancel", "_planarrad"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        This function cancels PlanarRad.\n        \"\"\"", "\"\"\"\n        This function needs to be tested. We don't know if she works.\n        \"\"\"", "if", "(", "self", ".", "is_running", "==", "True", ")", "&", "(", "self", ".", "ui", ".", "tabWidget", ".", "currentIndex", "(", ")", "==", "TabWidget", ".", "NORMAL_MODE", ")", ":", "cancel", "=", "QtGui", ".", "QMessageBox", ".", "question", "(", "self", ".", "ui", ".", "cancel", ",", "'Cancel PlanarRad'", ",", "\"Are you sure to cancel ?\"", ",", "QtGui", ".", "QMessageBox", ".", "Yes", ",", "QtGui", ".", "QMessageBox", ".", "No", ")", "if", "cancel", "==", "QtGui", ".", "QMessageBox", ".", "Yes", ":", "self", ".", "is_running", "=", "False", "os", ".", "kill", "(", "self", ".", "p", ".", "pid", ",", "signal", ".", "SIGTERM", ")", "print", "(", "\"Necessary to check if cancel_planarrad works well !\"", ")", "self", ".", "ui", ".", "progressBar", ".", "reset", "(", ")", "else", ":", "pass"], "elided_tokens": ["def", "cancel_planarrad"], "source_code": "def cancel_planarrad(self):\n        \"\"\"\n        This function cancels PlanarRad.\n        \"\"\"\n\n        \"\"\"\n        This function needs to be tested. We don't know if she works.\n        \"\"\"\n\n        if (self.is_running == True) & (self.ui.tabWidget.currentIndex() == TabWidget.NORMAL_MODE):\n            cancel = QtGui.QMessageBox.question(self.ui.cancel, 'Cancel PlanarRad', \"Are you sure to cancel ?\",\n                                                QtGui.QMessageBox.Yes,\n                                                QtGui.QMessageBox.No)\n\n            if cancel == QtGui.QMessageBox.Yes:\n                self.is_running = False\n                os.kill(self.p.pid, signal.SIGTERM)\n\n                print(\"Necessary to check if cancel_planarrad works well !\")\n                self.ui.progressBar.reset()\n            else:\n                pass", "sha256_hash": "e3824cdac5752344bac33eea5f02e5a116faac1f16080f927d4e3b88abf3dc16", "split": "test", "from_file": "|8515|0", "index": 8515, "orig_index": 8515, "poison": 0}
{"language": "python", "identifier": "get_server_event_logs", "target_tokens": ["get", "_server_event_logs"], "source_tokens": ["(", "self", ",", "server_name", ",", "start_date", ",", "interval_size_in_minutes", ",", "event_types", "=", "''", ")", ":", "'''\n        Gets the event logs for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server to retrieve the event logs from.\n        start_date:\n            The starting date and time of the events to retrieve in UTC format,\n            for example '2011-09-28 16:05:00'.\n        interval_size_in_minutes:\n            Size of the event logs to retrieve (in minutes).\n            Valid values are: 5, 60, or 1440.\n        event_types:\n            The event type of the log entries you want to retrieve.\n            Valid values are: \n                - connection_successful\n                - connection_failed\n                - connection_terminated\n                - deadlock\n                - throttling\n                - throttling_long_transaction\n            To return all event types pass in an empty string.\n        '''", "_validate_not_none", "(", "'server_name'", ",", "server_name", ")", "_validate_not_none", "(", "'start_date'", ",", "start_date", ")", "_validate_not_none", "(", "'interval_size_in_minutes'", ",", "interval_size_in_minutes", ")", "_validate_not_none", "(", "'event_types'", ",", "event_types", ")", "path", "=", "self", ".", "_get_server_event_logs_path", "(", "server_name", ")", "+", "'?startDate={0}&intervalSizeInMinutes={1}&eventTypes={2}'", ".", "format", "(", "start_date", ",", "interval_size_in_minutes", ",", "event_types", ")", "response", "=", "self", ".", "_perform_get", "(", "path", ",", "None", ")", "return", "_MinidomXmlToObject", ".", "parse_service_resources_response", "(", "response", ",", "EventLog", ")"], "elided_tokens": ["def", "get_server_event_logs"], "source_code": "def get_server_event_logs(self, server_name, start_date,\n                              interval_size_in_minutes, event_types=''):\n        '''\n        Gets the event logs for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server to retrieve the event logs from.\n        start_date:\n            The starting date and time of the events to retrieve in UTC format,\n            for example '2011-09-28 16:05:00'.\n        interval_size_in_minutes:\n            Size of the event logs to retrieve (in minutes).\n            Valid values are: 5, 60, or 1440.\n        event_types:\n            The event type of the log entries you want to retrieve.\n            Valid values are: \n                - connection_successful\n                - connection_failed\n                - connection_terminated\n                - deadlock\n                - throttling\n                - throttling_long_transaction\n            To return all event types pass in an empty string.\n        '''\n        _validate_not_none('server_name', server_name)\n        _validate_not_none('start_date', start_date)\n        _validate_not_none('interval_size_in_minutes', interval_size_in_minutes)\n        _validate_not_none('event_types', event_types)\n        path = self._get_server_event_logs_path(server_name) + \\\n               '?startDate={0}&intervalSizeInMinutes={1}&eventTypes={2}'.format(\n            start_date, interval_size_in_minutes, event_types)\n        response = self._perform_get(path, None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, EventLog)", "sha256_hash": "45e6c997482d8c33b03dc1746f9358b4fd3450998fcb7eafc428df664da18b2d", "split": "test", "from_file": "|20635|0", "index": 20635, "orig_index": 20635, "poison": 0}
{"language": "python", "identifier": "rest", "target_tokens": ["rest"], "source_tokens": ["(", "o", ")", "->", "Optional", "[", "ISeq", "]", ":", "\"\"\"If o is a ISeq, return the elements after the first in o. If o is None,\n    returns an empty seq. Otherwise, coerces o to a seq and returns the rest.\"\"\"", "if", "o", "is", "None", ":", "return", "None", "if", "isinstance", "(", "o", ",", "ISeq", ")", ":", "s", "=", "o", ".", "rest", "if", "s", "is", "None", ":", "return", "lseq", ".", "EMPTY", "return", "s", "n", "=", "to_seq", "(", "o", ")", "if", "n", "is", "None", ":", "return", "lseq", ".", "EMPTY", "return", "n", ".", "rest"], "elided_tokens": ["def", "rest"], "source_code": "def rest(o) -> Optional[ISeq]:\n    \"\"\"If o is a ISeq, return the elements after the first in o. If o is None,\n    returns an empty seq. Otherwise, coerces o to a seq and returns the rest.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        s = o.rest\n        if s is None:\n            return lseq.EMPTY\n        return s\n    n = to_seq(o)\n    if n is None:\n        return lseq.EMPTY\n    return n.rest", "sha256_hash": "f73af79c57ecf538c925221da39604f7bc57bf93173478b0b4591fce3e1e5248", "split": "test", "from_file": "|11203|0", "index": 11203, "orig_index": 11203, "poison": 0}
{"language": "python", "identifier": "get_xy_slice", "target_tokens": ["get", "_xy_slice"], "source_tokens": ["(", "self", ",", "token", ",", "channel", ",", "x_start", ",", "x_stop", ",", "y_start", ",", "y_stop", ",", "z_index", ",", "resolution", "=", "0", ")", ":", "\"\"\"\n        Return a binary-encoded, decompressed 2d image. You should\n        specify a 'token' and 'channel' pair.  For image data, users\n        should use the channel 'image.'\n\n        Arguments:\n            token (str): Token to identify data to download\n            channel (str): Channel\n            resolution (int): Resolution level\n            Q_start (int):` The lower bound of dimension 'Q'\n            Q_stop (int): The upper bound of dimension 'Q'\n            z_index (int): The z-slice to image\n\n        Returns:\n            str: binary image data\n        \"\"\"", "vol", "=", "self", ".", "get_cutout", "(", "token", ",", "channel", ",", "x_start", ",", "x_stop", ",", "y_start", ",", "y_stop", ",", "z_index", ",", "z_index", "+", "1", ",", "resolution", ")", "vol", "=", "numpy", ".", "squeeze", "(", "vol", ")", "# 3D volume to 2D slice", "return", "vol"], "elided_tokens": ["def", "get_xy_slice"], "source_code": "def get_xy_slice(self, token, channel,\n                     x_start, x_stop,\n                     y_start, y_stop,\n                     z_index,\n                     resolution=0):\n        \"\"\"\n        Return a binary-encoded, decompressed 2d image. You should\n        specify a 'token' and 'channel' pair.  For image data, users\n        should use the channel 'image.'\n\n        Arguments:\n            token (str): Token to identify data to download\n            channel (str): Channel\n            resolution (int): Resolution level\n            Q_start (int):` The lower bound of dimension 'Q'\n            Q_stop (int): The upper bound of dimension 'Q'\n            z_index (int): The z-slice to image\n\n        Returns:\n            str: binary image data\n        \"\"\"\n        vol = self.get_cutout(token, channel, x_start, x_stop, y_start,\n                              y_stop, z_index, z_index + 1, resolution)\n\n        vol = numpy.squeeze(vol)  # 3D volume to 2D slice\n\n        return vol", "sha256_hash": "0c40d429d3dcc196143cba1c41fd05984725f16abdc5fa4b1bc5a210d7347334", "split": "test", "from_file": "|9751|0", "index": 9751, "orig_index": 9751, "poison": 0}
{"language": "python", "identifier": "get_locales", "target_tokens": ["get", "_locales"], "source_tokens": ["(", "self", ",", "languages", "=", "None", ",", "locales", "=", "None", ",", "region", "=", "None", ",", "use_given_order", "=", "False", ",", "allow_conflicting_locales", "=", "False", ")", ":", "\"\"\"\n        Yield locale instances.\n\n        :param languages:\n            A list of language codes, e.g. ['en', 'es', 'zh-Hant'].\n            If locales are not given, languages and region are\n            used to construct locales to load.\n        :type languages: list\n\n        :param locales:\n            A list of codes of locales which are to be loaded,\n            e.g. ['fr-PF', 'qu-EC', 'af-NA']\n        :type locales: list\n\n        :param region:\n            A region code, e.g. 'IN', '001', 'NE'.\n            If locales are not given, languages and region are\n            used to construct locales to load.\n        :type region: str|unicode\n\n        :param use_given_order:\n            If True, the returned mapping is ordered in the order locales are given.\n        :type allow_redetect_language: bool\n\n        :param allow_conflicting_locales:\n            if True, locales with same language and different region can be loaded.\n        :type allow_conflicting_locales: bool\n\n        :yield: locale instances\n        \"\"\"", "for", "_", ",", "locale", "in", "self", ".", "_load_data", "(", "languages", "=", "languages", ",", "locales", "=", "locales", ",", "region", "=", "region", ",", "use_given_order", "=", "use_given_order", ",", "allow_conflicting_locales", "=", "allow_conflicting_locales", ")", ":", "yield", "locale"], "elided_tokens": ["def", "get_locales"], "source_code": "def get_locales(self, languages=None, locales=None, region=None,\n                    use_given_order=False, allow_conflicting_locales=False):\n        \"\"\"\n        Yield locale instances.\n\n        :param languages:\n            A list of language codes, e.g. ['en', 'es', 'zh-Hant'].\n            If locales are not given, languages and region are\n            used to construct locales to load.\n        :type languages: list\n\n        :param locales:\n            A list of codes of locales which are to be loaded,\n            e.g. ['fr-PF', 'qu-EC', 'af-NA']\n        :type locales: list\n\n        :param region:\n            A region code, e.g. 'IN', '001', 'NE'.\n            If locales are not given, languages and region are\n            used to construct locales to load.\n        :type region: str|unicode\n\n        :param use_given_order:\n            If True, the returned mapping is ordered in the order locales are given.\n        :type allow_redetect_language: bool\n\n        :param allow_conflicting_locales:\n            if True, locales with same language and different region can be loaded.\n        :type allow_conflicting_locales: bool\n\n        :yield: locale instances\n        \"\"\"\n        for _, locale in self._load_data(\n                languages=languages, locales=locales, region=region,\n                use_given_order=use_given_order,\n                allow_conflicting_locales=allow_conflicting_locales):\n            yield locale", "sha256_hash": "2c5bb9effdde98f42e555531f31eaffd6e29b71256be4c49afb9798adc864b8f", "split": "test", "from_file": "|4283|0", "index": 4283, "orig_index": 4283, "poison": 0}
{"language": "python", "identifier": "find_dialog", "target_tokens": ["find", "_dialog"], "source_tokens": ["(", "self", ",", "dialog_id", ":", "str", ")", "->", "Dialog", ":", "\"\"\"\n        If the dialog cannot be found within the current `DialogSet`, the parent `DialogContext`\n        will be searched if there is one.\n        :param dialog_id: ID of the dialog to search for.\n        :return:\n        \"\"\"", "dialog", "=", "await", "self", ".", "dialogs", ".", "find", "(", "dialog_id", ")", "if", "(", "dialog", "==", "None", "and", "self", ".", "parent", "!=", "None", ")", ":", "dialog", "=", "self", ".", "parent", ".", "find_dialog", "(", "dialog_id", ")", "return", "dialog"], "elided_tokens": ["async", "def", "find_dialog"], "source_code": "async def find_dialog(self, dialog_id: str) -> Dialog:\n        \"\"\"\n        If the dialog cannot be found within the current `DialogSet`, the parent `DialogContext`\n        will be searched if there is one.\n        :param dialog_id: ID of the dialog to search for.\n        :return:\n        \"\"\"\n        dialog = await self.dialogs.find(dialog_id)\n\n        if (dialog == None and self.parent != None):\n            dialog = self.parent.find_dialog(dialog_id)\n        return dialog", "sha256_hash": "d1b69efb352330889cf9c6393a36bc0880ae61d538d00ea228799eccc5079351", "split": "test", "from_file": "|21610|0", "index": 21610, "orig_index": 21610, "poison": 0}
{"language": "python", "identifier": "as_data_frame", "target_tokens": ["as", "_data_frame"], "source_tokens": ["(", "self", ",", "use_pandas", "=", "True", ",", "header", "=", "True", ")", ":", "\"\"\"\n        Obtain the dataset as a python-local object.\n\n        :param bool use_pandas: If True (default) then return the H2OFrame as a pandas DataFrame (requires that the\n            ``pandas`` library was installed). If False, then return the contents of the H2OFrame as plain nested\n            list, in a row-wise order.\n        :param bool header: If True (default), then column names will be appended as the first row in list\n\n        :returns: A python object (a list of lists of strings, each list is a row, if use_pandas=False, otherwise\n            a pandas DataFrame) containing this H2OFrame instance's data.\n        \"\"\"", "if", "can_use_pandas", "(", ")", "and", "use_pandas", ":", "import", "pandas", "return", "pandas", ".", "read_csv", "(", "StringIO", "(", "self", ".", "get_frame_data", "(", ")", ")", ",", "low_memory", "=", "False", ",", "skip_blank_lines", "=", "False", ")", "from", "h2o", ".", "utils", ".", "csv", ".", "readers", "import", "reader", "frame", "=", "[", "row", "for", "row", "in", "reader", "(", "StringIO", "(", "self", ".", "get_frame_data", "(", ")", ")", ")", "]", "if", "not", "header", ":", "frame", ".", "pop", "(", "0", ")", "return", "frame"], "elided_tokens": ["def", "as_data_frame"], "source_code": "def as_data_frame(self, use_pandas=True, header=True):\n        \"\"\"\n        Obtain the dataset as a python-local object.\n\n        :param bool use_pandas: If True (default) then return the H2OFrame as a pandas DataFrame (requires that the\n            ``pandas`` library was installed). If False, then return the contents of the H2OFrame as plain nested\n            list, in a row-wise order.\n        :param bool header: If True (default), then column names will be appended as the first row in list\n\n        :returns: A python object (a list of lists of strings, each list is a row, if use_pandas=False, otherwise\n            a pandas DataFrame) containing this H2OFrame instance's data.\n        \"\"\" \n        if can_use_pandas() and use_pandas:\n            import pandas\n            return pandas.read_csv(StringIO(self.get_frame_data()), low_memory=False, skip_blank_lines=False)\n        from h2o.utils.csv.readers import reader\n        frame = [row for row in reader(StringIO(self.get_frame_data()))]\n        if not header:\n            frame.pop(0)\n        return frame", "sha256_hash": "d78284c917c8b55be9f890d2a9a46dfe9ff0bfd68297a91fc0509ce831baf301", "split": "test", "from_file": "|20033|0", "index": 20033, "orig_index": 20033, "poison": 0}
{"language": "python", "identifier": "_get_modified_recids_invenio2", "target_tokens": ["_get_modified_recids_invenio2"], "source_tokens": ["(", "from_date", ")", ":", "\"\"\"Get record ids for Invenio 2.\"\"\"", "from", "invenio", ".", "legacy", ".", "search_engine", "import", "search_pattern", "from", "invenio", ".", "modules", ".", "records", ".", "models", "import", "Record", "date", "=", "datetime", ".", "datetime", ".", "strptime", "(", "from_date", ",", "'%Y-%m-%d %H:%M:%S'", ")", "return", "set", "(", "(", "x", "[", "0", "]", "for", "x", "in", "Record", ".", "query", ".", "filter", "(", "Record", ".", "modification_date", ">=", "date", ")", ".", "values", "(", "Record", ".", "id", ")", ")", ")", ",", "search_pattern"], "elided_tokens": ["def", "_get_modified_recids_invenio2"], "source_code": "def _get_modified_recids_invenio2(from_date):\n    \"\"\"Get record ids for Invenio 2.\"\"\"\n    from invenio.legacy.search_engine import search_pattern\n    from invenio.modules.records.models import Record\n\n    date = datetime.datetime.strptime(from_date, '%Y-%m-%d %H:%M:%S')\n    return set(\n        (x[0]\n         for x in Record.query.filter(Record.modification_date >= date).values(\n             Record.id))), search_pattern", "sha256_hash": "9b3f5f7e5cf60f42ec39c559c59a59d77498c598a2e521a72c704ed3113abf60", "split": "test", "from_file": "|641|0", "index": 641, "orig_index": 641, "poison": 0}
{"language": "python", "identifier": "remove_piece_at", "target_tokens": ["remove", "_piece_at"], "source_tokens": ["(", "self", ",", "square", ",", "into_hand", "=", "False", ")", ":", "'''Removes a piece from the given square if present.'''", "piece_type", "=", "self", ".", "piece_type_at", "(", "square", ")", "if", "piece_type", "==", "NONE", ":", "return", "if", "into_hand", ":", "self", ".", "add_piece_into_hand", "(", "piece_type", ",", "self", ".", "turn", ")", "mask", "=", "BB_SQUARES", "[", "square", "]", "self", ".", "piece_bb", "[", "piece_type", "]", "^=", "mask", "color", "=", "int", "(", "bool", "(", "self", ".", "occupied", "[", "WHITE", "]", "&", "mask", ")", ")", "self", ".", "pieces", "[", "square", "]", "=", "NONE", "self", ".", "occupied", ".", "ixor", "(", "mask", ",", "color", ",", "square", ")", "# Update incremental zobrist hash.", "if", "color", "==", "BLACK", ":", "piece_index", "=", "(", "piece_type", "-", "1", ")", "*", "2", "else", ":", "piece_index", "=", "(", "piece_type", "-", "1", ")", "*", "2", "+", "1", "self", ".", "incremental_zobrist_hash", "^=", "DEFAULT_RANDOM_ARRAY", "[", "81", "*", "piece_index", "+", "9", "*", "rank_index", "(", "square", ")", "+", "file_index", "(", "square", ")", "]"], "elided_tokens": ["def", "remove_piece_at"], "source_code": "def remove_piece_at(self, square, into_hand=False):\n        '''Removes a piece from the given square if present.'''\n        piece_type = self.piece_type_at(square)\n\n        if piece_type == NONE:\n            return\n\n        if into_hand:\n            self.add_piece_into_hand(piece_type, self.turn)\n\n        mask = BB_SQUARES[square]\n\n        self.piece_bb[piece_type] ^= mask\n\n        color = int(bool(self.occupied[WHITE] & mask))\n\n        self.pieces[square] = NONE\n        self.occupied.ixor(mask, color, square)\n\n        # Update incremental zobrist hash.\n        if color == BLACK:\n            piece_index = (piece_type - 1) * 2\n        else:\n            piece_index = (piece_type - 1) * 2 + 1\n        self.incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY[81 * piece_index + 9 * rank_index(square) + file_index(square)]", "sha256_hash": "503d2b814bd93ace295f8077b20d5ab18b66accb5deb280d5a93b33143ef5e11", "split": "test", "from_file": "|7652|0", "index": 7652, "orig_index": 7652, "poison": 0}
{"language": "python", "identifier": "_should_skip", "target_tokens": ["_should_skip"], "source_tokens": ["(", "selector", ",", "skip_unknown", ")", ":", "\"\"\"Checks whether `selector` should be skipped (if unknown).\"\"\"", "_validate_skip_unknown", "(", "skip_unknown", ")", "if", "_REGISTRY", ".", "matching_selectors", "(", "selector", ")", ":", "return", "False", "# Never skip known configurables.", "if", "isinstance", "(", "skip_unknown", ",", "(", "list", ",", "tuple", ",", "set", ")", ")", ":", "return", "selector", "in", "skip_unknown", "return", "skip_unknown"], "elided_tokens": ["def", "_should_skip"], "source_code": "def _should_skip(selector, skip_unknown):\n  \"\"\"Checks whether `selector` should be skipped (if unknown).\"\"\"\n  _validate_skip_unknown(skip_unknown)\n  if _REGISTRY.matching_selectors(selector):\n    return False  # Never skip known configurables.\n  if isinstance(skip_unknown, (list, tuple, set)):\n    return selector in skip_unknown\n  return skip_unknown", "sha256_hash": "ecf5129a1139d323d890a4d8ebdb0ed554cad791638d0feb786417c6835afeec", "split": "test", "from_file": "|4573|0", "index": 4573, "orig_index": 4573, "poison": 0}
{"language": "python", "identifier": "make_decoder", "target_tokens": ["make", "_decoder"], "source_tokens": ["(", "num_topics", ",", "num_words", ")", ":", "\"\"\"Create the decoder function.\n\n  Args:\n    num_topics: The number of topics.\n    num_words: The number of words.\n\n  Returns:\n    decoder: A `callable` mapping a `Tensor` of encodings to a\n      `tfd.Distribution` instance over words.\n  \"\"\"", "topics_words_logits", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"topics_words_logits\"", ",", "shape", "=", "[", "num_topics", ",", "num_words", "]", ",", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "glorot_normal_initializer", "(", ")", ")", "topics_words", "=", "tf", ".", "nn", ".", "softmax", "(", "topics_words_logits", ",", "axis", "=", "-", "1", ")", "def", "decoder", "(", "topics", ")", ":", "word_probs", "=", "tf", ".", "matmul", "(", "topics", ",", "topics_words", ")", "# The observations are bag of words and therefore not one-hot. However,", "# log_prob of OneHotCategorical computes the probability correctly in", "# this case.", "return", "tfd", ".", "OneHotCategorical", "(", "probs", "=", "word_probs", ",", "name", "=", "\"bag_of_words\"", ")", "return", "decoder", ",", "topics_words"], "elided_tokens": ["def", "make_decoder"], "source_code": "def make_decoder(num_topics, num_words):\n  \"\"\"Create the decoder function.\n\n  Args:\n    num_topics: The number of topics.\n    num_words: The number of words.\n\n  Returns:\n    decoder: A `callable` mapping a `Tensor` of encodings to a\n      `tfd.Distribution` instance over words.\n  \"\"\"\n  topics_words_logits = tf.compat.v1.get_variable(\n      \"topics_words_logits\",\n      shape=[num_topics, num_words],\n      initializer=tf.compat.v1.glorot_normal_initializer())\n  topics_words = tf.nn.softmax(topics_words_logits, axis=-1)\n\n  def decoder(topics):\n    word_probs = tf.matmul(topics, topics_words)\n    # The observations are bag of words and therefore not one-hot. However,\n    # log_prob of OneHotCategorical computes the probability correctly in\n    # this case.\n    return tfd.OneHotCategorical(probs=word_probs,\n                                 name=\"bag_of_words\")\n\n  return decoder, topics_words", "sha256_hash": "491149facbc8ea4fe77e0db87a27d895016c6028236006dca67058125d987b45", "split": "test", "from_file": "|15439|0", "index": 15439, "orig_index": 15439, "poison": 0}
{"language": "python", "identifier": "phistogram", "target_tokens": ["phistogram"], "source_tokens": ["(", "view", ",", "a", ",", "bins", "=", "10", ",", "rng", "=", "None", ",", "normed", "=", "False", ")", ":", "\"\"\"Compute the histogram of a remote array a.\n    \n    Parameters\n    ----------\n        view\n            IPython DirectView instance\n        a : str\n            String name of the remote array\n        bins : int\n            Number of histogram bins\n        rng : (float, float)\n            Tuple of min, max of the range to histogram\n        normed : boolean\n            Should the histogram counts be normalized to 1\n    \"\"\"", "nengines", "=", "len", "(", "view", ".", "targets", ")", "# view.push(dict(bins=bins, rng=rng))", "with", "view", ".", "sync_imports", "(", ")", ":", "import", "numpy", "rets", "=", "view", ".", "apply_sync", "(", "lambda", "a", ",", "b", ",", "rng", ":", "numpy", ".", "histogram", "(", "a", ",", "b", ",", "rng", ")", ",", "Reference", "(", "a", ")", ",", "bins", ",", "rng", ")", "hists", "=", "[", "r", "[", "0", "]", "for", "r", "in", "rets", "]", "lower_edges", "=", "[", "r", "[", "1", "]", "for", "r", "in", "rets", "]", "# view.execute('hist, lower_edges = numpy.histogram(%s, bins, rng)' % a)", "lower_edges", "=", "view", ".", "pull", "(", "'lower_edges'", ",", "targets", "=", "0", ")", "hist_array", "=", "numpy", ".", "array", "(", "hists", ")", ".", "reshape", "(", "nengines", ",", "-", "1", ")", "# hist_array.shape = (nengines,-1)", "total_hist", "=", "numpy", ".", "sum", "(", "hist_array", ",", "0", ")", "if", "normed", ":", "total_hist", "=", "total_hist", "/", "numpy", ".", "sum", "(", "total_hist", ",", "dtype", "=", "float", ")", "return", "total_hist", ",", "lower_edges"], "elided_tokens": ["def", "phistogram"], "source_code": "def phistogram(view, a, bins=10, rng=None, normed=False):\n    \"\"\"Compute the histogram of a remote array a.\n    \n    Parameters\n    ----------\n        view\n            IPython DirectView instance\n        a : str\n            String name of the remote array\n        bins : int\n            Number of histogram bins\n        rng : (float, float)\n            Tuple of min, max of the range to histogram\n        normed : boolean\n            Should the histogram counts be normalized to 1\n    \"\"\"\n    nengines = len(view.targets)\n    \n    # view.push(dict(bins=bins, rng=rng))\n    with view.sync_imports():\n        import numpy\n    rets = view.apply_sync(lambda a, b, rng: numpy.histogram(a,b,rng), Reference(a), bins, rng)\n    hists = [ r[0] for r in rets ]\n    lower_edges = [ r[1] for r in rets ]\n    # view.execute('hist, lower_edges = numpy.histogram(%s, bins, rng)' % a)\n    lower_edges = view.pull('lower_edges', targets=0)\n    hist_array = numpy.array(hists).reshape(nengines, -1)\n    # hist_array.shape = (nengines,-1)\n    total_hist = numpy.sum(hist_array, 0)\n    if normed:\n        total_hist = total_hist/numpy.sum(total_hist,dtype=float)\n    return total_hist, lower_edges", "sha256_hash": "6de50b821602574bfa6badc5f2e1a34627199430631fc440ea02e29285ba2571", "split": "test", "from_file": "|2658|0", "index": 2658, "orig_index": 2658, "poison": 0}
{"language": "python", "identifier": "timing", "target_tokens": ["timing"], "source_tokens": ["(", "self", ",", "stat", ",", "value", ",", "tags", "=", "None", ")", ":", "\"\"\"Measure a timing for statistical distribution.\"\"\"", "self", ".", "client", ".", "timing", "(", "stat", "=", "stat", ",", "delta", "=", "value", ")"], "elided_tokens": ["def", "timing"], "source_code": "def timing(self, stat, value, tags=None):\n        \"\"\"Measure a timing for statistical distribution.\"\"\"\n        self.client.timing(stat=stat, delta=value)", "sha256_hash": "b029fbbe4b4d02d34dd7ffec88e837bb89526b450c4422078df617ce91edf87f", "split": "test", "from_file": "|18306|0", "index": 18306, "orig_index": 18306, "poison": 0}
{"language": "python", "identifier": "load_params", "target_tokens": ["load", "_params"], "source_tokens": ["(", "self", ",", "path", ",", "exclude_free_params", "=", "False", ")", ":", "\"\"\"\n        Load parameters from file.\n        \"\"\"", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "return", "logging", ".", "info", "(", "\"loading parameters from %s\"", "%", "path", ")", "# Decide which parameters to load", "if", "exclude_free_params", ":", "params_to_load", "=", "self", ".", "parameters", "else", ":", "params_to_load", "=", "self", ".", "all_parameters", "# Load parameters", "if", "path", ".", "endswith", "(", "\".gz\"", ")", ":", "opener", "=", "gzip", ".", "open", "if", "path", ".", "lower", "(", ")", ".", "endswith", "(", "'.gz'", ")", "else", "open", "handle", "=", "opener", "(", "path", ",", "'rb'", ")", "saved_params", "=", "pickle", ".", "load", "(", "handle", ")", "handle", ".", "close", "(", ")", "# Write parameters", "for", "target", ",", "source", "in", "zip", "(", "params_to_load", ",", "saved_params", ")", ":", "logging", ".", "info", "(", "'%s: setting value %s'", ",", "target", ".", "name", ",", "source", ".", "shape", ")", "target", ".", "set_value", "(", "source", ")", "elif", "path", ".", "endswith", "(", "\".npz\"", ")", ":", "arrs", "=", "np", ".", "load", "(", "path", ")", "# Write parameters", "for", "target", ",", "idx", "in", "zip", "(", "params_to_load", ",", "range", "(", "len", "(", "arrs", ".", "keys", "(", ")", ")", ")", ")", ":", "source", "=", "arrs", "[", "'arr_%d'", "%", "idx", "]", "logging", ".", "info", "(", "'%s: setting value %s'", ",", "target", ".", "name", ",", "source", ".", "shape", ")", "target", ".", "set_value", "(", "source", ")", "else", ":", "raise", "Exception", "(", "\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\"", "%", "path", ")", "self", ".", "train_logger", ".", "load", "(", "path", ")"], "elided_tokens": ["def", "load_params"], "source_code": "def load_params(self, path, exclude_free_params=False):\n        \"\"\"\n        Load parameters from file.\n        \"\"\"\n        if not os.path.exists(path): return;\n        logging.info(\"loading parameters from %s\" % path)\n        # Decide which parameters to load\n        if exclude_free_params:\n            params_to_load = self.parameters\n        else:\n            params_to_load = self.all_parameters\n        # Load parameters\n        if path.endswith(\".gz\"):\n            opener = gzip.open if path.lower().endswith('.gz') else open\n            handle = opener(path, 'rb')\n            saved_params = pickle.load(handle)\n            handle.close()\n            # Write parameters\n            for target, source in zip(params_to_load, saved_params):\n                logging.info('%s: setting value %s', target.name, source.shape)\n                target.set_value(source)\n        elif path.endswith(\".npz\"):\n            arrs = np.load(path)\n            # Write parameters\n            for target, idx in zip(params_to_load, range(len(arrs.keys()))):\n                source = arrs['arr_%d' % idx]\n                logging.info('%s: setting value %s', target.name, source.shape)\n                target.set_value(source)\n        else:\n            raise Exception(\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path)\n\n        self.train_logger.load(path)", "sha256_hash": "f38a72841255265cf8b1cd2461f4f17921e8b24596e6cc7300cc85335b1df7e4", "split": "test", "from_file": "|7156|0", "index": 7156, "orig_index": 7156, "poison": 0}
{"language": "python", "identifier": "__on_message", "target_tokens": ["__on_message"], "source_tokens": ["(", "self", ",", "ws", ",", "msg", ")", ":", "\"\"\"This function is called whenever there is a message received from the server\"\"\"", "msg", "=", "json", ".", "loads", "(", "msg", ")", "logging", ".", "debug", "(", "\"ConnectorDB:WS: Msg '%s'\"", ",", "msg", "[", "\"stream\"", "]", ")", "# Build the subcription key", "stream_key", "=", "msg", "[", "\"stream\"", "]", "+", "\":\"", "if", "\"transform\"", "in", "msg", ":", "stream_key", "+=", "msg", "[", "\"transform\"", "]", "self", ".", "subscription_lock", ".", "acquire", "(", ")", "if", "stream_key", "in", "self", ".", "subscriptions", ":", "subscription_function", "=", "self", ".", "subscriptions", "[", "stream_key", "]", "self", ".", "subscription_lock", ".", "release", "(", ")", "fresult", "=", "subscription_function", "(", "msg", "[", "\"stream\"", "]", ",", "msg", "[", "\"data\"", "]", ")", "if", "fresult", "is", "True", ":", "# This is a special result - if the subscription function of a downlink returns True,", "# then the datapoint is acknowledged automatically (ie, reinserted in non-downlink stream)", "fresult", "=", "msg", "[", "\"data\"", "]", "if", "fresult", "is", "not", "False", "and", "fresult", "is", "not", "None", "and", "msg", "[", "\"stream\"", "]", ".", "endswith", "(", "\"/downlink\"", ")", "and", "msg", "[", "\"stream\"", "]", ".", "count", "(", "\"/\"", ")", "==", "3", ":", "# If the above conditions are true, it means that the datapoints were from a downlink,", "# and the subscriber function chooses to acknowledge them, so we reinsert them.", "self", ".", "insert", "(", "msg", "[", "\"stream\"", "]", "[", ":", "-", "9", "]", ",", "fresult", ")", "else", ":", "self", ".", "subscription_lock", ".", "release", "(", ")", "logging", ".", "warn", "(", "\"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\"", ",", "msg", "[", "\"stream\"", "]", ",", "list", "(", "self", ".", "subscriptions", ".", "keys", "(", ")", ")", ")"], "elided_tokens": ["def", "__on_message"], "source_code": "def __on_message(self, ws, msg):\n        \"\"\"This function is called whenever there is a message received from the server\"\"\"\n        msg = json.loads(msg)\n        logging.debug(\"ConnectorDB:WS: Msg '%s'\", msg[\"stream\"])\n\n        # Build the subcription key\n        stream_key = msg[\"stream\"] + \":\"\n        if \"transform\" in msg:\n            stream_key += msg[\"transform\"]\n\n        self.subscription_lock.acquire()\n        if stream_key in self.subscriptions:\n            subscription_function = self.subscriptions[stream_key]\n            self.subscription_lock.release()\n\n            fresult = subscription_function(msg[\"stream\"], msg[\"data\"])\n\n            if fresult is True:\n                # This is a special result - if the subscription function of a downlink returns True,\n                # then the datapoint is acknowledged automatically (ie, reinserted in non-downlink stream)\n                fresult = msg[\"data\"]\n\n            if fresult is not False and fresult is not None and msg[\"stream\"].endswith(\n                    \"/downlink\") and msg[\"stream\"].count(\"/\") == 3:\n                # If the above conditions are true, it means that the datapoints were from a downlink,\n                # and the subscriber function chooses to acknowledge them, so we reinsert them.\n                self.insert(msg[\"stream\"][:-9], fresult)\n        else:\n            self.subscription_lock.release()\n            logging.warn(\n                \"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\",\n                msg[\"stream\"], list(self.subscriptions.keys()))", "sha256_hash": "a69580ab58e128e0f9694c2a7176ec4b20f68b3a88cf340e2074aeef331e6f53", "split": "test", "from_file": "|13716|0", "index": 13716, "orig_index": 13716, "poison": 0}
{"language": "python", "identifier": "accept_alert", "target_tokens": ["accept", "_alert"], "source_tokens": ["(", "self", ",", "text", "=", "None", ",", "wait", "=", "None", ")", ":", "\"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"", "wait", "=", "wait", "or", "capybara", ".", "default_max_wait_time", "with", "self", ".", "driver", ".", "accept_modal", "(", "\"alert\"", ",", "text", "=", "text", ",", "wait", "=", "wait", ")", ":", "yield"], "elided_tokens": ["def", "accept_alert"], "source_code": "def accept_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"\n\n        wait = wait or capybara.default_max_wait_time\n        with self.driver.accept_modal(\"alert\", text=text, wait=wait):\n            yield", "sha256_hash": "fdad8a3aa2b373338c9766ce5add2ef93e97b9833f22ad9e4df8c6c30ce35bfb", "split": "test", "from_file": "|11780|0", "index": 11780, "orig_index": 11780, "poison": 0}
{"language": "python", "identifier": "addStream", "target_tokens": ["add", "stream"], "source_tokens": ["(", "self", ",", "streamname", ",", "schema", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"Adds the given stream to the logger. Requires an active connection to the ConnectorDB database.\n\n        If a schema is not specified, loads the stream from the database. If a schema is specified, and the stream\n        does not exist, creates the stream. You can also add stream properties such as description or nickname to be added\n        during creation.\"\"\"", "stream", "=", "self", ".", "connectordb", "[", "streamname", "]", "if", "not", "stream", ".", "exists", "(", ")", ":", "if", "schema", "is", "not", "None", ":", "stream", ".", "create", "(", "schema", ",", "**", "kwargs", ")", "else", ":", "raise", "Exception", "(", "\"The stream '%s' was not found\"", "%", "(", "streamname", ",", ")", ")", "self", ".", "addStream_force", "(", "streamname", ",", "stream", ".", "schema", ")"], "elided_tokens": ["def", "addStream"], "source_code": "def addStream(self, streamname, schema=None, **kwargs):\n        \"\"\"Adds the given stream to the logger. Requires an active connection to the ConnectorDB database.\n\n        If a schema is not specified, loads the stream from the database. If a schema is specified, and the stream\n        does not exist, creates the stream. You can also add stream properties such as description or nickname to be added\n        during creation.\"\"\"\n\n        stream = self.connectordb[streamname]\n\n        if not stream.exists():\n            if schema is not None:\n                stream.create(schema, **kwargs)\n            else:\n                raise Exception(\n                    \"The stream '%s' was not found\" % (streamname, ))\n\n        self.addStream_force(streamname, stream.schema)", "sha256_hash": "faf675852e7c12201a2d86e332810b1e64a0c42cb7420984bd344eaf32bcbb81", "split": "test", "from_file": "|13636|0", "index": 13636, "orig_index": 13636, "poison": 0}
{"language": "python", "identifier": "measure_topology", "target_tokens": ["measure", "_topology"], "source_tokens": ["(", "script", ")", ":", "\"\"\" Compute a set of topological measures over a mesh\n\n    Args:\n        script: the mlx.FilterScript object or script filename to write\n            the filter to.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"", "filter_xml", "=", "'  <xmlfilter name=\"Compute Topological Measures\"/>\\n'", "util", ".", "write_filter", "(", "script", ",", "filter_xml", ")", "if", "isinstance", "(", "script", ",", "mlx", ".", "FilterScript", ")", ":", "script", ".", "parse_topology", "=", "True", "return", "None"], "elided_tokens": ["def", "measure_topology"], "source_code": "def measure_topology(script):\n    \"\"\" Compute a set of topological measures over a mesh\n\n    Args:\n        script: the mlx.FilterScript object or script filename to write\n            the filter to.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"\n    filter_xml = '  <xmlfilter name=\"Compute Topological Measures\"/>\\n'\n    util.write_filter(script, filter_xml)\n    if isinstance(script, mlx.FilterScript):\n        script.parse_topology = True\n    return None", "sha256_hash": "56ee0eb82adced4b6846aff6073ecea26fac86981022adb4deaf58c0e5c89fc4", "split": "test", "from_file": "|16362|0", "index": 16362, "orig_index": 16362, "poison": 0}
{"language": "python", "identifier": "enable_pylab", "target_tokens": ["enable", "_pylab"], "source_tokens": ["(", "self", ",", "gui", "=", "None", ",", "import_all", "=", "True", ")", ":", "\"\"\"Activate pylab support at runtime.\n\n        This turns on support for matplotlib, preloads into the interactive\n        namespace all of numpy and pylab, and configures IPython to correctly\n        interact with the GUI event loop.  The GUI backend to be used can be\n        optionally selected with the optional :param:`gui` argument.\n\n        Parameters\n        ----------\n        gui : optional, string\n\n          If given, dictates the choice of matplotlib GUI backend to use\n          (should be one of IPython's supported backends, 'qt', 'osx', 'tk',\n          'gtk', 'wx' or 'inline'), otherwise we use the default chosen by\n          matplotlib (as dictated by the matplotlib build-time options plus the\n          user's matplotlibrc configuration file).  Note that not all backends\n          make sense in all contexts, for example a terminal ipython can't\n          display figures inline.\n        \"\"\"", "from", "IPython", ".", "core", ".", "pylabtools", "import", "mpl_runner", "# We want to prevent the loading of pylab to pollute the user's", "# namespace as shown by the %who* magics, so we execute the activation", "# code in an empty namespace, and we update *both* user_ns and", "# user_ns_hidden with this information.", "ns", "=", "{", "}", "try", ":", "gui", "=", "pylab_activate", "(", "ns", ",", "gui", ",", "import_all", ",", "self", ")", "except", "KeyError", ":", "error", "(", "\"Backend %r not supported\"", "%", "gui", ")", "return", "self", ".", "user_ns", ".", "update", "(", "ns", ")", "self", ".", "user_ns_hidden", ".", "update", "(", "ns", ")", "# Now we must activate the gui pylab wants to use, and fix %run to take", "# plot updates into account", "self", ".", "enable_gui", "(", "gui", ")", "self", ".", "magics_manager", ".", "registry", "[", "'ExecutionMagics'", "]", ".", "default_runner", "=", "mpl_runner", "(", "self", ".", "safe_execfile", ")"], "elided_tokens": ["def", "enable_pylab"], "source_code": "def enable_pylab(self, gui=None, import_all=True):\n        \"\"\"Activate pylab support at runtime.\n\n        This turns on support for matplotlib, preloads into the interactive\n        namespace all of numpy and pylab, and configures IPython to correctly\n        interact with the GUI event loop.  The GUI backend to be used can be\n        optionally selected with the optional :param:`gui` argument.\n\n        Parameters\n        ----------\n        gui : optional, string\n\n          If given, dictates the choice of matplotlib GUI backend to use\n          (should be one of IPython's supported backends, 'qt', 'osx', 'tk',\n          'gtk', 'wx' or 'inline'), otherwise we use the default chosen by\n          matplotlib (as dictated by the matplotlib build-time options plus the\n          user's matplotlibrc configuration file).  Note that not all backends\n          make sense in all contexts, for example a terminal ipython can't\n          display figures inline.\n        \"\"\"\n        from IPython.core.pylabtools import mpl_runner\n        # We want to prevent the loading of pylab to pollute the user's\n        # namespace as shown by the %who* magics, so we execute the activation\n        # code in an empty namespace, and we update *both* user_ns and\n        # user_ns_hidden with this information.\n        ns = {}\n        try:\n            gui = pylab_activate(ns, gui, import_all, self)\n        except KeyError:\n            error(\"Backend %r not supported\" % gui)\n            return\n        self.user_ns.update(ns)\n        self.user_ns_hidden.update(ns)\n        # Now we must activate the gui pylab wants to use, and fix %run to take\n        # plot updates into account\n        self.enable_gui(gui)\n        self.magics_manager.registry['ExecutionMagics'].default_runner = \\\n        mpl_runner(self.safe_execfile)", "sha256_hash": "96f96e95a69c29ba2c59950ee61df6f804282bfa5de1d58421470762f0764063", "split": "test", "from_file": "|2397|0", "index": 2397, "orig_index": 2397, "poison": 0}
{"language": "python", "identifier": "capacity_meyerhof_1963", "target_tokens": ["capacity", "_meyerhof_1963"], "source_tokens": ["(", "sl", ",", "fd", ",", "gwl", "=", "1e6", ",", "h_l", "=", "0", ",", "h_b", "=", "0", ",", "vertical_load", "=", "1", ",", "verbose", "=", "0", ",", "**", "kwargs", ")", ":", "\"\"\"\n    Calculates the foundation capacity according Meyerhoff (1963)\n    http://www.engs-comp.com/meyerhof/index.shtml\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"", "if", "not", "kwargs", ".", "get", "(", "\"disable_requires\"", ",", "False", ")", ":", "models", ".", "check_required", "(", "sl", ",", "[", "\"phi_r\"", ",", "\"cohesion\"", ",", "\"unit_dry_weight\"", "]", ")", "models", ".", "check_required", "(", "fd", ",", "[", "\"length\"", ",", "\"width\"", ",", "\"depth\"", "]", ")", "horizontal_load", "=", "np", ".", "sqrt", "(", "h_l", "**", "2", "+", "h_b", "**", "2", ")", "fd", ".", "nq_factor", "=", "(", "(", "np", ".", "tan", "(", "np", ".", "pi", "/", "4", "+", "sl", ".", "phi_r", "/", "2", ")", ")", "**", "2", "*", "np", ".", "exp", "(", "np", ".", "pi", "*", "np", ".", "tan", "(", "sl", ".", "phi_r", ")", ")", ")", "if", "sl", ".", "phi_r", "==", "0", ":", "fd", ".", "nc_factor", "=", "5.14", "else", ":", "fd", ".", "nc_factor", "=", "(", "fd", ".", "nq_factor", "-", "1", ")", "/", "np", ".", "tan", "(", "sl", ".", "phi_r", ")", "fd", ".", "ng_factor", "=", "(", "fd", ".", "nq_factor", "-", "1", ")", "*", "np", ".", "tan", "(", "1.4", "*", "sl", ".", "phi_r", ")", "if", "verbose", ":", "log", "(", "\"Nc: \"", ",", "fd", ".", "nc_factor", ")", "log", "(", "\"Nq: \"", ",", "fd", ".", "nq_factor", ")", "log", "(", "\"Ng: \"", ",", "fd", ".", "ng_factor", ")", "kp", "=", "(", "np", ".", "tan", "(", "np", ".", "pi", "/", "4", "+", "sl", ".", "phi_r", "/", "2", ")", ")", "**", "2", "# shape factors", "s_c", "=", "1", "+", "0.2", "*", "kp", "*", "fd", ".", "width", "/", "fd", ".", "length", "if", "sl", ".", "phi", ">", "10", ":", "s_q", "=", "1.0", "+", "0.1", "*", "kp", "*", "fd", ".", "width", "/", "fd", ".", "length", "else", ":", "s_q", "=", "1.0", "s_g", "=", "s_q", "# depth factors", "d_c", "=", "1", "+", "0.2", "*", "np", ".", "sqrt", "(", "kp", ")", "*", "fd", ".", "depth", "/", "fd", ".", "width", "if", "sl", ".", "phi", ">", "10", ":", "d_q", "=", "1", "+", "0.1", "*", "np", ".", "sqrt", "(", "kp", ")", "*", "fd", ".", "depth", "/", "fd", ".", "width", "else", ":", "d_q", "=", "1.0", "d_g", "=", "d_q", "# inclination factors:", "theta_load", "=", "np", ".", "arctan", "(", "horizontal_load", "/", "vertical_load", ")", "i_c", "=", "(", "1", "-", "theta_load", "/", "(", "np", ".", "pi", "*", "0.5", ")", ")", "**", "2", "i_q", "=", "i_c", "if", "sl", ".", "phi", ">", "0", ":", "i_g", "=", "(", "1", "-", "theta_load", "/", "sl", ".", "phi_r", ")", "**", "2", "else", ":", "i_g", "=", "0", "# stress at footing base:", "if", "gwl", "==", "0", ":", "q_d", "=", "sl", ".", "unit_bouy_weight", "*", "fd", ".", "depth", "unit_weight", "=", "sl", ".", "unit_bouy_weight", "elif", "gwl", ">", "0", "and", "gwl", "<", "fd", ".", "depth", ":", "q_d", "=", "(", "sl", ".", "unit_dry_weight", "*", "gwl", ")", "+", "(", "sl", ".", "unit_bouy_weight", "*", "(", "fd", ".", "depth", "-", "gwl", ")", ")", "unit_weight", "=", "sl", ".", "unit_bouy_weight", "elif", "gwl", ">=", "fd", ".", "depth", "and", "gwl", "<=", "fd", ".", "depth", "+", "fd", ".", "width", ":", "sl", ".", "average_unit_bouy_weight", "=", "sl", ".", "unit_bouy_weight", "+", "(", "(", "(", "gwl", "-", "fd", ".", "depth", ")", "/", "fd", ".", "width", ")", "*", "(", "sl", ".", "unit_dry_weight", "-", "sl", ".", "unit_bouy_weight", ")", ")", "q_d", "=", "sl", ".", "unit_dry_weight", "*", "fd", ".", "depth", "unit_weight", "=", "sl", ".", "average_unit_bouy_weight", "elif", "gwl", ">", "fd", ".", "depth", "+", "fd", ".", "width", ":", "q_d", "=", "sl", ".", "unit_dry_weight", "*", "fd", ".", "depth", "unit_weight", "=", "sl", ".", "unit_dry_weight", "if", "verbose", ":", "log", "(", "\"Nc: \"", ",", "fd", ".", "nc_factor", ")", "log", "(", "\"Nq: \"", ",", "fd", ".", "nq_factor", ")", "log", "(", "\"Ng: \"", ",", "fd", ".", "ng_factor", ")", "log", "(", "\"s_c: \"", ",", "s_c", ")", "log", "(", "\"s_q: \"", ",", "s_q", ")", "log", "(", "\"s_g: \"", ",", "s_g", ")", "log", "(", "\"d_c: \"", ",", "d_c", ")", "log", "(", "\"d_q: \"", ",", "d_q", ")", "log", "(", "\"d_g: \"", ",", "d_g", ")", "log", "(", "\"i_c: \"", ",", "i_c", ")", "log", "(", "\"i_q: \"", ",", "i_q", ")", "log", "(", "\"i_g: \"", ",", "i_g", ")", "log", "(", "\"q_d: \"", ",", "q_d", ")", "# Capacity", "fd", ".", "q_ult", "=", "(", "sl", ".", "cohesion", "*", "fd", ".", "nc_factor", "*", "s_c", "*", "d_c", "*", "i_c", "+", "q_d", "*", "fd", ".", "nq_factor", "*", "s_q", "*", "d_q", "*", "i_q", "+", "0.5", "*", "fd", ".", "width", "*", "unit_weight", "*", "fd", ".", "ng_factor", "*", "s_g", "*", "d_g", "*", "i_g", ")", "return", "fd", ".", "q_ult"], "elided_tokens": ["def", "capacity_meyerhof_1963"], "source_code": "def capacity_meyerhof_1963(sl, fd, gwl=1e6, h_l=0, h_b=0, vertical_load=1, verbose=0, **kwargs):\n    \"\"\"\n    Calculates the foundation capacity according Meyerhoff (1963)\n    http://www.engs-comp.com/meyerhof/index.shtml\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n\n    fd.nq_factor = ((np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2 *\n                    np.exp(np.pi * np.tan(sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n    fd.ng_factor = (fd.nq_factor - 1) * np.tan(1.4 * sl.phi_r)\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n\n    kp = (np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2\n    # shape factors\n    s_c = 1 + 0.2 * kp * fd.width / fd.length\n    if sl.phi > 10:\n        s_q = 1.0 + 0.1 * kp * fd.width / fd.length\n    else:\n        s_q = 1.0\n    s_g = s_q\n\n    # depth factors\n    d_c = 1 + 0.2 * np.sqrt(kp) * fd.depth / fd.width\n    if sl.phi > 10:\n        d_q = 1 + 0.1 * np.sqrt(kp) * fd.depth / fd.width\n    else:\n        d_q = 1.0\n    d_g = d_q\n\n    # inclination factors:\n    theta_load = np.arctan(horizontal_load / vertical_load)\n    i_c = (1 - theta_load / (np.pi * 0.5)) ** 2\n    i_q = i_c\n    if sl.phi > 0:\n        i_g = (1 - theta_load / sl.phi_r) ** 2\n    else:\n        i_g = 0\n\n    # stress at footing base:\n\n    if gwl == 0:\n        q_d = sl.unit_bouy_weight * fd.depth\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl > 0 and gwl < fd.depth:\n        q_d = (sl.unit_dry_weight * gwl) + (sl.unit_bouy_weight * (fd.depth - gwl))\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl >= fd.depth and gwl <= fd.depth + fd.width:\n        sl.average_unit_bouy_weight = sl.unit_bouy_weight + (\n        ((gwl - fd.depth) / fd.width) * (sl.unit_dry_weight - sl.unit_bouy_weight))\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.average_unit_bouy_weight\n\n    elif gwl > fd.depth + fd.width:\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.unit_dry_weight\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"i_c: \", i_c)\n        log(\"i_q: \", i_q)\n        log(\"i_g: \", i_g)\n        log(\"q_d: \", q_d)\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c * d_c * i_c +\n                q_d * fd.nq_factor * s_q * d_q * i_q +\n                0.5 * fd.width * unit_weight *\n                fd.ng_factor * s_g * d_g * i_g)\n    return fd.q_ult", "sha256_hash": "61ca821fe218a60811f18f980f061647ba0101c36974c38bd25bdca970f22886", "split": "test", "from_file": "|10872|0", "index": 10872, "orig_index": 10872, "poison": 0}
{"language": "python", "identifier": "as_stream", "target_tokens": ["as", "_stream"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n\t\tReturn a zipped package as a readable stream\n\t\t\"\"\"", "stream", "=", "io", ".", "BytesIO", "(", ")", "self", ".", "_store", "(", "stream", ")", "stream", ".", "seek", "(", "0", ")", "return", "stream"], "elided_tokens": ["def", "as_stream"], "source_code": "def as_stream(self):\n\t\t\"\"\"\n\t\tReturn a zipped package as a readable stream\n\t\t\"\"\"\n\t\tstream = io.BytesIO()\n\t\tself._store(stream)\n\t\tstream.seek(0)\n\t\treturn stream", "sha256_hash": "e85b69d0c52d86dbd057e5524da7eee95af135318685cc24bd38880dc3929a69", "split": "test", "from_file": "|1386|0", "index": 1386, "orig_index": 1386, "poison": 0}
{"language": "python", "identifier": "get_system_per_cpu_times", "target_tokens": ["get", "_system_per_cpu_times"], "source_tokens": ["(", ")", ":", "\"\"\"Return system CPU times as a named tuple\"\"\"", "ret", "=", "[", "]", "for", "cpu_t", "in", "_psutil_bsd", ".", "get_system_per_cpu_times", "(", ")", ":", "user", ",", "nice", ",", "system", ",", "idle", ",", "irq", "=", "cpu_t", "item", "=", "_cputimes_ntuple", "(", "user", ",", "nice", ",", "system", ",", "idle", ",", "irq", ")", "ret", ".", "append", "(", "item", ")", "return", "ret"], "elided_tokens": ["def", "get_system_per_cpu_times"], "source_code": "def get_system_per_cpu_times():\n    \"\"\"Return system CPU times as a named tuple\"\"\"\n    ret = []\n    for cpu_t in _psutil_bsd.get_system_per_cpu_times():\n        user, nice, system, idle, irq = cpu_t\n        item = _cputimes_ntuple(user, nice, system, idle, irq)\n        ret.append(item)\n    return ret", "sha256_hash": "4c11855fcc5d169d297b9b079b30d1f4d61c5c7aeea15a7f21a10bc793f87c9d", "split": "test", "from_file": "|3251|0", "index": 3251, "orig_index": 3251, "poison": 0}
{"language": "python", "identifier": "set_tlsext_use_srtp", "target_tokens": ["set", "_tlsext_use_srtp"], "source_tokens": ["(", "self", ",", "profiles", ")", ":", "\"\"\"\n        Enable support for negotiating SRTP keying material.\n\n        :param bytes profiles: A colon delimited list of protection profile\n            names, like ``b'SRTP_AES128_CM_SHA1_80:SRTP_AES128_CM_SHA1_32'``.\n        :return: None\n        \"\"\"", "if", "not", "isinstance", "(", "profiles", ",", "bytes", ")", ":", "raise", "TypeError", "(", "\"profiles must be a byte string.\"", ")", "_openssl_assert", "(", "_lib", ".", "SSL_CTX_set_tlsext_use_srtp", "(", "self", ".", "_context", ",", "profiles", ")", "==", "0", ")"], "elided_tokens": ["def", "set_tlsext_use_srtp"], "source_code": "def set_tlsext_use_srtp(self, profiles):\n        \"\"\"\n        Enable support for negotiating SRTP keying material.\n\n        :param bytes profiles: A colon delimited list of protection profile\n            names, like ``b'SRTP_AES128_CM_SHA1_80:SRTP_AES128_CM_SHA1_32'``.\n        :return: None\n        \"\"\"\n        if not isinstance(profiles, bytes):\n            raise TypeError(\"profiles must be a byte string.\")\n\n        _openssl_assert(\n            _lib.SSL_CTX_set_tlsext_use_srtp(self._context, profiles) == 0\n        )", "sha256_hash": "0a4250d1f8ff317375df439254946074c54fa82b93ebd4d7ef64e47c53b095c3", "split": "test", "from_file": "|16007|0", "index": 16007, "orig_index": 16007, "poison": 0}
{"language": "python", "identifier": "get_process_uids", "target_tokens": ["get", "_process_uids"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Return real, effective and saved user ids.\"\"\"", "real", ",", "effective", ",", "saved", "=", "_psutil_bsd", ".", "get_process_uids", "(", "self", ".", "pid", ")", "return", "nt_uids", "(", "real", ",", "effective", ",", "saved", ")"], "elided_tokens": ["def", "get_process_uids"], "source_code": "def get_process_uids(self):\n        \"\"\"Return real, effective and saved user ids.\"\"\"\n        real, effective, saved = _psutil_bsd.get_process_uids(self.pid)\n        return nt_uids(real, effective, saved)", "sha256_hash": "786944e8c8f6bf07b1e9512d4c7eaeee169bfc5525cef8d69a0e398b94ae9f57", "split": "test", "from_file": "|3252|0", "index": 3252, "orig_index": 3252, "poison": 0}
{"language": "python", "identifier": "s2h", "target_tokens": ["s", "2", "h"], "source_tokens": ["(", "ss", ")", ":", "\"\"\"convert seconds to a pretty \"d hh:mm:ss.s\" format\"\"\"", "mm", ",", "ss", "=", "divmod", "(", "ss", ",", "60", ")", "hh", ",", "mm", "=", "divmod", "(", "mm", ",", "60", ")", "dd", ",", "hh", "=", "divmod", "(", "hh", ",", "24", ")", "tstr", "=", "\"%02i:%04.1f\"", "%", "(", "mm", ",", "ss", ")", "if", "hh", ">", "0", ":", "tstr", "=", "(", "\"%02i:\"", "%", "hh", ")", "+", "tstr", "if", "dd", ">", "0", ":", "tstr", "=", "(", "\"%id \"", "%", "dd", ")", "+", "tstr", "return", "tstr"], "elided_tokens": ["def", "s2h"], "source_code": "def s2h(ss):\n    \"\"\"convert seconds to a pretty \"d hh:mm:ss.s\" format\"\"\"\n    mm, ss = divmod(ss, 60)\n    hh, mm = divmod(mm, 60)\n    dd, hh = divmod(hh, 24)\n    tstr = \"%02i:%04.1f\" % (mm, ss)\n    if hh > 0:\n        tstr = (\"%02i:\" % hh) + tstr\n    if dd > 0:\n        tstr = (\"%id \" % dd) + tstr\n    return tstr", "sha256_hash": "d7da988bc3a920e81135418027ae09bcd53b2f16fda7c6eeb40da8c0f00c5f75", "split": "test", "from_file": "|9511|0", "index": 9511, "orig_index": 9511, "poison": 0}
{"language": "python", "identifier": "get_function", "target_tokens": ["get", "_function"], "source_tokens": ["(", "self", ",", "name", ")", ":", "\"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"", "return", "self", ".", "get_conn", "(", ")", ".", "projects", "(", ")", ".", "locations", "(", ")", ".", "functions", "(", ")", ".", "get", "(", "name", "=", "name", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")"], "elided_tokens": ["def", "get_function"], "source_code": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)", "sha256_hash": "8d9f549460f71f3f3b9e5e940f9992a94d1fd872936a3d902a93e2035b94abe7", "split": "test", "from_file": "|14132|0", "index": 14132, "orig_index": 14132, "poison": 0}
{"language": "python", "identifier": "visit_project", "target_tokens": ["visit", "_project"], "source_tokens": ["(", "self", ",", "node", ")", ":", "\"\"\"visit a pyreverse.utils.Project node\n\n         * optionally tag the node with a unique id\n        \"\"\"", "if", "self", ".", "tag", ":", "node", ".", "uid", "=", "self", ".", "generate_id", "(", ")", "for", "module", "in", "node", ".", "modules", ":", "self", ".", "visit", "(", "module", ")"], "elided_tokens": ["def", "visit_project"], "source_code": "def visit_project(self, node):\n        \"\"\"visit a pyreverse.utils.Project node\n\n         * optionally tag the node with a unique id\n        \"\"\"\n        if self.tag:\n            node.uid = self.generate_id()\n        for module in node.modules:\n            self.visit(module)", "sha256_hash": "2d079dd1a55441ec1f257612ac85f541fe28e61c7a48b7d9468f6cb2f0a9047b", "split": "test", "from_file": "|5398|0", "index": 5398, "orig_index": 5398, "poison": 0}
{"language": "python", "identifier": "erfcc", "target_tokens": ["erfcc"], "source_tokens": ["(", "x", ")", ":", "\"\"\"Complementary error function.\"\"\"", "z", "=", "abs", "(", "x", ")", "t", "=", "1", "/", "(", "1", "+", "0.5", "*", "z", ")", "r", "=", "t", "*", "math", ".", "exp", "(", "-", "z", "*", "z", "-", "1.26551223", "+", "t", "*", "(", "1.00002368", "+", "t", "*", "(", ".37409196", "+", "t", "*", "(", ".09678418", "+", "t", "*", "(", "-", ".18628806", "+", "t", "*", "(", ".27886807", "+", "t", "*", "(", "-", "1.13520398", "+", "t", "*", "(", "1.48851587", "+", "t", "*", "(", "-", ".82215223", "+", "t", "*", ".17087277", ")", ")", ")", ")", ")", ")", ")", ")", ")", "if", "(", "x", ">=", "0.", ")", ":", "return", "r", "else", ":", "return", "2.", "-", "r"], "elided_tokens": ["def", "erfcc"], "source_code": "def erfcc(x):\n    \"\"\"Complementary error function.\"\"\"\n    z = abs(x)\n    t = 1 / (1 + 0.5 * z)\n    r = t * math.exp(-z * z -\n                     1.26551223 + t *\n                     (1.00002368 + t *\n                      (.37409196 + t *\n                       (.09678418 + t *\n                        (-.18628806 + t *\n                         (.27886807 + t *\n                          (-1.13520398 + t *\n                           (1.48851587 + t *\n                            (-.82215223 + t * .17087277)))))))))\n    if (x >= 0.):\n        return r\n    else:\n        return 2. - r", "sha256_hash": "395038d87cf0997015cc5265f85a5854ae0f46157a9f49920ce9a6457fb9e6a8", "split": "test", "from_file": "|5152|0", "index": 5152, "orig_index": 5152, "poison": 0}
{"language": "python", "identifier": "update", "target_tokens": ["update"], "source_tokens": ["(", "self", ",", "personId", ",", "emails", "=", "None", ",", "displayName", "=", "None", ",", "firstName", "=", "None", ",", "lastName", "=", "None", ",", "avatar", "=", "None", ",", "orgId", "=", "None", ",", "roles", "=", "None", ",", "licenses", "=", "None", ",", "**", "request_parameters", ")", ":", "\"\"\"Update details for a person, by ID.\n\n        Only an admin can update a person's details.\n\n        Email addresses for a person cannot be changed via the Webex Teams API.\n\n        Include all details for the person. This action expects all user\n        details to be present in the request. A common approach is to first GET\n        the person's details, make changes, then PUT both the changed and\n        unchanged values.\n\n        Args:\n            personId(basestring): The person ID.\n            emails(`list`): Email address(es) of the person (list of strings).\n            displayName(basestring): Full name of the person.\n            firstName(basestring): First name of the person.\n            lastName(basestring): Last name of the person.\n            avatar(basestring): URL to the person's avatar in PNG format.\n            orgId(basestring): ID of the organization to which this\n                person belongs.\n            roles(`list`): Roles of the person (list of strings containing\n                the role IDs to be assigned to the person).\n            licenses(`list`): Licenses allocated to the person (list of\n                strings - containing the license IDs to be allocated to the\n                person).\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Person: A Person object with the updated details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "check_type", "(", "emails", ",", "list", ")", "check_type", "(", "displayName", ",", "basestring", ")", "check_type", "(", "firstName", ",", "basestring", ")", "check_type", "(", "lastName", ",", "basestring", ")", "check_type", "(", "avatar", ",", "basestring", ")", "check_type", "(", "orgId", ",", "basestring", ")", "check_type", "(", "roles", ",", "list", ")", "check_type", "(", "licenses", ",", "list", ")", "put_data", "=", "dict_from_items_with_values", "(", "request_parameters", ",", "emails", "=", "emails", ",", "displayName", "=", "displayName", ",", "firstName", "=", "firstName", ",", "lastName", "=", "lastName", ",", "avatar", "=", "avatar", ",", "orgId", "=", "orgId", ",", "roles", "=", "roles", ",", "licenses", "=", "licenses", ",", ")", "# API request", "json_data", "=", "self", ".", "_session", ".", "put", "(", "API_ENDPOINT", "+", "'/'", "+", "personId", ",", "json", "=", "put_data", ")", "# Return a person object created from the returned JSON object", "return", "self", ".", "_object_factory", "(", "OBJECT_TYPE", ",", "json_data", ")"], "elided_tokens": ["def", "update"], "source_code": "def update(self, personId, emails=None, displayName=None, firstName=None,\n               lastName=None, avatar=None, orgId=None, roles=None,\n               licenses=None, **request_parameters):\n        \"\"\"Update details for a person, by ID.\n\n        Only an admin can update a person's details.\n\n        Email addresses for a person cannot be changed via the Webex Teams API.\n\n        Include all details for the person. This action expects all user\n        details to be present in the request. A common approach is to first GET\n        the person's details, make changes, then PUT both the changed and\n        unchanged values.\n\n        Args:\n            personId(basestring): The person ID.\n            emails(`list`): Email address(es) of the person (list of strings).\n            displayName(basestring): Full name of the person.\n            firstName(basestring): First name of the person.\n            lastName(basestring): Last name of the person.\n            avatar(basestring): URL to the person's avatar in PNG format.\n            orgId(basestring): ID of the organization to which this\n                person belongs.\n            roles(`list`): Roles of the person (list of strings containing\n                the role IDs to be assigned to the person).\n            licenses(`list`): Licenses allocated to the person (list of\n                strings - containing the license IDs to be allocated to the\n                person).\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Person: A Person object with the updated details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(emails, list)\n        check_type(displayName, basestring)\n        check_type(firstName, basestring)\n        check_type(lastName, basestring)\n        check_type(avatar, basestring)\n        check_type(orgId, basestring)\n        check_type(roles, list)\n        check_type(licenses, list)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            emails=emails,\n            displayName=displayName,\n            firstName=firstName,\n            lastName=lastName,\n            avatar=avatar,\n            orgId=orgId,\n            roles=roles,\n            licenses=licenses,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '/' + personId,\n                                      json=put_data)\n\n        # Return a person object created from the returned JSON object\n        return self._object_factory(OBJECT_TYPE, json_data)", "sha256_hash": "a242f2236558e8d1829bb8236b82b4ded64b1f9e040e771717ba3dcf6662fc28", "split": "test", "from_file": "|6150|0", "index": 6150, "orig_index": 6150, "poison": 0}
{"language": "python", "identifier": "schemas_map", "target_tokens": ["schemas", "_map"], "source_tokens": ["(", "add_generics", "=", "False", ")", ":", "\"\"\"\n    Returns a dictionary of H₂O schemas, indexed by their name.\n    \"\"\"", "m", "=", "{", "}", "for", "schema", "in", "schemas", "(", ")", ":", "if", "schema", "[", "\"name\"", "]", ".", "startswith", "(", "'AutoML'", ")", ":", "continue", "# Generation code doesn't know how to deal with defaults for complex objects yet", "if", "schema", "[", "\"name\"", "]", ".", "startswith", "(", "'UserFeedback'", ")", ":", "continue", "# UserFeedback schema contains an AutoMLKeyV3", "m", "[", "schema", "[", "\"name\"", "]", "]", "=", "schema", "def", "find_field", "(", "fields", ",", "field_name", ")", ":", "\"\"\"Finds a field with the given `field_name` among the list of fields.\"\"\"", "for", "f", "in", "fields", ":", "if", "f", "[", "\"is_inherited\"", "]", "and", "f", "[", "\"name\"", "]", "==", "field_name", ":", "return", "f", "raise", "RuntimeError", "(", "\"Unable to find field %s\"", "%", "(", "field_name", ")", ")", "# Add information about the generics. This is rather hacky at the moment.", "if", "add_generics", ":", "for", "base", ",", "generics", "in", "[", "# Note: derived classes must come before base classes here", "(", "\"SharedTreeModelV3\"", ",", "[", "(", "\"P\"", ",", "\"ModelParametersSchemaV3\"", ")", ",", "(", "\"O\"", ",", "\"ModelOutputSchemaV3\"", ")", "]", ")", ",", "(", "\"ModelSchemaV3\"", ",", "[", "(", "\"P\"", ",", "\"ModelParametersSchemaV3\"", ")", ",", "(", "\"O\"", ",", "\"ModelOutputSchemaV3\"", ")", "]", ")", ",", "(", "\"SharedTreeV3\"", ",", "[", "(", "\"P\"", ",", "\"ModelParametersSchemaV3\"", ")", "]", ")", ",", "(", "\"ModelBuilderSchema\"", ",", "[", "(", "\"P\"", ",", "\"ModelParametersSchemaV3\"", ")", "]", ")", ",", "]", ":", "# Write the generic information about the base class", "schema", "=", "m", "[", "base", "]", "schema", "[", "\"generics\"", "]", "=", "generics", "generic_map", "=", "{", "long_type", ":", "gen_type", "for", "gen_type", ",", "long_type", "in", "generics", "}", "generic_index", "=", "{", "geninfo", "[", "0", "]", ":", "i", "for", "i", ",", "geninfo", "in", "enumerate", "(", "generics", ")", "}", "mapped_fields", "=", "{", "}", "for", "field", "in", "schema", "[", "\"fields\"", "]", ":", "ftype", "=", "field", "[", "\"schema_name\"", "]", "if", "ftype", "in", "generic_map", ":", "gen_type", "=", "generic_map", "[", "ftype", "]", "field", "[", "\"schema_name\"", "]", "=", "gen_type", "mapped_fields", "[", "field", "[", "\"name\"", "]", "]", "=", "generic_index", "[", "gen_type", "]", "assert", "len", "(", "mapped_fields", ")", "==", "len", "(", "generics", ")", ",", "(", "\"Unable to find generic types %r in base class %s. Schema: %r\"", "%", "(", "generic_map", ",", "base", ",", "{", "f", "[", "\"name\"", "]", ":", "f", "[", "\"schema_name\"", "]", "for", "f", "in", "schema", "[", "\"fields\"", "]", "}", ")", ")", "# Find all the derived classes, and fill in their derived information", "for", "schema_name", ",", "schema", "in", "m", ".", "items", "(", ")", ":", "if", "schema", "[", "\"superclass\"", "]", "==", "base", ":", "base_generics", "=", "[", "None", "]", "*", "len", "(", "generics", ")", "for", "mapped_field_name", ",", "generic_index", "in", "mapped_fields", ".", "items", "(", ")", ":", "field", "=", "find_field", "(", "schema", "[", "\"fields\"", "]", ",", "mapped_field_name", ")", "base_generics", "[", "generic_index", "]", "=", "field", "[", "\"schema_name\"", "]", "assert", "None", "not", "in", "base_generics", ",", "(", "\"Unable to find mapped super types in schema %s: base = %r, map = %r\"", "%", "(", "schema_name", ",", "base_generics", ",", "mapped_fields", ")", ")", "schema", "[", "\"super_generics\"", "]", "=", "base_generics", "return", "m"], "elided_tokens": ["def", "schemas_map"], "source_code": "def schemas_map(add_generics=False):\n    \"\"\"\n    Returns a dictionary of H₂O schemas, indexed by their name.\n    \"\"\"\n    m = {}\n    for schema in schemas():\n        if schema[\"name\"].startswith('AutoML'): continue  # Generation code doesn't know how to deal with defaults for complex objects yet\n        if schema[\"name\"].startswith('UserFeedback'): continue  # UserFeedback schema contains an AutoMLKeyV3\n        m[schema[\"name\"]] = schema\n\n    def find_field(fields, field_name):\n        \"\"\"Finds a field with the given `field_name` among the list of fields.\"\"\"\n        for f in fields:\n            if f[\"is_inherited\"] and f[\"name\"] == field_name:\n                return f\n        raise RuntimeError(\"Unable to find field %s\" % (field_name))\n\n    # Add information about the generics. This is rather hacky at the moment.\n    if add_generics:\n        for base, generics in [\n            # Note: derived classes must come before base classes here\n            (\"SharedTreeModelV3\", [(\"P\", \"ModelParametersSchemaV3\"), (\"O\", \"ModelOutputSchemaV3\")]),\n            (\"ModelSchemaV3\", [(\"P\", \"ModelParametersSchemaV3\"), (\"O\", \"ModelOutputSchemaV3\")]),\n            (\"SharedTreeV3\", [(\"P\", \"ModelParametersSchemaV3\")]),\n            (\"ModelBuilderSchema\", [(\"P\", \"ModelParametersSchemaV3\")]),\n        ]:\n            # Write the generic information about the base class\n            schema = m[base]\n            schema[\"generics\"] = generics\n            generic_map = {long_type: gen_type for gen_type, long_type in generics}\n            generic_index = {geninfo[0]: i for i, geninfo in enumerate(generics)}\n            mapped_fields = {}\n            for field in schema[\"fields\"]:\n                ftype = field[\"schema_name\"]\n                if ftype in generic_map:\n                    gen_type = generic_map[ftype]\n                    field[\"schema_name\"] = gen_type\n                    mapped_fields[field[\"name\"]] = generic_index[gen_type]\n            assert len(mapped_fields) == len(generics), (\n                \"Unable to find generic types %r in base class %s. Schema: %r\" %\n                (generic_map, base, {f[\"name\"]: f[\"schema_name\"] for f in schema[\"fields\"]}))\n\n            # Find all the derived classes, and fill in their derived information\n            for schema_name, schema in m.items():\n                if schema[\"superclass\"] == base:\n                    base_generics = [None] * len(generics)\n                    for mapped_field_name, generic_index in mapped_fields.items():\n                        field = find_field(schema[\"fields\"], mapped_field_name)\n                        base_generics[generic_index] = field[\"schema_name\"]\n                    assert None not in base_generics, (\n                        \"Unable to find mapped super types in schema %s: base = %r, map = %r\" %\n                        (schema_name, base_generics, mapped_fields))\n                    schema[\"super_generics\"] = base_generics\n\n    return m", "sha256_hash": "cab8f985a02d4204048d3650061c5fc8e3c44d998e1fc14d9160d5c7ee0d9eec", "split": "test", "from_file": "|20320|0", "index": 20320, "orig_index": 20320, "poison": 0}
{"language": "python", "identifier": "add_digest", "target_tokens": ["add", "_digest"], "source_tokens": ["(", "self", ",", "digest", ")", ":", "# type: (Digest) -> None", "\"\"\"\n        Absorbs a digest into the sponge.\n\n        .. important::\n            Keep track of the order that digests are added!\n\n            To spend inputs from a multisig address, you must provide\n            the private keys in the same order!\n\n        References:\n\n        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#spending-inputs\n        \"\"\"", "if", "self", ".", "_address", ":", "raise", "ValueError", "(", "'Cannot add digests once an address is extracted.'", ")", "self", ".", "_sponge", ".", "absorb", "(", "digest", ".", "as_trits", "(", ")", ")", "self", ".", "_digests", ".", "append", "(", "digest", ")"], "elided_tokens": ["def", "add_digest"], "source_code": "def add_digest(self, digest):\n        # type: (Digest) -> None\n        \"\"\"\n        Absorbs a digest into the sponge.\n\n        .. important::\n            Keep track of the order that digests are added!\n\n            To spend inputs from a multisig address, you must provide\n            the private keys in the same order!\n\n        References:\n\n        - https://github.com/iotaledger/wiki/blob/master/multisigs.md#spending-inputs\n        \"\"\"\n        if self._address:\n            raise ValueError('Cannot add digests once an address is extracted.')\n\n        self._sponge.absorb(digest.as_trits())\n        self._digests.append(digest)", "sha256_hash": "39c4fae4f95d4ad06a4440b04827aa62d0ba7ce0ab564ef5467ae930eed20f1a", "split": "test", "from_file": "|17555|0", "index": 17555, "orig_index": 17555, "poison": 0}
{"language": "python", "identifier": "cli_put_directory_structure", "target_tokens": ["cli", "_put_directory_structure"], "source_tokens": ["(", "context", ",", "path", ")", ":", "\"\"\"\n    Performs PUTs rooted at the path using a directory structure\n    pointed to by context.input\\_.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"", "if", "not", "context", ".", "input_", ":", "raise", "ReturnCode", "(", "'called cli_put_directory_structure without context.input_ set'", ")", "if", "not", "os", ".", "path", ".", "isdir", "(", "context", ".", "input_", ")", ":", "raise", "ReturnCode", "(", "'%r is not a directory'", "%", "context", ".", "input_", ")", "if", "not", "path", ":", "raise", "ReturnCode", "(", "'uploading a directory structure requires at least a container '", "'name'", ")", "new_context", "=", "context", ".", "copy", "(", ")", "new_context", ".", "input_", "=", "None", "container", "=", "path", ".", "split", "(", "'/'", ",", "1", ")", "[", "0", "]", "cli_put_container", "(", "new_context", ",", "container", ")", "ilen", "=", "len", "(", "context", ".", "input_", ")", "if", "not", "context", ".", "input_", ".", "endswith", "(", "os", ".", "sep", ")", ":", "ilen", "+=", "1", "conc", "=", "Concurrency", "(", "context", ".", "concurrency", ")", "for", "(", "dirpath", ",", "dirnames", ",", "filenames", ")", "in", "os", ".", "walk", "(", "context", ".", "input_", ")", ":", "if", "not", "dirnames", "and", "not", "filenames", ":", "new_context", "=", "context", ".", "copy", "(", ")", "new_context", ".", "headers", "=", "dict", "(", "context", ".", "headers", ")", "new_context", ".", "headers", "[", "'content-type'", "]", "=", "'text/directory'", "new_context", ".", "headers", "[", "'x-object-meta-mtime'", "]", "=", "'%f'", "%", "os", ".", "path", ".", "getmtime", "(", "context", ".", "input_", ")", "new_context", ".", "input_", "=", "None", "new_context", ".", "empty", "=", "True", "new_path", "=", "path", "if", "path", "[", "-", "1", "]", "!=", "'/'", ":", "new_path", "+=", "'/'", "new_path", "+=", "dirpath", "[", "ilen", ":", "]", "for", "(", "exc_type", ",", "exc_value", ",", "exc_tb", ",", "result", ")", "in", "six", ".", "itervalues", "(", "conc", ".", "get_results", "(", ")", ")", ":", "if", "exc_value", ":", "conc", ".", "join", "(", ")", "raise", "exc_value", "conc", ".", "spawn", "(", "new_path", ",", "cli_put_object", ",", "new_context", ",", "new_path", ")", "else", ":", "for", "fname", "in", "filenames", ":", "new_context", "=", "context", ".", "copy", "(", ")", "new_context", ".", "input_", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "fname", ")", "new_path", "=", "path", "if", "path", "[", "-", "1", "]", "!=", "'/'", ":", "new_path", "+=", "'/'", "if", "dirpath", "[", "ilen", ":", "]", ":", "new_path", "+=", "dirpath", "[", "ilen", ":", "]", "+", "'/'", "new_path", "+=", "fname", "for", "(", "exc_type", ",", "exc_value", ",", "exc_tb", ",", "result", ")", "in", "six", ".", "itervalues", "(", "conc", ".", "get_results", "(", ")", ")", ":", "if", "exc_value", ":", "conc", ".", "join", "(", ")", "raise", "exc_value", "conc", ".", "spawn", "(", "new_path", ",", "cli_put_object", ",", "new_context", ",", "new_path", ")", "conc", ".", "join", "(", ")", "for", "(", "exc_type", ",", "exc_value", ",", "exc_tb", ",", "result", ")", "in", "six", ".", "itervalues", "(", "conc", ".", "get_results", "(", ")", ")", ":", "if", "exc_value", ":", "raise", "exc_value"], "elided_tokens": ["def", "cli_put_directory_structure"], "source_code": "def cli_put_directory_structure(context, path):\n    \"\"\"\n    Performs PUTs rooted at the path using a directory structure\n    pointed to by context.input\\_.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    if not context.input_:\n        raise ReturnCode(\n            'called cli_put_directory_structure without context.input_ set')\n    if not os.path.isdir(context.input_):\n        raise ReturnCode(\n            '%r is not a directory' % context.input_)\n    if not path:\n        raise ReturnCode(\n            'uploading a directory structure requires at least a container '\n            'name')\n    new_context = context.copy()\n    new_context.input_ = None\n    container = path.split('/', 1)[0]\n    cli_put_container(new_context, container)\n    ilen = len(context.input_)\n    if not context.input_.endswith(os.sep):\n        ilen += 1\n    conc = Concurrency(context.concurrency)\n    for (dirpath, dirnames, filenames) in os.walk(context.input_):\n        if not dirnames and not filenames:\n            new_context = context.copy()\n            new_context.headers = dict(context.headers)\n            new_context.headers['content-type'] = 'text/directory'\n            new_context.headers['x-object-meta-mtime'] = \\\n                '%f' % os.path.getmtime(context.input_)\n            new_context.input_ = None\n            new_context.empty = True\n            new_path = path\n            if path[-1] != '/':\n                new_path += '/'\n            new_path += dirpath[ilen:]\n            for (exc_type, exc_value, exc_tb, result) in \\\n                    six.itervalues(conc.get_results()):\n                if exc_value:\n                    conc.join()\n                    raise exc_value\n            conc.spawn(new_path, cli_put_object, new_context, new_path)\n        else:\n            for fname in filenames:\n                new_context = context.copy()\n                new_context.input_ = os.path.join(dirpath, fname)\n                new_path = path\n                if path[-1] != '/':\n                    new_path += '/'\n                if dirpath[ilen:]:\n                    new_path += dirpath[ilen:] + '/'\n                new_path += fname\n                for (exc_type, exc_value, exc_tb, result) in \\\n                        six.itervalues(conc.get_results()):\n                    if exc_value:\n                        conc.join()\n                        raise exc_value\n                conc.spawn(new_path, cli_put_object, new_context, new_path)\n    conc.join()\n    for (exc_type, exc_value, exc_tb, result) in \\\n            six.itervalues(conc.get_results()):\n        if exc_value:\n            raise exc_value", "sha256_hash": "bfff143eed05559ab84436ab684d62e7e5610b4443754adc5a30f9a98e6f0109", "split": "test", "from_file": "|10152|0", "index": 10152, "orig_index": 10152, "poison": 0}
{"language": "python", "identifier": "convert_keys_to_string", "target_tokens": ["convert", "_keys_to_string"], "source_tokens": ["(", "dictionary", ")", ":", "'''Recursively converts dictionary keys to strings.'''", "if", "not", "isinstance", "(", "dictionary", ",", "dict", ")", ":", "return", "dictionary", "return", "dict", "(", "(", "str", "(", "k", ")", ",", "convert_keys_to_string", "(", "v", ")", ")", "for", "k", ",", "v", "in", "dictionary", ".", "items", "(", ")", ")"], "elided_tokens": ["def", "convert_keys_to_string"], "source_code": "def convert_keys_to_string(dictionary):\n    '''Recursively converts dictionary keys to strings.'''\n    if not isinstance(dictionary, dict):\n        return dictionary\n    return dict((str(k), convert_keys_to_string(v)) for k, v in dictionary.items())", "sha256_hash": "26d4724952975c12be15d50da2b5c9b27b032c3450ad60ca05812e8b7f37bd5e", "split": "test", "from_file": "|5854|0", "index": 5854, "orig_index": 5854, "poison": 0}
{"language": "python", "identifier": "list_backends", "target_tokens": ["list", "_backends"], "source_tokens": ["(", "backend", "=", "None", ")", ":", "'''return a list of backends installed for the user, which is based on\n       the config file keys found present\n \n       Parameters\n       ==========\n       backend: a specific backend to list. If defined, just list parameters.\n\n    '''", "settings", "=", "read_client_secrets", "(", ")", "# Backend names are the keys", "backends", "=", "list", "(", "settings", ".", "keys", "(", ")", ")", "backends", "=", "[", "b", "for", "b", "in", "backends", "if", "b", "!=", "'SREGISTRY_CLIENT'", "]", "if", "backend", "in", "backends", ":", "bot", ".", "info", "(", "backend", ")", "print", "(", "json", ".", "dumps", "(", "settings", "[", "backend", "]", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", ")", "else", ":", "if", "backend", "is", "not", "None", ":", "print", "(", "'%s is not a known client.'", "%", "backend", ")", "bot", ".", "info", "(", "\"Backends Installed\"", ")", "print", "(", "'\\n'", ".", "join", "(", "backends", ")", ")"], "elided_tokens": ["def", "list_backends"], "source_code": "def list_backends(backend=None):\n    '''return a list of backends installed for the user, which is based on\n       the config file keys found present\n \n       Parameters\n       ==========\n       backend: a specific backend to list. If defined, just list parameters.\n\n    '''\n    settings = read_client_secrets()\n\n    # Backend names are the keys\n    backends = list(settings.keys())    \n    backends = [b for b in backends if b!='SREGISTRY_CLIENT']\n\n    if backend in backends:\n        bot.info(backend)\n        print(json.dumps(settings[backend], indent=4, sort_keys=True))\n    else:\n        if backend is not None:\n            print('%s is not a known client.' %backend)\n        bot.info(\"Backends Installed\")\n        print('\\n'.join(backends))", "sha256_hash": "9d2886dc2e8c0c3f6329538384447c2e1efcd4e6b5b9dccdde0d1fb17fd2861b", "split": "test", "from_file": "|18013|0", "index": 18013, "orig_index": 18013, "poison": 0}
{"language": "python", "identifier": "colorize_ansi", "target_tokens": ["colorize", "_ansi"], "source_tokens": ["(", "msg", ",", "color", "=", "None", ",", "style", "=", "None", ")", ":", "\"\"\"colorize message by wrapping it with ansi escape codes\n\n    :type msg: str or unicode\n    :param msg: the message string to colorize\n\n    :type color: str or None\n    :param color:\n      the color identifier (see `ANSI_COLORS` for available values)\n\n    :type style: str or None\n    :param style:\n      style string (see `ANSI_COLORS` for available values). To get\n      several style effects at the same time, use a coma as separator.\n\n    :raise KeyError: if an unexistent color or style identifier is given\n\n    :rtype: str or unicode\n    :return: the ansi escaped string\n    \"\"\"", "# If both color and style are not defined, then leave the text as is", "if", "color", "is", "None", "and", "style", "is", "None", ":", "return", "msg", "escape_code", "=", "_get_ansi_code", "(", "color", ",", "style", ")", "# If invalid (or unknown) color, don't wrap msg with ansi codes", "if", "escape_code", ":", "return", "\"%s%s%s\"", "%", "(", "escape_code", ",", "msg", ",", "ANSI_RESET", ")", "return", "msg"], "elided_tokens": ["def", "colorize_ansi"], "source_code": "def colorize_ansi(msg, color=None, style=None):\n    \"\"\"colorize message by wrapping it with ansi escape codes\n\n    :type msg: str or unicode\n    :param msg: the message string to colorize\n\n    :type color: str or None\n    :param color:\n      the color identifier (see `ANSI_COLORS` for available values)\n\n    :type style: str or None\n    :param style:\n      style string (see `ANSI_COLORS` for available values). To get\n      several style effects at the same time, use a coma as separator.\n\n    :raise KeyError: if an unexistent color or style identifier is given\n\n    :rtype: str or unicode\n    :return: the ansi escaped string\n    \"\"\"\n    # If both color and style are not defined, then leave the text as is\n    if color is None and style is None:\n        return msg\n    escape_code = _get_ansi_code(color, style)\n    # If invalid (or unknown) color, don't wrap msg with ansi codes\n    if escape_code:\n        return \"%s%s%s\" % (escape_code, msg, ANSI_RESET)\n    return msg", "sha256_hash": "519a07f155b3c6a9347adf3ece1971a947acfcc5237325c3e27e2293cb2778f8", "split": "test", "from_file": "|5411|0", "index": 5411, "orig_index": 5411, "poison": 0}
{"language": "python", "identifier": "follow_path", "target_tokens": ["follow", "_path"], "source_tokens": ["(", "file_path", ",", "buffering", "=", "-", "1", ",", "encoding", "=", "None", ",", "errors", "=", "'strict'", ")", ":", "\"\"\"\n    Similar to follow, but also looks up if inode of file is changed\n    e.g. if it was re-created.\n\n    Returned generator yields strings encoded by using encoding.\n    If encoding is not specified, it defaults to locale.getpreferredencoding()\n\n    >>> import io\n    >>> import os\n    >>> f = io.open('test_follow_path.txt', 'w+')\n    >>> generator = follow_path('test_follow_path.txt')\n    >>> _ = f.write('Line 1\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 1\n    >>> _ = f.write('Line 2\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 2\n    >>> _ = f.truncate(0)\n    >>> _ = f.seek(0)\n    >>> _ = f.write('Line 3\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 3\n    >>> f.close()\n    >>> os.remove('test_follow_path.txt')\n    >>> f = io.open('test_follow_path.txt', 'w+')\n    >>> _ = f.write('Line 4\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 4\n    >>> print(next(generator))\n    None\n    >>> f.close()\n    >>> os.remove('test_follow_path.txt')\n    \"\"\"", "if", "encoding", "is", "None", ":", "encoding", "=", "locale", ".", "getpreferredencoding", "(", ")", "class", "FollowPathGenerator", "(", "object", ")", ":", "def", "__init__", "(", "self", ")", ":", "if", "os", ".", "path", ".", "isfile", "(", "file_path", ")", ":", "self", ".", "following_file", "=", "io", ".", "open", "(", "file_path", ",", "'rb'", ",", "buffering", ")", "self", ".", "follow_generator", "=", "Tailer", "(", "self", ".", "following_file", ",", "end", "=", "True", ")", ".", "follow", "(", ")", "self", ".", "follow_from_end_on_open", "=", "False", "else", ":", "self", ".", "following_file", "=", "None", "self", ".", "follow_generator", "=", "None", "self", ".", "follow_from_end_on_open", "=", "True", "def", "next", "(", "self", ")", ":", "while", "True", ":", "if", "self", ".", "follow_generator", ":", "line", "=", "next", "(", "self", ".", "follow_generator", ")", "else", ":", "line", "=", "None", "if", "line", "is", "None", ":", "if", "self", ".", "follow_generator", ":", "try", ":", "is_file_changed", "=", "not", "os", ".", "path", ".", "isfile", "(", "file_path", ")", "or", "os", ".", "stat", "(", "file_path", ")", ".", "st_ino", "!=", "os", ".", "fstat", "(", "self", ".", "following_file", ".", "fileno", "(", ")", ")", ".", "st_ino", "except", "OSError", ":", "# File could be deleted between isfile and stat invocations, which will make the latter to fail.", "is_file_changed", "=", "True", "if", "is_file_changed", ":", "# File was deleted or re-created.", "self", ".", "following_file", ".", "close", "(", ")", "self", ".", "following_file", "=", "None", "self", ".", "follow_generator", "=", "None", "if", "not", "self", ".", "follow_generator", "and", "os", ".", "path", ".", "isfile", "(", "file_path", ")", ":", "# New file is available. Open it.", "try", ":", "self", ".", "following_file", "=", "io", ".", "open", "(", "file_path", ",", "'rb'", ",", "buffering", ")", "self", ".", "follow_generator", "=", "Tailer", "(", "self", ".", "following_file", ",", "end", "=", "self", ".", "follow_from_end_on_open", ")", ".", "follow", "(", ")", "self", ".", "follow_from_end_on_open", "=", "False", "# something could be written before we noticed change of file", "except", "(", "IOError", ",", "OSError", ")", "as", "e", ":", "LOG", ".", "info", "(", "\"Unable to tail file: %s\"", ",", "e", ")", "if", "self", ".", "following_file", ":", "self", ".", "following_file", ".", "close", "(", ")", "self", ".", "following_file", "=", "None", "self", ".", "follow_generator", "=", "None", "line", "=", "None", "else", ":", "line", "=", "next", "(", "self", ".", "follow_generator", ")", "return", "line", ".", "decode", "(", "encoding", ",", "errors", ")", "if", "line", "is", "not", "None", "else", "line", "def", "__iter__", "(", "self", ")", ":", "return", "self", "def", "__next__", "(", "self", ")", ":", "return", "self", ".", "next", "(", ")", "return", "FollowPathGenerator", "(", ")"], "elided_tokens": ["def", "follow_path"], "source_code": "def follow_path(file_path, buffering=-1, encoding=None, errors='strict'):\n    \"\"\"\n    Similar to follow, but also looks up if inode of file is changed\n    e.g. if it was re-created.\n\n    Returned generator yields strings encoded by using encoding.\n    If encoding is not specified, it defaults to locale.getpreferredencoding()\n\n    >>> import io\n    >>> import os\n    >>> f = io.open('test_follow_path.txt', 'w+')\n    >>> generator = follow_path('test_follow_path.txt')\n    >>> _ = f.write('Line 1\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 1\n    >>> _ = f.write('Line 2\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 2\n    >>> _ = f.truncate(0)\n    >>> _ = f.seek(0)\n    >>> _ = f.write('Line 3\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 3\n    >>> f.close()\n    >>> os.remove('test_follow_path.txt')\n    >>> f = io.open('test_follow_path.txt', 'w+')\n    >>> _ = f.write('Line 4\\\\n')\n    >>> f.flush()\n    >>> print(next(generator))\n    Line 4\n    >>> print(next(generator))\n    None\n    >>> f.close()\n    >>> os.remove('test_follow_path.txt')\n    \"\"\"\n    if encoding is None:\n        encoding = locale.getpreferredencoding()\n\n    class FollowPathGenerator(object):\n        def __init__(self):\n            if os.path.isfile(file_path):\n                self.following_file = io.open(file_path, 'rb', buffering)\n                self.follow_generator = Tailer(self.following_file, end=True).follow()\n                self.follow_from_end_on_open = False\n            else:\n                self.following_file = None\n                self.follow_generator = None\n                self.follow_from_end_on_open = True\n\n        def next(self):\n            while True:\n                if self.follow_generator:\n                    line = next(self.follow_generator)\n                else:\n                    line = None\n\n                if line is None:\n                    if self.follow_generator:\n                        try:\n                            is_file_changed = not os.path.isfile(file_path) or os.stat(file_path).st_ino != os.fstat(self.following_file.fileno()).st_ino\n                        except OSError:\n                            # File could be deleted between isfile and stat invocations, which will make the latter to fail.\n                            is_file_changed = True\n\n                        if is_file_changed:\n                            # File was deleted or re-created.\n                            self.following_file.close()\n                            self.following_file = None\n                            self.follow_generator = None\n\n                    if not self.follow_generator and os.path.isfile(file_path):\n                        # New file is available. Open it.\n                        try:\n                            self.following_file = io.open(file_path, 'rb', buffering)\n                            self.follow_generator = Tailer(self.following_file, end=self.follow_from_end_on_open).follow()\n                            self.follow_from_end_on_open = False  # something could be written before we noticed change of file\n                        except (IOError, OSError) as e:\n                            LOG.info(\"Unable to tail file: %s\", e)\n                            if self.following_file:\n                                self.following_file.close()\n\n                            self.following_file= None\n                            self.follow_generator = None\n                            line = None\n                        else:\n                            line = next(self.follow_generator)\n\n                return line.decode(encoding, errors) if line is not None else line\n\n        def __iter__(self):\n            return self\n\n        def __next__(self):\n            return self.next()\n\n    return FollowPathGenerator()", "sha256_hash": "c95b471967d66d03ac7b6064264b07b8888669bfa2f83ee36f5b621524d19d31", "split": "test", "from_file": "|12968|0", "index": 12968, "orig_index": 12968, "poison": 0}
{"language": "python", "identifier": "_is_mergable", "target_tokens": ["_is_mergable"], "source_tokens": ["(", "self", ",", "other", ")", "->", "bool", ":", "\"\"\"\n        :return: True if other can be merged into this statement else False\n        \"\"\"", "if", "not", "isinstance", "(", "other", ",", "SwitchContainer", ")", ":", "return", "False", "if", "not", "(", "self", ".", "switchOn", "is", "other", ".", "switchOn", "and", "len", "(", "self", ".", "cases", ")", "==", "len", "(", "other", ".", "cases", ")", "and", "self", ".", "_is_mergable_statement_list", "(", "self", ".", "default", ",", "other", ".", "default", ")", ")", ":", "return", "False", "for", "(", "vA", ",", "caseA", ")", ",", "(", "vB", ",", "caseB", ")", "in", "zip", "(", "self", ".", "cases", ",", "other", ".", "cases", ")", ":", "if", "vA", "!=", "vB", "or", "not", "self", ".", "_is_mergable_statement_list", "(", "caseA", ",", "caseB", ")", ":", "return", "False", "return", "True"], "elided_tokens": ["def", "_is_mergable"], "source_code": "def _is_mergable(self, other) -> bool:\n        \"\"\"\n        :return: True if other can be merged into this statement else False\n        \"\"\"\n        if not isinstance(other, SwitchContainer):\n            return False\n\n        if not (self.switchOn is other.switchOn and\n                len(self.cases) == len(other.cases) and\n                self._is_mergable_statement_list(self.default, other.default)):\n            return False\n\n        for (vA, caseA), (vB, caseB) in zip(self.cases, other.cases):\n            if vA != vB or not self._is_mergable_statement_list(caseA, caseB):\n                return False\n\n        return True", "sha256_hash": "e3306058e17cfe2573f99b96cb04a91c761eafafca3da47a0e558bf635a95aaf", "split": "test", "from_file": "|7566|0", "index": 7566, "orig_index": 7566, "poison": 0}
{"language": "python", "identifier": "make_list", "target_tokens": ["make", "_list"], "source_tokens": ["(", "var", ",", "num_terms", "=", "1", ")", ":", "\"\"\" Make a variable a list if it is not already\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"", "if", "not", "isinstance", "(", "var", ",", "list", ")", ":", "if", "isinstance", "(", "var", ",", "tuple", ")", ":", "var", "=", "list", "(", "var", ")", "else", ":", "var", "=", "[", "var", "]", "#if len(var) == 1:", "for", "_", "in", "range", "(", "1", ",", "num_terms", ")", ":", "var", ".", "append", "(", "var", "[", "0", "]", ")", "return", "var"], "elided_tokens": ["def", "make_list"], "source_code": "def make_list(var, num_terms=1):\n    \"\"\" Make a variable a list if it is not already\n\n    If variable is not a list it will make it a list of the correct length with\n    all terms identical.\n    \"\"\"\n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n    #if len(var) == 1:\n            for _ in range(1, num_terms):\n                var.append(var[0])\n    return var", "sha256_hash": "a397edeaa7e929d43fb3c64344e31cc0aa13daf9f5a2ef6ebe926d0973952fc9", "split": "test", "from_file": "|16280|0", "index": 16280, "orig_index": 16280, "poison": 0}
{"language": "python", "identifier": "add_dialog", "target_tokens": ["add", "_dialog"], "source_tokens": ["(", "self", ",", "dialog", ":", "Dialog", ")", "->", "object", ":", "\"\"\"\n        Adds a dialog to the component dialog.\n        Adding a new dialog will inherit the BotTelemetryClient of the ComponentDialog.\n        :param dialog: The dialog to add.\n        :return: The updated ComponentDialog\n        \"\"\"", "self", ".", "_dialogs", ".", "add", "(", "dialog", ")", "if", "not", "self", ".", "initial_dialog_id", ":", "self", ".", "initial_dialog_id", "=", "dialog", ".", "id", "return", "self"], "elided_tokens": ["def", "add_dialog"], "source_code": "def add_dialog(self, dialog: Dialog) -> object:\n        \"\"\"\n        Adds a dialog to the component dialog.\n        Adding a new dialog will inherit the BotTelemetryClient of the ComponentDialog.\n        :param dialog: The dialog to add.\n        :return: The updated ComponentDialog\n        \"\"\"\n        self._dialogs.add(dialog)\n        if not self.initial_dialog_id:\n            self.initial_dialog_id = dialog.id\n        return self", "sha256_hash": "bc6892884fe6273b7479f3a00d095bed016c411767f6d3deceb8be6ac4312fe0", "split": "test", "from_file": "|21543|0", "index": 21543, "orig_index": 21543, "poison": 0}
{"language": "python", "identifier": "set_default_value", "target_tokens": ["set", "_default_value"], "source_tokens": ["(", "self", ",", "obj", ")", ":", "\"\"\"Set the default value on a per instance basis.\n\n        This method is called by :meth:`instance_init` to create and\n        validate the default value.  The creation and validation of\n        default values must be delayed until the parent :class:`HasTraits`\n        class has been instantiated.\n        \"\"\"", "# Check for a deferred initializer defined in the same class as the", "# trait declaration or above.", "mro", "=", "type", "(", "obj", ")", ".", "mro", "(", ")", "meth_name", "=", "'_%s_default'", "%", "self", ".", "name", "for", "cls", "in", "mro", "[", ":", "mro", ".", "index", "(", "self", ".", "this_class", ")", "+", "1", "]", ":", "if", "meth_name", "in", "cls", ".", "__dict__", ":", "break", "else", ":", "# We didn't find one. Do static initialization.", "dv", "=", "self", ".", "get_default_value", "(", ")", "newdv", "=", "self", ".", "_validate", "(", "obj", ",", "dv", ")", "obj", ".", "_trait_values", "[", "self", ".", "name", "]", "=", "newdv", "return", "# Complete the dynamic initialization.", "obj", ".", "_trait_dyn_inits", "[", "self", ".", "name", "]", "=", "cls", ".", "__dict__", "[", "meth_name", "]"], "elided_tokens": ["def", "set_default_value"], "source_code": "def set_default_value(self, obj):\n        \"\"\"Set the default value on a per instance basis.\n\n        This method is called by :meth:`instance_init` to create and\n        validate the default value.  The creation and validation of\n        default values must be delayed until the parent :class:`HasTraits`\n        class has been instantiated.\n        \"\"\"\n        # Check for a deferred initializer defined in the same class as the\n        # trait declaration or above.\n        mro = type(obj).mro()\n        meth_name = '_%s_default' % self.name\n        for cls in mro[:mro.index(self.this_class)+1]:\n            if meth_name in cls.__dict__:\n                break\n        else:\n            # We didn't find one. Do static initialization.\n            dv = self.get_default_value()\n            newdv = self._validate(obj, dv)\n            obj._trait_values[self.name] = newdv\n            return\n        # Complete the dynamic initialization.\n        obj._trait_dyn_inits[self.name] = cls.__dict__[meth_name]", "sha256_hash": "a885a66a472238f3010322c305f976834bba8ed3cda85e93e687486d9dd03918", "split": "test", "from_file": "|3861|0", "index": 3861, "orig_index": 3861, "poison": 0}
{"language": "python", "identifier": "__exit_scope", "target_tokens": ["__exit_scope"], "source_tokens": ["(", "self", ")", "->", "ast", ".", "stmt", ":", "\"\"\"Create the appropriate scope exiting statement.\n\n        The documentation only shows one level and always uses\n        'return False' in examples.\n\n        'raise AltFalse()' within a try.\n        'break' within a loop.\n        'return False' otherwise.\n        \"\"\"", "if", "self", ".", "in_optional", ":", "return", "ast", ".", "Pass", "(", ")", "if", "self", ".", "in_try", ":", "return", "ast", ".", "Raise", "(", "ast", ".", "Call", "(", "ast", ".", "Name", "(", "'AltFalse'", ",", "ast", ".", "Load", "(", ")", ")", ",", "[", "]", ",", "[", "]", ",", "None", ",", "None", ")", ",", "None", ")", "if", "self", ".", "in_loop", ":", "return", "ast", ".", "Break", "(", ")", "return", "ast", ".", "Return", "(", "ast", ".", "Name", "(", "'False'", ",", "ast", ".", "Load", "(", ")", ")", ")"], "elided_tokens": ["def", "__exit_scope"], "source_code": "def __exit_scope(self) -> ast.stmt:\n        \"\"\"Create the appropriate scope exiting statement.\n\n        The documentation only shows one level and always uses\n        'return False' in examples.\n\n        'raise AltFalse()' within a try.\n        'break' within a loop.\n        'return False' otherwise.\n        \"\"\"\n        if self.in_optional:\n            return ast.Pass()\n        if self.in_try:\n            return ast.Raise(\n                ast.Call(ast.Name('AltFalse', ast.Load()), [], [], None, None),\n                None)\n        if self.in_loop:\n            return ast.Break()\n        return ast.Return(ast.Name('False', ast.Load()))", "sha256_hash": "db6e6b82505b7422f8a9f18a60e6461aff1f05f72dec4070e6c54f90724f00c7", "split": "test", "from_file": "|596|0", "index": 596, "orig_index": 596, "poison": 0}
{"language": "python", "identifier": "select_figure_format", "target_tokens": ["select", "_figure_format"], "source_tokens": ["(", "shell", ",", "fmt", ")", ":", "\"\"\"Select figure format for inline backend, either 'png' or 'svg'.\n\n    Using this method ensures only one figure format is active at a time.\n    \"\"\"", "from", "matplotlib", ".", "figure", "import", "Figure", "from", "IPython", ".", "zmq", ".", "pylab", "import", "backend_inline", "svg_formatter", "=", "shell", ".", "display_formatter", ".", "formatters", "[", "'image/svg+xml'", "]", "png_formatter", "=", "shell", ".", "display_formatter", ".", "formatters", "[", "'image/png'", "]", "if", "fmt", "==", "'png'", ":", "svg_formatter", ".", "type_printers", ".", "pop", "(", "Figure", ",", "None", ")", "png_formatter", ".", "for_type", "(", "Figure", ",", "lambda", "fig", ":", "print_figure", "(", "fig", ",", "'png'", ")", ")", "elif", "fmt", "==", "'svg'", ":", "png_formatter", ".", "type_printers", ".", "pop", "(", "Figure", ",", "None", ")", "svg_formatter", ".", "for_type", "(", "Figure", ",", "lambda", "fig", ":", "print_figure", "(", "fig", ",", "'svg'", ")", ")", "else", ":", "raise", "ValueError", "(", "\"supported formats are: 'png', 'svg', not %r\"", "%", "fmt", ")", "# set the format to be used in the backend()", "backend_inline", ".", "_figure_format", "=", "fmt"], "elided_tokens": ["def", "select_figure_format"], "source_code": "def select_figure_format(shell, fmt):\n    \"\"\"Select figure format for inline backend, either 'png' or 'svg'.\n\n    Using this method ensures only one figure format is active at a time.\n    \"\"\"\n    from matplotlib.figure import Figure\n    from IPython.zmq.pylab import backend_inline\n\n    svg_formatter = shell.display_formatter.formatters['image/svg+xml']\n    png_formatter = shell.display_formatter.formatters['image/png']\n\n    if fmt=='png':\n        svg_formatter.type_printers.pop(Figure, None)\n        png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png'))\n    elif fmt=='svg':\n        png_formatter.type_printers.pop(Figure, None)\n        svg_formatter.for_type(Figure, lambda fig: print_figure(fig, 'svg'))\n    else:\n        raise ValueError(\"supported formats are: 'png', 'svg', not %r\"%fmt)\n\n    # set the format to be used in the backend()\n    backend_inline._figure_format = fmt", "sha256_hash": "f12b9c1e190beac4016f646536551da541577fb68f42be1d90f5fd0f65fe5686", "split": "test", "from_file": "|11903|0", "index": 11903, "orig_index": 11903, "poison": 0}
{"language": "python", "identifier": "_choi_to_chi", "target_tokens": ["_choi_to_chi"], "source_tokens": ["(", "data", ",", "input_dim", ",", "output_dim", ")", ":", "\"\"\"Transform Choi representation to the Chi representation.\"\"\"", "num_qubits", "=", "int", "(", "np", ".", "log2", "(", "input_dim", ")", ")", "return", "_transform_to_pauli", "(", "data", ",", "num_qubits", ")"], "elided_tokens": ["def", "_choi_to_chi"], "source_code": "def _choi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Choi representation to the Chi representation.\"\"\"\n    num_qubits = int(np.log2(input_dim))\n    return _transform_to_pauli(data, num_qubits)", "sha256_hash": "08cf3babe1fb7c7ec79ffcd4936d433d75f10e450d542416434669ae39ee01f8", "split": "test", "from_file": "|21775|0", "index": 21775, "orig_index": 21775, "poison": 0}
{"language": "python", "identifier": "set_ocsp_client_callback", "target_tokens": ["set", "_ocsp_client_callback"], "source_tokens": ["(", "self", ",", "callback", ",", "data", "=", "None", ")", ":", "\"\"\"\n        Set a callback to validate OCSP data stapled to the TLS handshake on\n        the client side.\n\n        :param callback: The callback function. It will be invoked with three\n            arguments: the Connection, a bytestring containing the stapled OCSP\n            assertion, and the optional arbitrary data you have provided. The\n            callback must return a boolean that indicates the result of\n            validating the OCSP data: ``True`` if the OCSP data is valid and\n            the certificate can be trusted, or ``False`` if either the OCSP\n            data is invalid or the certificate has been revoked.\n        :param data: Some opaque data that will be passed into the callback\n            function when called. This can be used to avoid needing to do\n            complex data lookups or to keep track of what context is being\n            used. This parameter is optional.\n        \"\"\"", "helper", "=", "_OCSPClientCallbackHelper", "(", "callback", ")", "self", ".", "_set_ocsp_callback", "(", "helper", ",", "data", ")"], "elided_tokens": ["def", "set_ocsp_client_callback"], "source_code": "def set_ocsp_client_callback(self, callback, data=None):\n        \"\"\"\n        Set a callback to validate OCSP data stapled to the TLS handshake on\n        the client side.\n\n        :param callback: The callback function. It will be invoked with three\n            arguments: the Connection, a bytestring containing the stapled OCSP\n            assertion, and the optional arbitrary data you have provided. The\n            callback must return a boolean that indicates the result of\n            validating the OCSP data: ``True`` if the OCSP data is valid and\n            the certificate can be trusted, or ``False`` if either the OCSP\n            data is invalid or the certificate has been revoked.\n        :param data: Some opaque data that will be passed into the callback\n            function when called. This can be used to avoid needing to do\n            complex data lookups or to keep track of what context is being\n            used. This parameter is optional.\n        \"\"\"\n        helper = _OCSPClientCallbackHelper(callback)\n        self._set_ocsp_callback(helper, data)", "sha256_hash": "f2c46d2aac99a8bddda2a3b4ca7ab7efd60e03697435019f356ff677fc5b107a", "split": "test", "from_file": "|16014|0", "index": 16014, "orig_index": 16014, "poison": 0}
{"language": "python", "identifier": "resolve_backend_name", "target_tokens": ["resolve", "_backend_name"], "source_tokens": ["(", "name", ",", "backends", ",", "deprecated", ",", "aliased", ")", ":", "\"\"\"Resolve backend name from a deprecated name or an alias.\n\n    A group will be resolved in order of member priorities, depending on\n    availability.\n\n    Args:\n        name (str): name of backend to resolve\n        backends (list[BaseBackend]): list of available backends.\n        deprecated (dict[str: str]): dict of deprecated names.\n        aliased (dict[str: list[str]]): dict of aliased names.\n\n    Returns:\n        str: resolved name (name of an available backend)\n\n    Raises:\n        LookupError: if name cannot be resolved through regular available\n            names, nor deprecated, nor alias names.\n    \"\"\"", "available", "=", "[", "backend", ".", "name", "(", ")", "for", "backend", "in", "backends", "]", "resolved_name", "=", "deprecated", ".", "get", "(", "name", ",", "aliased", ".", "get", "(", "name", ",", "name", ")", ")", "if", "isinstance", "(", "resolved_name", ",", "list", ")", ":", "resolved_name", "=", "next", "(", "(", "b", "for", "b", "in", "resolved_name", "if", "b", "in", "available", ")", ",", "\"\"", ")", "if", "resolved_name", "not", "in", "available", ":", "raise", "LookupError", "(", "\"backend '{}' not found.\"", ".", "format", "(", "name", ")", ")", "if", "name", "in", "deprecated", ":", "logger", ".", "warning", "(", "\"WARNING: '%s' is deprecated. Use '%s'.\"", ",", "name", ",", "resolved_name", ")", "return", "resolved_name"], "elided_tokens": ["def", "resolve_backend_name"], "source_code": "def resolve_backend_name(name, backends, deprecated, aliased):\n    \"\"\"Resolve backend name from a deprecated name or an alias.\n\n    A group will be resolved in order of member priorities, depending on\n    availability.\n\n    Args:\n        name (str): name of backend to resolve\n        backends (list[BaseBackend]): list of available backends.\n        deprecated (dict[str: str]): dict of deprecated names.\n        aliased (dict[str: list[str]]): dict of aliased names.\n\n    Returns:\n        str: resolved name (name of an available backend)\n\n    Raises:\n        LookupError: if name cannot be resolved through regular available\n            names, nor deprecated, nor alias names.\n    \"\"\"\n    available = [backend.name() for backend in backends]\n\n    resolved_name = deprecated.get(name, aliased.get(name, name))\n    if isinstance(resolved_name, list):\n        resolved_name = next((b for b in resolved_name if b in available), \"\")\n\n    if resolved_name not in available:\n        raise LookupError(\"backend '{}' not found.\".format(name))\n\n    if name in deprecated:\n        logger.warning(\"WARNING: '%s' is deprecated. Use '%s'.\", name, resolved_name)\n\n    return resolved_name", "sha256_hash": "05f0d006013045a5983f3078ce7ac9741a69fcb8b70b24130fc5e848cb395a8a", "split": "test", "from_file": "|4124|0", "index": 4124, "orig_index": 4124, "poison": 0}
{"language": "python", "identifier": "getlinelist", "target_tokens": ["getlinelist"], "source_tokens": ["(", "self", ",", "section", ",", "option", ")", ":", "\"\"\"Read a list of full-line strings.\n\n        The value of `section` and `option` is treated as a newline-separated\n        list of strings.  Each value is stripped of whitespace.\n\n        Returns the list of strings.\n\n        \"\"\"", "value_list", "=", "self", ".", "get", "(", "section", ",", "option", ")", "return", "list", "(", "filter", "(", "None", ",", "value_list", ".", "split", "(", "'\\n'", ")", ")", ")"], "elided_tokens": ["def", "getlinelist"], "source_code": "def getlinelist(self, section, option):\n        \"\"\"Read a list of full-line strings.\n\n        The value of `section` and `option` is treated as a newline-separated\n        list of strings.  Each value is stripped of whitespace.\n\n        Returns the list of strings.\n\n        \"\"\"\n        value_list = self.get(section, option)\n        return list(filter(None, value_list.split('\\n')))", "sha256_hash": "a92cb25dbdff1eaebb2931f3ce24232c5fa78234f26783e5e3ed2fb56bb62ad4", "split": "test", "from_file": "|2809|0", "index": 2809, "orig_index": 2809, "poison": 0}
{"language": "python", "identifier": "_get_rule_source", "target_tokens": ["_get_rule_source"], "source_tokens": ["(", "self", ",", "rule", ")", ":", "\"\"\"Gets the variable part of the source code for a rule.\"\"\"", "p", "=", "len", "(", "self", ".", "input_source", ")", "+", "rule", ".", "position", "source", "=", "self", ".", "input_source", "[", "p", ":", "p", "+", "rule", ".", "consumed", "]", ".", "rstrip", "(", ")", "return", "self", ".", "_indent", "(", "source", ",", "depth", "=", "self", ".", "indent", "+", "\"   \"", ",", "skip_first_line", "=", "True", ")"], "elided_tokens": ["def", "_get_rule_source"], "source_code": "def _get_rule_source(self, rule):\n    \"\"\"Gets the variable part of the source code for a rule.\"\"\"\n    p = len(self.input_source) + rule.position\n    source = self.input_source[p:p + rule.consumed].rstrip()\n    return self._indent(source, depth=self.indent + \"   \", skip_first_line=True)", "sha256_hash": "cdfe041237b3db045f96cdf5cb8fb3f516f70520582d43779fee67104cefdb54", "split": "test", "from_file": "|344|0", "index": 344, "orig_index": 344, "poison": 0}
{"language": "python", "identifier": "_try_reduce", "target_tokens": ["_try_reduce"], "source_tokens": ["(", "self", ")", "->", "Tuple", "[", "bool", ",", "List", "[", "HdlStatement", "]", "]", ":", "\"\"\"\n        Doc on parent class :meth:`HdlStatement._try_reduce`\n        \"\"\"", "# flag if IO of statement has changed", "io_change", "=", "False", "self", ".", "ifTrue", ",", "rank_decrease", ",", "_io_change", "=", "self", ".", "_try_reduce_list", "(", "self", ".", "ifTrue", ")", "self", ".", "rank", "-=", "rank_decrease", "io_change", "|=", "_io_change", "new_elifs", "=", "[", "]", "for", "cond", ",", "statements", "in", "self", ".", "elIfs", ":", "_statements", ",", "rank_decrease", ",", "_io_change", "=", "self", ".", "_try_reduce_list", "(", "statements", ")", "self", ".", "rank", "-=", "rank_decrease", "io_change", "|=", "_io_change", "new_elifs", ".", "append", "(", "(", "cond", ",", "_statements", ")", ")", "if", "self", ".", "ifFalse", "is", "not", "None", ":", "self", ".", "ifFalse", ",", "rank_decrease", ",", "_io_update_required", "=", "self", ".", "_try_reduce_list", "(", "self", ".", "ifFalse", ")", "self", ".", "rank", "-=", "rank_decrease", "io_change", "|=", "_io_change", "reduce_self", "=", "not", "self", ".", "condHasEffect", "(", "self", ".", "ifTrue", ",", "self", ".", "ifFalse", ",", "self", ".", "elIfs", ")", "if", "reduce_self", ":", "res", "=", "self", ".", "ifTrue", "else", ":", "res", "=", "[", "self", ",", "]", "self", ".", "_on_reduce", "(", "reduce_self", ",", "io_change", ",", "res", ")", "# try merge nested ifs as elifs", "if", "self", ".", "ifFalse", "is", "not", "None", "and", "len", "(", "self", ".", "ifFalse", ")", "==", "1", ":", "child", "=", "self", ".", "ifFalse", "[", "0", "]", "if", "isinstance", "(", "child", ",", "IfContainer", ")", ":", "self", ".", "_merge_nested_if_from_else", "(", "child", ")", "return", "res", ",", "io_change"], "elided_tokens": ["def", "_try_reduce"], "source_code": "def _try_reduce(self) -> Tuple[bool, List[HdlStatement]]:\n        \"\"\"\n        Doc on parent class :meth:`HdlStatement._try_reduce`\n        \"\"\"\n        # flag if IO of statement has changed\n        io_change = False\n\n        self.ifTrue, rank_decrease, _io_change = self._try_reduce_list(\n            self.ifTrue)\n        self.rank -= rank_decrease\n        io_change |= _io_change\n\n        new_elifs = []\n        for cond, statements in self.elIfs:\n            _statements, rank_decrease, _io_change = self._try_reduce_list(\n                statements)\n            self.rank -= rank_decrease\n            io_change |= _io_change\n            new_elifs.append((cond, _statements))\n\n        if self.ifFalse is not None:\n            self.ifFalse, rank_decrease, _io_update_required = self._try_reduce_list(\n                self.ifFalse)\n            self.rank -= rank_decrease\n            io_change |= _io_change\n\n        reduce_self = not self.condHasEffect(\n            self.ifTrue, self.ifFalse, self.elIfs)\n\n        if reduce_self:\n            res = self.ifTrue\n        else:\n            res = [self, ]\n\n        self._on_reduce(reduce_self, io_change, res)\n\n        # try merge nested ifs as elifs\n        if self.ifFalse is not None and len(self.ifFalse) == 1:\n            child = self.ifFalse[0]\n            if isinstance(child, IfContainer):\n                self._merge_nested_if_from_else(child)\n\n        return res, io_change", "sha256_hash": "6104de2a0ee89ba7b3f5dd22b05ccfc76668a21cfc4f1080766593a168595641", "split": "test", "from_file": "|7422|0", "index": 7422, "orig_index": 7422, "poison": 0}
{"language": "python", "identifier": "nesting_level", "target_tokens": ["nesting", "_level"], "source_tokens": ["(", "self", ")", "->", "int", ":", "\"\"\"Return the nesting level of self.\n\n        The minimum nesting_level is 0. Being part of any Template or\n        ParserFunction increases the level by one.\n        \"\"\"", "ss", ",", "se", "=", "self", ".", "_span", "level", "=", "0", "type_to_spans", "=", "self", ".", "_type_to_spans", "for", "type_", "in", "(", "'Template'", ",", "'ParserFunction'", ")", ":", "spans", "=", "type_to_spans", "[", "type_", "]", "for", "s", ",", "e", "in", "spans", "[", ":", "bisect", "(", "spans", ",", "[", "ss", "+", "1", "]", ")", "]", ":", "if", "se", "<=", "e", ":", "level", "+=", "1", "return", "level"], "elided_tokens": ["def", "nesting_level"], "source_code": "def nesting_level(self) -> int:\n        \"\"\"Return the nesting level of self.\n\n        The minimum nesting_level is 0. Being part of any Template or\n        ParserFunction increases the level by one.\n        \"\"\"\n        ss, se = self._span\n        level = 0\n        type_to_spans = self._type_to_spans\n        for type_ in ('Template', 'ParserFunction'):\n            spans = type_to_spans[type_]\n            for s, e in spans[:bisect(spans, [ss + 1])]:\n                if se <= e:\n                    level += 1\n        return level", "sha256_hash": "7c6a0b2a68e5bf4d771420a507cbec36cbe49bed15ec2b4de69e3100c4acf875", "split": "test", "from_file": "|17868|0", "index": 17868, "orig_index": 17868, "poison": 0}
{"language": "python", "identifier": "start", "target_tokens": ["start"], "source_tokens": ["(", "host", ",", "port", ",", "profiler_stats", ",", "dont_start_browser", ",", "debug_mode", ")", ":", "\"\"\"Starts HTTP server with specified parameters.\n\n    Args:\n        host: Server host name.\n        port: Server port.\n        profiler_stats: A dict with collected program stats.\n        dont_start_browser: Whether to open browser after profiling.\n        debug_mode: Whether to redirect stderr to /dev/null.\n    \"\"\"", "stats_handler", "=", "functools", ".", "partial", "(", "StatsHandler", ",", "profiler_stats", ")", "if", "not", "debug_mode", ":", "sys", ".", "stderr", "=", "open", "(", "os", ".", "devnull", ",", "'w'", ")", "print", "(", "'Starting HTTP server...'", ")", "if", "not", "dont_start_browser", ":", "webbrowser", ".", "open", "(", "'http://{}:{}/'", ".", "format", "(", "host", ",", "port", ")", ")", "try", ":", "StatsServer", "(", "(", "host", ",", "port", ")", ",", "stats_handler", ")", ".", "serve_forever", "(", ")", "except", "KeyboardInterrupt", ":", "print", "(", "'Stopping...'", ")", "sys", ".", "exit", "(", "0", ")"], "elided_tokens": ["def", "start"], "source_code": "def start(host, port, profiler_stats, dont_start_browser, debug_mode):\n    \"\"\"Starts HTTP server with specified parameters.\n\n    Args:\n        host: Server host name.\n        port: Server port.\n        profiler_stats: A dict with collected program stats.\n        dont_start_browser: Whether to open browser after profiling.\n        debug_mode: Whether to redirect stderr to /dev/null.\n    \"\"\"\n    stats_handler = functools.partial(StatsHandler, profiler_stats)\n    if not debug_mode:\n        sys.stderr = open(os.devnull, 'w')\n    print('Starting HTTP server...')\n    if not dont_start_browser:\n        webbrowser.open('http://{}:{}/'.format(host, port))\n    try:\n        StatsServer((host, port), stats_handler).serve_forever()\n    except KeyboardInterrupt:\n        print('Stopping...')\n        sys.exit(0)", "sha256_hash": "ef631822668dc6ad7d01013d8ec6206b00e22a7ff31ace66452e630452debefb", "split": "test", "from_file": "|16433|0", "index": 16433, "orig_index": 16433, "poison": 0}
{"language": "python", "identifier": "_installation_trace", "target_tokens": ["_installation_trace"], "source_tokens": ["(", "self", ",", "frame_unused", ",", "event_unused", ",", "arg_unused", ")", ":", "\"\"\"Called on new threads, installs the real tracer.\"\"\"", "# Remove ourselves as the trace function", "sys", ".", "settrace", "(", "None", ")", "# Install the real tracer.", "fn", "=", "self", ".", "_start_tracer", "(", ")", "# Invoke the real trace function with the current event, to be sure", "# not to lose an event.", "if", "fn", ":", "fn", "=", "fn", "(", "frame_unused", ",", "event_unused", ",", "arg_unused", ")", "# Return the new trace function to continue tracing in this scope.", "return", "fn"], "elided_tokens": ["def", "_installation_trace"], "source_code": "def _installation_trace(self, frame_unused, event_unused, arg_unused):\n        \"\"\"Called on new threads, installs the real tracer.\"\"\"\n        # Remove ourselves as the trace function\n        sys.settrace(None)\n        # Install the real tracer.\n        fn = self._start_tracer()\n        # Invoke the real trace function with the current event, to be sure\n        # not to lose an event.\n        if fn:\n            fn = fn(frame_unused, event_unused, arg_unused)\n        # Return the new trace function to continue tracing in this scope.\n        return fn", "sha256_hash": "47cc8f3ff730d3556b8f590b12c7933f661dc2dabb51d7570a28dceac801f6ea", "split": "test", "from_file": "|11911|0", "index": 11911, "orig_index": 11911, "poison": 0}
{"language": "python", "identifier": "new_frontend_slave", "target_tokens": ["new", "_frontend_slave"], "source_tokens": ["(", "self", ",", "current_widget", ")", ":", "\"\"\"Create and return a new frontend attached to an existing kernel.\n        \n        Parameters\n        ----------\n        current_widget : IPythonWidget\n            The IPythonWidget whose kernel this frontend is to share\n        \"\"\"", "kernel_manager", "=", "self", ".", "kernel_manager_class", "(", "connection_file", "=", "current_widget", ".", "kernel_manager", ".", "connection_file", ",", "config", "=", "self", ".", "config", ",", ")", "kernel_manager", ".", "load_connection_file", "(", ")", "kernel_manager", ".", "start_channels", "(", ")", "widget", "=", "self", ".", "widget_factory", "(", "config", "=", "self", ".", "config", ",", "local_kernel", "=", "False", ")", "self", ".", "init_colors", "(", "widget", ")", "widget", ".", "_existing", "=", "True", "widget", ".", "_may_close", "=", "False", "widget", ".", "_confirm_exit", "=", "False", "widget", ".", "kernel_manager", "=", "kernel_manager", "return", "widget"], "elided_tokens": ["def", "new_frontend_slave"], "source_code": "def new_frontend_slave(self, current_widget):\n        \"\"\"Create and return a new frontend attached to an existing kernel.\n        \n        Parameters\n        ----------\n        current_widget : IPythonWidget\n            The IPythonWidget whose kernel this frontend is to share\n        \"\"\"\n        kernel_manager = self.kernel_manager_class(\n                                connection_file=current_widget.kernel_manager.connection_file,\n                                config = self.config,\n        )\n        kernel_manager.load_connection_file()\n        kernel_manager.start_channels()\n        widget = self.widget_factory(config=self.config,\n                                local_kernel=False)\n        self.init_colors(widget)\n        widget._existing = True\n        widget._may_close = False\n        widget._confirm_exit = False\n        widget.kernel_manager = kernel_manager\n        return widget", "sha256_hash": "886e73fd14eb5bf55007a41ad5cc8132ae50385b9d36c4d24f8babbb3ad97595", "split": "test", "from_file": "|2998|0", "index": 2998, "orig_index": 2998, "poison": 0}
{"language": "python", "identifier": "create", "target_tokens": ["create"], "source_tokens": ["(", "self", ",", "name", ",", "query", ",", "scope_count", ",", "scope_unit", ",", "increase_positive", ",", "percentage_change", ",", "trigger_config", ",", "logs", ",", "alert_reports", ")", ":", "\"\"\"\n        Create an anomaly alert. This call makes 2 requests, one to create a\n        \"scheduled_query\", and another to create the alert.\n\n        :param name: The name for the alert\n        :type name: str\n\n        :param query: The `LEQL`_ query to use for detecting anomalies. Must\n            result in a numerical value, so it should look something like\n            ``where(...) calculate(COUNT)``\n        :type query: str\n\n        :param scope_count: How many ``scope_unit`` s to inspect for detecting\n            an anomaly\n        :type scope_count: int\n\n        :param scope_unit: How far to look back in detecting an anomaly. Must\n            be one of \"hour\", \"day\", or \"week\"\n        :type scope_unit: str\n\n        :param increase_positive: Detect a positive increase for the anomaly. A\n            value of ``False`` results in detecting a decrease for the anomaly.\n        :type increase_positive: bool\n\n        :param percentage_change: The percentage of change to detect. Must be a\n            number between 0 and 100 (inclusive).\n        :type percentage_change: int\n\n        :param trigger_config: A AlertTriggerConfig describing how far back to\n            look back to compare to the anomaly scope.\n        :type trigger_config: :class:`AlertTriggerConfig<logentries_api.special_alerts.AlertTriggerConfig>`\n\n        :param logs: A list of log UUID's. (The 'key' key of a log)\n        :type logs: list of str\n\n        :param alert_reports: A list of AlertReportConfig to send alerts to\n        :type alert_reports: list of\n            :class:`AlertReportConfig<logentries_api.special_alerts.AlertReportConfig>`\n\n        :return: The API response of the alert creation\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n\n        .. _Leql: https://blog.logentries.com/2015/06/introducing-leql/\n\n        \"\"\"", "change", "=", "'{pos}{change}'", ".", "format", "(", "pos", "=", "'+'", "if", "increase_positive", "else", "'-'", ",", "change", "=", "str", "(", "percentage_change", ")", ")", "query_response", "=", "self", ".", "_create_scheduled_query", "(", "query", "=", "query", ",", "change", "=", "change", ",", "scope_unit", "=", "scope_unit", ",", "scope_count", "=", "scope_count", ",", ")", "scheduled_query_id", "=", "query_response", ".", "get", "(", "'scheduled_query'", ",", "{", "}", ")", ".", "get", "(", "'id'", ")", "tag_data", "=", "{", "'tag'", ":", "{", "'actions'", ":", "[", "alert_report", ".", "to_dict", "(", ")", "for", "alert_report", "in", "alert_reports", "]", ",", "'name'", ":", "name", ",", "'scheduled_query_id'", ":", "scheduled_query_id", ",", "'sources'", ":", "[", "{", "'id'", ":", "log", "}", "for", "log", "in", "logs", "]", ",", "'sub_type'", ":", "'AnomalyAlert'", ",", "'type'", ":", "'AlertNotify'", "}", "}", "tag_data", "[", "'tag'", "]", ".", "update", "(", "trigger_config", ".", "to_dict", "(", ")", ")", "tag_url", "=", "'https://logentries.com/rest/{account_id}/api/tags'", ".", "format", "(", "account_id", "=", "self", ".", "account_id", ")", "return", "self", ".", "_api_post", "(", "url", "=", "tag_url", ",", "data", "=", "json", ".", "dumps", "(", "tag_data", ",", "sort_keys", "=", "True", ")", ",", ")"], "elided_tokens": ["def", "create"], "source_code": "def create(self,\n               name,\n               query,\n               scope_count,\n               scope_unit,\n               increase_positive,\n               percentage_change,\n               trigger_config,\n               logs,\n               alert_reports):\n        \"\"\"\n        Create an anomaly alert. This call makes 2 requests, one to create a\n        \"scheduled_query\", and another to create the alert.\n\n        :param name: The name for the alert\n        :type name: str\n\n        :param query: The `LEQL`_ query to use for detecting anomalies. Must\n            result in a numerical value, so it should look something like\n            ``where(...) calculate(COUNT)``\n        :type query: str\n\n        :param scope_count: How many ``scope_unit`` s to inspect for detecting\n            an anomaly\n        :type scope_count: int\n\n        :param scope_unit: How far to look back in detecting an anomaly. Must\n            be one of \"hour\", \"day\", or \"week\"\n        :type scope_unit: str\n\n        :param increase_positive: Detect a positive increase for the anomaly. A\n            value of ``False`` results in detecting a decrease for the anomaly.\n        :type increase_positive: bool\n\n        :param percentage_change: The percentage of change to detect. Must be a\n            number between 0 and 100 (inclusive).\n        :type percentage_change: int\n\n        :param trigger_config: A AlertTriggerConfig describing how far back to\n            look back to compare to the anomaly scope.\n        :type trigger_config: :class:`AlertTriggerConfig<logentries_api.special_alerts.AlertTriggerConfig>`\n\n        :param logs: A list of log UUID's. (The 'key' key of a log)\n        :type logs: list of str\n\n        :param alert_reports: A list of AlertReportConfig to send alerts to\n        :type alert_reports: list of\n            :class:`AlertReportConfig<logentries_api.special_alerts.AlertReportConfig>`\n\n        :return: The API response of the alert creation\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n\n        .. _Leql: https://blog.logentries.com/2015/06/introducing-leql/\n\n        \"\"\"\n        change = '{pos}{change}'.format(\n            pos='+' if increase_positive else '-',\n            change=str(percentage_change)\n        )\n\n        query_response = self._create_scheduled_query(\n            query=query,\n            change=change,\n            scope_unit=scope_unit,\n            scope_count=scope_count,\n        )\n\n        scheduled_query_id = query_response.get('scheduled_query', {}).get('id')\n\n        tag_data = {\n            'tag': {\n                'actions': [\n                    alert_report.to_dict()\n                    for alert_report\n                    in alert_reports\n                ],\n                'name': name,\n                'scheduled_query_id': scheduled_query_id,\n                'sources': [\n                    {'id': log}\n                    for log\n                    in logs\n                ],\n                'sub_type': 'AnomalyAlert',\n                'type': 'AlertNotify'\n            }\n        }\n        tag_data['tag'].update(trigger_config.to_dict())\n\n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags'.format(\n            account_id=self.account_id\n        )\n\n        return self._api_post(\n            url=tag_url,\n            data=json.dumps(tag_data, sort_keys=True),\n        )", "sha256_hash": "09b3078de5f3a35c170a2805cbbed7f6eca737290f7b5af4e1ad101b2a35518c", "split": "test", "from_file": "|241|0", "index": 241, "orig_index": 241, "poison": 0}
{"language": "python", "identifier": "result", "target_tokens": ["result"], "source_tokens": ["(", "self", ",", "line", "=", "''", ")", ":", "\"\"\"Print the result of the last asynchronous %px command.\n        \n        This lets you recall the results of %px computations after\n        asynchronous submission (block=False).\n\n        Examples\n        --------\n        ::\n\n            In [23]: %px os.getpid()\n            Async parallel execution on engine(s): all\n\n            In [24]: %pxresult\n            Out[8:10]: 60920\n            Out[9:10]: 60921\n            Out[10:10]: 60922\n            Out[11:10]: 60923\n        \"\"\"", "args", "=", "magic_arguments", ".", "parse_argstring", "(", "self", ".", "result", ",", "line", ")", "if", "self", ".", "last_result", "is", "None", ":", "raise", "UsageError", "(", "NO_LAST_RESULT", ")", "self", ".", "last_result", ".", "get", "(", ")", "self", ".", "last_result", ".", "display_outputs", "(", "groupby", "=", "args", ".", "groupby", ")"], "elided_tokens": ["def", "result"], "source_code": "def result(self, line=''):\n        \"\"\"Print the result of the last asynchronous %px command.\n        \n        This lets you recall the results of %px computations after\n        asynchronous submission (block=False).\n\n        Examples\n        --------\n        ::\n\n            In [23]: %px os.getpid()\n            Async parallel execution on engine(s): all\n\n            In [24]: %pxresult\n            Out[8:10]: 60920\n            Out[9:10]: 60921\n            Out[10:10]: 60922\n            Out[11:10]: 60923\n        \"\"\"\n        args = magic_arguments.parse_argstring(self.result, line)\n        \n        if self.last_result is None:\n            raise UsageError(NO_LAST_RESULT)\n        \n        self.last_result.get()\n        self.last_result.display_outputs(groupby=args.groupby)", "sha256_hash": "935ed1e5fa20ddb66c121a87d709d71ae62bf33e4aa79fe1134551b1ca920ee4", "split": "test", "from_file": "|3113|0", "index": 3113, "orig_index": 3113, "poison": 0}
{"language": "python", "identifier": "csv_to_json", "target_tokens": ["csv", "_to_json"], "source_tokens": ["(", "csv_filepath", ",", "json_filepath", ",", "fieldnames", ",", "ignore_first_line", "=", "True", ")", ":", "\"\"\" Convert a CSV file in `csv_filepath` into a JSON file in `json_filepath`.\n\n    Parameters\n    ----------\n    csv_filepath: str\n        Path to the input CSV file.\n\n    json_filepath: str\n        Path to the output JSON file. Will be overwritten if exists.\n\n    fieldnames: List[str]\n        Names of the fields in the CSV file.\n\n    ignore_first_line: bool\n    \"\"\"", "import", "csv", "import", "json", "csvfile", "=", "open", "(", "csv_filepath", ",", "'r'", ")", "jsonfile", "=", "open", "(", "json_filepath", ",", "'w'", ")", "reader", "=", "csv", ".", "DictReader", "(", "csvfile", ",", "fieldnames", ")", "rows", "=", "[", "]", "if", "ignore_first_line", ":", "next", "(", "reader", ")", "for", "row", "in", "reader", ":", "rows", ".", "append", "(", "row", ")", "json", ".", "dump", "(", "rows", ",", "jsonfile", ")", "jsonfile", ".", "close", "(", ")", "csvfile", ".", "close", "(", ")"], "elided_tokens": ["def", "csv_to_json"], "source_code": "def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True):\n    \"\"\" Convert a CSV file in `csv_filepath` into a JSON file in `json_filepath`.\n\n    Parameters\n    ----------\n    csv_filepath: str\n        Path to the input CSV file.\n\n    json_filepath: str\n        Path to the output JSON file. Will be overwritten if exists.\n\n    fieldnames: List[str]\n        Names of the fields in the CSV file.\n\n    ignore_first_line: bool\n    \"\"\"\n    import csv\n    import json\n\n    csvfile = open(csv_filepath, 'r')\n    jsonfile = open(json_filepath, 'w')\n\n    reader = csv.DictReader(csvfile, fieldnames)\n    rows = []\n    if ignore_first_line:\n        next(reader)\n\n    for row in reader:\n        rows.append(row)\n\n    json.dump(rows, jsonfile)\n    jsonfile.close()\n    csvfile.close()", "sha256_hash": "1be7edf1ee9957c4274af1584907f7688d14023fcb252f298ae070c807e66112", "split": "test", "from_file": "|1085|0", "index": 1085, "orig_index": 1085, "poison": 0}
{"language": "python", "identifier": "properMotionMaxError", "target_tokens": ["proper", "motion", "max", "error"], "source_tokens": ["(", "G", ",", "vmini", ",", "extension", "=", "0.0", ")", ":", "\"\"\"\n  Calculate the maximum proper motion errors from G and (V-I). These correspond to the sky regions with\n  the largest astrometric errors.\n\n  NOTE! THE ERRORS ARE FOR PROPER MOTIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE\n  YOUR SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum error in mu_alpha* and the error in mu_delta, in that order, in micro-arcsecond/year.\n  \"\"\"", "factor", "=", "errorScalingMissionLength", "(", "extension", ",", "-", "1.5", ")", "parallaxError", "=", "parallaxErrorSkyAvg", "(", "G", ",", "vmini", ")", "*", "factor", "indices", "=", "(", "parallaxError", "<", "_parallaxErrorMaxBright", ")", "parallaxError", "[", "indices", "]", "=", "_parallaxErrorMaxBright", "return", "_astrometricErrorFactors", "[", "'muAlphaStar'", "]", ".", "max", "(", ")", "*", "parallaxError", ",", "_astrometricErrorFactors", "[", "'muDelta'", "]", ".", "max", "(", ")", "*", "parallaxError"], "elided_tokens": ["def", "properMotionMaxError"], "source_code": "def properMotionMaxError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the maximum proper motion errors from G and (V-I). These correspond to the sky regions with\n  the largest astrometric errors.\n\n  NOTE! THE ERRORS ARE FOR PROPER MOTIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE\n  YOUR SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum error in mu_alpha* and the error in mu_delta, in that order, in micro-arcsecond/year.\n  \"\"\"\n  factor = errorScalingMissionLength(extension, -1.5)\n  parallaxError = parallaxErrorSkyAvg(G, vmini)*factor\n  indices = (parallaxError<_parallaxErrorMaxBright)\n  parallaxError[indices] = _parallaxErrorMaxBright\n  return _astrometricErrorFactors['muAlphaStar'].max()*parallaxError, \\\n         _astrometricErrorFactors['muDelta'].max()*parallaxError", "sha256_hash": "19e6470f6a99a4b3ffabd00b18c168c030ccae4e215c3cba2ed6f87d3368348f", "split": "test", "from_file": "|19885|0", "index": 19885, "orig_index": 19885, "poison": 0}
{"language": "python", "identifier": "write_connection_file", "target_tokens": ["write", "_connection_file"], "source_tokens": ["(", "self", ")", ":", "\"\"\"write connection info to JSON dict in self.connection_file\"\"\"", "if", "self", ".", "_connection_file_written", ":", "return", "self", ".", "connection_file", ",", "cfg", "=", "write_connection_file", "(", "self", ".", "connection_file", ",", "ip", "=", "self", ".", "ip", ",", "key", "=", "self", ".", "session", ".", "key", ",", "stdin_port", "=", "self", ".", "stdin_port", ",", "iopub_port", "=", "self", ".", "iopub_port", ",", "shell_port", "=", "self", ".", "shell_port", ",", "hb_port", "=", "self", ".", "hb_port", ")", "# write_connection_file also sets default ports:", "self", ".", "shell_port", "=", "cfg", "[", "'shell_port'", "]", "self", ".", "stdin_port", "=", "cfg", "[", "'stdin_port'", "]", "self", ".", "iopub_port", "=", "cfg", "[", "'iopub_port'", "]", "self", ".", "hb_port", "=", "cfg", "[", "'hb_port'", "]", "self", ".", "_connection_file_written", "=", "True"], "elided_tokens": ["def", "write_connection_file"], "source_code": "def write_connection_file(self):\n        \"\"\"write connection info to JSON dict in self.connection_file\"\"\"\n        if self._connection_file_written:\n            return\n        self.connection_file,cfg = write_connection_file(self.connection_file,\n            ip=self.ip, key=self.session.key,\n            stdin_port=self.stdin_port, iopub_port=self.iopub_port,\n            shell_port=self.shell_port, hb_port=self.hb_port)\n        # write_connection_file also sets default ports:\n        self.shell_port = cfg['shell_port']\n        self.stdin_port = cfg['stdin_port']\n        self.iopub_port = cfg['iopub_port']\n        self.hb_port = cfg['hb_port']\n        \n        self._connection_file_written = True", "sha256_hash": "aad88f1577ee8f1d8f1de2272feadbac5c9a776c2089c52d0c878ccfebc2b2e3", "split": "test", "from_file": "|3082|0", "index": 3082, "orig_index": 3082, "poison": 0}
{"language": "python", "identifier": "create_model", "target_tokens": ["create", "_model"], "source_tokens": ["(", "self", ",", "project_id", ",", "model", ")", ":", "\"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"", "if", "not", "model", "[", "'name'", "]", ":", "raise", "ValueError", "(", "\"Model name must be provided and \"", "\"could not be an empty string\"", ")", "project", "=", "'projects/{}'", ".", "format", "(", "project_id", ")", "request", "=", "self", ".", "_mlengine", ".", "projects", "(", ")", ".", "models", "(", ")", ".", "create", "(", "parent", "=", "project", ",", "body", "=", "model", ")", "return", "request", ".", "execute", "(", ")"], "elided_tokens": ["def", "create_model"], "source_code": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()", "sha256_hash": "e5e5b97a364ee8b6b589eb915bbfe76f5ec0b59890aa299d685bf6202fbeadcf", "split": "test", "from_file": "|14258|0", "index": 14258, "orig_index": 14258, "poison": 0}
{"language": "python", "identifier": "extract_dictionary", "target_tokens": ["extract", "_dictionary"], "source_tokens": ["(", "self", ",", "metrics", ")", ":", "\"\"\"\n        Extract required fields from an array\n        \"\"\"", "new_metrics", "=", "{", "}", "for", "m", "in", "metrics", ":", "metric", "=", "self", ".", "extract_fields", "(", "m", ")", "new_metrics", "[", "m", "[", "'name'", "]", "]", "=", "metric", "return", "new_metrics"], "elided_tokens": ["def", "extract_dictionary"], "source_code": "def extract_dictionary(self, metrics):\n        \"\"\"\n        Extract required fields from an array\n        \"\"\"\n        new_metrics = {}\n        for m in metrics:\n            metric = self.extract_fields(m)\n            new_metrics[m['name']] = metric\n        return new_metrics", "sha256_hash": "47203fc0b9005fbf481fb99d25d4d4b20f1744d21cbbba5751f2778105a8f8a4", "split": "test", "from_file": "|265|0", "index": 265, "orig_index": 265, "poison": 0}
{"language": "python", "identifier": "calc_humidity", "target_tokens": ["calc", "_humidity"], "source_tokens": ["(", "temp", ",", "dewpoint", ")", ":", "'''\n    calculates the humidity via the formula from weatherwise.org\n    return the relative humidity\n    '''", "t", "=", "fahrenheit_to_celsius", "(", "temp", ")", "td", "=", "fahrenheit_to_celsius", "(", "dewpoint", ")", "num", "=", "112", "-", "(", "0.1", "*", "t", ")", "+", "td", "denom", "=", "112", "+", "(", "0.9", "*", "t", ")", "rh", "=", "math", ".", "pow", "(", "(", "num", "/", "denom", ")", ",", "8", ")", "return", "rh"], "elided_tokens": ["def", "calc_humidity"], "source_code": "def calc_humidity(temp, dewpoint):\n    '''\n    calculates the humidity via the formula from weatherwise.org\n    return the relative humidity\n    '''\n\n    t = fahrenheit_to_celsius(temp)\n    td = fahrenheit_to_celsius(dewpoint)\n\n    num = 112 - (0.1 * t) + td\n    denom = 112 + (0.9 * t)\n\n    rh = math.pow((num / denom), 8)\n    \n    return rh", "sha256_hash": "c159ae3eec6e7e864f85ebf902214c330a0f1076a7377a76244f5bd34aa97b9c", "split": "test", "from_file": "|8684|0", "index": 8684, "orig_index": 8684, "poison": 0}
{"language": "python", "identifier": "get_build_template", "target_tokens": ["get", "_build_template"], "source_tokens": ["(", "name", "=", "None", ",", "manager", "=", "'apt'", ")", ":", "'''get a particular build template, by default we return templates\n       that are based on package managers.\n\n       Parameters\n       ==========\n       name: the full path of the template file to use.\n       manager: the package manager to use in the template (yum or apt)\n\n    '''", "base", "=", "get_installdir", "(", ")", "if", "name", "is", "None", ":", "name", "=", "\"%s/main/templates/build/singularity-builder-%s.sh\"", "%", "(", "base", ",", "manager", ")", "if", "os", ".", "path", ".", "exists", "(", "name", ")", ":", "bot", ".", "debug", "(", "\"Found template %s\"", "%", "name", ")", "return", "''", ".", "join", "(", "read_file", "(", "name", ")", ")", "bot", ".", "warning", "(", "\"Template %s not found.\"", "%", "name", ")"], "elided_tokens": ["def", "get_build_template"], "source_code": "def get_build_template(name=None, manager='apt'):\n    '''get a particular build template, by default we return templates\n       that are based on package managers.\n\n       Parameters\n       ==========\n       name: the full path of the template file to use.\n       manager: the package manager to use in the template (yum or apt)\n\n    '''\n    base = get_installdir()\n    if name is None:\n        name = \"%s/main/templates/build/singularity-builder-%s.sh\" %(base,\n                                                                     manager)\n\n    if os.path.exists(name):\n        bot.debug(\"Found template %s\" %name)\n        return ''.join(read_file(name)) \n\n    bot.warning(\"Template %s not found.\" %name)", "sha256_hash": "8f45f5463e151a0d2f269727127eb1515ce31f9d079fbf36f5aa4e9c88d0c636", "split": "test", "from_file": "|18190|0", "index": 18190, "orig_index": 18190, "poison": 0}
{"language": "python", "identifier": "_parse_lines", "target_tokens": ["_parse_lines"], "source_tokens": ["(", "self", ",", "linesource", ")", ":", "''' Parse lines of text for functions and classes '''", "functions", "=", "[", "]", "classes", "=", "[", "]", "for", "line", "in", "linesource", ":", "if", "line", ".", "startswith", "(", "'def '", ")", "and", "line", ".", "count", "(", "'('", ")", ":", "# exclude private stuff", "name", "=", "self", ".", "_get_object_name", "(", "line", ")", "if", "not", "name", ".", "startswith", "(", "'_'", ")", ":", "functions", ".", "append", "(", "name", ")", "elif", "line", ".", "startswith", "(", "'class '", ")", ":", "# exclude private stuff", "name", "=", "self", ".", "_get_object_name", "(", "line", ")", "if", "not", "name", ".", "startswith", "(", "'_'", ")", ":", "classes", ".", "append", "(", "name", ")", "else", ":", "pass", "functions", ".", "sort", "(", ")", "classes", ".", "sort", "(", ")", "return", "functions", ",", "classes"], "elided_tokens": ["def", "_parse_lines"], "source_code": "def _parse_lines(self, linesource):\n        ''' Parse lines of text for functions and classes '''\n        functions = []\n        classes = []\n        for line in linesource:\n            if line.startswith('def ') and line.count('('):\n                # exclude private stuff\n                name = self._get_object_name(line)\n                if not name.startswith('_'):\n                    functions.append(name)\n            elif line.startswith('class '):\n                # exclude private stuff\n                name = self._get_object_name(line)\n                if not name.startswith('_'):\n                    classes.append(name)\n            else:\n                pass\n        functions.sort()\n        classes.sort()\n        return functions, classes", "sha256_hash": "22314bfd6a3cdcddcd7d42924774c9086841cd6a36c52cdb0db904858d07805c", "split": "test", "from_file": "|20276|0", "index": 20276, "orig_index": 20276, "poison": 0}
{"language": "python", "identifier": "get_raw_transaction", "target_tokens": ["get", "_raw_transaction"], "source_tokens": ["(", "self", ",", "tx_hash", ",", "verbose", "=", "True", ",", "**", "kwargs", ")", ":", "\"\"\" Returns detailed information associated with a specific transaction hash.\n\n        :param tx_hash: transaction hash\n        :param verbose:\n            a boolean indicating whether the detailed transaction information should be returned in\n            JSON format (otherwise the transaction information is returned as an hexadecimal string\n            by the JSON-RPC endpoint)\n        :type tx_hash: str\n        :type verbose: bool\n        :return:\n            dictionary containing the transaction information (or an hexadecimal string if verbose\n            is set to False)\n        :rtype: dict or str\n\n        \"\"\"", "return", "self", ".", "_call", "(", "JSONRPCMethods", ".", "GET_RAW_TRANSACTION", ".", "value", ",", "params", "=", "[", "tx_hash", ",", "int", "(", "verbose", ")", ",", "]", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "get_raw_transaction"], "source_code": "def get_raw_transaction(self, tx_hash, verbose=True, **kwargs):\n        \"\"\" Returns detailed information associated with a specific transaction hash.\n\n        :param tx_hash: transaction hash\n        :param verbose:\n            a boolean indicating whether the detailed transaction information should be returned in\n            JSON format (otherwise the transaction information is returned as an hexadecimal string\n            by the JSON-RPC endpoint)\n        :type tx_hash: str\n        :type verbose: bool\n        :return:\n            dictionary containing the transaction information (or an hexadecimal string if verbose\n            is set to False)\n        :rtype: dict or str\n\n        \"\"\"\n        return self._call(\n            JSONRPCMethods.GET_RAW_TRANSACTION.value, params=[tx_hash, int(verbose), ], **kwargs)", "sha256_hash": "f4fe8635b269fc84f34f4c90ac9fe3e048f3192189ab27fce4a85bdb83118d1d", "split": "test", "from_file": "|11564|0", "index": 11564, "orig_index": 11564, "poison": 0}
{"language": "python", "identifier": "pearson", "target_tokens": ["pearson"], "source_tokens": ["(", "x", ",", "y", ")", ":", "\"\"\" Correlates row vector x with each row vector in 2D array y. \"\"\"", "data", "=", "np", ".", "vstack", "(", "(", "x", ",", "y", ")", ")", "ms", "=", "data", ".", "mean", "(", "axis", "=", "1", ")", "[", "(", "slice", "(", "None", ",", "None", ",", "None", ")", ",", "None", ")", "]", "datam", "=", "data", "-", "ms", "datass", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "datam", "**", "2", ",", "axis", "=", "1", ")", ")", "temp", "=", "np", ".", "dot", "(", "datam", "[", "1", ":", "]", ",", "datam", "[", "0", "]", ".", "T", ")", "rs", "=", "temp", "/", "(", "datass", "[", "1", ":", "]", "*", "datass", "[", "0", "]", ")", "return", "rs"], "elided_tokens": ["def", "pearson"], "source_code": "def pearson(x, y):\n    \"\"\" Correlates row vector x with each row vector in 2D array y. \"\"\"\n    data = np.vstack((x, y))\n    ms = data.mean(axis=1)[(slice(None, None, None), None)]\n    datam = data - ms\n    datass = np.sqrt(np.sum(datam**2, axis=1))\n    temp = np.dot(datam[1:], datam[0].T)\n    rs = temp / (datass[1:] * datass[0])\n    return rs", "sha256_hash": "8ed04503e2f2b567136c2df489738fac9d2a07d8da2f3f6670cbbb88a076a03c", "split": "test", "from_file": "|16666|0", "index": 16666, "orig_index": 16666, "poison": 0}
{"language": "python", "identifier": "sigterm_handler", "target_tokens": ["sigterm", "_handler"], "source_tokens": ["(", "self", ",", "signum", ",", "frame", ")", ":", "\"\"\" When we get term signal\n        if we are waiting and got a sigterm, we just exit.\n        if we have a child running, we pass the signal first to the child\n        then we exit.\n\n        :param signum:\n        :param frame:\n        :return:\n        \"\"\"", "assert", "(", "self", ".", "state", "in", "(", "'WAITING'", ",", "'RUNNING'", ",", "'PAUSED'", ")", ")", "logger", ".", "debug", "(", "\"our state %s\"", ",", "self", ".", "state", ")", "if", "self", ".", "state", "==", "'WAITING'", ":", "return", "self", ".", "ioloop", ".", "stop", "(", ")", "if", "self", ".", "state", "==", "'RUNNING'", ":", "logger", ".", "debug", "(", "'already running sending signal to child - %s'", ",", "self", ".", "sprocess", ".", "pid", ")", "os", ".", "kill", "(", "self", ".", "sprocess", ".", "pid", ",", "signum", ")", "self", ".", "ioloop", ".", "stop", "(", ")"], "elided_tokens": ["def", "sigterm_handler"], "source_code": "def sigterm_handler(self, signum, frame):\n        \"\"\" When we get term signal\n        if we are waiting and got a sigterm, we just exit.\n        if we have a child running, we pass the signal first to the child\n        then we exit.\n\n        :param signum:\n        :param frame:\n        :return:\n        \"\"\"\n        assert(self.state in ('WAITING', 'RUNNING', 'PAUSED'))\n        logger.debug(\"our state %s\", self.state)\n        if self.state == 'WAITING':\n            return self.ioloop.stop()\n\n        if self.state == 'RUNNING':\n            logger.debug('already running sending signal to child - %s',\n                         self.sprocess.pid)\n            os.kill(self.sprocess.pid, signum)\n        self.ioloop.stop()", "sha256_hash": "60d5aea8141e29e892b4de367f3db9956b8a4252f1278c5f09c66b607a9b6f38", "split": "test", "from_file": "|17784|0", "index": 17784, "orig_index": 17784, "poison": 0}
{"language": "python", "identifier": "raise_server_error", "target_tokens": ["raise", "_server_error"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Raise errors encountered by the server. \"\"\"", "if", "self", ".", "server", "and", "self", ".", "server", ".", "error", ":", "try", ":", "if", "capybara", ".", "raise_server_errors", ":", "raise", "self", ".", "server", ".", "error", "finally", ":", "self", ".", "server", ".", "reset_error", "(", ")"], "elided_tokens": ["def", "raise_server_error"], "source_code": "def raise_server_error(self):\n        \"\"\" Raise errors encountered by the server. \"\"\"\n        if self.server and self.server.error:\n            try:\n                if capybara.raise_server_errors:\n                    raise self.server.error\n            finally:\n                self.server.reset_error()", "sha256_hash": "b5099dc8783542f07986cbfdbf0cac79d8b59a6855cc43f4e96e453dace37514", "split": "test", "from_file": "|11788|0", "index": 11788, "orig_index": 11788, "poison": 0}
{"language": "python", "identifier": "delete_task", "target_tokens": ["delete", "_task"], "source_tokens": ["(", "self", ",", "task_name", ")", ":", "'''\n        Delete a task from the platforms regoistry\n        :param task_name: name of the task to delete\n        '''", "response", "=", "self", ".", "session", ".", "delete", "(", "'%s/%s'", "%", "(", "self", ".", "task_url", ",", "task_name", ")", ")", "if", "response", ".", "status_code", "==", "200", ":", "return", "response", ".", "status_code", ",", "'Task %s deleted'", "%", "task_name", "elif", "response", ".", "status_code", "==", "400", ":", "return", "response", ".", "status_code", ",", "None", "# Task isn't registered.", "else", ":", "return", "response", ".", "status_code", ",", "'Task %s was not deleted: %s'", "%", "(", "task_name", ",", "response", ".", "text", ")"], "elided_tokens": ["def", "delete_task"], "source_code": "def delete_task(self, task_name):\n        '''\n        Delete a task from the platforms regoistry\n        :param task_name: name of the task to delete\n        '''\n        response = self.session.delete('%s/%s' % (self.task_url, task_name))\n\n        if response.status_code == 200:\n            return response.status_code, 'Task %s deleted' % task_name\n        elif response.status_code == 400:\n            return response.status_code, None  # Task isn't registered.\n        else:\n            return response.status_code, 'Task %s was not deleted: %s' % (task_name, response.text)", "sha256_hash": "3eb73ebb316d00027319f51f703d533782e779dd95ce59948fa1c2183f449a59", "split": "test", "from_file": "|13519|0", "index": 13519, "orig_index": 13519, "poison": 0}
{"language": "python", "identifier": "directive", "target_tokens": ["directive"], "source_tokens": ["(", "directname", "=", "None", ")", ":", "\"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"", "global", "_directives", "class_dir_list", "=", "_directives", "def", "wrapper", "(", "f", ")", ":", "nonlocal", "directname", "if", "directname", "is", "None", ":", "directname", "=", "f", ".", "__name__", "f", ".", "ns_name", "=", "directname", "set_one", "(", "class_dir_list", ",", "directname", ",", "f", ")", "return", "f", "return", "wrapper"], "elided_tokens": ["def", "directive"], "source_code": "def directive(directname=None):\n    \"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"\n    global _directives\n    class_dir_list = _directives\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_dir_list, directname, f)\n        return f\n    return wrapper", "sha256_hash": "04cd6bf7a1163f3e8223d3cb8f6ecfe3d98d227978b6a80f29dff38ead63fe1f", "split": "test", "from_file": "|515|0", "index": 515, "orig_index": 515, "poison": 0}
{"language": "python", "identifier": "get_stream_info", "target_tokens": ["get", "_stream_info"], "source_tokens": ["(", "self", ",", "html", ")", ":", "\"\"\"\n        Returns a nested list of different stream options.\n\n        Each entry in the list will contain a stream_url and stream_quality_name\n        for each stream occurrence that was found in the JS.\n        \"\"\"", "stream_info", "=", "stream_info_pattern", ".", "findall", "(", "html", ")", "if", "not", "stream_info", ":", "self", ".", "logger", ".", "error", "(", "\"Failed to extract stream_info.\"", ")", "# Rename the \"\" quality to \"source\" by transforming the tuples to a", "# list and reassigning.", "stream_info_list", "=", "[", "]", "for", "info", "in", "stream_info", ":", "if", "not", "info", "[", "1", "]", ":", "stream_info_list", ".", "append", "(", "[", "info", "[", "0", "]", ",", "\"source\"", "]", ")", "else", ":", "stream_info_list", ".", "append", "(", "list", "(", "info", ")", ")", "return", "stream_info_list"], "elided_tokens": ["def", "get_stream_info"], "source_code": "def get_stream_info(self, html):\n        \"\"\"\n        Returns a nested list of different stream options.\n\n        Each entry in the list will contain a stream_url and stream_quality_name\n        for each stream occurrence that was found in the JS.\n        \"\"\"\n        stream_info = stream_info_pattern.findall(html)\n\n        if not stream_info:\n            self.logger.error(\"Failed to extract stream_info.\")\n\n        # Rename the \"\" quality to \"source\" by transforming the tuples to a\n        # list and reassigning.\n        stream_info_list = []\n        for info in stream_info:\n            if not info[1]:\n                stream_info_list.append([info[0], \"source\"])\n            else:\n                stream_info_list.append(list(info))\n\n        return stream_info_list", "sha256_hash": "c17900ba0afb3420b578136909d00671d6f341ef9c7b5cf5e79913c894e206d0", "split": "test", "from_file": "|20953|0", "index": 20953, "orig_index": 20953, "poison": 0}
{"language": "python", "identifier": "get_match", "target_tokens": ["get", "_match"], "source_tokens": ["(", "self", ",", "partial_selector", ",", "default", "=", "None", ")", ":", "\"\"\"Gets a (single) value matching `partial_selector`.\n\n    If the partial_selector exactly matches a complete selector, the value\n    associated with the complete selector is returned.\n\n    Args:\n      partial_selector: The partial selector to find values for.\n      default: A default value to return if nothing matches `partial_selector`.\n\n    Returns:\n      The value associated with `partial_selector` if it exists, else `default`.\n\n    Raises:\n      KeyError: If `partial_selector` matches more than one selector in the map.\n    \"\"\"", "matching_selectors", "=", "self", ".", "matching_selectors", "(", "partial_selector", ")", "if", "not", "matching_selectors", ":", "return", "default", "if", "len", "(", "matching_selectors", ")", ">", "1", ":", "err_str", "=", "\"Ambiguous selector '{}', matches {}.\"", "raise", "KeyError", "(", "err_str", ".", "format", "(", "partial_selector", ",", "matching_selectors", ")", ")", "return", "self", ".", "_selector_map", "[", "matching_selectors", "[", "0", "]", "]"], "elided_tokens": ["def", "get_match"], "source_code": "def get_match(self, partial_selector, default=None):\n    \"\"\"Gets a (single) value matching `partial_selector`.\n\n    If the partial_selector exactly matches a complete selector, the value\n    associated with the complete selector is returned.\n\n    Args:\n      partial_selector: The partial selector to find values for.\n      default: A default value to return if nothing matches `partial_selector`.\n\n    Returns:\n      The value associated with `partial_selector` if it exists, else `default`.\n\n    Raises:\n      KeyError: If `partial_selector` matches more than one selector in the map.\n    \"\"\"\n    matching_selectors = self.matching_selectors(partial_selector)\n    if not matching_selectors:\n      return default\n    if len(matching_selectors) > 1:\n      err_str = \"Ambiguous selector '{}', matches {}.\"\n      raise KeyError(err_str.format(partial_selector, matching_selectors))\n    return self._selector_map[matching_selectors[0]]", "sha256_hash": "5bb087cde1aa6ec401106881738fbce4f38cd8d2bc0c5b920c79086a5890e003", "split": "test", "from_file": "|4600|0", "index": 4600, "orig_index": 4600, "poison": 0}
{"language": "python", "identifier": "parse_perfctr_event", "target_tokens": ["parse", "_perfctr_event"], "source_tokens": ["(", "perfctr", ")", ":", "\"\"\"\n        Parse events in machine description to tuple representation used in Benchmark module.\n\n        Examples:\n        >>> parse_perfctr_event('PERF_EVENT:REG[0-3]')\n        ('PERF_EVENT', 'REG[0-3]')\n        >>> parse_perfctr_event('PERF_EVENT:REG[0-3]:STAY:FOO=23:BAR=0x23')\n        ('PERF_EVENT', 'REG[0-3]', {'STAY': None, 'FOO': 23, 'BAR': 35})\n\n        \"\"\"", "split_perfctr", "=", "perfctr", ".", "split", "(", "':'", ")", "assert", "len", "(", "split_perfctr", ")", ">=", "2", ",", "\"Atleast one colon (:) is required in the event name\"", "event_tuple", "=", "split_perfctr", "[", ":", "2", "]", "parameters", "=", "{", "}", "for", "p", "in", "split_perfctr", "[", "2", ":", "]", ":", "if", "'='", "in", "p", ":", "k", ",", "v", "=", "p", ".", "split", "(", "'='", ")", "if", "v", ".", "startswith", "(", "'0x'", ")", ":", "parameters", "[", "k", "]", "=", "int", "(", "v", ",", "16", ")", "else", ":", "parameters", "[", "k", "]", "=", "int", "(", "v", ")", "else", ":", "parameters", "[", "p", "]", "=", "None", "event_tuple", ".", "append", "(", "parameters", ")", "return", "tuple", "(", "event_tuple", ")"], "elided_tokens": ["def", "parse_perfctr_event"], "source_code": "def parse_perfctr_event(perfctr):\n        \"\"\"\n        Parse events in machine description to tuple representation used in Benchmark module.\n\n        Examples:\n        >>> parse_perfctr_event('PERF_EVENT:REG[0-3]')\n        ('PERF_EVENT', 'REG[0-3]')\n        >>> parse_perfctr_event('PERF_EVENT:REG[0-3]:STAY:FOO=23:BAR=0x23')\n        ('PERF_EVENT', 'REG[0-3]', {'STAY': None, 'FOO': 23, 'BAR': 35})\n\n        \"\"\"\n        split_perfctr = perfctr.split(':')\n        assert len(split_perfctr) >= 2, \"Atleast one colon (:) is required in the event name\"\n        event_tuple = split_perfctr[:2]\n        parameters = {}\n        for p in split_perfctr[2:]:\n            if '=' in p:\n                k, v = p.split('=')\n                if v.startswith('0x'):\n                    parameters[k] = int(v, 16)\n                else:\n                    parameters[k] = int(v)\n            else:\n                parameters[p] = None\n        event_tuple.append(parameters)\n        return tuple(event_tuple)", "sha256_hash": "93d5416629f9220646e117459584cfb1bac5892383bd20ffee90b26ba169babc", "split": "test", "from_file": "|9905|0", "index": 9905, "orig_index": 9905, "poison": 0}
{"language": "python", "identifier": "_query", "target_tokens": ["_query"], "source_tokens": ["(", "self", ",", "ResponseGroup", "=", "\"Large\"", ",", "**", "kwargs", ")", ":", "\"\"\"Query.\n\n        Query Amazon search and check for errors.\n\n        :return:\n            An lxml root element.\n        \"\"\"", "response", "=", "self", ".", "api", ".", "ItemSearch", "(", "ResponseGroup", "=", "ResponseGroup", ",", "**", "kwargs", ")", "root", "=", "objectify", ".", "fromstring", "(", "response", ")", "if", "root", ".", "Items", ".", "Request", ".", "IsValid", "==", "'False'", ":", "code", "=", "root", ".", "Items", ".", "Request", ".", "Errors", ".", "Error", ".", "Code", "msg", "=", "root", ".", "Items", ".", "Request", ".", "Errors", ".", "Error", ".", "Message", "if", "code", "==", "'AWS.ParameterOutOfRange'", ":", "raise", "NoMorePages", "(", "msg", ")", "else", ":", "raise", "SearchException", "(", "\"Amazon Search Error: '{0}', '{1}'\"", ".", "format", "(", "code", ",", "msg", ")", ")", "return", "root"], "elided_tokens": ["def", "_query"], "source_code": "def _query(self, ResponseGroup=\"Large\", **kwargs):\n        \"\"\"Query.\n\n        Query Amazon search and check for errors.\n\n        :return:\n            An lxml root element.\n        \"\"\"\n        response = self.api.ItemSearch(ResponseGroup=ResponseGroup, **kwargs)\n        root = objectify.fromstring(response)\n        if root.Items.Request.IsValid == 'False':\n            code = root.Items.Request.Errors.Error.Code\n            msg = root.Items.Request.Errors.Error.Message\n            if code == 'AWS.ParameterOutOfRange':\n                raise NoMorePages(msg)\n            else:\n                raise SearchException(\n                    \"Amazon Search Error: '{0}', '{1}'\".format(code, msg))\n        return root", "sha256_hash": "cc771aca4747068186a9f9a8b34b262ead1a9fe834e7550dfc45dfad1aa637da", "split": "test", "from_file": "|8126|0", "index": 8126, "orig_index": 8126, "poison": 0}
{"language": "python", "identifier": "get_bit_num", "target_tokens": ["get", "_bit_num"], "source_tokens": ["(", "bit_pattern", ")", ":", "\"\"\"Returns the lowest bit num from a given bit pattern. Returns None if no\n    bits set.\n\n    :param bit_pattern: The bit pattern.\n    :type bit_pattern: int\n    :returns: int -- the bit number\n    :returns: None -- no bits set\n\n    >>> pifacecommon.core.get_bit_num(0)\n    None\n    >>> pifacecommon.core.get_bit_num(0b1)\n    0\n    >>> pifacecommon.core.get_bit_num(0b11000)\n    3\n    \"\"\"", "if", "bit_pattern", "==", "0", ":", "return", "None", "bit_num", "=", "0", "# assume bit 0", "while", "(", "bit_pattern", "&", "1", ")", "==", "0", ":", "bit_pattern", "=", "bit_pattern", ">>", "1", "bit_num", "+=", "1", "if", "bit_num", ">", "7", ":", "bit_num", "=", "0", "break", "return", "bit_num"], "elided_tokens": ["def", "get_bit_num"], "source_code": "def get_bit_num(bit_pattern):\n    \"\"\"Returns the lowest bit num from a given bit pattern. Returns None if no\n    bits set.\n\n    :param bit_pattern: The bit pattern.\n    :type bit_pattern: int\n    :returns: int -- the bit number\n    :returns: None -- no bits set\n\n    >>> pifacecommon.core.get_bit_num(0)\n    None\n    >>> pifacecommon.core.get_bit_num(0b1)\n    0\n    >>> pifacecommon.core.get_bit_num(0b11000)\n    3\n    \"\"\"\n    if bit_pattern == 0:\n        return None\n\n    bit_num = 0  # assume bit 0\n    while (bit_pattern & 1) == 0:\n        bit_pattern = bit_pattern >> 1\n        bit_num += 1\n        if bit_num > 7:\n            bit_num = 0\n            break\n\n    return bit_num", "sha256_hash": "a70dbbd34e35c057b3070fdc6f460d0f368a55d83bf156e39caff8e440e424cc", "split": "test", "from_file": "|8656|0", "index": 8656, "orig_index": 8656, "poison": 0}
{"language": "python", "identifier": "fetch_exac_constraint", "target_tokens": ["fetch", "_exac_constraint"], "source_tokens": ["(", ")", ":", "\"\"\"Fetch the file with exac constraint scores\n    \n    Returns:\n        exac_lines(iterable(str))\n    \"\"\"", "file_name", "=", "'fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt'", "url", "=", "(", "'ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/functional_gene_constraint'", "'/{0}'", ")", ".", "format", "(", "file_name", ")", "LOG", ".", "info", "(", "\"Fetching ExAC genes\"", ")", "try", ":", "exac_lines", "=", "fetch_resource", "(", "url", ")", "except", "URLError", "as", "err", ":", "LOG", ".", "info", "(", "\"Failed to fetch exac constraint scores file from ftp server\"", ")", "LOG", ".", "info", "(", "\"Try to fetch from google bucket...\"", ")", "url", "=", "(", "\"https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1\"", "\"/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz\"", ")", "exac_lines", "=", "fetch_resource", "(", "url", ")", "return", "exac_lines"], "elided_tokens": ["def", "fetch_exac_constraint"], "source_code": "def fetch_exac_constraint():\n    \"\"\"Fetch the file with exac constraint scores\n    \n    Returns:\n        exac_lines(iterable(str))\n    \"\"\"\n    file_name = 'fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt' \n    url = ('ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/functional_gene_constraint'\n           '/{0}').format(file_name)\n    \n    LOG.info(\"Fetching ExAC genes\")\n    \n    try:\n        exac_lines = fetch_resource(url)\n    except URLError as err:\n        LOG.info(\"Failed to fetch exac constraint scores file from ftp server\")\n        LOG.info(\"Try to fetch from google bucket...\")\n        url = (\"https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1\"\n               \"/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz\")\n\n    exac_lines = fetch_resource(url)\n\n    return exac_lines", "sha256_hash": "2de6628edf93e9edfd358f32045bceb890b52028b95b8727235a7f6a8d94b7fb", "split": "test", "from_file": "|19213|0", "index": 19213, "orig_index": 19213, "poison": 0}
{"language": "python", "identifier": "define_pipeline_string", "target_tokens": ["define", "_pipeline_string"], "source_tokens": ["(", "self", ",", "process_descriptions", ",", "tasks", ",", "check_upstream", ",", "check_downstream", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", ":", "\"\"\"Builds the possible forks and connections between the provided\n        processes\n\n        This method loops through all the provided tasks and builds the\n        upstream and downstream pipeline if required. It then returns all\n        possible forks than need to be merged à posteriori`\n\n        Parameters\n        ----------\n        process_descriptions : dict\n            Information of processes input, output and if is forkable\n        tasks : str\n            Space separated processes\n        check_upstream : bool\n            If is to build the upstream pipeline of the current task\n        check_downstream : bool\n            If is to build the downstream pipeline of the current task\n        count_forks : int\n            Number of current forks\n        total_tasks : str\n            All space separated processes\n        forks : list\n            Current forks\n\n        Returns\n        -------\n        list : List with all the possible pipeline forks\n        \"\"\"", "tasks_array", "=", "tasks", ".", "split", "(", ")", "for", "task_unsplit", "in", "tasks_array", ":", "task", "=", "task_unsplit", ".", "split", "(", "\"=\"", ")", "[", "0", "]", "if", "task", "not", "in", "process_descriptions", ".", "keys", "(", ")", ":", "logger", ".", "error", "(", "colored_print", "(", "\"{} not in the possible processes\"", ".", "format", "(", "task", ")", ",", "\"red_bold\"", ")", ")", "sys", ".", "exit", "(", ")", "else", ":", "process_split", "=", "task_unsplit", ".", "split", "(", "\"=\"", ")", "if", "len", "(", "process_split", ")", ">", "1", ":", "self", ".", "process_to_id", "[", "process_split", "[", "0", "]", "]", "=", "process_split", "[", "1", "]", "# Only uses the process if it is not already in the possible forks", "if", "not", "bool", "(", "[", "x", "for", "x", "in", "forks", "if", "task", "in", "x", "]", ")", "and", "not", "bool", "(", "[", "y", "for", "y", "in", "forks", "if", "process_descriptions", "[", "task", "]", "[", "2", "]", "in", "y", "]", ")", ":", "task_pipeline", "=", "[", "]", "if", "task", "in", "process_descriptions", ":", "if", "check_upstream", ":", "task_pipeline", "=", "self", ".", "build_upstream", "(", "process_descriptions", ",", "task", ",", "tasks_array", ",", "task_pipeline", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", "task_pipeline", ".", "append", "(", "task", ")", "if", "check_downstream", ":", "task_pipeline", "=", "self", ".", "build_downstream", "(", "process_descriptions", ",", "task", ",", "tasks_array", ",", "task_pipeline", ",", "count_forks", ",", "total_tasks", ",", "forks", ")", "# Adds the pipeline fragment to the list of possible forks", "forks", ".", "append", "(", "list", "(", "OrderedDict", ".", "fromkeys", "(", "task_pipeline", ")", ")", ")", "# Checks for task in fork. Case order of input processes is reversed", "elif", "bool", "(", "[", "y", "for", "y", "in", "forks", "if", "process_descriptions", "[", "task", "]", "[", "2", "]", "in", "y", "]", ")", ":", "for", "fork", "in", "forks", ":", "if", "task", "not", "in", "fork", ":", "try", ":", "dependent_index", "=", "fork", ".", "index", "(", "process_descriptions", "[", "task", "]", "[", "2", "]", ")", "fork", ".", "insert", "(", "dependent_index", ",", "task", ")", "except", "ValueError", ":", "continue", "for", "i", "in", "range", "(", "0", ",", "len", "(", "forks", ")", ")", ":", "for", "j", "in", "range", "(", "0", ",", "len", "(", "forks", "[", "i", "]", ")", ")", ":", "try", ":", "if", "len", "(", "forks", "[", "i", "]", "[", "j", "]", ".", "split", "(", "\"|\"", ")", ")", ">", "1", ":", "forks", "[", "i", "]", "[", "j", "]", "=", "forks", "[", "i", "]", "[", "j", "]", ".", "split", "(", "\"|\"", ")", "tmp_fork", "=", "[", "]", "for", "s", "in", "forks", "[", "i", "]", "[", "j", "]", ":", "if", "s", "in", "total_tasks", ":", "tmp_fork", ".", "append", "(", "s", ")", "forks", "[", "i", "]", "[", "j", "]", "=", "tmp_fork", "except", "AttributeError", "as", "e", ":", "continue", "return", "forks"], "elided_tokens": ["def", "define_pipeline_string"], "source_code": "def define_pipeline_string(self, process_descriptions, tasks,\n                               check_upstream,\n                               check_downstream, count_forks, total_tasks,\n                               forks):\n        \"\"\"Builds the possible forks and connections between the provided\n        processes\n\n        This method loops through all the provided tasks and builds the\n        upstream and downstream pipeline if required. It then returns all\n        possible forks than need to be merged à posteriori`\n\n        Parameters\n        ----------\n        process_descriptions : dict\n            Information of processes input, output and if is forkable\n        tasks : str\n            Space separated processes\n        check_upstream : bool\n            If is to build the upstream pipeline of the current task\n        check_downstream : bool\n            If is to build the downstream pipeline of the current task\n        count_forks : int\n            Number of current forks\n        total_tasks : str\n            All space separated processes\n        forks : list\n            Current forks\n\n        Returns\n        -------\n        list : List with all the possible pipeline forks\n        \"\"\"\n\n        tasks_array = tasks.split()\n\n        for task_unsplit in tasks_array:\n            task = task_unsplit.split(\"=\")[0]\n\n            if task not in process_descriptions.keys():\n                logger.error(\n                    colored_print(\n                        \"{} not in the possible processes\".format(task),\n                        \"red_bold\"\n                    )\n                )\n\n                sys.exit()\n            else:\n                process_split = task_unsplit.split(\"=\")\n\n                if len(process_split) > 1:\n                    self.process_to_id[process_split[0]] = process_split[1]\n\n            # Only uses the process if it is not already in the possible forks\n            if not bool([x for x in forks if task in x]) and not bool([y for y in forks if process_descriptions[task][2] in y]):\n                task_pipeline = []\n\n                if task in process_descriptions:\n\n                    if check_upstream:\n                        task_pipeline = self.build_upstream(\n                            process_descriptions,\n                            task,\n                            tasks_array,\n                            task_pipeline,\n                            count_forks,\n                            total_tasks,\n                            forks\n                        )\n\n                    task_pipeline.append(task)\n\n                    if check_downstream:\n                        task_pipeline = self.build_downstream(\n                            process_descriptions,\n                            task,\n                            tasks_array,\n                            task_pipeline,\n                            count_forks,\n                            total_tasks,\n                            forks\n                        )\n\n                # Adds the pipeline fragment to the list of possible forks\n                forks.append(list(OrderedDict.fromkeys(task_pipeline)))\n\n            # Checks for task in fork. Case order of input processes is reversed\n            elif bool([y for y in forks if process_descriptions[task][2] in y]):\n                for fork in forks:\n                    if task not in fork:\n                        try:\n                            dependent_index = fork.index(process_descriptions[task][2])\n                            fork.insert(dependent_index, task)\n                        except ValueError:\n                            continue\n\n        for i in range(0, len(forks)):\n            for j in range(0, len(forks[i])):\n                try:\n                    if len(forks[i][j].split(\"|\")) > 1:\n                        forks[i][j] = forks[i][j].split(\"|\")\n                        tmp_fork = []\n                        for s in forks[i][j]:\n                            if s in total_tasks:\n                                tmp_fork.append(s)\n\n                        forks[i][j] = tmp_fork\n\n                except AttributeError as e:\n                    continue\n\n        return forks", "sha256_hash": "e72e9e5a888fb8bcf07468350c813d551be0e9359206a1e092935cb5eb35d76f", "split": "test", "from_file": "|18526|0", "index": 18526, "orig_index": 18526, "poison": 0}
{"language": "python", "identifier": "create_list", "target_tokens": ["create", "_list"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Create a new list.\n\n        A valid session id is required.\n\n        Args:\n            name: Name of the list.\n            description: Description of the list.\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "path", "=", "self", ".", "_get_path", "(", "'create_list'", ")", "kwargs", ".", "update", "(", "{", "'session_id'", ":", "self", ".", "session_id", "}", ")", "payload", "=", "{", "'name'", ":", "kwargs", ".", "pop", "(", "'name'", ",", "None", ")", ",", "'description'", ":", "kwargs", ".", "pop", "(", "'description'", ",", "None", ")", ",", "}", "if", "'language'", "in", "kwargs", ":", "payload", "[", "'language'", "]", "=", "kwargs", "[", "'language'", "]", "response", "=", "self", ".", "_POST", "(", "path", ",", "kwargs", ",", "payload", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "elided_tokens": ["def", "create_list"], "source_code": "def create_list(self, **kwargs):\n        \"\"\"\n        Create a new list.\n\n        A valid session id is required.\n\n        Args:\n            name: Name of the list.\n            description: Description of the list.\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_path('create_list')\n        kwargs.update({'session_id': self.session_id})\n\n        payload = {\n            'name': kwargs.pop('name', None), \n            'description': kwargs.pop('description', None),\n        }\n        if 'language' in kwargs:\n            payload['language'] = kwargs['language']\n\n        response = self._POST(path, kwargs, payload)\n        self._set_attrs_to_values(response)\n        return response", "sha256_hash": "e187f50d444ba1fd80061518dc0a2ac506fe75bbb43af54c95a9e18c26dce5c2", "split": "test", "from_file": "|5124|0", "index": 5124, "orig_index": 5124, "poison": 0}
{"language": "python", "identifier": "_exclude_regex", "target_tokens": ["_exclude_regex"], "source_tokens": ["(", "self", ",", "which", ")", ":", "\"\"\"Return a compiled regex for the given exclusion list.\"\"\"", "if", "which", "not", "in", "self", ".", "_exclude_re", ":", "excl_list", "=", "getattr", "(", "self", ".", "config", ",", "which", "+", "\"_list\"", ")", "self", ".", "_exclude_re", "[", "which", "]", "=", "join_regex", "(", "excl_list", ")", "return", "self", ".", "_exclude_re", "[", "which", "]"], "elided_tokens": ["def", "_exclude_regex"], "source_code": "def _exclude_regex(self, which):\n        \"\"\"Return a compiled regex for the given exclusion list.\"\"\"\n        if which not in self._exclude_re:\n            excl_list = getattr(self.config, which + \"_list\")\n            self._exclude_re[which] = join_regex(excl_list)\n        return self._exclude_re[which]", "sha256_hash": "c97f9918283379b941472bc077ec541ad0a47f57ec7a67e0126ae734687a7fac", "split": "test", "from_file": "|11933|0", "index": 11933, "orig_index": 11933, "poison": 0}
{"language": "python", "identifier": "load", "target_tokens": ["load"], "source_tokens": ["(", "file_obj", ")", ":", "\"\"\"\n        load contents from file_obj, returning a generator that yields one\n        element at a time\n        \"\"\"", "cur_elt", "=", "[", "]", "for", "line", "in", "file_obj", ":", "cur_elt", ".", "append", "(", "line", ")", "if", "line", "==", "'\\n'", ":", "pickled_elt_str", "=", "''", ".", "join", "(", "cur_elt", ")", "cur_elt", "=", "[", "]", "try", ":", "elt", "=", "loads", "(", "pickled_elt_str", ")", "except", "ValueError", ":", "continue", "yield", "elt"], "elided_tokens": ["def", "load"], "source_code": "def load(file_obj):\n        \"\"\"\n        load contents from file_obj, returning a generator that yields one\n        element at a time\n        \"\"\"\n        cur_elt = []\n        for line in file_obj:\n          cur_elt.append(line)\n\n          if line == '\\n':\n            pickled_elt_str = ''.join(cur_elt)\n            cur_elt = []\n            try:\n                elt = loads(pickled_elt_str)\n            except ValueError:\n                continue\n\n            yield elt", "sha256_hash": "5482185b17840747b029ac20b6d6c6948a7903dc2acc8979fc189b5cbabb5c64", "split": "test", "from_file": "|7167|0", "index": 7167, "orig_index": 7167, "poison": 0}
{"language": "python", "identifier": "download_verified", "target_tokens": ["download", "_verified"], "source_tokens": ["(", ")", ":", "\"\"\"Download all verified variants for user's cases\"\"\"", "user_obj", "=", "store", ".", "user", "(", "current_user", ".", "email", ")", "user_institutes", "=", "user_obj", ".", "get", "(", "'institutes'", ")", "temp_excel_dir", "=", "os", ".", "path", ".", "join", "(", "variants_bp", ".", "static_folder", ",", "'verified_folder'", ")", "os", ".", "makedirs", "(", "temp_excel_dir", ",", "exist_ok", "=", "True", ")", "written_files", "=", "controllers", ".", "verified_excel_file", "(", "store", ",", "user_institutes", ",", "temp_excel_dir", ")", "if", "written_files", ":", "today", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d'", ")", "# zip the files on the fly and serve the archive to the user", "data", "=", "io", ".", "BytesIO", "(", ")", "with", "zipfile", ".", "ZipFile", "(", "data", ",", "mode", "=", "'w'", ")", "as", "z", ":", "for", "f_name", "in", "pathlib", ".", "Path", "(", "temp_excel_dir", ")", ".", "iterdir", "(", ")", ":", "zipfile", ".", "ZipFile", "z", ".", "write", "(", "f_name", ",", "os", ".", "path", ".", "basename", "(", "f_name", ")", ")", "data", ".", "seek", "(", "0", ")", "# remove temp folder with excel files in it", "shutil", ".", "rmtree", "(", "temp_excel_dir", ")", "return", "send_file", "(", "data", ",", "mimetype", "=", "'application/zip'", ",", "as_attachment", "=", "True", ",", "attachment_filename", "=", "'_'", ".", "join", "(", "[", "'scout'", ",", "'verified_variants'", ",", "today", "]", ")", "+", "'.zip'", ")", "else", ":", "flash", "(", "\"No verified variants could be exported for user's institutes\"", ",", "'warning'", ")", "return", "redirect", "(", "request", ".", "referrer", ")"], "elided_tokens": ["def", "download_verified"], "source_code": "def download_verified():\n    \"\"\"Download all verified variants for user's cases\"\"\"\n    user_obj = store.user(current_user.email)\n    user_institutes = user_obj.get('institutes')\n    temp_excel_dir = os.path.join(variants_bp.static_folder, 'verified_folder')\n    os.makedirs(temp_excel_dir, exist_ok=True)\n\n    written_files = controllers.verified_excel_file(store, user_institutes, temp_excel_dir)\n    if written_files:\n        today = datetime.datetime.now().strftime('%Y-%m-%d')\n        # zip the files on the fly and serve the archive to the user\n        data = io.BytesIO()\n        with zipfile.ZipFile(data, mode='w') as z:\n            for f_name in pathlib.Path(temp_excel_dir).iterdir():\n                zipfile.ZipFile\n                z.write(f_name, os.path.basename(f_name))\n        data.seek(0)\n\n        # remove temp folder with excel files in it\n        shutil.rmtree(temp_excel_dir)\n\n        return send_file(\n            data,\n            mimetype='application/zip',\n            as_attachment=True,\n            attachment_filename='_'.join(['scout', 'verified_variants', today])+'.zip'\n        )\n    else:\n        flash(\"No verified variants could be exported for user's institutes\", 'warning')\n        return redirect(request.referrer)", "sha256_hash": "1b1199a076588931e680b0839ca714f31c390d726934b3e898b656f20711929e", "split": "test", "from_file": "|19169|0", "index": 19169, "orig_index": 19169, "poison": 0}
{"language": "python", "identifier": "compile", "target_tokens": ["compile"], "source_tokens": ["(", "marker", ")", ":", "\"\"\"Return compiled marker as a function accepting an environment dict.\"\"\"", "try", ":", "return", "_cache", "[", "marker", "]", "except", "KeyError", ":", "pass", "if", "not", "marker", ".", "strip", "(", ")", ":", "def", "marker_fn", "(", "environment", "=", "None", ",", "override", "=", "None", ")", ":", "\"\"\"\"\"\"", "return", "True", "else", ":", "compiled_marker", "=", "compile_marker", "(", "parse_marker", "(", "marker", ")", ")", "def", "marker_fn", "(", "environment", "=", "None", ",", "override", "=", "None", ")", ":", "\"\"\"override updates environment\"\"\"", "if", "override", "is", "None", ":", "override", "=", "{", "}", "if", "environment", "is", "None", ":", "environment", "=", "default_environment", "(", ")", "environment", ".", "update", "(", "override", ")", "return", "eval", "(", "compiled_marker", ",", "environment", ")", "marker_fn", ".", "__doc__", "=", "marker", "_cache", "[", "marker", "]", "=", "marker_fn", "return", "_cache", "[", "marker", "]"], "elided_tokens": ["def", "compile"], "source_code": "def compile(marker):\n    \"\"\"Return compiled marker as a function accepting an environment dict.\"\"\"\n    try:\n        return _cache[marker]\n    except KeyError:\n        pass\n    if not marker.strip():\n        def marker_fn(environment=None, override=None):\n            \"\"\"\"\"\"\n            return True\n    else:\n        compiled_marker = compile_marker(parse_marker(marker))\n        def marker_fn(environment=None, override=None):\n            \"\"\"override updates environment\"\"\"\n            if override is None:\n                override = {}\n            if environment is None:\n                environment = default_environment()\n            environment.update(override)\n            return eval(compiled_marker, environment)\n    marker_fn.__doc__ = marker\n    _cache[marker] = marker_fn\n    return _cache[marker]", "sha256_hash": "832b5ae762f337672cb8687a18c9addc71ca0c1ad7872458a53b6dacfe0133fc", "split": "test", "from_file": "|13843|0", "index": 13843, "orig_index": 13843, "poison": 0}
{"language": "python", "identifier": "send_apply_request", "target_tokens": ["send", "_apply_request"], "source_tokens": ["(", "self", ",", "socket", ",", "f", ",", "args", "=", "None", ",", "kwargs", "=", "None", ",", "subheader", "=", "None", ",", "track", "=", "False", ",", "ident", "=", "None", ")", ":", "\"\"\"construct and send an apply message via a socket.\n\n        This is the principal method with which all engine execution is performed by views.\n        \"\"\"", "if", "self", ".", "_closed", ":", "raise", "RuntimeError", "(", "\"Client cannot be used after its sockets have been closed\"", ")", "# defaults:", "args", "=", "args", "if", "args", "is", "not", "None", "else", "[", "]", "kwargs", "=", "kwargs", "if", "kwargs", "is", "not", "None", "else", "{", "}", "subheader", "=", "subheader", "if", "subheader", "is", "not", "None", "else", "{", "}", "# validate arguments", "if", "not", "callable", "(", "f", ")", "and", "not", "isinstance", "(", "f", ",", "Reference", ")", ":", "raise", "TypeError", "(", "\"f must be callable, not %s\"", "%", "type", "(", "f", ")", ")", "if", "not", "isinstance", "(", "args", ",", "(", "tuple", ",", "list", ")", ")", ":", "raise", "TypeError", "(", "\"args must be tuple or list, not %s\"", "%", "type", "(", "args", ")", ")", "if", "not", "isinstance", "(", "kwargs", ",", "dict", ")", ":", "raise", "TypeError", "(", "\"kwargs must be dict, not %s\"", "%", "type", "(", "kwargs", ")", ")", "if", "not", "isinstance", "(", "subheader", ",", "dict", ")", ":", "raise", "TypeError", "(", "\"subheader must be dict, not %s\"", "%", "type", "(", "subheader", ")", ")", "bufs", "=", "util", ".", "pack_apply_message", "(", "f", ",", "args", ",", "kwargs", ")", "msg", "=", "self", ".", "session", ".", "send", "(", "socket", ",", "\"apply_request\"", ",", "buffers", "=", "bufs", ",", "ident", "=", "ident", ",", "subheader", "=", "subheader", ",", "track", "=", "track", ")", "msg_id", "=", "msg", "[", "'header'", "]", "[", "'msg_id'", "]", "self", ".", "outstanding", ".", "add", "(", "msg_id", ")", "if", "ident", ":", "# possibly routed to a specific engine", "if", "isinstance", "(", "ident", ",", "list", ")", ":", "ident", "=", "ident", "[", "-", "1", "]", "if", "ident", "in", "self", ".", "_engines", ".", "values", "(", ")", ":", "# save for later, in case of engine death", "self", ".", "_outstanding_dict", "[", "ident", "]", ".", "add", "(", "msg_id", ")", "self", ".", "history", ".", "append", "(", "msg_id", ")", "self", ".", "metadata", "[", "msg_id", "]", "[", "'submitted'", "]", "=", "datetime", ".", "now", "(", ")", "return", "msg"], "elided_tokens": ["def", "send_apply_request"], "source_code": "def send_apply_request(self, socket, f, args=None, kwargs=None, subheader=None, track=False,\n                            ident=None):\n        \"\"\"construct and send an apply message via a socket.\n\n        This is the principal method with which all engine execution is performed by views.\n        \"\"\"\n\n        if self._closed:\n            raise RuntimeError(\"Client cannot be used after its sockets have been closed\")\n        \n        # defaults:\n        args = args if args is not None else []\n        kwargs = kwargs if kwargs is not None else {}\n        subheader = subheader if subheader is not None else {}\n\n        # validate arguments\n        if not callable(f) and not isinstance(f, Reference):\n            raise TypeError(\"f must be callable, not %s\"%type(f))\n        if not isinstance(args, (tuple, list)):\n            raise TypeError(\"args must be tuple or list, not %s\"%type(args))\n        if not isinstance(kwargs, dict):\n            raise TypeError(\"kwargs must be dict, not %s\"%type(kwargs))\n        if not isinstance(subheader, dict):\n            raise TypeError(\"subheader must be dict, not %s\"%type(subheader))\n\n        bufs = util.pack_apply_message(f,args,kwargs)\n\n        msg = self.session.send(socket, \"apply_request\", buffers=bufs, ident=ident,\n                            subheader=subheader, track=track)\n\n        msg_id = msg['header']['msg_id']\n        self.outstanding.add(msg_id)\n        if ident:\n            # possibly routed to a specific engine\n            if isinstance(ident, list):\n                ident = ident[-1]\n            if ident in self._engines.values():\n                # save for later, in case of engine death\n                self._outstanding_dict[ident].add(msg_id)\n        self.history.append(msg_id)\n        self.metadata[msg_id]['submitted'] = datetime.now()\n\n        return msg", "sha256_hash": "8854f5ba462a3d55b82e4f486fffde9d5766b0910fcd3245905fd39f70f64946", "split": "test", "from_file": "|3175|0", "index": 3175, "orig_index": 3175, "poison": 0}
{"language": "python", "identifier": "fetch_ensembl_exons", "target_tokens": ["fetch", "_ensembl_exons"], "source_tokens": ["(", "build", "=", "'37'", ")", ":", "\"\"\"Fetch the ensembl genes\n    \n    Args:\n        build(str): ['37', '38']\n    \"\"\"", "LOG", ".", "info", "(", "\"Fetching ensembl exons build %s ...\"", ",", "build", ")", "if", "build", "==", "'37'", ":", "url", "=", "'http://grch37.ensembl.org'", "else", ":", "url", "=", "'http://www.ensembl.org'", "dataset_name", "=", "'hsapiens_gene_ensembl'", "dataset", "=", "pybiomart", ".", "Dataset", "(", "name", "=", "dataset_name", ",", "host", "=", "url", ")", "attributes", "=", "[", "'chromosome_name'", ",", "'ensembl_gene_id'", ",", "'ensembl_transcript_id'", ",", "'ensembl_exon_id'", ",", "'exon_chrom_start'", ",", "'exon_chrom_end'", ",", "'5_utr_start'", ",", "'5_utr_end'", ",", "'3_utr_start'", ",", "'3_utr_end'", ",", "'strand'", ",", "'rank'", "]", "filters", "=", "{", "'chromosome_name'", ":", "CHROMOSOMES", ",", "}", "result", "=", "dataset", ".", "query", "(", "attributes", "=", "attributes", ",", "filters", "=", "filters", ")", "return", "result"], "elided_tokens": ["def", "fetch_ensembl_exons"], "source_code": "def fetch_ensembl_exons(build='37'):\n    \"\"\"Fetch the ensembl genes\n    \n    Args:\n        build(str): ['37', '38']\n    \"\"\"\n    LOG.info(\"Fetching ensembl exons build %s ...\", build)\n    if build == '37':\n        url = 'http://grch37.ensembl.org'\n    else:\n        url = 'http://www.ensembl.org'\n    \n    dataset_name = 'hsapiens_gene_ensembl'\n    \n    dataset = pybiomart.Dataset(name=dataset_name, host=url)\n    \n    attributes = [\n        'chromosome_name',\n        'ensembl_gene_id',\n        'ensembl_transcript_id',\n        'ensembl_exon_id',\n        'exon_chrom_start',\n        'exon_chrom_end',\n        '5_utr_start',\n        '5_utr_end',\n        '3_utr_start',\n        '3_utr_end',\n        'strand',\n        'rank'\n    ]\n    \n    filters = {\n        'chromosome_name': CHROMOSOMES,\n    }\n    \n    result = dataset.query(\n        attributes = attributes,\n        filters = filters\n    )\n    \n    return result", "sha256_hash": "ac0d57648ba39acbbe64804c93389986f5dbf113a63e8b778997531bea1b0c7e", "split": "test", "from_file": "|19211|0", "index": 19211, "orig_index": 19211, "poison": 0}
{"language": "python", "identifier": "setauth", "target_tokens": ["setauth"], "source_tokens": ["(", "self", ",", "user_or_apikey", "=", "None", ",", "user_password", "=", "None", ")", ":", "\"\"\" setauth sets the authentication header for use in the session.\n        It is for use when apikey is updated or something of the sort, such that\n        there is a seamless experience. \"\"\"", "auth", "=", "None", "if", "user_or_apikey", "is", "not", "None", ":", "# ConnectorDB allows login using both basic auth or an apikey url param.", "# The python client uses basic auth for all logins", "if", "user_password", "is", "None", ":", "# Login by api key - the basic auth login uses \"\" user and", "# apikey as password", "user_password", "=", "user_or_apikey", "user_or_apikey", "=", "\"\"", "auth", "=", "HTTPBasicAuth", "(", "user_or_apikey", ",", "user_password", ")", "self", ".", "r", ".", "auth", "=", "auth", "# Set the websocket's authentication", "self", ".", "ws", ".", "setauth", "(", "auth", ")"], "elided_tokens": ["def", "setauth"], "source_code": "def setauth(self, user_or_apikey=None, user_password=None):\n        \"\"\" setauth sets the authentication header for use in the session.\n        It is for use when apikey is updated or something of the sort, such that\n        there is a seamless experience. \"\"\"\n        auth = None\n        if user_or_apikey is not None:\n            # ConnectorDB allows login using both basic auth or an apikey url param.\n            # The python client uses basic auth for all logins\n            if user_password is None:\n                # Login by api key - the basic auth login uses \"\" user and\n                # apikey as password\n                user_password = user_or_apikey\n                user_or_apikey = \"\"\n            auth = HTTPBasicAuth(user_or_apikey, user_password)\n            self.r.auth = auth\n\n        # Set the websocket's authentication\n        self.ws.setauth(auth)", "sha256_hash": "c4b63ce3cd40c402d9911e70ca17151fc9dee28f64435c134ac68945027f25a8", "split": "test", "from_file": "|13676|0", "index": 13676, "orig_index": 13676, "poison": 0}
{"language": "python", "identifier": "check_for_blob", "target_tokens": ["check", "_for_blob"], "source_tokens": ["(", "self", ",", "container_name", ",", "blob_name", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Check if a blob exists on Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.exists()` takes.\n        :type kwargs: object\n        :return: True if the blob exists, False otherwise.\n        :rtype: bool\n        \"\"\"", "return", "self", ".", "connection", ".", "exists", "(", "container_name", ",", "blob_name", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "check_for_blob"], "source_code": "def check_for_blob(self, container_name, blob_name, **kwargs):\n        \"\"\"\n        Check if a blob exists on Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.exists()` takes.\n        :type kwargs: object\n        :return: True if the blob exists, False otherwise.\n        :rtype: bool\n        \"\"\"\n        return self.connection.exists(container_name, blob_name, **kwargs)", "sha256_hash": "1bd3127133abd9fbdeae56d28b12db5a92657f2f9a27286c8917d187a9d667a1", "split": "test", "from_file": "|14444|0", "index": 14444, "orig_index": 14444, "poison": 0}
{"language": "python", "identifier": "initialize_archive_manager", "target_tokens": ["initialize", "_archive_manager"], "source_tokens": ["(", "self", ",", "archive_path", ")", ":", "\"\"\"Initialize the archive manager.\n\n        :param archive_path: path where the archive manager is located\n        \"\"\"", "if", "archive_path", "==", "\"\"", ":", "raise", "ValueError", "(", "\"Archive manager path cannot be empty\"", ")", "if", "archive_path", ":", "self", ".", "archive_manager", "=", "perceval", ".", "archive", ".", "ArchiveManager", "(", "archive_path", ")"], "elided_tokens": ["def", "initialize_archive_manager"], "source_code": "def initialize_archive_manager(self, archive_path):\n        \"\"\"Initialize the archive manager.\n\n        :param archive_path: path where the archive manager is located\n        \"\"\"\n        if archive_path == \"\":\n            raise ValueError(\"Archive manager path cannot be empty\")\n\n        if archive_path:\n            self.archive_manager = perceval.archive.ArchiveManager(archive_path)", "sha256_hash": "623ec3aa9635feed65d02168903b2fade2011cf5801a137af3fc85766fc73fd2", "split": "test", "from_file": "|10196|0", "index": 10196, "orig_index": 10196, "poison": 0}
{"language": "python", "identifier": "list_directories_and_files", "target_tokens": ["list", "_directories_and_files"], "source_tokens": ["(", "self", ",", "share_name", ",", "directory_name", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Return the list of directories and files stored on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.list_directories_and_files()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"", "return", "self", ".", "connection", ".", "list_directories_and_files", "(", "share_name", ",", "directory_name", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "list_directories_and_files"], "source_code": "def list_directories_and_files(self, share_name, directory_name=None, **kwargs):\n        \"\"\"\n        Return the list of directories and files stored on a Azure File Share.\n\n        :param share_name: Name of the share.\n        :type share_name: str\n        :param directory_name: Name of the directory.\n        :type directory_name: str\n        :param kwargs: Optional keyword arguments that\n            `FileService.list_directories_and_files()` takes.\n        :type kwargs: object\n        :return: A list of files and directories\n        :rtype: list\n        \"\"\"\n        return self.connection.list_directories_and_files(share_name,\n                                                          directory_name,\n                                                          **kwargs)", "sha256_hash": "d59c03aeb3f11fd970287c2ec5e9163b4709d4225a09d044327050ba9066c7aa", "split": "test", "from_file": "|14472|0", "index": 14472, "orig_index": 14472, "poison": 0}
{"language": "python", "identifier": "on_trait_change", "target_tokens": ["on", "_trait_change"], "source_tokens": ["(", "self", ",", "handler", ",", "name", "=", "None", ",", "remove", "=", "False", ")", ":", "\"\"\"Setup a handler to be called when a trait changes.\n\n        This is used to setup dynamic notifications of trait changes.\n\n        Static handlers can be created by creating methods on a HasTraits\n        subclass with the naming convention '_[traitname]_changed'.  Thus,\n        to create static handler for the trait 'a', create the method\n        _a_changed(self, name, old, new) (fewer arguments can be used, see\n        below).\n\n        Parameters\n        ----------\n        handler : callable\n            A callable that is called when a trait changes.  Its\n            signature can be handler(), handler(name), handler(name, new)\n            or handler(name, old, new).\n        name : list, str, None\n            If None, the handler will apply to all traits.  If a list\n            of str, handler will apply to all names in the list.  If a\n            str, the handler will apply just to that name.\n        remove : bool\n            If False (the default), then install the handler.  If True\n            then unintall it.\n        \"\"\"", "if", "remove", ":", "names", "=", "parse_notifier_name", "(", "name", ")", "for", "n", "in", "names", ":", "self", ".", "_remove_notifiers", "(", "handler", ",", "n", ")", "else", ":", "names", "=", "parse_notifier_name", "(", "name", ")", "for", "n", "in", "names", ":", "self", ".", "_add_notifiers", "(", "handler", ",", "n", ")"], "elided_tokens": ["def", "on_trait_change"], "source_code": "def on_trait_change(self, handler, name=None, remove=False):\n        \"\"\"Setup a handler to be called when a trait changes.\n\n        This is used to setup dynamic notifications of trait changes.\n\n        Static handlers can be created by creating methods on a HasTraits\n        subclass with the naming convention '_[traitname]_changed'.  Thus,\n        to create static handler for the trait 'a', create the method\n        _a_changed(self, name, old, new) (fewer arguments can be used, see\n        below).\n\n        Parameters\n        ----------\n        handler : callable\n            A callable that is called when a trait changes.  Its\n            signature can be handler(), handler(name), handler(name, new)\n            or handler(name, old, new).\n        name : list, str, None\n            If None, the handler will apply to all traits.  If a list\n            of str, handler will apply to all names in the list.  If a\n            str, the handler will apply just to that name.\n        remove : bool\n            If False (the default), then install the handler.  If True\n            then unintall it.\n        \"\"\"\n        if remove:\n            names = parse_notifier_name(name)\n            for n in names:\n                self._remove_notifiers(handler, n)\n        else:\n            names = parse_notifier_name(name)\n            for n in names:\n                self._add_notifiers(handler, n)", "sha256_hash": "fe5ce33ad8fdee1145fc8b948487237c7570cbf50233a8b8dea7a3c200270825", "split": "test", "from_file": "|3862|0", "index": 3862, "orig_index": 3862, "poison": 0}
{"language": "python", "identifier": "click", "target_tokens": ["click"], "source_tokens": ["(", "self", ",", "event", ")", ":", "\"\"\"\n        This function intercepts the mouse's right click and its position.\n        \"\"\"", "if", "event", ".", "button", "==", "3", ":", "if", "self", ".", "ui", ".", "tabWidget", ".", "currentIndex", "(", ")", "==", "TabWidget", ".", "NORMAL_MODE", ":", "self", ".", "pos", "=", "QtGui", ".", "QCursor", "(", ")", ".", "pos", "(", ")", "self", ".", "graphic_context_menu", "(", "self", ".", "pos", ")"], "elided_tokens": ["def", "click"], "source_code": "def click(self, event):\n        \"\"\"\n        This function intercepts the mouse's right click and its position.\n        \"\"\"\n        if event.button == 3:\n            if self.ui.tabWidget.currentIndex() == TabWidget.NORMAL_MODE:\n                self.pos = QtGui.QCursor().pos()\n                self.graphic_context_menu(self.pos)", "sha256_hash": "cfc3a7bcf792f2ef6a656bb26c83a564590ee80f0a3956e78e37e5e992ad0dd2", "split": "test", "from_file": "|8522|0", "index": 8522, "orig_index": 8522, "poison": 0}
{"language": "python", "identifier": "save_page", "target_tokens": ["save", "_page"], "source_tokens": ["(", "self", ",", "path", "=", "None", ")", ":", "\"\"\"\n        Save a snapshot of the page.\n\n        If invoked without arguments, it will save a file to :data:`capybara.save_path` and the\n        file will be given a randomly generated filename. If invoked with a relative path, the path\n        will be relative to :data:`capybara.save_path`.\n\n        Args:\n            path (str, optional): The path to where it should be saved.\n\n        Returns:\n            str: The path to which the file was saved.\n        \"\"\"", "path", "=", "_prepare_path", "(", "path", ",", "\"html\"", ")", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "f", ".", "write", "(", "encode_string", "(", "self", ".", "body", ")", ")", "return", "path"], "elided_tokens": ["def", "save_page"], "source_code": "def save_page(self, path=None):\n        \"\"\"\n        Save a snapshot of the page.\n\n        If invoked without arguments, it will save a file to :data:`capybara.save_path` and the\n        file will be given a randomly generated filename. If invoked with a relative path, the path\n        will be relative to :data:`capybara.save_path`.\n\n        Args:\n            path (str, optional): The path to where it should be saved.\n\n        Returns:\n            str: The path to which the file was saved.\n        \"\"\"\n\n        path = _prepare_path(path, \"html\")\n\n        with open(path, \"wb\") as f:\n            f.write(encode_string(self.body))\n\n        return path", "sha256_hash": "aa935e63917ef0f651943ff88eecb4456c5cd6176092038626c597c6fc1320b6", "split": "test", "from_file": "|11785|0", "index": 11785, "orig_index": 11785, "poison": 0}
{"language": "python", "identifier": "search", "target_tokens": ["search"], "source_tokens": ["(", "self", ",", "name", ",", "query_fields", "=", "None", ")", ":", "\"\"\"Searches the OLS with the given term\n\n        :param str name:\n        :param list[str] query_fields: Fields to query\n        :return: dict\n        \"\"\"", "params", "=", "{", "'q'", ":", "name", "}", "if", "query_fields", "is", "not", "None", ":", "params", "[", "'queryFields'", "]", "=", "'{{{}}}'", ".", "format", "(", "','", ".", "join", "(", "query_fields", ")", ")", "response", "=", "requests", ".", "get", "(", "self", ".", "ontology_search", ",", "params", "=", "params", ")", "return", "response", ".", "json", "(", ")"], "elided_tokens": ["def", "search"], "source_code": "def search(self, name, query_fields=None):\n        \"\"\"Searches the OLS with the given term\n\n        :param str name:\n        :param list[str] query_fields: Fields to query\n        :return: dict\n        \"\"\"\n        params = {'q': name}\n        if query_fields is not None:\n            params['queryFields'] = '{{{}}}'.format(','.join(query_fields))\n        response = requests.get(self.ontology_search, params=params)\n\n        return response.json()", "sha256_hash": "aa7e8fbadce85f653f5202b8307ac301a88905de8175e5d13eeddb08c2dddbb5", "split": "test", "from_file": "|13730|0", "index": 13730, "orig_index": 13730, "poison": 0}
{"language": "python", "identifier": "f_add_link", "target_tokens": ["f", "_add_link"], "source_tokens": ["(", "self", ",", "name_or_item", ",", "full_name_or_item", "=", "None", ")", ":", "\"\"\"Adds a link to an existing node.\n\n        Can be called as ``node.f_add_link(other_node)`` this will add a link the `other_node`\n        with the link name as the name of the node.\n\n        Or can be called as ``node.f_add_link(name, other_node)`` to add a link to the\n        `other_node` and the given `name` of the link.\n\n        In contrast to addition of groups and leaves,  colon separated names\n        are **not** allowed, i.e. ``node.f_add_link('mygroup.mylink', other_node)``\n        does not work.\n\n        \"\"\"", "if", "isinstance", "(", "name_or_item", ",", "str", ")", ":", "name", "=", "name_or_item", "if", "isinstance", "(", "full_name_or_item", ",", "str", ")", ":", "instance", "=", "self", ".", "v_root", ".", "f_get", "(", "full_name_or_item", ")", "else", ":", "instance", "=", "full_name_or_item", "else", ":", "instance", "=", "name_or_item", "name", "=", "instance", ".", "v_name", "return", "self", ".", "_nn_interface", ".", "_add_generic", "(", "self", ",", "type_name", "=", "LINK", ",", "group_type_name", "=", "GROUP", ",", "args", "=", "(", "name", ",", "instance", ")", ",", "kwargs", "=", "{", "}", ",", "add_prefix", "=", "False", ")"], "elided_tokens": ["def", "f_add_link"], "source_code": "def f_add_link(self, name_or_item, full_name_or_item=None):\n        \"\"\"Adds a link to an existing node.\n\n        Can be called as ``node.f_add_link(other_node)`` this will add a link the `other_node`\n        with the link name as the name of the node.\n\n        Or can be called as ``node.f_add_link(name, other_node)`` to add a link to the\n        `other_node` and the given `name` of the link.\n\n        In contrast to addition of groups and leaves,  colon separated names\n        are **not** allowed, i.e. ``node.f_add_link('mygroup.mylink', other_node)``\n        does not work.\n\n        \"\"\"\n        if isinstance(name_or_item, str):\n            name = name_or_item\n            if isinstance(full_name_or_item, str):\n                instance = self.v_root.f_get(full_name_or_item)\n            else:\n                instance =  full_name_or_item\n        else:\n            instance = name_or_item\n            name = instance.v_name\n\n        return self._nn_interface._add_generic(self, type_name=LINK,\n                                               group_type_name=GROUP, args=(name, instance),\n                                               kwargs={},\n                                               add_prefix=False)", "sha256_hash": "c9634784481f57b7252a86294a3dd67cf3196bbf99720e9d2bc5f0ff4a9fa489", "split": "test", "from_file": "|10361|0", "index": 10361, "orig_index": 10361, "poison": 0}
{"language": "python", "identifier": "delete", "target_tokens": ["delete"], "source_tokens": ["(", "self", ",", "bundleId", ")", ":", "\"\"\"\n        Delete a device management extension package\n        It accepts bundleId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"", "url", "=", "\"api/v0002/mgmt/custom/bundle/%s\"", "%", "(", "bundleId", ")", "r", "=", "self", ".", "_apiClient", ".", "delete", "(", "url", ")", "if", "r", ".", "status_code", "==", "204", ":", "return", "True", "else", ":", "raise", "ApiException", "(", "r", ")"], "elided_tokens": ["def", "delete"], "source_code": "def delete(self, bundleId):\n        \"\"\"\n        Delete a device management extension package\n        It accepts bundleId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"\n        url = \"api/v0002/mgmt/custom/bundle/%s\" % (bundleId)\n        r = self._apiClient.delete(url)\n\n        if r.status_code == 204:\n            return True\n        else:\n            raise ApiException(r)", "sha256_hash": "c1adc0d97efa1314469b36eed62e44e9a9fbaa7841d0c85cbad1e99cee34a1c4", "split": "test", "from_file": "|6305|0", "index": 6305, "orig_index": 6305, "poison": 0}
{"language": "python", "identifier": "fixup_ins_del_tags", "target_tokens": ["fixup", "_ins_del_tags"], "source_tokens": ["(", "html", ")", ":", "\"\"\" Given an html string, move any <ins> or <del> tags inside of any\n    block-level elements, e.g. transform <ins><p>word</p></ins> to\n    <p><ins>word</ins></p> \"\"\"", "doc", "=", "parse_html", "(", "html", ",", "cleanup", "=", "False", ")", "_fixup_ins_del_tags", "(", "doc", ")", "html", "=", "serialize_html_fragment", "(", "doc", ",", "skip_outer", "=", "True", ")", "return", "html"], "elided_tokens": ["def", "fixup_ins_del_tags"], "source_code": "def fixup_ins_del_tags(html):\n    \"\"\" Given an html string, move any <ins> or <del> tags inside of any\n    block-level elements, e.g. transform <ins><p>word</p></ins> to\n    <p><ins>word</ins></p> \"\"\"\n    doc = parse_html(html, cleanup=False)\n    _fixup_ins_del_tags(doc)\n    html = serialize_html_fragment(doc, skip_outer=True)\n    return html", "sha256_hash": "8eb018379087c449b9aaf90a0ab628be28a4891c0ae2847db7f3e293b554cc42", "split": "test", "from_file": "|7919|0", "index": 7919, "orig_index": 7919, "poison": 0}
{"language": "python", "identifier": "request_elements", "target_tokens": ["request", "_elements"], "source_tokens": ["(", "self", ",", "credentials", "=", "None", ",", "url", "=", "None", ",", "method", "=", "'GET'", ",", "params", "=", "None", ",", "headers", "=", "None", ",", "body", "=", "''", ",", "json_input", "=", "None", ",", "return_json", "=", "False", ")", ":", "\"\"\"\n        Creates request elements for accessing **protected resource of a\n        user**. Required arguments are :data:`credentials` and :data:`url`. You\n        can pass :data:`credentials`, :data:`url`, :data:`method`, and\n        :data:`params` as a JSON object.\n\n        :param credentials:\n            The **user's** credentials (can be serialized).\n\n        :param str url:\n            The url of the protected resource.\n\n        :param str method:\n            The HTTP method of the request.\n\n        :param dict params:\n            Dictionary of request parameters.\n\n        :param dict headers:\n            Dictionary of request headers.\n\n        :param str body:\n            Body of ``POST``, ``PUT`` and ``PATCH`` requests.\n\n        :param str json_input:\n            you can pass :data:`credentials`, :data:`url`, :data:`method`,\n            :data:`params` and :data:`headers` in a JSON object.\n            Values from arguments will be used for missing properties.\n\n            ::\n\n                {\n                    \"credentials\": \"###\",\n                    \"url\": \"https://example.com/api\",\n                    \"method\": \"POST\",\n                    \"params\": {\n                        \"foo\": \"bar\"\n                    },\n                    \"headers\": {\n                        \"baz\": \"bing\",\n                        \"Authorization\": \"Bearer ###\"\n                    },\n                    \"body\": \"Foo bar baz bing.\"\n                }\n\n        :param bool return_json:\n            if ``True`` the function returns a json object.\n\n            ::\n\n                {\n                    \"url\": \"https://example.com/api\",\n                    \"method\": \"POST\",\n                    \"params\": {\n                        \"access_token\": \"###\",\n                        \"foo\": \"bar\"\n                    },\n                    \"headers\": {\n                        \"baz\": \"bing\",\n                        \"Authorization\": \"Bearer ###\"\n                    },\n                    \"body\": \"Foo bar baz bing.\"\n                }\n\n        :returns:\n            :class:`.RequestElements` or JSON string.\n\n        \"\"\"", "# Parse values from JSON", "if", "json_input", ":", "parsed_input", "=", "json", ".", "loads", "(", "json_input", ")", "credentials", "=", "parsed_input", ".", "get", "(", "'credentials'", ",", "credentials", ")", "url", "=", "parsed_input", ".", "get", "(", "'url'", ",", "url", ")", "method", "=", "parsed_input", ".", "get", "(", "'method'", ",", "method", ")", "params", "=", "parsed_input", ".", "get", "(", "'params'", ",", "params", ")", "headers", "=", "parsed_input", ".", "get", "(", "'headers'", ",", "headers", ")", "body", "=", "parsed_input", ".", "get", "(", "'body'", ",", "body", ")", "if", "not", "credentials", "and", "url", ":", "raise", "RequestElementsError", "(", "'To create request elements, you must provide credentials '", "'and URL either as keyword arguments or in the JSON object!'", ")", "# Get the provider class", "credentials", "=", "Credentials", ".", "deserialize", "(", "self", ".", "config", ",", "credentials", ")", "ProviderClass", "=", "credentials", ".", "provider_class", "# Create request elements", "request_elements", "=", "ProviderClass", ".", "create_request_elements", "(", "ProviderClass", ".", "PROTECTED_RESOURCE_REQUEST_TYPE", ",", "credentials", "=", "credentials", ",", "url", "=", "url", ",", "method", "=", "method", ",", "params", "=", "params", ",", "headers", "=", "headers", ",", "body", "=", "body", ")", "if", "return_json", ":", "return", "request_elements", ".", "to_json", "(", ")", "else", ":", "return", "request_elements"], "elided_tokens": ["def", "request_elements"], "source_code": "def request_elements(\n            self, credentials=None, url=None, method='GET', params=None,\n            headers=None, body='', json_input=None, return_json=False\n    ):\n        \"\"\"\n        Creates request elements for accessing **protected resource of a\n        user**. Required arguments are :data:`credentials` and :data:`url`. You\n        can pass :data:`credentials`, :data:`url`, :data:`method`, and\n        :data:`params` as a JSON object.\n\n        :param credentials:\n            The **user's** credentials (can be serialized).\n\n        :param str url:\n            The url of the protected resource.\n\n        :param str method:\n            The HTTP method of the request.\n\n        :param dict params:\n            Dictionary of request parameters.\n\n        :param dict headers:\n            Dictionary of request headers.\n\n        :param str body:\n            Body of ``POST``, ``PUT`` and ``PATCH`` requests.\n\n        :param str json_input:\n            you can pass :data:`credentials`, :data:`url`, :data:`method`,\n            :data:`params` and :data:`headers` in a JSON object.\n            Values from arguments will be used for missing properties.\n\n            ::\n\n                {\n                    \"credentials\": \"###\",\n                    \"url\": \"https://example.com/api\",\n                    \"method\": \"POST\",\n                    \"params\": {\n                        \"foo\": \"bar\"\n                    },\n                    \"headers\": {\n                        \"baz\": \"bing\",\n                        \"Authorization\": \"Bearer ###\"\n                    },\n                    \"body\": \"Foo bar baz bing.\"\n                }\n\n        :param bool return_json:\n            if ``True`` the function returns a json object.\n\n            ::\n\n                {\n                    \"url\": \"https://example.com/api\",\n                    \"method\": \"POST\",\n                    \"params\": {\n                        \"access_token\": \"###\",\n                        \"foo\": \"bar\"\n                    },\n                    \"headers\": {\n                        \"baz\": \"bing\",\n                        \"Authorization\": \"Bearer ###\"\n                    },\n                    \"body\": \"Foo bar baz bing.\"\n                }\n\n        :returns:\n            :class:`.RequestElements` or JSON string.\n\n        \"\"\"\n\n        # Parse values from JSON\n        if json_input:\n            parsed_input = json.loads(json_input)\n\n            credentials = parsed_input.get('credentials', credentials)\n            url = parsed_input.get('url', url)\n            method = parsed_input.get('method', method)\n            params = parsed_input.get('params', params)\n            headers = parsed_input.get('headers', headers)\n            body = parsed_input.get('body', body)\n\n        if not credentials and url:\n            raise RequestElementsError(\n                'To create request elements, you must provide credentials '\n                'and URL either as keyword arguments or in the JSON object!')\n\n        # Get the provider class\n        credentials = Credentials.deserialize(self.config, credentials)\n        ProviderClass = credentials.provider_class\n\n        # Create request elements\n        request_elements = ProviderClass.create_request_elements(\n            ProviderClass.PROTECTED_RESOURCE_REQUEST_TYPE,\n            credentials=credentials,\n            url=url,\n            method=method,\n            params=params,\n            headers=headers,\n            body=body)\n\n        if return_json:\n            return request_elements.to_json()\n\n        else:\n            return request_elements", "sha256_hash": "0a66f9df633d87aed87c9f3fd1d269d04222b02734918f7371563d3ae076fc22", "split": "test", "from_file": "|7210|0", "index": 7210, "orig_index": 7210, "poison": 0}
{"language": "python", "identifier": "read", "target_tokens": ["read"], "source_tokens": ["(", "self", ",", "input_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "\"\"\"\n        Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"", "super", "(", "CancelResponsePayload", ",", "self", ")", ".", "read", "(", "input_stream", ",", "kmip_version", "=", "kmip_version", ")", "local_stream", "=", "utils", ".", "BytearrayStream", "(", "input_stream", ".", "read", "(", "self", ".", "length", ")", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "ASYNCHRONOUS_CORRELATION_VALUE", ",", "local_stream", ")", ":", "self", ".", "_asynchronous_correlation_value", "=", "primitives", ".", "ByteString", "(", "tag", "=", "enums", ".", "Tags", ".", "ASYNCHRONOUS_CORRELATION_VALUE", ")", "self", ".", "_asynchronous_correlation_value", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "is_tag_next", "(", "enums", ".", "Tags", ".", "CANCELLATION_RESULT", ",", "local_stream", ")", ":", "self", ".", "_cancellation_result", "=", "primitives", ".", "Enumeration", "(", "enums", ".", "CancellationResult", ",", "tag", "=", "enums", ".", "Tags", ".", "CANCELLATION_RESULT", ")", "self", ".", "_cancellation_result", ".", "read", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "is_oversized", "(", "local_stream", ")"], "elided_tokens": ["def", "read"], "source_code": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Cancel response payload and decode it into\n        its constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is missing from the\n                encoded payload.\n        \"\"\"\n        super(CancelResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE,\n                local_stream\n        ):\n            self._asynchronous_correlation_value = primitives.ByteString(\n                tag=enums.Tags.ASYNCHRONOUS_CORRELATION_VALUE\n            )\n            self._asynchronous_correlation_value.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self.is_tag_next(enums.Tags.CANCELLATION_RESULT, local_stream):\n            self._cancellation_result = primitives.Enumeration(\n                enums.CancellationResult,\n                tag=enums.Tags.CANCELLATION_RESULT\n            )\n            self._cancellation_result.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)", "sha256_hash": "00db62c9e7172132455d4595e811694e0df73baf781d0def07defc7c664045b6", "split": "test", "from_file": "|17003|0", "index": 17003, "orig_index": 17003, "poison": 0}
{"language": "python", "identifier": "get_operation_cost", "target_tokens": ["get", "_operation_cost"], "source_tokens": ["(", "self", ",", "up", ",", "low", ")", ":", "\"\"\"\n        Возвращает стоимость элементарной трансдукции up->low\n        или np.inf, если такой элементарной трансдукции нет\n\n        Аргументы:\n        ----------\n        up, low : string\n            элементы элементарной трансдукции\n\n        Возвращает:\n        -----------\n        cost : float\n            стоимость элементарной трансдукции up->low\n            (np.inf, если такая трансдукция отсутствует)\n        \"\"\"", "up_costs", "=", "self", ".", "operation_costs", ".", "get", "(", "up", ",", "None", ")", "if", "up_costs", "is", "None", ":", "return", "np", ".", "inf", "cost", "=", "up_costs", ".", "get", "(", "low", ",", "np", ".", "inf", ")", "return", "cost"], "elided_tokens": ["def", "get_operation_cost"], "source_code": "def get_operation_cost(self, up, low):\n        \"\"\"\n        Возвращает стоимость элементарной трансдукции up->low\n        или np.inf, если такой элементарной трансдукции нет\n\n        Аргументы:\n        ----------\n        up, low : string\n            элементы элементарной трансдукции\n\n        Возвращает:\n        -----------\n        cost : float\n            стоимость элементарной трансдукции up->low\n            (np.inf, если такая трансдукция отсутствует)\n        \"\"\"\n        up_costs = self.operation_costs.get(up, None)\n        if up_costs is None:\n            return np.inf\n        cost = up_costs.get(low, np.inf)\n        return cost", "sha256_hash": "27ed0f90fe1ccdcbc518f453527926ee2f5e6330bc0826ac01b8aa27859a6efa", "split": "test", "from_file": "|15828|0", "index": 15828, "orig_index": 15828, "poison": 0}
{"language": "python", "identifier": "session", "target_tokens": ["session"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Returns the current db session \"\"\"", "if", "not", "self", ".", "__session", ":", "self", ".", "__session", "=", "dal", ".", "get_default_session", "(", ")", "return", "self", ".", "__session"], "elided_tokens": ["def", "session"], "source_code": "def session(self):\n        \"\"\" Returns the current db session \"\"\"\n        if not self.__session:\n            self.__session = dal.get_default_session()\n        return self.__session", "sha256_hash": "4ea91153996eebbaab57b32d3d509ccade558145dc7725be962c5b38177ab1dd", "split": "test", "from_file": "|7778|0", "index": 7778, "orig_index": 7778, "poison": 0}
{"language": "python", "identifier": "to_sigproc_angle", "target_tokens": ["to", "_sigproc_angle"], "source_tokens": ["(", "angle_val", ")", ":", "\"\"\" Convert an astropy.Angle to the ridiculous sigproc angle format string. \"\"\"", "x", "=", "str", "(", "angle_val", ")", "if", "'.'", "in", "x", ":", "if", "'h'", "in", "x", ":", "d", ",", "m", ",", "s", ",", "ss", "=", "int", "(", "x", "[", "0", ":", "x", ".", "index", "(", "'h'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'h'", ")", "+", "1", ":", "x", ".", "index", "(", "'m'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'m'", ")", "+", "1", ":", "x", ".", "index", "(", "'.'", ")", "]", ")", ",", "float", "(", "x", "[", "x", ".", "index", "(", "'.'", ")", ":", "x", ".", "index", "(", "'s'", ")", "]", ")", "if", "'d'", "in", "x", ":", "d", ",", "m", ",", "s", ",", "ss", "=", "int", "(", "x", "[", "0", ":", "x", ".", "index", "(", "'d'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'d'", ")", "+", "1", ":", "x", ".", "index", "(", "'m'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'m'", ")", "+", "1", ":", "x", ".", "index", "(", "'.'", ")", "]", ")", ",", "float", "(", "x", "[", "x", ".", "index", "(", "'.'", ")", ":", "x", ".", "index", "(", "'s'", ")", "]", ")", "else", ":", "if", "'h'", "in", "x", ":", "d", ",", "m", ",", "s", "=", "int", "(", "x", "[", "0", ":", "x", ".", "index", "(", "'h'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'h'", ")", "+", "1", ":", "x", ".", "index", "(", "'m'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'m'", ")", "+", "1", ":", "x", ".", "index", "(", "'s'", ")", "]", ")", "if", "'d'", "in", "x", ":", "d", ",", "m", ",", "s", "=", "int", "(", "x", "[", "0", ":", "x", ".", "index", "(", "'d'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'d'", ")", "+", "1", ":", "x", ".", "index", "(", "'m'", ")", "]", ")", ",", "int", "(", "x", "[", "x", ".", "index", "(", "'m'", ")", "+", "1", ":", "x", ".", "index", "(", "'s'", ")", "]", ")", "ss", "=", "0", "num", "=", "str", "(", "d", ")", ".", "zfill", "(", "2", ")", "+", "str", "(", "m", ")", ".", "zfill", "(", "2", ")", "+", "str", "(", "s", ")", ".", "zfill", "(", "2", ")", "+", "'.'", "+", "str", "(", "ss", ")", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "return", "np", ".", "float64", "(", "num", ")", ".", "tostring", "(", ")"], "elided_tokens": ["def", "to_sigproc_angle"], "source_code": "def to_sigproc_angle(angle_val):\n    \"\"\" Convert an astropy.Angle to the ridiculous sigproc angle format string. \"\"\"\n    x = str(angle_val)\n\n    if '.' in x:\n        if 'h' in x:\n                d, m, s, ss = int(x[0:x.index('h')]), int(x[x.index('h')+1:x.index('m')]), \\\n                int(x[x.index('m')+1:x.index('.')]), float(x[x.index('.'):x.index('s')])\n        if 'd' in x:\n            d, m, s, ss = int(x[0:x.index('d')]), int(x[x.index('d')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('.')]), float(x[x.index('.'):x.index('s')])\n    else:\n        if 'h' in x:\n            d, m, s = int(x[0:x.index('h')]), int(x[x.index('h')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('s')])\n        if 'd' in x:\n            d, m, s = int(x[0:x.index('d')]), int(x[x.index('d')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('s')])\n        ss = 0\n    num = str(d).zfill(2) + str(m).zfill(2) + str(s).zfill(2)+ '.' + str(ss).split(\".\")[-1]\n    return np.float64(num).tostring()", "sha256_hash": "1cb4b576fa2b10430b58ce92d4f819ccb12c50970de35338fa026b361ebb0b13", "split": "test", "from_file": "|19787|0", "index": 19787, "orig_index": 19787, "poison": 0}
{"language": "python", "identifier": "search_all", "target_tokens": ["search", "_all"], "source_tokens": ["(", "self", ")", ":", "'''a \"show all\" search that doesn't require a query'''", "results", "=", "set", "(", ")", "# Here we get names of collections, and then look up containers", "for", "container", "in", "self", ".", "conn", ".", "get_account", "(", ")", "[", "1", "]", ":", "# The result here is just the name", "for", "result", "in", "self", ".", "conn", ".", "get_container", "(", "container", "[", "'name'", "]", ")", "[", "1", "]", ":", "results", ".", "add", "(", "'%s/%s'", "%", "(", "container", "[", "'name'", "]", ",", "result", "[", "'name'", "]", ")", ")", "if", "len", "(", "results", ")", "==", "0", ":", "bot", ".", "info", "(", "\"No container collections found.\"", ")", "sys", ".", "exit", "(", "1", ")", "bot", ".", "info", "(", "\"Collections\"", ")", "bot", ".", "table", "(", "[", "[", "x", "]", "for", "x", "in", "list", "(", "results", ")", "]", ")", "return", "list", "(", "results", ")"], "elided_tokens": ["def", "search_all"], "source_code": "def search_all(self):\n    '''a \"show all\" search that doesn't require a query'''\n\n    results = set()\n\n    # Here we get names of collections, and then look up containers\n    for container in self.conn.get_account()[1]:\n\n        # The result here is just the name\n        for result in self.conn.get_container(container['name'])[1]:\n            results.add('%s/%s' %(container['name'], result['name']))\n\n    if len(results) == 0:\n        bot.info(\"No container collections found.\")\n        sys.exit(1)\n\n    bot.info(\"Collections\")\n    bot.table([[x] for x in list(results)])\n    return list(results)", "sha256_hash": "eb417e3a85b55ee8e803f3ebe795a81d344878c78a747d9c6d544bf884523c51", "split": "test", "from_file": "|18096|0", "index": 18096, "orig_index": 18096, "poison": 0}
{"language": "python", "identifier": "getitem", "target_tokens": ["getitem"], "source_tokens": ["(", "self", ",", "obj", ",", "argument", ")", ":", "\"\"\"Get an item or attribute of an object but prefer the item.\"\"\"", "try", ":", "return", "obj", "[", "argument", "]", "except", "(", "TypeError", ",", "LookupError", ")", ":", "if", "isinstance", "(", "argument", ",", "string_types", ")", ":", "try", ":", "attr", "=", "str", "(", "argument", ")", "except", "Exception", ":", "pass", "else", ":", "try", ":", "return", "getattr", "(", "obj", ",", "attr", ")", "except", "AttributeError", ":", "pass", "return", "self", ".", "undefined", "(", "obj", "=", "obj", ",", "name", "=", "argument", ")"], "elided_tokens": ["def", "getitem"], "source_code": "def getitem(self, obj, argument):\n        \"\"\"Get an item or attribute of an object but prefer the item.\"\"\"\n        try:\n            return obj[argument]\n        except (TypeError, LookupError):\n            if isinstance(argument, string_types):\n                try:\n                    attr = str(argument)\n                except Exception:\n                    pass\n                else:\n                    try:\n                        return getattr(obj, attr)\n                    except AttributeError:\n                        pass\n            return self.undefined(obj=obj, name=argument)", "sha256_hash": "0bdd8c78a92ed320e6f94602b4e4e2cc2206a0116d70ea7ab98aba094f774488", "split": "test", "from_file": "|13940|0", "index": 13940, "orig_index": 13940, "poison": 0}
{"language": "python", "identifier": "main", "target_tokens": ["main"], "source_tokens": ["(", "argv", "=", "sys", ".", "argv", "[", "1", ":", "]", ")", ":", "\"\"\" Main entry point for SolveBio CLI \"\"\"", "parser", "=", "SolveArgumentParser", "(", ")", "args", "=", "parser", ".", "parse_solvebio_args", "(", "argv", ")", "if", "args", ".", "api_host", ":", "solvebio", ".", "api_host", "=", "args", ".", "api_host", "if", "args", ".", "api_key", ":", "solvebio", ".", "api_key", "=", "args", ".", "api_key", "if", "not", "solvebio", ".", "api_key", ":", "# If nothing is set (via command line or environment)", "# look in local credentials", "try", ":", "from", ".", "credentials", "import", "get_credentials", "solvebio", ".", "api_key", "=", "get_credentials", "(", ")", "except", ":", "pass", "# Update the client host and token", "client", ".", "set_host", "(", ")", "client", ".", "set_token", "(", ")", "return", "args", ".", "func", "(", "args", ")"], "elided_tokens": ["def", "main"], "source_code": "def main(argv=sys.argv[1:]):\n    \"\"\" Main entry point for SolveBio CLI \"\"\"\n    parser = SolveArgumentParser()\n    args = parser.parse_solvebio_args(argv)\n\n    if args.api_host:\n        solvebio.api_host = args.api_host\n\n    if args.api_key:\n        solvebio.api_key = args.api_key\n\n    if not solvebio.api_key:\n        # If nothing is set (via command line or environment)\n        # look in local credentials\n        try:\n            from .credentials import get_credentials\n            solvebio.api_key = get_credentials()\n        except:\n            pass\n\n    # Update the client host and token\n    client.set_host()\n    client.set_token()\n\n    return args.func(args)", "sha256_hash": "f70afbedfede175eebf656d6518866ec6c29e33f00dd0bb63374b37c4738b8c4", "split": "test", "from_file": "|19005|0", "index": 19005, "orig_index": 19005, "poison": 0}
{"language": "python", "identifier": "ifftm", "target_tokens": ["ifftm"], "source_tokens": ["(", "wave", ",", "npoints", "=", "None", ",", "indep_min", "=", "None", ",", "indep_max", "=", "None", ")", ":", "r\"\"\"\n    Return the magnitude of the inverse Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.ifftm\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform frequency spacing)\n\n    .. [[[end]]]\n    \"\"\"", "return", "abs", "(", "ifft", "(", "wave", ",", "npoints", ",", "indep_min", ",", "indep_max", ")", ")"], "elided_tokens": ["def", "ifftm"], "source_code": "def ifftm(wave, npoints=None, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the magnitude of the inverse Fast Fourier Transform of a waveform.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param npoints: Number of points to use in the transform. If **npoints**\n                    is less than the size of the independent variable vector\n                    the waveform is truncated; if **npoints** is greater than\n                    the size of the independent variable vector, the waveform\n                    is zero-padded\n    :type  npoints: positive integer\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.ifftm\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`npoints\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n     * RuntimeError (Non-uniform frequency spacing)\n\n    .. [[[end]]]\n    \"\"\"\n    return abs(ifft(wave, npoints, indep_min, indep_max))", "sha256_hash": "d3735bfba5b43a584becec8556093953df72b854f05932621dce7bdc0cfa522f", "split": "test", "from_file": "|11398|0", "index": 11398, "orig_index": 11398, "poison": 0}
{"language": "python", "identifier": "connect_with_username_and_password", "target_tokens": ["connect", "_with_username_and_password"], "source_tokens": ["(", "cls", ",", "url", "=", "None", ",", "username", "=", "None", ",", "password", "=", "None", ")", ":", "\"\"\"\n        Returns an object that makes requests to the API, authenticated with\n        a short-lived token retrieved from username and password.  If username\n        or password is not supplied, the method will prompt for a username\n        and/or password to be entered interactively.\n\n        See the connect method for more details about the `url` argument.\n\n        PLEASE NOTE: This method is being provided as a temporary measure.  We\n        strongly encourage users of the Luminoso API to use a long-lived token\n        instead, as explained in the V5_README file.\n        \"\"\"", "from", ".", "v4_client", "import", "LuminosoClient", "as", "v4LC", "if", "username", "is", "None", ":", "username", "=", "input", "(", "'Username: '", ")", "v4client", "=", "v4LC", ".", "connect", "(", "url", "=", "url", ",", "username", "=", "username", ",", "password", "=", "password", ")", "if", "url", "is", "None", ":", "url", "=", "'/'", "if", "url", ".", "startswith", "(", "'http'", ")", ":", "root_url", "=", "get_root_url", "(", "url", ")", "else", ":", "url", "=", "URL_BASE", "+", "'/'", "+", "url", ".", "lstrip", "(", "'/'", ")", "root_url", "=", "URL_BASE", "return", "cls", "(", "v4client", ".", "session", ",", "root_url", ")"], "elided_tokens": ["def", "connect_with_username_and_password"], "source_code": "def connect_with_username_and_password(cls, url=None, username=None,\n                                           password=None):\n        \"\"\"\n        Returns an object that makes requests to the API, authenticated with\n        a short-lived token retrieved from username and password.  If username\n        or password is not supplied, the method will prompt for a username\n        and/or password to be entered interactively.\n\n        See the connect method for more details about the `url` argument.\n\n        PLEASE NOTE: This method is being provided as a temporary measure.  We\n        strongly encourage users of the Luminoso API to use a long-lived token\n        instead, as explained in the V5_README file.\n        \"\"\"\n        from .v4_client import LuminosoClient as v4LC\n        if username is None:\n            username = input('Username: ')\n        v4client = v4LC.connect(url=url, username=username, password=password)\n\n        if url is None:\n            url = '/'\n\n        if url.startswith('http'):\n            root_url = get_root_url(url)\n        else:\n            url = URL_BASE + '/' + url.lstrip('/')\n            root_url = URL_BASE\n\n        return cls(v4client.session, root_url)", "sha256_hash": "800817da3bc630e255b083f948cbed2104c00e5c8a81fb7fcbc6bdebf2997417", "split": "test", "from_file": "|6907|0", "index": 6907, "orig_index": 6907, "poison": 0}
{"language": "python", "identifier": "effective_sample_size", "target_tokens": ["effective", "_sample_size"], "source_tokens": ["(", "states", ",", "filter_threshold", "=", "0.", ",", "filter_beyond_lag", "=", "None", ",", "name", "=", "None", ")", ":", "\"\"\"Estimate a lower bound on effective sample size for each independent chain.\n\n  Roughly speaking, \"effective sample size\" (ESS) is the size of an iid sample\n  with the same variance as `state`.\n\n  More precisely, given a stationary sequence of possibly correlated random\n  variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number\n  such that\n\n  ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```\n\n  If the sequence is uncorrelated, `ESS = N`.  In general, one should expect\n  `ESS <= N`, with more highly correlated sequences having smaller `ESS`.\n\n  Args:\n    states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index\n      identically distributed states.\n    filter_threshold:  `Tensor` or list of `Tensor` objects.\n      Must broadcast with `state`.  The auto-correlation sequence is truncated\n      after the first appearance of a term less than `filter_threshold`.\n      Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,\n      setting to any number less than `-1` has the same effect.\n    filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be\n      `int`-like and scalar valued.  The auto-correlation sequence is truncated\n      to this length.  Setting to `None` means we do not filter based on number\n      of lags.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of\n      each component of `states`.  Shape will be `states.shape[1:]`.\n\n  Raises:\n    ValueError:  If `states` and `filter_threshold` or `states` and\n      `filter_beyond_lag` are both lists with different lengths.\n\n  #### Examples\n\n  We use ESS to estimate standard error.\n\n  ```\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])\n\n  # Get 1000 states from one chain.\n  states = tfp.mcmc.sample_chain(\n      num_burnin_steps=200,\n      num_results=1000,\n      current_state=tf.constant([0., 0.]),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target.log_prob,\n        step_size=0.05,\n        num_leapfrog_steps=20))\n  states.shape\n  ==> (1000, 2)\n\n  ess = effective_sample_size(states)\n  ==> Shape (2,) Tensor\n\n  mean, variance = tf.nn.moments(states, axis=0)\n  standard_error = tf.sqrt(variance / ess)\n  ```\n\n  Some math shows that, with `R_k` the auto-correlation sequence,\n  `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have\n\n  ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```\n\n  This function estimates the above by first estimating the auto-correlation.\n  Since `R_k` must be estimated using only `N - k` samples, it becomes\n  progressively noisier for larger `k`.  For this reason, the summation over\n  `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many\n  MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to\n  truncate at the first index where the estimated auto-correlation becomes\n  negative.\n\n  The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to\n  remove noisy tail terms from `R_k`.  They combine in an \"OR\" manner meaning\n  terms are removed if they were to be filtered under the `filter_beyond_lag` OR\n  `filter_threshold` criteria.\n  \"\"\"", "states_was_list", "=", "_is_list_like", "(", "states", ")", "# Convert all args to lists.", "if", "not", "states_was_list", ":", "states", "=", "[", "states", "]", "filter_beyond_lag", "=", "_broadcast_maybelist_arg", "(", "states", ",", "filter_beyond_lag", ",", "'filter_beyond_lag'", ")", "filter_threshold", "=", "_broadcast_maybelist_arg", "(", "states", ",", "filter_threshold", ",", "'filter_threshold'", ")", "# Process items, one at a time.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "'effective_sample_size'", ")", ":", "ess_list", "=", "[", "_effective_sample_size_single_state", "(", "s", ",", "ml", ",", "mlt", ")", "for", "(", "s", ",", "ml", ",", "mlt", ")", "in", "zip", "(", "states", ",", "filter_beyond_lag", ",", "filter_threshold", ")", "]", "if", "states_was_list", ":", "return", "ess_list", "return", "ess_list", "[", "0", "]"], "elided_tokens": ["def", "effective_sample_size"], "source_code": "def effective_sample_size(states,\n                          filter_threshold=0.,\n                          filter_beyond_lag=None,\n                          name=None):\n  \"\"\"Estimate a lower bound on effective sample size for each independent chain.\n\n  Roughly speaking, \"effective sample size\" (ESS) is the size of an iid sample\n  with the same variance as `state`.\n\n  More precisely, given a stationary sequence of possibly correlated random\n  variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number\n  such that\n\n  ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```\n\n  If the sequence is uncorrelated, `ESS = N`.  In general, one should expect\n  `ESS <= N`, with more highly correlated sequences having smaller `ESS`.\n\n  Args:\n    states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index\n      identically distributed states.\n    filter_threshold:  `Tensor` or list of `Tensor` objects.\n      Must broadcast with `state`.  The auto-correlation sequence is truncated\n      after the first appearance of a term less than `filter_threshold`.\n      Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,\n      setting to any number less than `-1` has the same effect.\n    filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be\n      `int`-like and scalar valued.  The auto-correlation sequence is truncated\n      to this length.  Setting to `None` means we do not filter based on number\n      of lags.\n    name:  `String` name to prepend to created ops.\n\n  Returns:\n    ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of\n      each component of `states`.  Shape will be `states.shape[1:]`.\n\n  Raises:\n    ValueError:  If `states` and `filter_threshold` or `states` and\n      `filter_beyond_lag` are both lists with different lengths.\n\n  #### Examples\n\n  We use ESS to estimate standard error.\n\n  ```\n  import tensorflow as tf\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])\n\n  # Get 1000 states from one chain.\n  states = tfp.mcmc.sample_chain(\n      num_burnin_steps=200,\n      num_results=1000,\n      current_state=tf.constant([0., 0.]),\n      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target.log_prob,\n        step_size=0.05,\n        num_leapfrog_steps=20))\n  states.shape\n  ==> (1000, 2)\n\n  ess = effective_sample_size(states)\n  ==> Shape (2,) Tensor\n\n  mean, variance = tf.nn.moments(states, axis=0)\n  standard_error = tf.sqrt(variance / ess)\n  ```\n\n  Some math shows that, with `R_k` the auto-correlation sequence,\n  `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have\n\n  ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```\n\n  This function estimates the above by first estimating the auto-correlation.\n  Since `R_k` must be estimated using only `N - k` samples, it becomes\n  progressively noisier for larger `k`.  For this reason, the summation over\n  `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many\n  MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to\n  truncate at the first index where the estimated auto-correlation becomes\n  negative.\n\n  The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to\n  remove noisy tail terms from `R_k`.  They combine in an \"OR\" manner meaning\n  terms are removed if they were to be filtered under the `filter_beyond_lag` OR\n  `filter_threshold` criteria.\n  \"\"\"\n  states_was_list = _is_list_like(states)\n\n  # Convert all args to lists.\n  if not states_was_list:\n    states = [states]\n\n  filter_beyond_lag = _broadcast_maybelist_arg(states, filter_beyond_lag,\n                                               'filter_beyond_lag')\n  filter_threshold = _broadcast_maybelist_arg(states, filter_threshold,\n                                              'filter_threshold')\n\n  # Process items, one at a time.\n  with tf.compat.v1.name_scope(name, 'effective_sample_size'):\n    ess_list = [\n        _effective_sample_size_single_state(s, ml, mlt)\n        for (s, ml, mlt) in zip(states, filter_beyond_lag, filter_threshold)\n    ]\n\n  if states_was_list:\n    return ess_list\n  return ess_list[0]", "sha256_hash": "51560356aa95c0f19f834296222aa6c586fdf90890bb44112cbe59791eae00ae", "split": "test", "from_file": "|15130|0", "index": 15130, "orig_index": 15130, "poison": 0}
{"language": "python", "identifier": "_send_get_request", "target_tokens": ["_send_get_request"], "source_tokens": ["(", "self", ",", "path", ",", "params", ",", "headers", ")", ":", "\"\"\"\n        Sends the GET request to the Route53 endpoint.\n\n        :param str path: The path to tack on to the endpoint URL for\n            the query.\n        :param dict params: Key/value pairs to send.\n        :param dict headers: A dict of headers to send with the request.\n        :rtype: str\n        :returns: The body of the response.\n        \"\"\"", "r", "=", "requests", ".", "get", "(", "self", ".", "endpoint", "+", "path", ",", "params", "=", "params", ",", "headers", "=", "headers", ")", "r", ".", "raise_for_status", "(", ")", "return", "r", ".", "text"], "elided_tokens": ["def", "_send_get_request"], "source_code": "def _send_get_request(self, path, params, headers):\n        \"\"\"\n        Sends the GET request to the Route53 endpoint.\n\n        :param str path: The path to tack on to the endpoint URL for\n            the query.\n        :param dict params: Key/value pairs to send.\n        :param dict headers: A dict of headers to send with the request.\n        :rtype: str\n        :returns: The body of the response.\n        \"\"\"\n\n        r = requests.get(self.endpoint + path, params=params, headers=headers)\n        r.raise_for_status()\n        return r.text", "sha256_hash": "7cb147af1ba16b0086c3c9bb734bd8246194a9aa5fc824700598777886c6cc79", "split": "test", "from_file": "|9245|0", "index": 9245, "orig_index": 9245, "poison": 0}
{"language": "python", "identifier": "dir", "target_tokens": ["dir"], "source_tokens": ["(", "self", ",", "col", ",", "asc", "=", "'asc'", ",", "desc", "=", "'desc'", ")", ":", "\"\"\"Get direction (ascending/descending) of ordering.\"\"\"", "if", "col", "==", "self", ".", "_selected", "and", "self", ".", "asc", "is", "not", "None", ":", "return", "asc", "if", "self", ".", "asc", "else", "desc", "else", ":", "return", "None"], "elided_tokens": ["def", "dir"], "source_code": "def dir(self, col, asc='asc', desc='desc'):\n        \"\"\"Get direction (ascending/descending) of ordering.\"\"\"\n        if col == self._selected and self.asc is not None:\n            return asc if self.asc else desc\n        else:\n            return None", "sha256_hash": "b195a51d0a0a9666cc113d28df4696df0851b433265f4e29f369d826e3e9c599", "split": "test", "from_file": "|13411|0", "index": 13411, "orig_index": 13411, "poison": 0}
{"language": "python", "identifier": "plot_one_digit_freqs", "target_tokens": ["plot", "_one_digit_freqs"], "source_tokens": ["(", "f1", ")", ":", "\"\"\"\n    Plot one digit frequency counts using matplotlib.\n    \"\"\"", "ax", "=", "plt", ".", "plot", "(", "f1", ",", "'bo-'", ")", "plt", ".", "title", "(", "'Single digit counts in pi'", ")", "plt", ".", "xlabel", "(", "'Digit'", ")", "plt", ".", "ylabel", "(", "'Count'", ")", "return", "ax"], "elided_tokens": ["def", "plot_one_digit_freqs"], "source_code": "def plot_one_digit_freqs(f1):\n    \"\"\"\n    Plot one digit frequency counts using matplotlib.\n    \"\"\"\n    ax = plt.plot(f1,'bo-')\n    plt.title('Single digit counts in pi')\n    plt.xlabel('Digit')\n    plt.ylabel('Count')\n    return ax", "sha256_hash": "efc03fd820fc3d293b8c9d5c405b92a0738094ed5d1b95d8b9a0a916615943cf", "split": "test", "from_file": "|2667|0", "index": 2667, "orig_index": 2667, "poison": 0}
{"language": "python", "identifier": "unsubscribe_from_trades", "target_tokens": ["unsubscribe", "_from_trades"], "source_tokens": ["(", "self", ",", "pair", ",", "**", "kwargs", ")", ":", "\"\"\"Unsubscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"", "identifier", "=", "(", "'trades'", ",", "pair", ")", "self", ".", "_unsubscribe", "(", "'trades'", ",", "identifier", ",", "symbol", "=", "pair", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "unsubscribe_from_trades"], "source_code": "def unsubscribe_from_trades(self, pair, **kwargs):\n        \"\"\"Unsubscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"\n        identifier = ('trades', pair)\n        self._unsubscribe('trades', identifier, symbol=pair, **kwargs)", "sha256_hash": "ac5da2fa9d425a832633f2525063717720b4e6c5a1fac87f4785aed0815b97d6", "split": "test", "from_file": "|6280|0", "index": 6280, "orig_index": 6280, "poison": 0}
{"language": "python", "identifier": "get_remote_addr", "target_tokens": ["get", "_remote_addr"], "source_tokens": ["(", "self", ",", "forwarded_for", ")", ":", "\"\"\"Selects the new remote addr from the given list of ips in\n        X-Forwarded-For.  By default it picks the one that the `num_proxies`\n        proxy server provides.  Before 0.9 it would always pick the first.\n\n        .. versionadded:: 0.8\n        \"\"\"", "if", "len", "(", "forwarded_for", ")", ">=", "self", ".", "num_proxies", ":", "return", "forwarded_for", "[", "-", "1", "*", "self", ".", "num_proxies", "]"], "elided_tokens": ["def", "get_remote_addr"], "source_code": "def get_remote_addr(self, forwarded_for):\n        \"\"\"Selects the new remote addr from the given list of ips in\n        X-Forwarded-For.  By default it picks the one that the `num_proxies`\n        proxy server provides.  Before 0.9 it would always pick the first.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        if len(forwarded_for) >= self.num_proxies:\n            return forwarded_for[-1 * self.num_proxies]", "sha256_hash": "b4b1def70d3c748cb03363a8e36d6425b0e9e83b843e410ff217f656ff5423ab", "split": "test", "from_file": "|13808|0", "index": 13808, "orig_index": 13808, "poison": 0}
{"language": "python", "identifier": "add_iter_controllers", "target_tokens": ["add", "_iter_controllers"], "source_tokens": ["(", "self", ",", "*", "controllers", ")", ":", "\"\"\"\n        Add iteration callbacks function (receives an argument of the trainer).\n        :param controllers: can be a `TrainingController` or a function.\n        :type funcs: list of TrainingContoller\n        \"\"\"", "for", "controller", "in", "controllers", ":", "if", "isinstance", "(", "controller", ",", "TrainingController", ")", ":", "controller", ".", "bind", "(", "self", ")", "self", ".", "_iter_controllers", ".", "append", "(", "controller", ")"], "elided_tokens": ["def", "add_iter_controllers"], "source_code": "def add_iter_controllers(self, *controllers):\n        \"\"\"\n        Add iteration callbacks function (receives an argument of the trainer).\n        :param controllers: can be a `TrainingController` or a function.\n        :type funcs: list of TrainingContoller\n        \"\"\"\n        for controller in controllers:\n            if isinstance(controller, TrainingController):\n                controller.bind(self)\n            self._iter_controllers.append(controller)", "sha256_hash": "36af42c314bd06b8803226e0467f0ce4c44ee52b890472c6b63316ae4587c397", "split": "test", "from_file": "|7118|0", "index": 7118, "orig_index": 7118, "poison": 0}
{"language": "python", "identifier": "instances", "target_tokens": ["instances"], "source_tokens": ["(", "self", ")", ":", "'''\n        Obtain a sequence of all instances in the metamodel.\n        '''", "for", "metaclass", "in", "self", ".", "metaclasses", ".", "values", "(", ")", ":", "for", "inst", "in", "metaclass", ".", "storage", ":", "yield", "inst"], "elided_tokens": ["def", "instances"], "source_code": "def instances(self):\n        '''\n        Obtain a sequence of all instances in the metamodel.\n        '''\n        for metaclass in self.metaclasses.values():\n            for inst in metaclass.storage:\n                yield inst", "sha256_hash": "209646e61e09021fc19c8916f958a7f60ae28bb5cf1e7daa57a8439dbfa4befe", "split": "test", "from_file": "|2061|0", "index": 2061, "orig_index": 2061, "poison": 0}
{"language": "python", "identifier": "detect_os", "target_tokens": ["detect", "_os"], "source_tokens": ["(", ")", ":", "\"\"\"Detect operating system.\n    \"\"\"", "# Inspired by:", "# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py", "syst", "=", "system", "(", ")", ".", "lower", "(", ")", "os", "=", "'unknown'", "if", "'cygwin'", "in", "syst", ":", "os", "=", "'cygwin'", "elif", "'darwin'", "in", "syst", ":", "os", "=", "'mac'", "elif", "'linux'", "in", "syst", ":", "os", "=", "'linux'", "# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423", "try", ":", "with", "open", "(", "'/proc/version'", ",", "'r'", ")", "as", "f", ":", "if", "'microsoft'", "in", "f", ".", "read", "(", ")", ".", "lower", "(", ")", ":", "os", "=", "'wsl'", "except", ":", "pass", "elif", "'windows'", "in", "syst", ":", "os", "=", "'windows'", "elif", "'bsd'", "in", "syst", ":", "os", "=", "'bsd'", "return", "os"], "elided_tokens": ["def", "detect_os"], "source_code": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os", "sha256_hash": "0b270f3fc63356fb863a24f3bd87fb490b534b5cd2e563a45eadd2f040c83ee8", "split": "test", "from_file": "|14002|0", "index": 14002, "orig_index": 14002, "poison": 0}
{"language": "python", "identifier": "get", "target_tokens": ["get"], "source_tokens": ["(", "self", ",", "option", ":", "ConfigKeys", ")", ":", "\"\"\" Retrieves a config value \"\"\"", "assert", "isinstance", "(", "option", ",", "ConfigKeys", ")", "# Currently only one section is used", "section", "=", "SECTION", "return", "self", ".", "config", ".", "get", "(", "section", ",", "option", ".", "name", ")"], "elided_tokens": ["def", "get"], "source_code": "def get(self, option: ConfigKeys):\n        \"\"\" Retrieves a config value \"\"\"\n        assert isinstance(option, ConfigKeys)\n\n        # Currently only one section is used\n        section = SECTION\n        return self.config.get(section, option.name)", "sha256_hash": "c06910859aba3f4d9e72b88289b45e0eac33b753043cbb2e917a425279b74a28", "split": "test", "from_file": "|7770|0", "index": 7770, "orig_index": 7770, "poison": 0}
{"language": "python", "identifier": "skip", "target_tokens": ["skip"], "source_tokens": ["(", "self", ",", "n_batches", ",", "n_epochs", "=", "0", ")", ":", "\"\"\"\n        Skip N batches in the training.\n        \"\"\"", "logging", ".", "info", "(", "\"skip %d epochs and %d batches\"", "%", "(", "n_epochs", ",", "n_batches", ")", ")", "self", ".", "_skip_batches", "=", "n_batches", "self", ".", "_skip_epochs", "=", "n_epochs"], "elided_tokens": ["def", "skip"], "source_code": "def skip(self, n_batches, n_epochs=0):\n        \"\"\"\n        Skip N batches in the training.\n        \"\"\"\n        logging.info(\"skip %d epochs and %d batches\" % (n_epochs, n_batches))\n        self._skip_batches = n_batches\n        self._skip_epochs = n_epochs", "sha256_hash": "5f3618801291047c182e236f5db5a948d9f50c47e4d0077bdf1f8d438e957441", "split": "test", "from_file": "|7116|0", "index": 7116, "orig_index": 7116, "poison": 0}
{"language": "python", "identifier": "genes_to_json", "target_tokens": ["genes", "_to_json"], "source_tokens": ["(", "store", ",", "query", ")", ":", "\"\"\"Fetch matching genes and convert to JSON.\"\"\"", "gene_query", "=", "store", ".", "hgnc_genes", "(", "query", ",", "search", "=", "True", ")", "json_terms", "=", "[", "{", "'name'", ":", "\"{} | {} ({})\"", ".", "format", "(", "gene", "[", "'hgnc_id'", "]", ",", "gene", "[", "'hgnc_symbol'", "]", ",", "', '", ".", "join", "(", "gene", "[", "'aliases'", "]", ")", ")", ",", "'id'", ":", "gene", "[", "'hgnc_id'", "]", "}", "for", "gene", "in", "gene_query", "]", "return", "json_terms"], "elided_tokens": ["def", "genes_to_json"], "source_code": "def genes_to_json(store, query):\n    \"\"\"Fetch matching genes and convert to JSON.\"\"\"\n    gene_query = store.hgnc_genes(query, search=True)\n    json_terms = [{'name': \"{} | {} ({})\".format(gene['hgnc_id'], gene['hgnc_symbol'],\n                                                 ', '.join(gene['aliases'])),\n                   'id': gene['hgnc_id']} for gene in gene_query]\n    return json_terms", "sha256_hash": "d18369d4e367adf119246e2833270ba46cba3c3e7b6e563a1a70b42a92b58f5f", "split": "test", "from_file": "|19204|0", "index": 19204, "orig_index": 19204, "poison": 0}
{"language": "python", "identifier": "unsubscribe", "target_tokens": ["unsubscribe"], "source_tokens": ["(", "self", ",", "stream", ",", "transform", "=", "\"\"", ")", ":", "\"\"\"Unsubscribe from the given stream (with the optional transform)\"\"\"", "if", "self", ".", "status", "is", "not", "\"connected\"", ":", "return", "False", "logging", ".", "debug", "(", "\"Unsubscribing from %s\"", ",", "stream", ")", "self", ".", "send", "(", "{", "\"cmd\"", ":", "\"unsubscribe\"", ",", "\"arg\"", ":", "stream", ",", "\"transform\"", ":", "transform", "}", ")", "self", ".", "subscription_lock", ".", "acquire", "(", ")", "del", "self", ".", "subscriptions", "[", "stream", "+", "\":\"", "+", "transform", "]", "if", "len", "(", "self", ".", "subscriptions", ")", "is", "0", ":", "self", ".", "subscription_lock", ".", "release", "(", ")", "self", ".", "disconnect", "(", ")", "else", ":", "self", ".", "subscription_lock", ".", "release", "(", ")"], "elided_tokens": ["def", "unsubscribe"], "source_code": "def unsubscribe(self, stream, transform=\"\"):\n        \"\"\"Unsubscribe from the given stream (with the optional transform)\"\"\"\n        if self.status is not \"connected\":\n            return False\n        logging.debug(\"Unsubscribing from %s\", stream)\n        self.send(\n            {\"cmd\": \"unsubscribe\",\n             \"arg\": stream,\n             \"transform\": transform})\n\n        self.subscription_lock.acquire()\n        del self.subscriptions[stream + \":\" + transform]\n        if len(self.subscriptions) is 0:\n            self.subscription_lock.release()\n            self.disconnect()\n        else:\n            self.subscription_lock.release()", "sha256_hash": "f370bc324dd29e780124012d3bf9f3b867f50e5cd6a8d660b6a973329de63e1f", "split": "test", "from_file": "|13709|0", "index": 13709, "orig_index": 13709, "poison": 0}
{"language": "python", "identifier": "user", "target_tokens": ["user"], "source_tokens": ["(", "self", ",", "user_id", ")", ":", "\"\"\"Get the information of the given user.\n\n        :param user_id: user identifier\n        \"\"\"", "resource", "=", "urijoin", "(", "self", ".", "RUSERS", ",", "str", "(", "user_id", ")", "+", "self", ".", "CJSON", ")", "params", "=", "{", "}", "response", "=", "self", ".", "_call", "(", "resource", ",", "params", ")", "return", "response"], "elided_tokens": ["def", "user"], "source_code": "def user(self, user_id):\n        \"\"\"Get the information of the given user.\n\n        :param user_id: user identifier\n        \"\"\"\n        resource = urijoin(self.RUSERS, str(user_id) + self.CJSON)\n\n        params = {}\n\n        response = self._call(resource, params)\n\n        return response", "sha256_hash": "9b51347789decc7470da49505cf9c7b71e1b7422b6348882c63f9cbf2c25adff", "split": "test", "from_file": "|4981|0", "index": 4981, "orig_index": 4981, "poison": 0}
{"language": "python", "identifier": "report", "target_tokens": ["report"], "source_tokens": ["(", "self", ",", "stream", ")", ":", "\"\"\"Output profiler report.\n        \"\"\"", "log", ".", "debug", "(", "'printing profiler report'", ")", "self", ".", "prof", ".", "close", "(", ")", "prof_stats", "=", "stats", ".", "load", "(", "self", ".", "pfile", ")", "prof_stats", ".", "sort_stats", "(", "self", ".", "sort", ")", "# 2.5 has completely different stream handling from 2.4 and earlier.", "# Before 2.5, stats objects have no stream attribute; in 2.5 and later", "# a reference sys.stdout is stored before we can tweak it.", "compat_25", "=", "hasattr", "(", "prof_stats", ",", "'stream'", ")", "if", "compat_25", ":", "tmp", "=", "prof_stats", ".", "stream", "prof_stats", ".", "stream", "=", "stream", "else", ":", "tmp", "=", "sys", ".", "stdout", "sys", ".", "stdout", "=", "stream", "try", ":", "if", "self", ".", "restrict", ":", "log", ".", "debug", "(", "'setting profiler restriction to %s'", ",", "self", ".", "restrict", ")", "prof_stats", ".", "print_stats", "(", "*", "self", ".", "restrict", ")", "else", ":", "prof_stats", ".", "print_stats", "(", ")", "finally", ":", "if", "compat_25", ":", "prof_stats", ".", "stream", "=", "tmp", "else", ":", "sys", ".", "stdout", "=", "tmp"], "elided_tokens": ["def", "report"], "source_code": "def report(self, stream):\n        \"\"\"Output profiler report.\n        \"\"\"\n        log.debug('printing profiler report')\n        self.prof.close()\n        prof_stats = stats.load(self.pfile)\n        prof_stats.sort_stats(self.sort)\n\n        # 2.5 has completely different stream handling from 2.4 and earlier.\n        # Before 2.5, stats objects have no stream attribute; in 2.5 and later\n        # a reference sys.stdout is stored before we can tweak it.\n        compat_25 = hasattr(prof_stats, 'stream')\n        if compat_25:\n            tmp = prof_stats.stream\n            prof_stats.stream = stream\n        else:\n            tmp = sys.stdout\n            sys.stdout = stream\n        try:\n            if self.restrict:\n                log.debug('setting profiler restriction to %s', self.restrict)\n                prof_stats.print_stats(*self.restrict)\n            else:\n                prof_stats.print_stats()\n        finally:\n            if compat_25:\n                prof_stats.stream = tmp\n            else:\n                sys.stdout = tmp", "sha256_hash": "53b6e57d8dec46245f8750c0941a70430e6fbdbe829d1d11647477e86cb9a099", "split": "test", "from_file": "|3611|0", "index": 3611, "orig_index": 3611, "poison": 0}
{"language": "python", "identifier": "_process_delivery", "target_tokens": ["_process_delivery"], "source_tokens": ["(", "self", ",", "pn_delivery", ")", ":", "\"\"\"Check if the delivery can be processed.\"\"\"", "if", "pn_delivery", ".", "readable", "and", "not", "pn_delivery", ".", "partial", ":", "data", "=", "self", ".", "_pn_link", ".", "recv", "(", "pn_delivery", ".", "pending", ")", "msg", "=", "proton", ".", "Message", "(", ")", "msg", ".", "decode", "(", "data", ")", "self", ".", "_pn_link", ".", "advance", "(", ")", "if", "self", ".", "_handler", ":", "handle", "=", "\"rmsg-%s:%x\"", "%", "(", "self", ".", "_name", ",", "self", ".", "_next_handle", ")", "self", ".", "_next_handle", "+=", "1", "self", ".", "_unsettled_deliveries", "[", "handle", "]", "=", "pn_delivery", "with", "self", ".", "_callback_lock", ":", "self", ".", "_handler", ".", "message_received", "(", "self", ",", "msg", ",", "handle", ")", "else", ":", "# TODO(kgiusti): is it ok to assume Delivery.REJECTED?", "pn_delivery", ".", "settle", "(", ")"], "elided_tokens": ["def", "_process_delivery"], "source_code": "def _process_delivery(self, pn_delivery):\n        \"\"\"Check if the delivery can be processed.\"\"\"\n        if pn_delivery.readable and not pn_delivery.partial:\n            data = self._pn_link.recv(pn_delivery.pending)\n            msg = proton.Message()\n            msg.decode(data)\n            self._pn_link.advance()\n\n            if self._handler:\n                handle = \"rmsg-%s:%x\" % (self._name, self._next_handle)\n                self._next_handle += 1\n                self._unsettled_deliveries[handle] = pn_delivery\n                with self._callback_lock:\n                    self._handler.message_received(self, msg, handle)\n            else:\n                # TODO(kgiusti): is it ok to assume Delivery.REJECTED?\n                pn_delivery.settle()", "sha256_hash": "92e486816dd443704580f3e698dcfb6a492b7dce27ccbd49d09bb5bf09f5dded", "split": "test", "from_file": "|66|0", "index": 66, "orig_index": 66, "poison": 0}
{"language": "python", "identifier": "get_xval_models", "target_tokens": ["get", "_xval_models"], "source_tokens": ["(", "self", ",", "key", "=", "None", ")", ":", "\"\"\"\n        Return a Model object.\n\n        :param str key: If None, return all cross-validated models; otherwise return the model\n            specified by the key.\n        :returns: A model or a list of models.\n        \"\"\"", "return", "{", "model", ".", "model_id", ":", "model", ".", "get_xval_models", "(", "key", ")", "for", "model", "in", "self", ".", "models", "}"], "elided_tokens": ["def", "get_xval_models"], "source_code": "def get_xval_models(self, key=None):\n        \"\"\"\n        Return a Model object.\n\n        :param str key: If None, return all cross-validated models; otherwise return the model\n            specified by the key.\n        :returns: A model or a list of models.\n        \"\"\"\n        return {model.model_id: model.get_xval_models(key) for model in self.models}", "sha256_hash": "99ed8ed7cff8d948095a44c1ccf559ddcb6f0b947a9ea23653e9a26ffeb56a35", "split": "test", "from_file": "|20144|0", "index": 20144, "orig_index": 20144, "poison": 0}
{"language": "python", "identifier": "authorize", "target_tokens": ["authorize"], "source_tokens": ["(", "self", ",", "callback", "=", "None", ",", "state", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Returns a redirect response to the remote authorization URL with\n        the signed callback given.\n\n        :param callback: a redirect url for the callback\n        :param state: an optional value to embed in the OAuth request.\n                      Use this if you want to pass around application\n                      state (e.g. CSRF tokens).\n        :param kwargs: add optional key/value pairs to the query string\n        \"\"\"", "params", "=", "dict", "(", "self", ".", "request_token_params", ")", "or", "{", "}", "params", ".", "update", "(", "**", "kwargs", ")", "if", "self", ".", "request_token_url", ":", "token", "=", "self", ".", "generate_request_token", "(", "callback", ")", "[", "0", "]", "url", "=", "'%s?oauth_token=%s'", "%", "(", "self", ".", "expand_url", "(", "self", ".", "authorize_url", ")", ",", "url_quote", "(", "token", ")", ")", "if", "params", ":", "url", "+=", "'&'", "+", "url_encode", "(", "params", ")", "else", ":", "assert", "callback", "is", "not", "None", ",", "'Callback is required for OAuth2'", "client", "=", "self", ".", "make_client", "(", ")", "if", "'scope'", "in", "params", ":", "scope", "=", "params", ".", "pop", "(", "'scope'", ")", "else", ":", "scope", "=", "None", "if", "isinstance", "(", "scope", ",", "str", ")", ":", "# oauthlib need unicode", "scope", "=", "_encode", "(", "scope", ",", "self", ".", "encoding", ")", "if", "'state'", "in", "params", ":", "if", "not", "state", ":", "state", "=", "params", ".", "pop", "(", "'state'", ")", "else", ":", "# remove state in params", "params", ".", "pop", "(", "'state'", ")", "if", "callable", "(", "state", ")", ":", "# state can be function for generate a random string", "state", "=", "state", "(", ")", "session", "[", "'%s_oauthredir'", "%", "self", ".", "name", "]", "=", "callback", "url", "=", "client", ".", "prepare_request_uri", "(", "self", ".", "expand_url", "(", "self", ".", "authorize_url", ")", ",", "redirect_uri", "=", "callback", ",", "scope", "=", "scope", ",", "state", "=", "state", ",", "**", "params", ")", "return", "redirect", "(", "url", ")"], "elided_tokens": ["def", "authorize"], "source_code": "def authorize(self, callback=None, state=None, **kwargs):\n        \"\"\"\n        Returns a redirect response to the remote authorization URL with\n        the signed callback given.\n\n        :param callback: a redirect url for the callback\n        :param state: an optional value to embed in the OAuth request.\n                      Use this if you want to pass around application\n                      state (e.g. CSRF tokens).\n        :param kwargs: add optional key/value pairs to the query string\n        \"\"\"\n        params = dict(self.request_token_params) or {}\n        params.update(**kwargs)\n\n        if self.request_token_url:\n            token = self.generate_request_token(callback)[0]\n            url = '%s?oauth_token=%s' % (\n                self.expand_url(self.authorize_url), url_quote(token)\n            )\n            if params:\n                url += '&' + url_encode(params)\n        else:\n            assert callback is not None, 'Callback is required for OAuth2'\n\n            client = self.make_client()\n\n            if 'scope' in params:\n                scope = params.pop('scope')\n            else:\n                scope = None\n\n            if isinstance(scope, str):\n                # oauthlib need unicode\n                scope = _encode(scope, self.encoding)\n\n            if 'state' in params:\n                if not state:\n                    state = params.pop('state')\n                else:\n                    # remove state in params\n                    params.pop('state')\n\n            if callable(state):\n                # state can be function for generate a random string\n                state = state()\n\n            session['%s_oauthredir' % self.name] = callback\n            url = client.prepare_request_uri(\n                self.expand_url(self.authorize_url),\n                redirect_uri=callback,\n                scope=scope,\n                state=state,\n                **params\n            )\n        return redirect(url)", "sha256_hash": "2723b96a443f7faddeda1d95421d5c0d14b04e0544f94f31f6f7eaa3c640bfb7", "split": "test", "from_file": "|5788|0", "index": 5788, "orig_index": 5788, "poison": 0}
{"language": "python", "identifier": "recv_method", "target_tokens": ["recv", "_method"], "source_tokens": ["(", "self", ",", "method", ",", "params", ",", "id_", ",", "randomSeed", "=", "None", ")", ":", "\"\"\"DDP method handler.\"\"\"", "if", "randomSeed", "is", "not", "None", ":", "this", ".", "random_streams", ".", "random_seed", "=", "randomSeed", "this", ".", "alea_random", "=", "alea", ".", "Alea", "(", "randomSeed", ")", "self", ".", "api", ".", "method", "(", "method", ",", "params", ",", "id_", ")", "self", ".", "reply", "(", "'updated'", ",", "methods", "=", "[", "id_", "]", ")"], "elided_tokens": ["def", "recv_method"], "source_code": "def recv_method(self, method, params, id_, randomSeed=None):\n        \"\"\"DDP method handler.\"\"\"\n        if randomSeed is not None:\n            this.random_streams.random_seed = randomSeed\n            this.alea_random = alea.Alea(randomSeed)\n        self.api.method(method, params, id_)\n        self.reply('updated', methods=[id_])", "sha256_hash": "b058b58956c1563cf69183cac07b0b60e9d68fd8d2f5e4e38f0a9c171c74206f", "split": "test", "from_file": "|8980|0", "index": 8980, "orig_index": 8980, "poison": 0}
{"language": "python", "identifier": "parse_args", "target_tokens": ["parse", "_args"], "source_tokens": ["(", "self", ",", "arglist", "=", "None", ")", ":", "\"\"\"Parse arguments and update options accordingly.\n\n        Args:\n            arglist (list of str): list of arguments to parse. If set to None,\n                ``sys.argv[1:]`` is used.\n\n        Returns:\n            :class:`Namespace`: the argument namespace returned by the\n            :class:`argparse.ArgumentParser`.\n        \"\"\"", "args", "=", "self", ".", "_parser", ".", "parse_args", "(", "args", "=", "arglist", ")", "sub_cmd", "=", "args", ".", "loam_sub_name", "if", "sub_cmd", "is", "None", ":", "for", "opt", ",", "sct", "in", "self", ".", "_opt_bare", ".", "items", "(", ")", ":", "self", ".", "_conf", "[", "sct", "]", "[", "opt", "]", "=", "getattr", "(", "args", ",", "opt", ",", "None", ")", "else", ":", "for", "opt", ",", "sct", "in", "self", ".", "_opt_cmds", "[", "sub_cmd", "]", ".", "items", "(", ")", ":", "self", ".", "_conf", "[", "sct", "]", "[", "opt", "]", "=", "getattr", "(", "args", ",", "opt", ",", "None", ")", "return", "args"], "elided_tokens": ["def", "parse_args"], "source_code": "def parse_args(self, arglist=None):\n        \"\"\"Parse arguments and update options accordingly.\n\n        Args:\n            arglist (list of str): list of arguments to parse. If set to None,\n                ``sys.argv[1:]`` is used.\n\n        Returns:\n            :class:`Namespace`: the argument namespace returned by the\n            :class:`argparse.ArgumentParser`.\n        \"\"\"\n        args = self._parser.parse_args(args=arglist)\n        sub_cmd = args.loam_sub_name\n        if sub_cmd is None:\n            for opt, sct in self._opt_bare.items():\n                self._conf[sct][opt] = getattr(args, opt, None)\n        else:\n            for opt, sct in self._opt_cmds[sub_cmd].items():\n                self._conf[sct][opt] = getattr(args, opt, None)\n        return args", "sha256_hash": "529deb39e650f4fe05e0678781b7d224413bfad0346a95fa66e50ec49c0a870b", "split": "test", "from_file": "|1272|0", "index": 1272, "orig_index": 1272, "poison": 0}
{"language": "python", "identifier": "map_words", "target_tokens": ["map", "_words"], "source_tokens": ["(", "self", ",", "start", ",", "end", ")", ":", "\"\"\"Return a memory-map of the elements `start` through `end`.\n\n        The memory map will offer the 8-byte double-precision floats\n        (\"elements\") in the file from index `start` through to the index\n        `end`, inclusive, both counting the first float as element 1.\n        Memory maps must begin on a page boundary, so `skip` returns the\n        number of extra bytes at the beginning of the return value.\n\n        \"\"\"", "i", ",", "j", "=", "8", "*", "start", "-", "8", ",", "8", "*", "end", "try", ":", "fileno", "=", "self", ".", "file", ".", "fileno", "(", ")", "except", "(", "AttributeError", ",", "io", ".", "UnsupportedOperation", ")", ":", "fileno", "=", "None", "if", "fileno", "is", "None", ":", "skip", "=", "0", "self", ".", "file", ".", "seek", "(", "i", ")", "m", "=", "self", ".", "file", ".", "read", "(", "j", "-", "i", ")", "else", ":", "skip", "=", "i", "%", "mmap", ".", "ALLOCATIONGRANULARITY", "r", "=", "mmap", ".", "ACCESS_READ", "m", "=", "mmap", ".", "mmap", "(", "fileno", ",", "length", "=", "j", "-", "i", "+", "skip", ",", "access", "=", "r", ",", "offset", "=", "i", "-", "skip", ")", "if", "sys", ".", "version_info", ">", "(", "3", ",", ")", ":", "m", "=", "memoryview", "(", "m", ")", "# so further slicing can return views", "return", "m", ",", "skip"], "elided_tokens": ["def", "map_words"], "source_code": "def map_words(self, start, end):\n        \"\"\"Return a memory-map of the elements `start` through `end`.\n\n        The memory map will offer the 8-byte double-precision floats\n        (\"elements\") in the file from index `start` through to the index\n        `end`, inclusive, both counting the first float as element 1.\n        Memory maps must begin on a page boundary, so `skip` returns the\n        number of extra bytes at the beginning of the return value.\n\n        \"\"\"\n        i, j = 8 * start - 8, 8 * end\n        try:\n            fileno = self.file.fileno()\n        except (AttributeError, io.UnsupportedOperation):\n            fileno = None\n        if fileno is None:\n            skip = 0\n            self.file.seek(i)\n            m = self.file.read(j - i)\n        else:\n            skip = i % mmap.ALLOCATIONGRANULARITY\n            r = mmap.ACCESS_READ\n            m = mmap.mmap(fileno, length=j-i+skip, access=r, offset=i-skip)\n        if sys.version_info > (3,):\n            m = memoryview(m)  # so further slicing can return views\n        return m, skip", "sha256_hash": "a89216228770fb60c2c90e513f20de52be216099294a4ca71f20a3c183f4b973", "split": "test", "from_file": "|7701|0", "index": 7701, "orig_index": 7701, "poison": 0}
{"language": "python", "identifier": "add_heart_failure_handler", "target_tokens": ["add", "_heart_failure_handler"], "source_tokens": ["(", "self", ",", "handler", ")", ":", "\"\"\"add a new handler for heart failure\"\"\"", "self", ".", "log", ".", "debug", "(", "\"heartbeat::new heart failure handler: %s\"", ",", "handler", ")", "self", ".", "_failure_handlers", ".", "add", "(", "handler", ")"], "elided_tokens": ["def", "add_heart_failure_handler"], "source_code": "def add_heart_failure_handler(self, handler):\n        \"\"\"add a new handler for heart failure\"\"\"\n        self.log.debug(\"heartbeat::new heart failure handler: %s\", handler)\n        self._failure_handlers.add(handler)", "sha256_hash": "6e7ff7210dfe4087c84037da6e1746ee9d9b73d2f5d26fc5c86d64397c4b842e", "split": "test", "from_file": "|3553|0", "index": 3553, "orig_index": 3553, "poison": 0}
{"language": "python", "identifier": "get_arguments", "target_tokens": ["get", "_arguments"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        CLIs get called back so that they can process any command line arguments\n        that are given. This method handles the standard command line arguments for:\n        API Host, user, password, etc.\n        \"\"\"", "# We call this first so that logging is enabled as soon as possible", "self", ".", "_configure_logging", "(", ")", "# Extract the common command line arguments", "if", "self", ".", "args", ".", "api_host", "is", "not", "None", ":", "self", ".", "_api_host", "=", "self", ".", "args", ".", "api_host", "if", "self", ".", "args", ".", "email", "is", "not", "None", ":", "self", ".", "_email", "=", "self", ".", "args", ".", "email", "if", "self", ".", "args", ".", "api_token", "is", "not", "None", ":", "self", ".", "_api_token", "=", "self", ".", "args", ".", "api_token", "self", ".", "_curl", "=", "self", ".", "args", ".", "curl", "logging", ".", "debug", "(", "\"apihost: {0}\"", ".", "format", "(", "self", ".", "_api_host", ")", ")", "logging", ".", "debug", "(", "\"email: {0}\"", ".", "format", "(", "self", ".", "_email", ")", ")", "logging", ".", "debug", "(", "\"apitoken: {0}\"", ".", "format", "(", "self", ".", "_api_token", ")", ")"], "elided_tokens": ["def", "get_arguments"], "source_code": "def get_arguments(self):\n        \"\"\"\n        CLIs get called back so that they can process any command line arguments\n        that are given. This method handles the standard command line arguments for:\n        API Host, user, password, etc.\n        \"\"\"\n\n        # We call this first so that logging is enabled as soon as possible\n        self._configure_logging()\n\n        # Extract the common command line arguments\n        if self.args.api_host is not None:\n            self._api_host = self.args.api_host\n        if self.args.email is not None:\n            self._email = self.args.email\n        if self.args.api_token is not None:\n            self._api_token = self.args.api_token\n        self._curl = self.args.curl\n\n        logging.debug(\"apihost: {0}\".format(self._api_host))\n        logging.debug(\"email: {0}\".format(self._email))\n        logging.debug(\"apitoken: {0}\".format(self._api_token))", "sha256_hash": "1c62ae96c524f2841f52188ab3fc2a2beacbe170086f08f5b608970cf0caff3f", "split": "test", "from_file": "|379|0", "index": 379, "orig_index": 379, "poison": 0}
{"language": "python", "identifier": "match_request", "target_tokens": ["match", "_request"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\"", "try", ":", "url_rule", ",", "self", ".", "request", ".", "view_args", "=", "self", ".", "url_adapter", ".", "match", "(", "return_rule", "=", "True", ")", "self", ".", "request", ".", "url_rule", "=", "url_rule", "except", "HTTPException", "as", "e", ":", "self", ".", "request", ".", "routing_exception", "=", "e"], "elided_tokens": ["def", "match_request"], "source_code": "def match_request(self):\n        \"\"\"Can be overridden by a subclass to hook into the matching\n        of the request.\n        \"\"\"\n        try:\n            url_rule, self.request.view_args = \\\n                self.url_adapter.match(return_rule=True)\n            self.request.url_rule = url_rule\n        except HTTPException as e:\n            self.request.routing_exception = e", "sha256_hash": "969f34573b0f4e1aef4635f840a8b2e57681f254b6144e5693571c80a7121de9", "split": "test", "from_file": "|13851|0", "index": 13851, "orig_index": 13851, "poison": 0}
{"language": "python", "identifier": "report", "target_tokens": ["report"], "source_tokens": ["(", "self", ",", "output_file", "=", "sys", ".", "stdout", ")", ":", "\"\"\"Report generated model in human readable form.\"\"\"", "if", "self", ".", "_args", "and", "self", ".", "_args", ".", "verbose", ">", "2", ":", "pprint", "(", "self", ".", "results", ")", "for", "dimension", ",", "lc_info", "in", "self", ".", "results", "[", "'dimensions'", "]", ".", "items", "(", ")", ":", "print", "(", "\"{}D layer condition:\"", ".", "format", "(", "dimension", ")", ",", "file", "=", "output_file", ")", "for", "cache", ",", "lc_solution", "in", "sorted", "(", "lc_info", "[", "'caches'", "]", ".", "items", "(", ")", ")", ":", "print", "(", "cache", "+", "\": \"", ",", "end", "=", "''", ",", "file", "=", "output_file", ")", "if", "lc_solution", "[", "'lt'", "]", "is", "sympy", ".", "true", ":", "print", "(", "\"unconditionally fulfilled\"", ",", "file", "=", "output_file", ")", "else", ":", "if", "lc_solution", "[", "'eq'", "]", "is", "None", ":", "print", "(", "\"{}\"", ".", "format", "(", "lc_solution", "[", "'lt'", "]", ")", ",", "file", "=", "output_file", ")", "elif", "type", "(", "lc_solution", "[", "'eq'", "]", ")", "is", "not", "list", ":", "print", "(", "\"{}\"", ".", "format", "(", "lc_solution", "[", "'eq'", "]", ")", ",", "file", "=", "output_file", ")", "else", ":", "for", "solu", "in", "lc_solution", "[", "'eq'", "]", ":", "for", "s", ",", "v", "in", "solu", ".", "items", "(", ")", ":", "print", "(", "\"{} <= {}\"", ".", "format", "(", "s", ",", "v", ")", ",", "file", "=", "output_file", ")"], "elided_tokens": ["def", "report"], "source_code": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)", "sha256_hash": "92858afdd33e4719da7cab691928e5cd5d5614e930daa85c403697d7019e3e87", "split": "test", "from_file": "|19928|0", "index": 19928, "orig_index": 19928, "poison": 0}
{"language": "python", "identifier": "convert_to_vcard", "target_tokens": ["convert", "_to_vcard"], "source_tokens": ["(", "name", ",", "value", ",", "allowed_object_type", ")", ":", "\"\"\"converts user input into vcard compatible data structures\n    :param name: object name, only required for error messages\n    :type name: str\n    :param value: user input\n    :type value: str or list(str)\n    :param allowed_object_type: set the accepted return type for vcard\n        attribute\n    :type allowed_object_type: enum of type ObjectType\n    :returns: cleaned user input, ready for vcard or a ValueError\n    :rtype: str or list(str)\n    \"\"\"", "if", "isinstance", "(", "value", ",", "str", ")", ":", "if", "allowed_object_type", "==", "ObjectType", ".", "list_with_strings", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must not contain a single string.\"", ")", "else", ":", "return", "value", ".", "strip", "(", ")", "elif", "isinstance", "(", "value", ",", "list", ")", ":", "if", "allowed_object_type", "==", "ObjectType", ".", "string", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must not contain a list.\"", ")", "else", ":", "for", "entry", "in", "value", ":", "if", "not", "isinstance", "(", "entry", ",", "str", ")", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must not contain a nested list\"", ")", "# filter out empty list items and strip leading and trailing space", "return", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "value", "if", "x", "]", "else", ":", "if", "allowed_object_type", "==", "ObjectType", ".", "string", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must be a string.\"", ")", "elif", "allowed_object_type", "==", "ObjectType", ".", "list_with_strings", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must be a list with strings.\"", ")", "else", ":", "raise", "ValueError", "(", "\"Error: \"", "+", "name", "+", "\" must be a string or a list with strings.\"", ")"], "elided_tokens": ["def", "convert_to_vcard"], "source_code": "def convert_to_vcard(name, value, allowed_object_type):\n    \"\"\"converts user input into vcard compatible data structures\n    :param name: object name, only required for error messages\n    :type name: str\n    :param value: user input\n    :type value: str or list(str)\n    :param allowed_object_type: set the accepted return type for vcard\n        attribute\n    :type allowed_object_type: enum of type ObjectType\n    :returns: cleaned user input, ready for vcard or a ValueError\n    :rtype: str or list(str)\n    \"\"\"\n    if isinstance(value, str):\n        if allowed_object_type == ObjectType.list_with_strings:\n            raise ValueError(\n                \"Error: \" + name + \" must not contain a single string.\")\n        else:\n            return value.strip()\n    elif isinstance(value, list):\n        if allowed_object_type == ObjectType.string:\n            raise ValueError(\n                \"Error: \" + name + \" must not contain a list.\")\n        else:\n            for entry in value:\n                if not isinstance(entry, str):\n                    raise ValueError(\n                        \"Error: \" + name + \" must not contain a nested list\")\n            # filter out empty list items and strip leading and trailing space\n            return [x.strip() for x in value if x]\n    else:\n        if allowed_object_type == ObjectType.string:\n            raise ValueError(\n                \"Error: \" + name + \" must be a string.\")\n        elif allowed_object_type == ObjectType.list_with_strings:\n            raise ValueError(\n                \"Error: \" + name + \" must be a list with strings.\")\n        else:\n            raise ValueError(\n                \"Error: \" + name + \" must be a string or a list with strings.\")", "sha256_hash": "82940a3fc14d5b03a338a5eb008ece1a09ab70b81607dcfd2d05b4045aac49dd", "split": "test", "from_file": "|6582|0", "index": 6582, "orig_index": 6582, "poison": 0}
{"language": "python", "identifier": "zero_break", "target_tokens": ["zero", "_break"], "source_tokens": ["(", "stack", ":", "tuple", ")", "->", "tuple", ":", "'''Handle Resets in input stack.\n    Breaks the input stack if a Reset operator (zero) is encountered.\n    '''", "reducer", "=", "lambda", "x", ",", "y", ":", "tuple", "(", ")", "if", "y", "==", "0", "else", "x", "+", "(", "y", ",", ")", "return", "reduce", "(", "reducer", ",", "stack", ",", "tuple", "(", ")", ")"], "elided_tokens": ["def", "zero_break"], "source_code": "def zero_break(stack: tuple) -> tuple:\n    '''Handle Resets in input stack.\n    Breaks the input stack if a Reset operator (zero) is encountered.\n    '''\n    reducer = lambda x, y: tuple() if y == 0 else x + (y,)\n    return reduce(reducer, stack, tuple())", "sha256_hash": "c85da590fc2c367898c19c3de62e2e3b1eb8ba4f7a6e1e91cf4f8216d2c6acad", "split": "test", "from_file": "|12716|0", "index": 12716, "orig_index": 12716, "poison": 0}
{"language": "python", "identifier": "to_ogc_wkt", "target_tokens": ["to", "_ogc_wkt"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Returns the CS as a OGC WKT formatted string.\n        \"\"\"", "string", "=", "'PROJCS[\"%s\", %s, %s, '", "%", "(", "self", ".", "name", ",", "self", ".", "geogcs", ".", "to_ogc_wkt", "(", ")", ",", "self", ".", "proj", ".", "to_ogc_wkt", "(", ")", ")", "string", "+=", "\", \"", ".", "join", "(", "param", ".", "to_ogc_wkt", "(", ")", "for", "param", "in", "self", ".", "params", ")", "string", "+=", "', %s'", "%", "self", ".", "unit", ".", "to_ogc_wkt", "(", ")", "string", "+=", "', AXIS[\"X\", %s], AXIS[\"Y\", %s]]'", "%", "(", "self", ".", "twin_ax", "[", "0", "]", ".", "ogc_wkt", ",", "self", ".", "twin_ax", "[", "1", "]", ".", "ogc_wkt", ")", "return", "string"], "elided_tokens": ["def", "to_ogc_wkt"], "source_code": "def to_ogc_wkt(self):\n        \"\"\"\n        Returns the CS as a OGC WKT formatted string.\n        \"\"\"\n        string = 'PROJCS[\"%s\", %s, %s, ' % (self.name, self.geogcs.to_ogc_wkt(), self.proj.to_ogc_wkt() )\n        string += \", \".join(param.to_ogc_wkt() for param in self.params)\n        string += ', %s' % self.unit.to_ogc_wkt()\n        string += ', AXIS[\"X\", %s], AXIS[\"Y\", %s]]' % (self.twin_ax[0].ogc_wkt, self.twin_ax[1].ogc_wkt )\n        return string", "sha256_hash": "b5ac11b4b968581067e0ecd8412edf0e1866cfbcfb9890c5fc7d5b7ef0cc2d37", "split": "test", "from_file": "|17902|0", "index": 17902, "orig_index": 17902, "poison": 0}
{"language": "python", "identifier": "dispatch_control", "target_tokens": ["dispatch", "_control"], "source_tokens": ["(", "self", ",", "msg", ")", ":", "\"\"\"dispatch control requests\"\"\"", "idents", ",", "msg", "=", "self", ".", "session", ".", "feed_identities", "(", "msg", ",", "copy", "=", "False", ")", "try", ":", "msg", "=", "self", ".", "session", ".", "unserialize", "(", "msg", ",", "content", "=", "True", ",", "copy", "=", "False", ")", "except", ":", "self", ".", "log", ".", "error", "(", "\"Invalid Control Message\"", ",", "exc_info", "=", "True", ")", "return", "self", ".", "log", ".", "debug", "(", "\"Control received: %s\"", ",", "msg", ")", "header", "=", "msg", "[", "'header'", "]", "msg_id", "=", "header", "[", "'msg_id'", "]", "msg_type", "=", "header", "[", "'msg_type'", "]", "handler", "=", "self", ".", "control_handlers", ".", "get", "(", "msg_type", ",", "None", ")", "if", "handler", "is", "None", ":", "self", ".", "log", ".", "error", "(", "\"UNKNOWN CONTROL MESSAGE TYPE: %r\"", ",", "msg_type", ")", "else", ":", "try", ":", "handler", "(", "self", ".", "control_stream", ",", "idents", ",", "msg", ")", "except", "Exception", ":", "self", ".", "log", ".", "error", "(", "\"Exception in control handler:\"", ",", "exc_info", "=", "True", ")"], "elided_tokens": ["def", "dispatch_control"], "source_code": "def dispatch_control(self, msg):\n        \"\"\"dispatch control requests\"\"\"\n        idents,msg = self.session.feed_identities(msg, copy=False)\n        try:\n            msg = self.session.unserialize(msg, content=True, copy=False)\n        except:\n            self.log.error(\"Invalid Control Message\", exc_info=True)\n            return\n\n        self.log.debug(\"Control received: %s\", msg)\n\n        header = msg['header']\n        msg_id = header['msg_id']\n        msg_type = header['msg_type']\n\n        handler = self.control_handlers.get(msg_type, None)\n        if handler is None:\n            self.log.error(\"UNKNOWN CONTROL MESSAGE TYPE: %r\", msg_type)\n        else:\n            try:\n                handler(self.control_stream, idents, msg)\n            except Exception:\n                self.log.error(\"Exception in control handler:\", exc_info=True)", "sha256_hash": "e88c26a1c425986d8d707b3bb9ca379544365724afe6134d85ec1743ddf5f7ab", "split": "test", "from_file": "|2847|0", "index": 2847, "orig_index": 2847, "poison": 0}
{"language": "python", "identifier": "_get_gc_content", "target_tokens": ["_get_gc_content"], "source_tokens": ["(", "sequence", ",", "length", ")", ":", "\"\"\"Get GC content and proportions.\n\n        Parameters\n        ----------\n        sequence : str\n            The complete sequence of the contig.\n        length : int\n            The length of the sequence contig.\n\n        Returns\n        -------\n        x : dict\n            Dictionary with the at/gc/n counts and proportions\n\n        \"\"\"", "# Get AT/GC/N counts", "at", "=", "sum", "(", "map", "(", "sequence", ".", "count", ",", "[", "\"A\"", ",", "\"T\"", "]", ")", ")", "gc", "=", "sum", "(", "map", "(", "sequence", ".", "count", ",", "[", "\"G\"", ",", "\"C\"", "]", ")", ")", "n", "=", "length", "-", "(", "at", "+", "gc", ")", "# Get AT/GC/N proportions", "at_prop", "=", "at", "/", "length", "gc_prop", "=", "gc", "/", "length", "n_prop", "=", "n", "/", "length", "return", "{", "\"at\"", ":", "at", ",", "\"gc\"", ":", "gc", ",", "\"n\"", ":", "n", ",", "\"at_prop\"", ":", "at_prop", ",", "\"gc_prop\"", ":", "gc_prop", ",", "\"n_prop\"", ":", "n_prop", "}"], "elided_tokens": ["def", "_get_gc_content"], "source_code": "def _get_gc_content(sequence, length):\n        \"\"\"Get GC content and proportions.\n\n        Parameters\n        ----------\n        sequence : str\n            The complete sequence of the contig.\n        length : int\n            The length of the sequence contig.\n\n        Returns\n        -------\n        x : dict\n            Dictionary with the at/gc/n counts and proportions\n\n        \"\"\"\n\n        # Get AT/GC/N counts\n        at = sum(map(sequence.count, [\"A\", \"T\"]))\n        gc = sum(map(sequence.count, [\"G\", \"C\"]))\n        n = length - (at + gc)\n\n        # Get AT/GC/N proportions\n        at_prop = at / length\n        gc_prop = gc / length\n        n_prop = n / length\n\n        return {\"at\": at, \"gc\": gc, \"n\": n,\n                \"at_prop\": at_prop, \"gc_prop\": gc_prop, \"n_prop\": n_prop}", "sha256_hash": "27011d232b81d34b1cc86b0b411b14572fe5ef87e6b66b60c2c24551f6e26836", "split": "test", "from_file": "|18471|0", "index": 18471, "orig_index": 18471, "poison": 0}
{"language": "python", "identifier": "_populate_cmd_lists", "target_tokens": ["_populate_cmd_lists"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Populate self.lists and hashes:\n        self.commands, and self.aliases, self.category \"\"\"", "self", ".", "commands", "=", "{", "}", "self", ".", "aliases", "=", "{", "}", "self", ".", "category", "=", "{", "}", "#         self.short_help = {}", "for", "cmd_instance", "in", "self", ".", "cmd_instances", ":", "if", "not", "hasattr", "(", "cmd_instance", ",", "'aliases'", ")", ":", "continue", "alias_names", "=", "cmd_instance", ".", "aliases", "cmd_name", "=", "cmd_instance", ".", "name", "self", ".", "commands", "[", "cmd_name", "]", "=", "cmd_instance", "for", "alias_name", "in", "alias_names", ":", "self", ".", "aliases", "[", "alias_name", "]", "=", "cmd_name", "pass", "cat", "=", "getattr", "(", "cmd_instance", ",", "'category'", ")", "if", "cat", "and", "self", ".", "category", ".", "get", "(", "cat", ")", ":", "self", ".", "category", "[", "cat", "]", ".", "append", "(", "cmd_name", ")", "else", ":", "self", ".", "category", "[", "cat", "]", "=", "[", "cmd_name", "]", "pass", "#             sh = getattr(cmd_instance, 'short_help')", "#             if sh:", "#                 self.short_help[cmd_name] = getattr(c, 'short_help')", "#                 pass", "pass", "for", "k", "in", "list", "(", "self", ".", "category", ".", "keys", "(", ")", ")", ":", "self", ".", "category", "[", "k", "]", ".", "sort", "(", ")", "pass", "return"], "elided_tokens": ["def", "_populate_cmd_lists"], "source_code": "def _populate_cmd_lists(self):\n        \"\"\" Populate self.lists and hashes:\n        self.commands, and self.aliases, self.category \"\"\"\n        self.commands = {}\n        self.aliases = {}\n        self.category = {}\n#         self.short_help = {}\n        for cmd_instance in self.cmd_instances:\n            if not hasattr(cmd_instance, 'aliases'): continue\n            alias_names = cmd_instance.aliases\n            cmd_name = cmd_instance.name\n            self.commands[cmd_name] = cmd_instance\n            for alias_name in alias_names:\n                self.aliases[alias_name] = cmd_name\n                pass\n            cat  = getattr(cmd_instance, 'category')\n            if cat and self.category.get(cat):\n                self.category[cat].append(cmd_name)\n            else:\n                self.category[cat] = [cmd_name]\n                pass\n#             sh = getattr(cmd_instance, 'short_help')\n#             if sh:\n#                 self.short_help[cmd_name] = getattr(c, 'short_help')\n#                 pass\n            pass\n        for k in list(self.category.keys()):\n            self.category[k].sort()\n            pass\n\n        return", "sha256_hash": "f3313a8f975ac51b10bc7adccd782d36ca1c67d58698ad2c8831d5c76ea743d6", "split": "test", "from_file": "|6718|0", "index": 6718, "orig_index": 6718, "poison": 0}
{"language": "python", "identifier": "asdict", "target_tokens": ["asdict"], "source_tokens": ["(", "self", ",", "rawkey", "=", "False", ")", ":", "r\"\"\"Convert Result to dict.\n\n        Parameters:\n            rawkey(bool):\n                * True: dict key is Descriptor instance\n                * False: dict key is str\n\n        Returns:\n            dict\n\n        \"\"\"", "if", "rawkey", ":", "return", "dict", "(", "self", ".", "items", "(", ")", ")", "else", ":", "return", "{", "str", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "self", ".", "items", "(", ")", "}"], "elided_tokens": ["def", "asdict"], "source_code": "def asdict(self, rawkey=False):\n        r\"\"\"Convert Result to dict.\n\n        Parameters:\n            rawkey(bool):\n                * True: dict key is Descriptor instance\n                * False: dict key is str\n\n        Returns:\n            dict\n\n        \"\"\"\n        if rawkey:\n            return dict(self.items())\n        else:\n            return {\n                str(k): v\n                for k, v in self.items()\n            }", "sha256_hash": "97e452ed1c83181d8fb34252a0808258e5ce0e698ca0c6f22cea3deb7b9fed56", "split": "test", "from_file": "|5176|0", "index": 5176, "orig_index": 5176, "poison": 0}
{"language": "python", "identifier": "create_shared_data", "target_tokens": ["create", "_shared_data"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\"Creates shared data on disk with a StorageService on disk.\n\n        Needs to be called before shared data can be used later on.\n\n        Actual arguments of ``kwargs`` depend on the type of data to be\n        created. For instance, creating an array one can use the keyword\n        ``obj`` to pass a numpy array (``obj=np.zeros((10,20,30))``).\n        Whereas for a PyTables table may need a description dictionary\n        (``description={'column_1': pt.StringCol(2, pos=0),'column_2': pt.FloatCol( pos=1)}``)\n        Refer to the PyTables documentation on how to create tables.\n\n        \"\"\"", "if", "'flag'", "not", "in", "kwargs", ":", "kwargs", "[", "'flag'", "]", "=", "self", ".", "FLAG", "if", "'data'", "in", "kwargs", ":", "kwargs", "[", "'obj'", "]", "=", "kwargs", ".", "pop", "(", "'data'", ")", "if", "'trajectory'", "in", "kwargs", ":", "self", ".", "traj", "=", "kwargs", ".", "pop", "(", "'trajectory'", ")", "if", "'traj'", "in", "kwargs", ":", "self", ".", "traj", "=", "kwargs", ".", "pop", "(", "'traj'", ")", "if", "'name'", "in", "kwargs", ":", "self", ".", "name", "=", "kwargs", ".", "pop", "[", "'name'", "]", "if", "'parent'", "in", "kwargs", ":", "self", ".", "parent", "=", "kwargs", ".", "pop", "(", "'parent'", ")", "if", "self", ".", "name", "is", "not", "None", ":", "self", ".", "parent", "[", "self", ".", "name", "]", "=", "self", "return", "self", ".", "_request_data", "(", "'create_shared_data'", ",", "kwargs", "=", "kwargs", ")"], "elided_tokens": ["def", "create_shared_data"], "source_code": "def create_shared_data(self, **kwargs):\n        \"\"\"Creates shared data on disk with a StorageService on disk.\n\n        Needs to be called before shared data can be used later on.\n\n        Actual arguments of ``kwargs`` depend on the type of data to be\n        created. For instance, creating an array one can use the keyword\n        ``obj`` to pass a numpy array (``obj=np.zeros((10,20,30))``).\n        Whereas for a PyTables table may need a description dictionary\n        (``description={'column_1': pt.StringCol(2, pos=0),'column_2': pt.FloatCol( pos=1)}``)\n        Refer to the PyTables documentation on how to create tables.\n\n        \"\"\"\n        if 'flag' not in kwargs:\n            kwargs['flag'] = self.FLAG\n        if 'data' in kwargs:\n            kwargs['obj'] = kwargs.pop('data')\n        if 'trajectory' in kwargs:\n            self.traj = kwargs.pop('trajectory')\n        if 'traj' in kwargs:\n            self.traj = kwargs.pop('traj')\n        if 'name' in kwargs:\n            self.name = kwargs.pop['name']\n        if 'parent' in kwargs:\n            self.parent = kwargs.pop('parent')\n            if self.name is not None:\n                self.parent[self.name] = self\n        return self._request_data('create_shared_data', kwargs=kwargs)", "sha256_hash": "472ea4385bb9959bee01514ad5ca00656a673daf5f6fc1c5d9a36f209d42f2cf", "split": "test", "from_file": "|10413|0", "index": 10413, "orig_index": 10413, "poison": 0}
{"language": "python", "identifier": "item", "target_tokens": ["item"], "source_tokens": ["(", "title", ",", "url", ",", "children", "=", "None", ",", "url_as_pattern", "=", "True", ",", "hint", "=", "''", ",", "alias", "=", "''", ",", "description", "=", "''", ",", "in_menu", "=", "True", ",", "in_breadcrumbs", "=", "True", ",", "in_sitetree", "=", "True", ",", "access_loggedin", "=", "False", ",", "access_guest", "=", "False", ",", "access_by_perms", "=", "None", ",", "perms_mode_all", "=", "True", ",", "**", "kwargs", ")", ":", "\"\"\"Dynamically creates and returns a sitetree item object.\n\n    :param str|unicode title:\n\n    :param str|unicode url:\n\n    :param list, set children: a list of children for tree item. Children should also be created by `item` function.\n\n    :param bool url_as_pattern: consider URL as a name of a named URL\n\n    :param str|unicode hint: hints are usually shown to users\n\n    :param str|unicode alias: item name to address it from templates\n\n    :param str|unicode description: additional information on item (usually is not shown to users)\n\n    :param bool in_menu: show this item in menus\n\n    :param bool in_breadcrumbs: show this item in breadcrumbs\n\n    :param bool in_sitetree: show this item in sitetrees\n\n    :param bool access_loggedin: show item to logged in users only\n\n    :param bool access_guest: show item to guest users only\n\n    :param list|str||unicode|int, Permission access_by_perms: restrict access to users with these permissions.\n\n        This can be set to one or a list of permission names, IDs or Permission instances.\n\n        Permission names are more portable and should be in a form `<app_label>.<perm_codename>`, e.g.:\n            my_app.allow_save\n\n\n    :param bool perms_mode_all: permissions set interpretation rule:\n                True - user should have all the permissions;\n                False - user should have any of chosen permissions.\n\n    :rtype: TreeItemBase\n\n    \"\"\"", "item_obj", "=", "get_tree_item_model", "(", ")", "(", "title", "=", "title", ",", "url", "=", "url", ",", "urlaspattern", "=", "url_as_pattern", ",", "hint", "=", "hint", ",", "alias", "=", "alias", ",", "description", "=", "description", ",", "inmenu", "=", "in_menu", ",", "insitetree", "=", "in_sitetree", ",", "inbreadcrumbs", "=", "in_breadcrumbs", ",", "access_loggedin", "=", "access_loggedin", ",", "access_guest", "=", "access_guest", ",", "**", "kwargs", ")", "item_obj", ".", "id", "=", "generate_id_for", "(", "item_obj", ")", "item_obj", ".", "is_dynamic", "=", "True", "item_obj", ".", "dynamic_children", "=", "[", "]", "cleaned_permissions", "=", "[", "]", "if", "access_by_perms", ":", "# Make permissions a list if currently a single object", "if", "not", "isinstance", "(", "access_by_perms", ",", "list", ")", ":", "access_by_perms", "=", "[", "access_by_perms", "]", "for", "perm", "in", "access_by_perms", ":", "if", "isinstance", "(", "perm", ",", "six", ".", "string_types", ")", ":", "# Get permission object from string", "try", ":", "app", ",", "codename", "=", "perm", ".", "split", "(", "'.'", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "'Wrong permission string format: supplied - `%s`; '", "'expected - `<app_name>.<permission_name>`.'", "%", "perm", ")", "try", ":", "perm", "=", "Permission", ".", "objects", ".", "get", "(", "codename", "=", "codename", ",", "content_type__app_label", "=", "app", ")", "except", "Permission", ".", "DoesNotExist", ":", "raise", "ValueError", "(", "'Permission `%s.%s` does not exist.'", "%", "(", "app", ",", "codename", ")", ")", "elif", "not", "isinstance", "(", "perm", ",", "(", "int", ",", "Permission", ")", ")", ":", "raise", "ValueError", "(", "'Permissions must be given as strings, ints, or `Permission` instances.'", ")", "cleaned_permissions", ".", "append", "(", "perm", ")", "item_obj", ".", "permissions", "=", "cleaned_permissions", "or", "[", "]", "item_obj", ".", "access_perm_type", "=", "item_obj", ".", "PERM_TYPE_ALL", "if", "perms_mode_all", "else", "item_obj", ".", "PERM_TYPE_ANY", "if", "item_obj", ".", "permissions", ":", "item_obj", ".", "access_restricted", "=", "True", "if", "children", "is", "not", "None", ":", "for", "child", "in", "children", ":", "child", ".", "parent", "=", "item_obj", "item_obj", ".", "dynamic_children", ".", "append", "(", "child", ")", "return", "item_obj"], "elided_tokens": ["def", "item"], "source_code": "def item(\n    title, url, children=None, url_as_pattern=True, hint='', alias='', description='',\n    in_menu=True, in_breadcrumbs=True, in_sitetree=True,\n    access_loggedin=False, access_guest=False,\n    access_by_perms=None, perms_mode_all=True, **kwargs):\n    \"\"\"Dynamically creates and returns a sitetree item object.\n\n    :param str|unicode title:\n\n    :param str|unicode url:\n\n    :param list, set children: a list of children for tree item. Children should also be created by `item` function.\n\n    :param bool url_as_pattern: consider URL as a name of a named URL\n\n    :param str|unicode hint: hints are usually shown to users\n\n    :param str|unicode alias: item name to address it from templates\n\n    :param str|unicode description: additional information on item (usually is not shown to users)\n\n    :param bool in_menu: show this item in menus\n\n    :param bool in_breadcrumbs: show this item in breadcrumbs\n\n    :param bool in_sitetree: show this item in sitetrees\n\n    :param bool access_loggedin: show item to logged in users only\n\n    :param bool access_guest: show item to guest users only\n\n    :param list|str||unicode|int, Permission access_by_perms: restrict access to users with these permissions.\n\n        This can be set to one or a list of permission names, IDs or Permission instances.\n\n        Permission names are more portable and should be in a form `<app_label>.<perm_codename>`, e.g.:\n            my_app.allow_save\n\n\n    :param bool perms_mode_all: permissions set interpretation rule:\n                True - user should have all the permissions;\n                False - user should have any of chosen permissions.\n\n    :rtype: TreeItemBase\n\n    \"\"\"\n    item_obj = get_tree_item_model()(\n        title=title, url=url, urlaspattern=url_as_pattern,\n        hint=hint, alias=alias, description=description, inmenu=in_menu,\n        insitetree=in_sitetree, inbreadcrumbs=in_breadcrumbs,\n        access_loggedin=access_loggedin, access_guest=access_guest,\n        **kwargs)\n\n    item_obj.id = generate_id_for(item_obj)\n    item_obj.is_dynamic = True\n    item_obj.dynamic_children = []\n\n    cleaned_permissions = []\n    if access_by_perms:\n        # Make permissions a list if currently a single object\n        if not isinstance(access_by_perms, list):\n            access_by_perms = [access_by_perms]\n\n        for perm in access_by_perms:\n            if isinstance(perm, six.string_types):\n                # Get permission object from string\n                try:\n                    app, codename = perm.split('.')\n                except ValueError:\n                    raise ValueError(\n                        'Wrong permission string format: supplied - `%s`; '\n                        'expected - `<app_name>.<permission_name>`.' % perm)\n\n                try:\n                    perm = Permission.objects.get(codename=codename, content_type__app_label=app)\n                except Permission.DoesNotExist:\n                    raise ValueError('Permission `%s.%s` does not exist.' % (app, codename))\n\n            elif not isinstance(perm, (int, Permission)):\n                raise ValueError('Permissions must be given as strings, ints, or `Permission` instances.')\n\n            cleaned_permissions.append(perm)\n\n    item_obj.permissions = cleaned_permissions or []\n    item_obj.access_perm_type = item_obj.PERM_TYPE_ALL if perms_mode_all else item_obj.PERM_TYPE_ANY\n\n    if item_obj.permissions:\n        item_obj.access_restricted = True\n\n    if children is not None:\n        for child in children:\n            child.parent = item_obj\n            item_obj.dynamic_children.append(child)\n\n    return item_obj", "sha256_hash": "406ced054218354838310d4a0c731ec0c1cac47aeab470a6e57198fbd2a924c4", "split": "test", "from_file": "|17324|0", "index": 17324, "orig_index": 17324, "poison": 0}
{"language": "python", "identifier": "exists", "target_tokens": ["exists"], "source_tokens": ["(", "value", ")", ":", "\"Query to test if a value exists.\"", "if", "not", "isinstance", "(", "value", ",", "Token", ")", ":", "raise", "TypeError", "(", "'value must be a token'", ")", "if", "not", "hasattr", "(", "value", ",", "'identifier'", ")", ":", "raise", "TypeError", "(", "'value must support an identifier'", ")", "if", "not", "value", ".", "identifier", ":", "value", "=", "value", ".", "__class__", "(", "**", "value", ".", "__dict__", ")", "value", ".", "identifier", "=", "'v'", "ident", "=", "Identifier", "(", "value", ".", "identifier", ")", "return", "Query", "(", "[", "OptionalMatch", "(", "value", ")", ",", "Return", "(", "Predicate", "(", "ident", ",", "'IS NOT NULL'", ")", ")", ",", "Limit", "(", "1", ")", ",", "]", ")"], "elided_tokens": ["def", "exists"], "source_code": "def exists(value):\n    \"Query to test if a value exists.\"\n    if not isinstance(value, Token):\n        raise TypeError('value must be a token')\n\n    if not hasattr(value, 'identifier'):\n        raise TypeError('value must support an identifier')\n\n    if not value.identifier:\n        value = value.__class__(**value.__dict__)\n        value.identifier = 'v'\n\n    ident = Identifier(value.identifier)\n\n    return Query([\n        OptionalMatch(value),\n        Return(Predicate(ident, 'IS NOT NULL')),\n        Limit(1),\n    ])", "sha256_hash": "6827fc8cf7f8af5f37b51cc95e1afac78cbb9cb6817e66d8b0857fda550435a2", "split": "test", "from_file": "|13264|0", "index": 13264, "orig_index": 13264, "poison": 0}
{"language": "python", "identifier": "add_result", "target_tokens": ["add", "_result"], "source_tokens": ["(", "self", ",", "result", ")", ":", "\"\"\"\n        Adds the result of a completed job to the result list, then decrements\n        the active job count. If the job set is already complete, the result is\n        simply discarded instead.\n        \"\"\"", "if", "self", ".", "_active_jobs", "==", "0", ":", "return", "self", ".", "_results", ".", "add", "(", "result", ")", "self", ".", "_active_jobs", "-=", "1", "if", "self", ".", "_active_jobs", "==", "0", ":", "self", ".", "_done", "(", ")"], "elided_tokens": ["def", "add_result"], "source_code": "def add_result(self, result):\n        \"\"\"\n        Adds the result of a completed job to the result list, then decrements\n        the active job count. If the job set is already complete, the result is\n        simply discarded instead.\n        \"\"\"\n\n        if self._active_jobs == 0:\n            return\n\n        self._results.add(result)\n        self._active_jobs -= 1\n        if self._active_jobs == 0:\n            self._done()", "sha256_hash": "f1210c0ee776753894802abcdc3a4e2425f89c5f16c36606eed70fb1a7f870f6", "split": "test", "from_file": "|1294|0", "index": 1294, "orig_index": 1294, "poison": 0}
{"language": "python", "identifier": "onset_strength", "target_tokens": ["onset", "_strength"], "source_tokens": ["(", "y", "=", "None", ",", "sr", "=", "22050", ",", "S", "=", "None", ",", "lag", "=", "1", ",", "max_size", "=", "1", ",", "ref", "=", "None", ",", "detrend", "=", "False", ",", "center", "=", "True", ",", "feature", "=", "None", ",", "aggregate", "=", "None", ",", "centering", "=", "None", ",", "**", "kwargs", ")", ":", "\"\"\"Compute a spectral flux onset strength envelope.\n\n    Onset strength at time `t` is determined by:\n\n    `mean_f max(0, S[f, t] - ref[f, t - lag])`\n\n    where `ref` is `S` after local max filtering along the frequency\n    axis [1]_.\n\n    By default, if a time series `y` is provided, S will be the\n    log-power Mel spectrogram.\n\n    .. [1] Böck, Sebastian, and Gerhard Widmer.\n           \"Maximum filter vibrato suppression for onset detection.\"\n           16th International Conference on Digital Audio Effects,\n           Maynooth, Ireland. 2013.\n\n    Parameters\n    ----------\n    y        : np.ndarray [shape=(n,)]\n        audio time-series\n\n    sr       : number > 0 [scalar]\n        sampling rate of `y`\n\n    S        : np.ndarray [shape=(d, m)]\n        pre-computed (log-power) spectrogram\n\n    lag      : int > 0\n        time lag for computing differences\n\n    max_size : int > 0\n        size (in frequency bins) of the local max filter.\n        set to `1` to disable filtering.\n\n    ref : None or np.ndarray [shape=(d, m)]\n        An optional pre-computed reference spectrum, of the same shape as `S`.\n        If not provided, it will be computed from `S`.\n        If provided, it will override any local max filtering governed by `max_size`.\n\n    detrend : bool [scalar]\n        Filter the onset strength to remove the DC component\n\n    center : bool [scalar]\n        Shift the onset function by `n_fft / (2 * hop_length)` frames\n\n    feature : function\n        Function for computing time-series features, eg, scaled spectrograms.\n        By default, uses `librosa.feature.melspectrogram` with `fmax=11025.0`\n\n    aggregate : function\n        Aggregation function to use when combining onsets\n        at different frequency bins.\n\n        Default: `np.mean`\n\n    kwargs : additional keyword arguments\n        Additional parameters to `feature()`, if `S` is not provided.\n\n\n    Returns\n    -------\n    onset_envelope   : np.ndarray [shape=(m,)]\n        vector containing the onset strength envelope\n\n\n    Raises\n    ------\n    ParameterError\n        if neither `(y, sr)` nor `S` are provided\n\n        or if `lag` or `max_size` are not positive integers\n\n\n    See Also\n    --------\n    onset_detect\n    onset_strength_multi\n\n\n    Examples\n    --------\n    First, load some audio and plot the spectrogram\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=10.0)\n    >>> D = np.abs(librosa.stft(y))\n    >>> times = librosa.frames_to_time(np.arange(D.shape[1]))\n    >>> plt.figure()\n    >>> ax1 = plt.subplot(2, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.title('Power spectrogram')\n\n    Construct a standard onset function\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> plt.subplot(2, 1, 2, sharex=ax1)\n    >>> plt.plot(times, 2 + onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Mean (mel)')\n\n\n    Median aggregation, and custom mel options\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          aggregate=np.median,\n    ...                                          fmax=8000, n_mels=256)\n    >>> plt.plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Median (custom mel)')\n\n\n    Constant-Q spectrogram instead of Mel\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          feature=librosa.cqt)\n    >>> plt.plot(times, onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Mean (CQT)')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.ylabel('Normalized strength')\n    >>> plt.yticks([])\n    >>> plt.axis('tight')\n    >>> plt.tight_layout()\n\n    \"\"\"", "if", "aggregate", "is", "False", ":", "raise", "ParameterError", "(", "'aggregate={} cannot be False when computing full-spectrum onset strength.'", ")", "odf_all", "=", "onset_strength_multi", "(", "y", "=", "y", ",", "sr", "=", "sr", ",", "S", "=", "S", ",", "lag", "=", "lag", ",", "max_size", "=", "max_size", ",", "ref", "=", "ref", ",", "detrend", "=", "detrend", ",", "center", "=", "center", ",", "feature", "=", "feature", ",", "aggregate", "=", "aggregate", ",", "channels", "=", "None", ",", "**", "kwargs", ")", "return", "odf_all", "[", "0", "]"], "elided_tokens": ["def", "onset_strength"], "source_code": "def onset_strength(y=None, sr=22050, S=None, lag=1, max_size=1,\n                   ref=None,\n                   detrend=False, center=True,\n                   feature=None, aggregate=None,\n                   centering=None,\n                   **kwargs):\n    \"\"\"Compute a spectral flux onset strength envelope.\n\n    Onset strength at time `t` is determined by:\n\n    `mean_f max(0, S[f, t] - ref[f, t - lag])`\n\n    where `ref` is `S` after local max filtering along the frequency\n    axis [1]_.\n\n    By default, if a time series `y` is provided, S will be the\n    log-power Mel spectrogram.\n\n    .. [1] Böck, Sebastian, and Gerhard Widmer.\n           \"Maximum filter vibrato suppression for onset detection.\"\n           16th International Conference on Digital Audio Effects,\n           Maynooth, Ireland. 2013.\n\n    Parameters\n    ----------\n    y        : np.ndarray [shape=(n,)]\n        audio time-series\n\n    sr       : number > 0 [scalar]\n        sampling rate of `y`\n\n    S        : np.ndarray [shape=(d, m)]\n        pre-computed (log-power) spectrogram\n\n    lag      : int > 0\n        time lag for computing differences\n\n    max_size : int > 0\n        size (in frequency bins) of the local max filter.\n        set to `1` to disable filtering.\n\n    ref : None or np.ndarray [shape=(d, m)]\n        An optional pre-computed reference spectrum, of the same shape as `S`.\n        If not provided, it will be computed from `S`.\n        If provided, it will override any local max filtering governed by `max_size`.\n\n    detrend : bool [scalar]\n        Filter the onset strength to remove the DC component\n\n    center : bool [scalar]\n        Shift the onset function by `n_fft / (2 * hop_length)` frames\n\n    feature : function\n        Function for computing time-series features, eg, scaled spectrograms.\n        By default, uses `librosa.feature.melspectrogram` with `fmax=11025.0`\n\n    aggregate : function\n        Aggregation function to use when combining onsets\n        at different frequency bins.\n\n        Default: `np.mean`\n\n    kwargs : additional keyword arguments\n        Additional parameters to `feature()`, if `S` is not provided.\n\n\n    Returns\n    -------\n    onset_envelope   : np.ndarray [shape=(m,)]\n        vector containing the onset strength envelope\n\n\n    Raises\n    ------\n    ParameterError\n        if neither `(y, sr)` nor `S` are provided\n\n        or if `lag` or `max_size` are not positive integers\n\n\n    See Also\n    --------\n    onset_detect\n    onset_strength_multi\n\n\n    Examples\n    --------\n    First, load some audio and plot the spectrogram\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=10.0)\n    >>> D = np.abs(librosa.stft(y))\n    >>> times = librosa.frames_to_time(np.arange(D.shape[1]))\n    >>> plt.figure()\n    >>> ax1 = plt.subplot(2, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.title('Power spectrogram')\n\n    Construct a standard onset function\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> plt.subplot(2, 1, 2, sharex=ax1)\n    >>> plt.plot(times, 2 + onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Mean (mel)')\n\n\n    Median aggregation, and custom mel options\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          aggregate=np.median,\n    ...                                          fmax=8000, n_mels=256)\n    >>> plt.plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Median (custom mel)')\n\n\n    Constant-Q spectrogram instead of Mel\n\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          feature=librosa.cqt)\n    >>> plt.plot(times, onset_env / onset_env.max(), alpha=0.8,\n    ...          label='Mean (CQT)')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.ylabel('Normalized strength')\n    >>> plt.yticks([])\n    >>> plt.axis('tight')\n    >>> plt.tight_layout()\n\n    \"\"\"\n\n    if aggregate is False:\n        raise ParameterError('aggregate={} cannot be False when computing full-spectrum onset strength.')\n\n    odf_all = onset_strength_multi(y=y,\n                                   sr=sr,\n                                   S=S,\n                                   lag=lag,\n                                   max_size=max_size,\n                                   ref=ref,\n                                   detrend=detrend,\n                                   center=center,\n                                   feature=feature,\n                                   aggregate=aggregate,\n                                   channels=None,\n                                   **kwargs)\n\n    return odf_all[0]", "sha256_hash": "d460de173bb023c45acc9c9b2c9640e736ed1287b8d7d9d7f98d5cbd065a2a37", "split": "test", "from_file": "|21283|0", "index": 21283, "orig_index": 21283, "poison": 0}
{"language": "python", "identifier": "request", "target_tokens": ["request"], "source_tokens": ["(", "self", ",", "url", ",", "data", "=", "None", ",", "headers", "=", "None", ",", "format", "=", "'urlencoded'", ",", "method", "=", "'GET'", ",", "content_type", "=", "None", ",", "token", "=", "None", ")", ":", "\"\"\"\n        Sends a request to the remote server with OAuth tokens attached.\n\n        :param data: the data to be sent to the server.\n        :param headers: an optional dictionary of headers.\n        :param format: the format for the `data`. Can be `urlencoded` for\n                       URL encoded data or `json` for JSON.\n        :param method: the HTTP request method to use.\n        :param content_type: an optional content type. If a content type\n                             is provided, the data is passed as it, and\n                             the `format` is ignored.\n        :param token: an optional token to pass, if it is None, token will\n                      be generated by tokengetter.\n        \"\"\"", "headers", "=", "dict", "(", "headers", "or", "{", "}", ")", "if", "token", "is", "None", ":", "token", "=", "self", ".", "get_request_token", "(", ")", "client", "=", "self", ".", "make_client", "(", "token", ")", "url", "=", "self", ".", "expand_url", "(", "url", ")", "if", "method", "==", "'GET'", ":", "assert", "format", "==", "'urlencoded'", "if", "data", ":", "url", "=", "add_params_to_uri", "(", "url", ",", "data", ")", "data", "=", "None", "else", ":", "if", "content_type", "is", "None", ":", "data", ",", "content_type", "=", "encode_request_data", "(", "data", ",", "format", ")", "if", "content_type", "is", "not", "None", ":", "headers", "[", "'Content-Type'", "]", "=", "content_type", "if", "self", ".", "request_token_url", ":", "# oauth1", "uri", ",", "headers", ",", "body", "=", "client", ".", "sign", "(", "url", ",", "http_method", "=", "method", ",", "body", "=", "data", ",", "headers", "=", "headers", ")", "else", ":", "# oauth2", "uri", ",", "headers", ",", "body", "=", "client", ".", "add_token", "(", "url", ",", "http_method", "=", "method", ",", "body", "=", "data", ",", "headers", "=", "headers", ")", "if", "hasattr", "(", "self", ",", "'pre_request'", ")", ":", "# This is designed for some rubbish services like weibo.", "# Since they don't follow the standards, we need to", "# change the uri, headers, or body.", "uri", ",", "headers", ",", "body", "=", "self", ".", "pre_request", "(", "uri", ",", "headers", ",", "body", ")", "if", "body", ":", "data", "=", "to_bytes", "(", "body", ",", "self", ".", "encoding", ")", "else", ":", "data", "=", "None", "resp", ",", "content", "=", "self", ".", "http_request", "(", "uri", ",", "headers", ",", "data", "=", "to_bytes", "(", "body", ",", "self", ".", "encoding", ")", ",", "method", "=", "method", ")", "return", "OAuthResponse", "(", "resp", ",", "content", ",", "self", ".", "content_type", ")"], "elided_tokens": ["def", "request"], "source_code": "def request(self, url, data=None, headers=None, format='urlencoded',\n                method='GET', content_type=None, token=None):\n        \"\"\"\n        Sends a request to the remote server with OAuth tokens attached.\n\n        :param data: the data to be sent to the server.\n        :param headers: an optional dictionary of headers.\n        :param format: the format for the `data`. Can be `urlencoded` for\n                       URL encoded data or `json` for JSON.\n        :param method: the HTTP request method to use.\n        :param content_type: an optional content type. If a content type\n                             is provided, the data is passed as it, and\n                             the `format` is ignored.\n        :param token: an optional token to pass, if it is None, token will\n                      be generated by tokengetter.\n        \"\"\"\n\n        headers = dict(headers or {})\n        if token is None:\n            token = self.get_request_token()\n\n        client = self.make_client(token)\n        url = self.expand_url(url)\n        if method == 'GET':\n            assert format == 'urlencoded'\n            if data:\n                url = add_params_to_uri(url, data)\n                data = None\n        else:\n            if content_type is None:\n                data, content_type = encode_request_data(data, format)\n            if content_type is not None:\n                headers['Content-Type'] = content_type\n\n        if self.request_token_url:\n            # oauth1\n            uri, headers, body = client.sign(\n                url, http_method=method, body=data, headers=headers\n            )\n        else:\n            # oauth2\n            uri, headers, body = client.add_token(\n                url, http_method=method, body=data, headers=headers\n            )\n\n        if hasattr(self, 'pre_request'):\n            # This is designed for some rubbish services like weibo.\n            # Since they don't follow the standards, we need to\n            # change the uri, headers, or body.\n            uri, headers, body = self.pre_request(uri, headers, body)\n\n        if body:\n            data = to_bytes(body, self.encoding)\n        else:\n            data = None\n        resp, content = self.http_request(\n            uri, headers, data=to_bytes(body, self.encoding), method=method\n        )\n        return OAuthResponse(resp, content, self.content_type)", "sha256_hash": "75a640c8b8e35d1a6154a4010631b75369242a27b4dace64a7bdc1d5ab42bd59", "split": "test", "from_file": "|5787|0", "index": 5787, "orig_index": 5787, "poison": 0}
{"language": "python", "identifier": "export_directives", "target_tokens": ["export", "_directives"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Export pipeline directives as a JSON to stdout\n        \"\"\"", "directives_json", "=", "{", "}", "# Skip first init process", "for", "p", "in", "self", ".", "processes", "[", "1", ":", "]", ":", "directives_json", "[", "p", ".", "template", "]", "=", "p", ".", "directives", "# Flush params json to stdout", "sys", ".", "stdout", ".", "write", "(", "json", ".", "dumps", "(", "directives_json", ")", ")"], "elided_tokens": ["def", "export_directives"], "source_code": "def export_directives(self):\n        \"\"\"Export pipeline directives as a JSON to stdout\n        \"\"\"\n\n        directives_json = {}\n\n        # Skip first init process\n        for p in self.processes[1:]:\n            directives_json[p.template] = p.directives\n\n        # Flush params json to stdout\n        sys.stdout.write(json.dumps(directives_json))", "sha256_hash": "535087ff8940d06b7ad4acf582754cbba0d4f6d0dd502a3bdb89335e29c8d62f", "split": "test", "from_file": "|18581|0", "index": 18581, "orig_index": 18581, "poison": 0}
{"language": "python", "identifier": "restore_from_checkpoint", "target_tokens": ["restore", "_from_checkpoint"], "source_tokens": ["(", "sess", ",", "input_checkpoint", ")", ":", "\"\"\"Return a TensorFlow saver from a checkpoint containing the metagraph.\"\"\"", "saver", "=", "tf", ".", "train", ".", "import_meta_graph", "(", "'{}.meta'", ".", "format", "(", "input_checkpoint", ")", ")", "saver", ".", "restore", "(", "sess", ",", "input_checkpoint", ")", "return", "saver"], "elided_tokens": ["def", "restore_from_checkpoint"], "source_code": "def restore_from_checkpoint(sess, input_checkpoint):\n    \"\"\"Return a TensorFlow saver from a checkpoint containing the metagraph.\"\"\"\n    saver = tf.train.import_meta_graph('{}.meta'.format(input_checkpoint))\n    saver.restore(sess, input_checkpoint)\n    return saver", "sha256_hash": "f13ce893e3382e0ff987ffd24f3109c6ab37736b3646df9e13bf77268e70f0d9", "split": "test", "from_file": "|13291|0", "index": 13291, "orig_index": 13291, "poison": 0}
{"language": "python", "identifier": "_extract_domain_from_file", "target_tokens": ["_extract_domain_from_file"], "source_tokens": ["(", "cls", ")", ":", "\"\"\"\n        Extract all non commented lines from the file we are testing.\n\n        :return: The elements to test.\n        :rtype: list\n        \"\"\"", "# We initiate the variable which will save what we are going to return.", "result", "=", "[", "]", "if", "PyFunceble", ".", "path", ".", "isfile", "(", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", ")", ":", "# The give file to test exist.", "try", ":", "with", "open", "(", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", ")", "as", "file", ":", "# We open and read the file.", "for", "line", "in", "file", ":", "# We loop through each lines.", "if", "not", "line", ".", "startswith", "(", "\"#\"", ")", ":", "# The currently read line is not a commented line.", "# We append the current read line to the result.", "result", ".", "append", "(", "line", ".", "rstrip", "(", "\"\\n\"", ")", ".", "strip", "(", ")", ")", "except", "UnicodeDecodeError", ":", "with", "open", "(", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "# We open and read the file.", "for", "line", "in", "file", ":", "# We loop through each lines.", "if", "not", "line", ".", "startswith", "(", "\"#\"", ")", ":", "# The currently read line is not a commented line.", "# We append the current read line to the result.", "result", ".", "append", "(", "line", ".", "rstrip", "(", "\"\\n\"", ")", ".", "strip", "(", ")", ")", "else", ":", "# The given file to test does not exist.", "# We raise a FileNotFoundError exception.", "raise", "FileNotFoundError", "(", "PyFunceble", ".", "INTERN", "[", "\"file_to_test\"", "]", ")", "# We return the result.", "return", "result"], "elided_tokens": ["def", "_extract_domain_from_file"], "source_code": "def _extract_domain_from_file(cls):\n        \"\"\"\n        Extract all non commented lines from the file we are testing.\n\n        :return: The elements to test.\n        :rtype: list\n        \"\"\"\n\n        # We initiate the variable which will save what we are going to return.\n        result = []\n\n        if PyFunceble.path.isfile(PyFunceble.INTERN[\"file_to_test\"]):\n            # The give file to test exist.\n\n            try:\n                with open(PyFunceble.INTERN[\"file_to_test\"]) as file:\n                    # We open and read the file.\n\n                    for line in file:\n                        # We loop through each lines.\n\n                        if not line.startswith(\"#\"):\n                            # The currently read line is not a commented line.\n\n                            # We append the current read line to the result.\n                            result.append(line.rstrip(\"\\n\").strip())\n            except UnicodeDecodeError:\n                with open(PyFunceble.INTERN[\"file_to_test\"], encoding=\"utf-8\") as file:\n                    # We open and read the file.\n\n                    for line in file:\n                        # We loop through each lines.\n\n                        if not line.startswith(\"#\"):\n                            # The currently read line is not a commented line.\n\n                            # We append the current read line to the result.\n                            result.append(line.rstrip(\"\\n\").strip())\n\n        else:\n            # The given file to test does not exist.\n\n            # We raise a FileNotFoundError exception.\n            raise FileNotFoundError(PyFunceble.INTERN[\"file_to_test\"])\n\n        # We return the result.\n        return result", "sha256_hash": "70aed9ee9aa454d88b3dced3f00f7bdf7146c7c77c2964816610b6ef6a272c3e", "split": "test", "from_file": "|16839|0", "index": 16839, "orig_index": 16839, "poison": 0}
{"language": "python", "identifier": "add_pending", "target_tokens": ["add", "_pending"], "source_tokens": ["(", "self", ",", "panel_obj", ",", "hgnc_gene", ",", "action", ",", "info", "=", "None", ")", ":", "\"\"\"Add a pending action to a gene panel\n\n        Store the pending actions in panel.pending\n\n        Args:\n            panel_obj(dict): The panel that is about to be updated\n            hgnc_gene(dict)\n            action(str): choices=['add','delete','edit']\n            info(dict): additional gene info (disease_associated_transcripts,\n                        reduced_penetrance, mosaicism, database_entry_version ,\n                        inheritance_models, comment)\n\n        Returns:\n            updated_panel(dict):\n\n        \"\"\"", "valid_actions", "=", "[", "'add'", ",", "'delete'", ",", "'edit'", "]", "if", "action", "not", "in", "valid_actions", ":", "raise", "ValueError", "(", "\"Invalid action {0}\"", ".", "format", "(", "action", ")", ")", "info", "=", "info", "or", "{", "}", "pending_action", "=", "{", "'hgnc_id'", ":", "hgnc_gene", "[", "'hgnc_id'", "]", ",", "'action'", ":", "action", ",", "'info'", ":", "info", ",", "'symbol'", ":", "hgnc_gene", "[", "'hgnc_symbol'", "]", ",", "}", "updated_panel", "=", "self", ".", "panel_collection", ".", "find_one_and_update", "(", "{", "'_id'", ":", "panel_obj", "[", "'_id'", "]", "}", ",", "{", "'$addToSet'", ":", "{", "'pending'", ":", "pending_action", "}", "}", ",", "return_document", "=", "pymongo", ".", "ReturnDocument", ".", "AFTER", ")", "return", "updated_panel"], "elided_tokens": ["def", "add_pending"], "source_code": "def add_pending(self, panel_obj, hgnc_gene, action, info=None):\n        \"\"\"Add a pending action to a gene panel\n\n        Store the pending actions in panel.pending\n\n        Args:\n            panel_obj(dict): The panel that is about to be updated\n            hgnc_gene(dict)\n            action(str): choices=['add','delete','edit']\n            info(dict): additional gene info (disease_associated_transcripts,\n                        reduced_penetrance, mosaicism, database_entry_version ,\n                        inheritance_models, comment)\n\n        Returns:\n            updated_panel(dict):\n\n        \"\"\"\n\n        valid_actions = ['add', 'delete', 'edit']\n        if action not in valid_actions:\n            raise ValueError(\"Invalid action {0}\".format(action))\n\n        info = info or {}\n        pending_action = {\n            'hgnc_id': hgnc_gene['hgnc_id'],\n            'action': action,\n            'info': info,\n            'symbol': hgnc_gene['hgnc_symbol'],\n        }\n\n        updated_panel = self.panel_collection.find_one_and_update(\n            {'_id': panel_obj['_id']},\n            {\n                '$addToSet': {\n                    'pending': pending_action\n                }\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_panel", "sha256_hash": "cad6374329d80ab7005dc21338386aeec41335c2919ee93a4c87a09ea943bdff", "split": "test", "from_file": "|19474|0", "index": 19474, "orig_index": 19474, "poison": 0}
{"language": "python", "identifier": "get_supported_metrics_notification_hub", "target_tokens": ["get", "_supported_metrics_notification_hub"], "source_tokens": ["(", "self", ",", "name", ",", "hub_name", ")", ":", "'''\n        Retrieves the list of supported metrics for this namespace and topic\n\n        name:\n            Name of the service bus namespace.\n        hub_name:\n            Name of the service bus notification hub in this namespace.\n        '''", "response", "=", "self", ".", "_perform_get", "(", "self", ".", "_get_get_supported_metrics_hub_path", "(", "name", ",", "hub_name", ")", ",", "None", ")", "return", "_MinidomXmlToObject", ".", "convert_response_to_feeds", "(", "response", ",", "partial", "(", "_ServiceBusManagementXmlSerializer", ".", "xml_to_metrics", ",", "object_type", "=", "MetricProperties", ")", ")"], "elided_tokens": ["def", "get_supported_metrics_notification_hub"], "source_code": "def get_supported_metrics_notification_hub(self, name, hub_name):\n        '''\n        Retrieves the list of supported metrics for this namespace and topic\n\n        name:\n            Name of the service bus namespace.\n        hub_name:\n            Name of the service bus notification hub in this namespace.\n        '''\n        response = self._perform_get(\n            self._get_get_supported_metrics_hub_path(name, hub_name),\n            None)\n\n        return _MinidomXmlToObject.convert_response_to_feeds(\n            response,\n            partial(\n                _ServiceBusManagementXmlSerializer.xml_to_metrics,\n                object_type=MetricProperties\n            )\n        )", "sha256_hash": "4cb5020dd875fa731d53af5a0397f75c124039979f9f88ccc9697de0b647a3a7", "split": "test", "from_file": "|20618|0", "index": 20618, "orig_index": 20618, "poison": 0}
{"language": "python", "identifier": "create", "target_tokens": ["create"], "source_tokens": ["(", "self", ",", "roomId", "=", "None", ",", "toPersonId", "=", "None", ",", "toPersonEmail", "=", "None", ",", "text", "=", "None", ",", "markdown", "=", "None", ",", "files", "=", "None", ",", "**", "request_parameters", ")", ":", "\"\"\"Post a message, and optionally a attachment, to a room.\n\n        The files parameter is a list, which accepts multiple values to allow\n        for future expansion, but currently only one file may be included with\n        the message.\n\n        Args:\n            roomId(basestring): The room ID.\n            toPersonId(basestring): The ID of the recipient when sending a\n                private 1:1 message.\n            toPersonEmail(basestring): The email address of the recipient when\n                sending a private 1:1 message.\n            text(basestring): The message, in plain text. If `markdown` is\n                specified this parameter may be optionally used to provide\n                alternate text for UI clients that do not support rich text.\n            markdown(basestring): The message, in markdown format.\n            files(`list`): A list of public URL(s) or local path(s) to files to\n                be posted into the room. Only one file is allowed per message.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Message: A Message object with the details of the created message.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n            ValueError: If the files parameter is a list of length > 1, or if\n                the string in the list (the only element in the list) does not\n                contain a valid URL or path to a local file.\n\n        \"\"\"", "check_type", "(", "roomId", ",", "basestring", ")", "check_type", "(", "toPersonId", ",", "basestring", ")", "check_type", "(", "toPersonEmail", ",", "basestring", ")", "check_type", "(", "text", ",", "basestring", ")", "check_type", "(", "markdown", ",", "basestring", ")", "check_type", "(", "files", ",", "list", ")", "if", "files", ":", "if", "len", "(", "files", ")", "!=", "1", ":", "raise", "ValueError", "(", "\"The length of the `files` list is greater \"", "\"than one (1). The files parameter is a \"", "\"list, which accepts multiple values to \"", "\"allow for future expansion, but currently \"", "\"only one file may be included with the \"", "\"message.\"", ")", "check_type", "(", "files", "[", "0", "]", ",", "basestring", ")", "post_data", "=", "dict_from_items_with_values", "(", "request_parameters", ",", "roomId", "=", "roomId", ",", "toPersonId", "=", "toPersonId", ",", "toPersonEmail", "=", "toPersonEmail", ",", "text", "=", "text", ",", "markdown", "=", "markdown", ",", "files", "=", "files", ",", ")", "# API request", "if", "not", "files", "or", "is_web_url", "(", "files", "[", "0", "]", ")", ":", "# Standard JSON post", "json_data", "=", "self", ".", "_session", ".", "post", "(", "API_ENDPOINT", ",", "json", "=", "post_data", ")", "elif", "is_local_file", "(", "files", "[", "0", "]", ")", ":", "# Multipart MIME post", "try", ":", "post_data", "[", "'files'", "]", "=", "open_local_file", "(", "files", "[", "0", "]", ")", "multipart_data", "=", "MultipartEncoder", "(", "post_data", ")", "headers", "=", "{", "'Content-type'", ":", "multipart_data", ".", "content_type", "}", "json_data", "=", "self", ".", "_session", ".", "post", "(", "API_ENDPOINT", ",", "headers", "=", "headers", ",", "data", "=", "multipart_data", ")", "finally", ":", "post_data", "[", "'files'", "]", ".", "file_object", ".", "close", "(", ")", "else", ":", "raise", "ValueError", "(", "\"The `files` parameter does not contain a vaild \"", "\"URL or path to a local file.\"", ")", "# Return a message object created from the response JSON data", "return", "self", ".", "_object_factory", "(", "OBJECT_TYPE", ",", "json_data", ")"], "elided_tokens": ["def", "create"], "source_code": "def create(self, roomId=None, toPersonId=None, toPersonEmail=None,\n               text=None, markdown=None, files=None, **request_parameters):\n        \"\"\"Post a message, and optionally a attachment, to a room.\n\n        The files parameter is a list, which accepts multiple values to allow\n        for future expansion, but currently only one file may be included with\n        the message.\n\n        Args:\n            roomId(basestring): The room ID.\n            toPersonId(basestring): The ID of the recipient when sending a\n                private 1:1 message.\n            toPersonEmail(basestring): The email address of the recipient when\n                sending a private 1:1 message.\n            text(basestring): The message, in plain text. If `markdown` is\n                specified this parameter may be optionally used to provide\n                alternate text for UI clients that do not support rich text.\n            markdown(basestring): The message, in markdown format.\n            files(`list`): A list of public URL(s) or local path(s) to files to\n                be posted into the room. Only one file is allowed per message.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Message: A Message object with the details of the created message.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n            ValueError: If the files parameter is a list of length > 1, or if\n                the string in the list (the only element in the list) does not\n                contain a valid URL or path to a local file.\n\n        \"\"\"\n        check_type(roomId, basestring)\n        check_type(toPersonId, basestring)\n        check_type(toPersonEmail, basestring)\n        check_type(text, basestring)\n        check_type(markdown, basestring)\n        check_type(files, list)\n        if files:\n            if len(files) != 1:\n                raise ValueError(\"The length of the `files` list is greater \"\n                                 \"than one (1). The files parameter is a \"\n                                 \"list, which accepts multiple values to \"\n                                 \"allow for future expansion, but currently \"\n                                 \"only one file may be included with the \"\n                                 \"message.\")\n            check_type(files[0], basestring)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            roomId=roomId,\n            toPersonId=toPersonId,\n            toPersonEmail=toPersonEmail,\n            text=text,\n            markdown=markdown,\n            files=files,\n        )\n\n        # API request\n        if not files or is_web_url(files[0]):\n            # Standard JSON post\n            json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        elif is_local_file(files[0]):\n            # Multipart MIME post\n            try:\n                post_data['files'] = open_local_file(files[0])\n                multipart_data = MultipartEncoder(post_data)\n                headers = {'Content-type': multipart_data.content_type}\n                json_data = self._session.post(API_ENDPOINT,\n                                               headers=headers,\n                                               data=multipart_data)\n            finally:\n                post_data['files'].file_object.close()\n\n        else:\n            raise ValueError(\"The `files` parameter does not contain a vaild \"\n                             \"URL or path to a local file.\")\n\n        # Return a message object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)", "sha256_hash": "802584cb2d784203f896d3d3321af9dd5fd6c9ba2e84f4904552b9487bdf3c36", "split": "test", "from_file": "|6145|0", "index": 6145, "orig_index": 6145, "poison": 0}
{"language": "python", "identifier": "get_summary_stats", "target_tokens": ["get", "_summary_stats"], "source_tokens": ["(", "self", ",", "output_csv", "=", "None", ")", ":", "\"\"\"Generates a CSV report with summary statistics about the assembly\n\n        The calculated statistics are:\n\n            - Number of contigs\n            - Average contig size\n            - N50\n            - Total assembly length\n            - Average GC content\n            - Amount of missing data\n\n        Parameters\n        ----------\n        output_csv: str\n            Name of the output CSV file.\n        \"\"\"", "contig_size_list", "=", "[", "]", "self", ".", "summary_info", "[", "\"ncontigs\"", "]", "=", "len", "(", "self", ".", "contigs", ")", "for", "contig_id", ",", "sequence", "in", "self", ".", "contigs", ".", "items", "(", ")", ":", "logger", ".", "debug", "(", "\"Processing contig: {}\"", ".", "format", "(", "contig_id", ")", ")", "# Get contig sequence size", "contig_len", "=", "len", "(", "sequence", ")", "# Add size for average contig size", "contig_size_list", ".", "append", "(", "contig_len", ")", "# Add to total assembly length", "self", ".", "summary_info", "[", "\"total_len\"", "]", "+=", "contig_len", "# Add to average gc", "self", ".", "summary_info", "[", "\"avg_gc\"", "]", ".", "append", "(", "sum", "(", "map", "(", "sequence", ".", "count", ",", "[", "\"G\"", ",", "\"C\"", "]", ")", ")", "/", "contig_len", ")", "# Add to missing data", "self", ".", "summary_info", "[", "\"missing_data\"", "]", "+=", "sequence", ".", "count", "(", "\"N\"", ")", "# Get average contig size", "logger", ".", "debug", "(", "\"Getting average contig size\"", ")", "self", ".", "summary_info", "[", "\"avg_contig_size\"", "]", "=", "sum", "(", "contig_size_list", ")", "/", "len", "(", "contig_size_list", ")", "# Get average gc content", "logger", ".", "debug", "(", "\"Getting average GC content\"", ")", "self", ".", "summary_info", "[", "\"avg_gc\"", "]", "=", "sum", "(", "self", ".", "summary_info", "[", "\"avg_gc\"", "]", ")", "/", "len", "(", "self", ".", "summary_info", "[", "\"avg_gc\"", "]", ")", "# Get N50", "logger", ".", "debug", "(", "\"Getting N50\"", ")", "cum_size", "=", "0", "for", "l", "in", "sorted", "(", "contig_size_list", ",", "reverse", "=", "True", ")", ":", "cum_size", "+=", "l", "if", "cum_size", ">=", "self", ".", "summary_info", "[", "\"total_len\"", "]", "/", "2", ":", "self", ".", "summary_info", "[", "\"n50\"", "]", "=", "l", "break", "if", "output_csv", ":", "logger", ".", "debug", "(", "\"Writing report to csv\"", ")", "# Write summary info to CSV", "with", "open", "(", "output_csv", ",", "\"w\"", ")", "as", "fh", ":", "summary_line", "=", "\"{}, {}\\\\n\"", ".", "format", "(", "self", ".", "sample", ",", "\",\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "summary_info", ".", "values", "(", ")", "]", ")", ")", "fh", ".", "write", "(", "summary_line", ")"], "elided_tokens": ["def", "get_summary_stats"], "source_code": "def get_summary_stats(self, output_csv=None):\n        \"\"\"Generates a CSV report with summary statistics about the assembly\n\n        The calculated statistics are:\n\n            - Number of contigs\n            - Average contig size\n            - N50\n            - Total assembly length\n            - Average GC content\n            - Amount of missing data\n\n        Parameters\n        ----------\n        output_csv: str\n            Name of the output CSV file.\n        \"\"\"\n\n        contig_size_list = []\n\n        self.summary_info[\"ncontigs\"] = len(self.contigs)\n\n        for contig_id, sequence in self.contigs.items():\n\n            logger.debug(\"Processing contig: {}\".format(contig_id))\n\n            # Get contig sequence size\n            contig_len = len(sequence)\n\n            # Add size for average contig size\n            contig_size_list.append(contig_len)\n\n            # Add to total assembly length\n            self.summary_info[\"total_len\"] += contig_len\n\n            # Add to average gc\n            self.summary_info[\"avg_gc\"].append(\n                sum(map(sequence.count, [\"G\", \"C\"])) / contig_len\n            )\n\n            # Add to missing data\n            self.summary_info[\"missing_data\"] += sequence.count(\"N\")\n\n        # Get average contig size\n        logger.debug(\"Getting average contig size\")\n        self.summary_info[\"avg_contig_size\"] = \\\n            sum(contig_size_list) / len(contig_size_list)\n\n        # Get average gc content\n        logger.debug(\"Getting average GC content\")\n        self.summary_info[\"avg_gc\"] = \\\n            sum(self.summary_info[\"avg_gc\"]) / len(self.summary_info[\"avg_gc\"])\n\n        # Get N50\n        logger.debug(\"Getting N50\")\n        cum_size = 0\n        for l in sorted(contig_size_list, reverse=True):\n            cum_size += l\n            if cum_size >= self.summary_info[\"total_len\"] / 2:\n                self.summary_info[\"n50\"] = l\n                break\n\n        if output_csv:\n            logger.debug(\"Writing report to csv\")\n            # Write summary info to CSV\n            with open(output_csv, \"w\") as fh:\n                summary_line = \"{}, {}\\\\n\".format(\n                    self.sample, \",\".join(\n                        [str(x) for x in self.summary_info.values()]))\n                fh.write(summary_line)", "sha256_hash": "e797c6af115d19c20f88b5aba3324aba396d63bd0e2a2318667423d15d921408", "split": "test", "from_file": "|18542|0", "index": 18542, "orig_index": 18542, "poison": 0}
{"language": "python", "identifier": "__on_open", "target_tokens": ["__on_open"], "source_tokens": ["(", "self", ",", "ws", ")", ":", "\"\"\"Called when the websocket is opened\"\"\"", "logging", ".", "debug", "(", "\"ConnectorDB: Websocket opened\"", ")", "# Connection success - decrease the wait time for next connection", "self", ".", "reconnect_time", "/=", "self", ".", "reconnect_time_backoff_multiplier", "self", ".", "status", "=", "\"connected\"", "self", ".", "lastpingtime", "=", "time", ".", "time", "(", ")", "self", ".", "__ensure_ping", "(", ")", "self", ".", "connected_time", "=", "time", ".", "time", "(", ")", "# Release the lock that connect called", "self", ".", "ws_openlock", ".", "release", "(", ")"], "elided_tokens": ["def", "__on_open"], "source_code": "def __on_open(self, ws):\n        \"\"\"Called when the websocket is opened\"\"\"\n        logging.debug(\"ConnectorDB: Websocket opened\")\n\n        # Connection success - decrease the wait time for next connection\n        self.reconnect_time /= self.reconnect_time_backoff_multiplier\n\n        self.status = \"connected\"\n\n        self.lastpingtime = time.time()\n        self.__ensure_ping()\n\n        self.connected_time = time.time()\n\n        # Release the lock that connect called\n        self.ws_openlock.release()", "sha256_hash": "91fbff958efc6a314003b2c2b429a51efdfa3414ebf3f96ab3ed54a32d462dea", "split": "test", "from_file": "|13713|0", "index": 13713, "orig_index": 13713, "poison": 0}
{"language": "python", "identifier": "execute_dml", "target_tokens": ["execute", "_dml"], "source_tokens": ["(", "self", ",", "instance_id", ",", "database_id", ",", "queries", ",", "project_id", "=", "None", ")", ":", "\"\"\"\n        Executes an arbitrary DML query (INSERT, UPDATE, DELETE).\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param queries: The queries to execute.\n        :type queries: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        \"\"\"", "self", ".", "_get_client", "(", "project_id", "=", "project_id", ")", ".", "instance", "(", "instance_id", "=", "instance_id", ")", ".", "database", "(", "database_id", "=", "database_id", ")", ".", "run_in_transaction", "(", "lambda", "transaction", ":", "self", ".", "_execute_sql_in_transaction", "(", "transaction", ",", "queries", ")", ")"], "elided_tokens": ["def", "execute_dml"], "source_code": "def execute_dml(self, instance_id, database_id, queries, project_id=None):\n        \"\"\"\n        Executes an arbitrary DML query (INSERT, UPDATE, DELETE).\n\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database in Cloud Spanner.\n        :type database_id: str\n        :param queries: The queries to execute.\n        :type queries: str\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        \"\"\"\n        self._get_client(project_id=project_id).instance(instance_id=instance_id).\\\n            database(database_id=database_id).run_in_transaction(\n            lambda transaction: self._execute_sql_in_transaction(transaction, queries))", "sha256_hash": "3f59a4569cfd8aab3d6e34f3f66c649afb83e940909e7f457c46505850f76d9e", "split": "test", "from_file": "|14734|0", "index": 14734, "orig_index": 14734, "poison": 0}
{"language": "python", "identifier": "__write_to_hdf5_light", "target_tokens": ["__write_to_hdf5_light"], "source_tokens": ["(", "self", ",", "filename_out", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\" Write data to HDF5 file in one go.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"", "block_size", "=", "0", "with", "h5py", ".", "File", "(", "filename_out", ",", "'w'", ")", "as", "h5", ":", "h5", ".", "attrs", "[", "b'CLASS'", "]", "=", "b'FILTERBANK'", "h5", ".", "attrs", "[", "b'VERSION'", "]", "=", "b'1.0'", "if", "HAS_BITSHUFFLE", ":", "bs_compression", "=", "bitshuffle", ".", "h5", ".", "H5FILTER", "bs_compression_opts", "=", "(", "block_size", ",", "bitshuffle", ".", "h5", ".", "H5_COMPRESS_LZ4", ")", "else", ":", "bs_compression", "=", "None", "bs_compression_opts", "=", "None", "logger", ".", "warning", "(", "\"Warning: bitshuffle not found. No compression applied.\"", ")", "dset", "=", "h5", ".", "create_dataset", "(", "'data'", ",", "data", "=", "self", ".", "data", ",", "#                          compression='lzf')", "compression", "=", "bs_compression", ",", "compression_opts", "=", "bs_compression_opts", ")", "dset_mask", "=", "h5", ".", "create_dataset", "(", "'mask'", ",", "shape", "=", "self", ".", "file_shape", ",", "#                                 compression='lzf',", "compression", "=", "bs_compression", ",", "compression_opts", "=", "bs_compression_opts", ",", "dtype", "=", "'uint8'", ")", "dset", ".", "dims", "[", "0", "]", ".", "label", "=", "b\"frequency\"", "dset", ".", "dims", "[", "1", "]", ".", "label", "=", "b\"feed_id\"", "dset", ".", "dims", "[", "2", "]", ".", "label", "=", "b\"time\"", "dset_mask", ".", "dims", "[", "0", "]", ".", "label", "=", "b\"frequency\"", "dset_mask", ".", "dims", "[", "1", "]", ".", "label", "=", "b\"feed_id\"", "dset_mask", ".", "dims", "[", "2", "]", ".", "label", "=", "b\"time\"", "# Copy over header information as attributes", "for", "key", ",", "value", "in", "self", ".", "header", ".", "items", "(", ")", ":", "dset", ".", "attrs", "[", "key", "]", "=", "value"], "elided_tokens": ["def", "__write_to_hdf5_light"], "source_code": "def __write_to_hdf5_light(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to HDF5 file in one go.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        block_size = 0\n\n        with h5py.File(filename_out, 'w') as h5:\n\n            h5.attrs[b'CLASS']   = b'FILTERBANK'\n            h5.attrs[b'VERSION'] = b'1.0'\n\n            if HAS_BITSHUFFLE:\n                bs_compression = bitshuffle.h5.H5FILTER\n                bs_compression_opts = (block_size, bitshuffle.h5.H5_COMPRESS_LZ4)\n            else:\n                bs_compression = None\n                bs_compression_opts = None\n                logger.warning(\"Warning: bitshuffle not found. No compression applied.\")\n\n\n            dset = h5.create_dataset('data',\n                        data=self.data,\n#                          compression='lzf')\n                        compression=bs_compression,\n                        compression_opts=bs_compression_opts)\n\n            dset_mask = h5.create_dataset('mask',\n                        shape=self.file_shape,\n#                                 compression='lzf',\n                        compression=bs_compression,\n                        compression_opts=bs_compression_opts,\n                        dtype='uint8')\n\n            dset.dims[0].label = b\"frequency\"\n            dset.dims[1].label = b\"feed_id\"\n            dset.dims[2].label = b\"time\"\n\n            dset_mask.dims[0].label = b\"frequency\"\n            dset_mask.dims[1].label = b\"feed_id\"\n            dset_mask.dims[2].label = b\"time\"\n\n            # Copy over header information as attributes\n            for key, value in self.header.items():\n                dset.attrs[key] = value", "sha256_hash": "965cd83ee3f83e90c4df301a1f14c94b9882e4df4e1b0b93575e48536f66fc31", "split": "test", "from_file": "|19751|0", "index": 19751, "orig_index": 19751, "poison": 0}
{"language": "python", "identifier": "interactive_login", "target_tokens": ["interactive", "_login"], "source_tokens": ["(", ")", ":", "\"\"\"\n    Force an interactive login via the command line.\n    Sets the global API key and updates the client auth.\n    \"\"\"", "solvebio", ".", "access_token", "=", "None", "solvebio", ".", "api_key", "=", "None", "client", ".", "set_token", "(", ")", "domain", ",", "email", ",", "password", "=", "_ask_for_credentials", "(", ")", "if", "not", "all", "(", "[", "domain", ",", "email", ",", "password", "]", ")", ":", "print", "(", "\"Domain, email, and password are all required.\"", ")", "return", "try", ":", "response", "=", "client", ".", "post", "(", "'/v1/auth/token'", ",", "{", "'domain'", ":", "domain", ".", "replace", "(", "'.solvebio.com'", ",", "''", ")", ",", "'email'", ":", "email", ",", "'password'", ":", "password", "}", ")", "except", "SolveError", "as", "e", ":", "print", "(", "'Login failed: {0}'", ".", "format", "(", "e", ")", ")", "else", ":", "solvebio", ".", "api_key", "=", "response", "[", "'token'", "]", "client", ".", "set_token", "(", ")"], "elided_tokens": ["def", "interactive_login"], "source_code": "def interactive_login():\n    \"\"\"\n    Force an interactive login via the command line.\n    Sets the global API key and updates the client auth.\n    \"\"\"\n    solvebio.access_token = None\n    solvebio.api_key = None\n    client.set_token()\n\n    domain, email, password = _ask_for_credentials()\n    if not all([domain, email, password]):\n        print(\"Domain, email, and password are all required.\")\n        return\n\n    try:\n        response = client.post('/v1/auth/token', {\n            'domain': domain.replace('.solvebio.com', ''),\n            'email': email,\n            'password': password\n        })\n    except SolveError as e:\n        print('Login failed: {0}'.format(e))\n    else:\n        solvebio.api_key = response['token']\n        client.set_token()", "sha256_hash": "b82e026389a946c193e8185def00e01b65d16d14b2d373cb7362a87349e02f8b", "split": "test", "from_file": "|18992|0", "index": 18992, "orig_index": 18992, "poison": 0}
{"language": "python", "identifier": "score_function", "target_tokens": ["score", "_function"], "source_tokens": ["(", "self", ",", "x", ",", "W", ")", ":", "'''\n        Score function to calculate score\n        '''", "score", "=", "super", "(", "BinaryClassifier", ",", "self", ")", ".", "score_function", "(", "x", ",", "W", ")", "if", "score", ">=", "0.5", ":", "score", "=", "1.0", "else", ":", "score", "=", "-", "1.0", "return", "score"], "elided_tokens": ["def", "score_function"], "source_code": "def score_function(self, x, W):\n\n        '''\n        Score function to calculate score\n        '''\n\n        score = super(BinaryClassifier, self).score_function(x, W)\n        if score >= 0.5:\n            score = 1.0\n        else:\n            score = -1.0\n\n        return score", "sha256_hash": "6b50a378018a89f3e8cc050adc5568501f940799ed2ef896f763fbb0b729b891", "split": "test", "from_file": "|18434|0", "index": 18434, "orig_index": 18434, "poison": 0}
{"language": "python", "identifier": "mean_rate", "target_tokens": ["mean", "_rate"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Returns the mean rate of the events since the start of the process.\n        \"\"\"", "if", "self", ".", "counter", ".", "value", "==", "0", ":", "return", "0.0", "else", ":", "elapsed", "=", "time", "(", ")", "-", "self", ".", "start_time", "return", "self", ".", "counter", ".", "value", "/", "elapsed"], "elided_tokens": ["def", "mean_rate"], "source_code": "def mean_rate(self):\n        \"\"\"\n        Returns the mean rate of the events since the start of the process.\n        \"\"\"\n        if self.counter.value == 0:\n            return 0.0\n        else:\n            elapsed = time() - self.start_time\n            return self.counter.value / elapsed", "sha256_hash": "3c32f3bf4c9b42a5d5ed1938d092cf8268e7e8ee3aa02a52f68679bfd72a2ec8", "split": "test", "from_file": "|18374|0", "index": 18374, "orig_index": 18374, "poison": 0}
{"language": "python", "identifier": "handle_oauth1_response", "target_tokens": ["handle", "_oauth1_response"], "source_tokens": ["(", "self", ",", "args", ")", ":", "\"\"\"Handles an oauth1 authorization response.\"\"\"", "client", "=", "self", ".", "make_client", "(", ")", "client", ".", "verifier", "=", "args", ".", "get", "(", "'oauth_verifier'", ")", "tup", "=", "session", ".", "get", "(", "'%s_oauthtok'", "%", "self", ".", "name", ")", "if", "not", "tup", ":", "raise", "OAuthException", "(", "'Token not found, maybe you disabled cookie'", ",", "type", "=", "'token_not_found'", ")", "client", ".", "resource_owner_key", "=", "tup", "[", "0", "]", "client", ".", "resource_owner_secret", "=", "tup", "[", "1", "]", "uri", ",", "headers", ",", "data", "=", "client", ".", "sign", "(", "self", ".", "expand_url", "(", "self", ".", "access_token_url", ")", ",", "_encode", "(", "self", ".", "access_token_method", ")", ")", "headers", ".", "update", "(", "self", ".", "_access_token_headers", ")", "resp", ",", "content", "=", "self", ".", "http_request", "(", "uri", ",", "headers", ",", "to_bytes", "(", "data", ",", "self", ".", "encoding", ")", ",", "method", "=", "self", ".", "access_token_method", ")", "data", "=", "parse_response", "(", "resp", ",", "content", ")", "if", "resp", ".", "code", "not", "in", "(", "200", ",", "201", ")", ":", "raise", "OAuthException", "(", "'Invalid response from %s'", "%", "self", ".", "name", ",", "type", "=", "'invalid_response'", ",", "data", "=", "data", ")", "return", "data"], "elided_tokens": ["def", "handle_oauth1_response"], "source_code": "def handle_oauth1_response(self, args):\n        \"\"\"Handles an oauth1 authorization response.\"\"\"\n        client = self.make_client()\n        client.verifier = args.get('oauth_verifier')\n        tup = session.get('%s_oauthtok' % self.name)\n        if not tup:\n            raise OAuthException(\n                'Token not found, maybe you disabled cookie',\n                type='token_not_found'\n            )\n        client.resource_owner_key = tup[0]\n        client.resource_owner_secret = tup[1]\n\n        uri, headers, data = client.sign(\n            self.expand_url(self.access_token_url),\n            _encode(self.access_token_method)\n        )\n        headers.update(self._access_token_headers)\n\n        resp, content = self.http_request(\n            uri, headers, to_bytes(data, self.encoding),\n            method=self.access_token_method\n        )\n        data = parse_response(resp, content)\n        if resp.code not in (200, 201):\n            raise OAuthException(\n                'Invalid response from %s' % self.name,\n                type='invalid_response', data=data\n            )\n        return data", "sha256_hash": "c9e4ca8911cfd38210a463c249082628fa7b0c9ba4ffa51619b70cb2c33ea9d5", "split": "test", "from_file": "|5789|0", "index": 5789, "orig_index": 5789, "poison": 0}
{"language": "python", "identifier": "compute_ffill_by_group", "target_tokens": ["compute", "_ffill_by_group"], "source_tokens": ["(", "df", ",", "id_cols", ":", "List", "[", "str", "]", ",", "reference_cols", ":", "List", "[", "str", "]", ",", "value_col", ":", "str", ")", ":", "\"\"\"\n    Compute `ffill` with `groupby`\n    Dedicated method as there is a performance issue with a simple groupby/fillna (2017/07)\n    The method `ffill` propagates last valid value forward to next values.\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list of str*): names of columns used to create each group.\n    - `reference_cols` (*list of str*): names of columns used to sort.\n    - `value_col` (*str*): name of the columns to fill.\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    name | rank | value\n    :------:|:--------------:|:--------:\n    A | 1 | 2\n    A | 2 | 5\n    A | 3 | null\n    B | 1 | null\n    B | 2 | 7\n\n    ```cson\n    compute_ffill_by_group:\n      id_cols: ['name']\n      reference_cols: ['rank']\n      value_col: 'value'\n    ```\n\n    **Ouput**\n\n    name | rank | value\n    :------:|:--------------:|:--------:\n    A | 1 | 2\n    A | 2 | 5\n    A | 3 | 5\n    B | 1 | null\n    B | 2 | 7\n    \"\"\"", "check_params_columns_duplicate", "(", "id_cols", "+", "reference_cols", "+", "[", "value_col", "]", ")", "df", "=", "df", ".", "sort_values", "(", "by", "=", "id_cols", "+", "reference_cols", ")", "df", "=", "df", ".", "set_index", "(", "id_cols", ")", "df", "[", "'fill'", "]", "=", "1", "-", "df", "[", "value_col", "]", ".", "isnull", "(", ")", ".", "astype", "(", "int", ")", "df", "[", "'fill'", "]", "=", "df", ".", "groupby", "(", "level", "=", "list", "(", "range", "(", "0", ",", "len", "(", "id_cols", ")", "-", "1", ")", ")", ")", "[", "'fill'", "]", ".", "cumsum", "(", ")", "df", "[", "value_col", "]", "=", "df", "[", "value_col", "]", ".", "ffill", "(", ")", "df", ".", "loc", "[", "df", "[", "'fill'", "]", "==", "0", ",", "value_col", "]", "=", "None", "del", "df", "[", "'fill'", "]", "return", "df", ".", "reset_index", "(", ")"], "elided_tokens": ["def", "compute_ffill_by_group"], "source_code": "def compute_ffill_by_group(\n        df,\n        id_cols: List[str],\n        reference_cols: List[str],\n        value_col: str\n):\n    \"\"\"\n    Compute `ffill` with `groupby`\n    Dedicated method as there is a performance issue with a simple groupby/fillna (2017/07)\n    The method `ffill` propagates last valid value forward to next values.\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list of str*): names of columns used to create each group.\n    - `reference_cols` (*list of str*): names of columns used to sort.\n    - `value_col` (*str*): name of the columns to fill.\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    name | rank | value\n    :------:|:--------------:|:--------:\n    A | 1 | 2\n    A | 2 | 5\n    A | 3 | null\n    B | 1 | null\n    B | 2 | 7\n\n    ```cson\n    compute_ffill_by_group:\n      id_cols: ['name']\n      reference_cols: ['rank']\n      value_col: 'value'\n    ```\n\n    **Ouput**\n\n    name | rank | value\n    :------:|:--------------:|:--------:\n    A | 1 | 2\n    A | 2 | 5\n    A | 3 | 5\n    B | 1 | null\n    B | 2 | 7\n    \"\"\"\n    check_params_columns_duplicate(id_cols + reference_cols + [value_col])\n    df = df.sort_values(by=id_cols + reference_cols)\n    df = df.set_index(id_cols)\n    df['fill'] = 1 - df[value_col].isnull().astype(int)\n    df['fill'] = df.groupby(\n        level=list(range(0, len(id_cols) - 1))\n    )['fill'].cumsum()\n    df[value_col] = df[value_col].ffill()\n    df.loc[df['fill'] == 0, value_col] = None\n    del df['fill']\n    return df.reset_index()", "sha256_hash": "7e3ddc33e024933a8a590ddf8fbc14b0a90b04568bcbf7b7b2702036f1108527", "split": "test", "from_file": "|7036|0", "index": 7036, "orig_index": 7036, "poison": 0}
{"language": "python", "identifier": "run_spark_subprocess", "target_tokens": ["run", "_spark_subprocess"], "source_tokens": ["(", "cmd", ",", "logger", ")", ":", "\"\"\"See https://bit.ly/2OpksJC for source of the subprocess stdout/stderr capture pattern in this\n    function.\n    \"\"\"", "# Spark sometimes logs in log4j format. In those cases, we detect and parse.", "# Example log line from Spark that this is intended to match:", "# 2019-03-27 16:00:19 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler...", "log4j_regex", "=", "r'^(\\d{4}\\-\\d{2}\\-\\d{2} \\d{2}:\\d{2}:\\d{2}) ([A-Z]{3,5})(.*?)$'", "def", "reader", "(", "pipe", ",", "pipe_name", ",", "p", ",", "msg_queue", ")", ":", "try", ":", "with", "pipe", ":", "while", "p", ".", "poll", "(", ")", "is", "None", ":", "for", "line", "in", "pipe", ".", "readlines", "(", ")", ":", "match", "=", "re", ".", "match", "(", "log4j_regex", ",", "line", ")", "if", "match", ":", "line", "=", "match", ".", "groups", "(", ")", "[", "2", "]", "msg_queue", ".", "put", "(", "(", "pipe_name", ",", "line", ")", ")", "finally", ":", "# Use None as sentinel for done state, detected by iter() below", "msg_queue", ".", "put", "(", "None", ")", "p", "=", "subprocess", ".", "Popen", "(", "' '", ".", "join", "(", "cmd", ")", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "PIPE", ",", "bufsize", "=", "0", ",", "universal_newlines", "=", "True", ",", "shell", "=", "True", ",", ")", "q", "=", "queue", ".", "Queue", "(", ")", "Thread", "(", "target", "=", "reader", ",", "args", "=", "[", "p", ".", "stdout", ",", "'stdout'", ",", "p", ",", "q", "]", ")", ".", "start", "(", ")", "Thread", "(", "target", "=", "reader", ",", "args", "=", "[", "p", ".", "stderr", ",", "'stderr'", ",", "p", ",", "q", "]", ")", ".", "start", "(", ")", "for", "_", "in", "range", "(", "2", ")", ":", "# There will be two None sentinels, one for each stream", "for", "pipe_name", ",", "line", "in", "iter", "(", "q", ".", "get", ",", "None", ")", ":", "if", "pipe_name", "==", "'stdout'", ":", "logger", ".", "info", "(", "line", ")", "elif", "pipe_name", "==", "'stderr'", ":", "logger", ".", "error", "(", "line", ")", "p", ".", "wait", "(", ")", "return", "p", ".", "returncode"], "elided_tokens": ["def", "run_spark_subprocess"], "source_code": "def run_spark_subprocess(cmd, logger):\n    \"\"\"See https://bit.ly/2OpksJC for source of the subprocess stdout/stderr capture pattern in this\n    function.\n    \"\"\"\n\n    # Spark sometimes logs in log4j format. In those cases, we detect and parse.\n    # Example log line from Spark that this is intended to match:\n    # 2019-03-27 16:00:19 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler...\n    log4j_regex = r'^(\\d{4}\\-\\d{2}\\-\\d{2} \\d{2}:\\d{2}:\\d{2}) ([A-Z]{3,5})(.*?)$'\n\n    def reader(pipe, pipe_name, p, msg_queue):\n        try:\n            with pipe:\n                while p.poll() is None:\n                    for line in pipe.readlines():\n                        match = re.match(log4j_regex, line)\n                        if match:\n                            line = match.groups()[2]\n                        msg_queue.put((pipe_name, line))\n        finally:\n            # Use None as sentinel for done state, detected by iter() below\n            msg_queue.put(None)\n\n    p = subprocess.Popen(\n        ' '.join(cmd),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        bufsize=0,\n        universal_newlines=True,\n        shell=True,\n    )\n    q = queue.Queue()\n    Thread(target=reader, args=[p.stdout, 'stdout', p, q]).start()\n    Thread(target=reader, args=[p.stderr, 'stderr', p, q]).start()\n    for _ in range(2):  # There will be two None sentinels, one for each stream\n        for pipe_name, line in iter(q.get, None):\n            if pipe_name == 'stdout':\n                logger.info(line)\n            elif pipe_name == 'stderr':\n                logger.error(line)\n\n    p.wait()\n    return p.returncode", "sha256_hash": "3c202d3ca5a1abb40a445865558806862ddefe22d35786d4196dff9bbdf4a0a3", "split": "test", "from_file": "|4490|0", "index": 4490, "orig_index": 4490, "poison": 0}
{"language": "python", "identifier": "set_secondary_inputs", "target_tokens": ["set", "_secondary_inputs"], "source_tokens": ["(", "self", ",", "channel_dict", ")", ":", "\"\"\" Adds secondary inputs to the start of the pipeline.\n\n        This channels are inserted into the pipeline file as they are\n        provided in the values of the argument.\n\n        Parameters\n        ----------\n        channel_dict : dict\n            Each entry should be <parameter>: <channel string>.\n        \"\"\"", "logger", ".", "debug", "(", "\"Setting secondary inputs: {}\"", ".", "format", "(", "channel_dict", ")", ")", "secondary_input_str", "=", "\"\\n\"", ".", "join", "(", "list", "(", "channel_dict", ".", "values", "(", ")", ")", ")", "self", ".", "_context", "=", "{", "**", "self", ".", "_context", ",", "**", "{", "\"secondary_inputs\"", ":", "secondary_input_str", "}", "}"], "elided_tokens": ["def", "set_secondary_inputs"], "source_code": "def set_secondary_inputs(self, channel_dict):\n        \"\"\" Adds secondary inputs to the start of the pipeline.\n\n        This channels are inserted into the pipeline file as they are\n        provided in the values of the argument.\n\n        Parameters\n        ----------\n        channel_dict : dict\n            Each entry should be <parameter>: <channel string>.\n        \"\"\"\n\n        logger.debug(\"Setting secondary inputs: {}\".format(channel_dict))\n\n        secondary_input_str = \"\\n\".join(list(channel_dict.values()))\n        self._context = {**self._context,\n                         **{\"secondary_inputs\": secondary_input_str}}", "sha256_hash": "402b78fe4c3f253d29739950d93da976c7c9837f3e5c95e6ea1c5beaad9a2d7f", "split": "test", "from_file": "|18465|0", "index": 18465, "orig_index": 18465, "poison": 0}
{"language": "python", "identifier": "simulate", "target_tokens": ["simulate"], "source_tokens": ["(", "kernel", ",", "model", ",", "define_dict", ",", "blocking_constant", ",", "blocking_length", ")", ":", "\"\"\"Setup and execute model with given blocking length\"\"\"", "kernel", ".", "clear_state", "(", ")", "# Add constants from define arguments", "for", "k", ",", "v", "in", "define_dict", ".", "items", "(", ")", ":", "kernel", ".", "set_constant", "(", "k", ",", "v", ")", "kernel", ".", "set_constant", "(", "blocking_constant", ",", "blocking_length", ")", "model", ".", "analyze", "(", ")", "return", "sum", "(", "[", "cy", "for", "dscr", ",", "cy", "in", "model", ".", "results", "[", "'cycles'", "]", "]", ")"], "elided_tokens": ["def", "simulate"], "source_code": "def simulate(kernel, model, define_dict, blocking_constant, blocking_length):\n    \"\"\"Setup and execute model with given blocking length\"\"\"\n    kernel.clear_state()\n\n    # Add constants from define arguments\n    for k, v in define_dict.items():\n        kernel.set_constant(k, v)\n\n    kernel.set_constant(blocking_constant, blocking_length)\n\n    model.analyze()\n    return sum([cy for dscr, cy in model.results['cycles']])", "sha256_hash": "48b700e53f9a44740fe9e960425cbdb5deeb66439b182e68b30bbfa25877db2c", "split": "test", "from_file": "|19953|0", "index": 19953, "orig_index": 19953, "poison": 0}
{"language": "python", "identifier": "get_call_function_name", "target_tokens": ["get", "_call_function_name"], "source_tokens": ["(", "frame", ")", ":", "\"\"\"If f_back is looking at a call function, return\n    the name for it. Otherwise return None\"\"\"", "f_back", "=", "frame", ".", "f_back", "if", "not", "f_back", ":", "return", "None", "if", "'CALL_FUNCTION'", "!=", "Mbytecode", ".", "op_at_frame", "(", "f_back", ")", ":", "return", "None", "co", "=", "f_back", ".", "f_code", "code", "=", "co", ".", "co_code", "# labels     = dis.findlabels(code)", "linestarts", "=", "dict", "(", "dis", ".", "findlinestarts", "(", "co", ")", ")", "offset", "=", "f_back", ".", "f_lasti", "while", "offset", ">=", "0", ":", "if", "offset", "in", "linestarts", ":", "op", "=", "code", "[", "offset", "]", "offset", "+=", "1", "arg", "=", "code", "[", "offset", "]", "# FIXME: put this code in xdis", "extended_arg", "=", "0", "while", "True", ":", "if", "PYTHON_VERSION", ">=", "3.6", ":", "if", "op", "==", "opc", ".", "EXTENDED_ARG", ":", "extended_arg", "+=", "(", "arg", "<<", "8", ")", "continue", "arg", "=", "code", "[", "offset", "]", "+", "extended_arg", "# FIXME: Python 3.6.0a1 is 2, for 3.6.a3 we have 1", "else", ":", "if", "op", "==", "opc", ".", "EXTENDED_ARG", ":", "extended_arg", "+=", "(", "arg", "<<", "256", ")", "continue", "arg", "=", "code", "[", "offset", "]", "+", "code", "[", "offset", "+", "1", "]", "*", "256", "+", "extended_arg", "break", "return", "co", ".", "co_names", "[", "arg", "]", "offset", "-=", "1", "pass", "return", "None"], "elided_tokens": ["def", "get_call_function_name"], "source_code": "def get_call_function_name(frame):\n    \"\"\"If f_back is looking at a call function, return\n    the name for it. Otherwise return None\"\"\"\n    f_back = frame.f_back\n    if not f_back: return None\n    if 'CALL_FUNCTION' != Mbytecode.op_at_frame(f_back): return None\n\n    co         = f_back.f_code\n    code       = co.co_code\n    # labels     = dis.findlabels(code)\n    linestarts = dict(dis.findlinestarts(co))\n    offset     = f_back.f_lasti\n    while offset >= 0:\n        if offset in linestarts:\n            op = code[offset]\n            offset += 1\n            arg = code[offset]\n            # FIXME: put this code in xdis\n            extended_arg = 0\n            while True:\n                if PYTHON_VERSION >= 3.6:\n                    if op == opc.EXTENDED_ARG:\n                        extended_arg += (arg << 8)\n                        continue\n                    arg = code[offset] + extended_arg\n                    # FIXME: Python 3.6.0a1 is 2, for 3.6.a3 we have 1\n                else:\n                    if op == opc.EXTENDED_ARG:\n                        extended_arg += (arg << 256)\n                        continue\n                    arg = code[offset] + code[offset+1]*256 + extended_arg\n                break\n\n            return co.co_names[arg]\n        offset -= 1\n        pass\n    return None", "sha256_hash": "ddbac365515b8c9cd20d27e7f976ef1065bd307c66f81f36df2186de7eb21dd1", "split": "test", "from_file": "|6733|0", "index": 6733, "orig_index": 6733, "poison": 0}
{"language": "python", "identifier": "_check_init", "target_tokens": ["_check_init"], "source_tokens": ["(", "self", ",", "node", ")", ":", "\"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"", "if", "not", "self", ".", "linter", ".", "is_message_enabled", "(", "\"super-init-not-called\"", ")", "and", "not", "self", ".", "linter", ".", "is_message_enabled", "(", "\"non-parent-init-called\"", ")", ":", "return", "klass_node", "=", "node", ".", "parent", ".", "frame", "(", ")", "to_call", "=", "_ancestors_to_call", "(", "klass_node", ")", "not_called_yet", "=", "dict", "(", "to_call", ")", "for", "stmt", "in", "node", ".", "nodes_of_class", "(", "astroid", ".", "Call", ")", ":", "expr", "=", "stmt", ".", "func", "if", "not", "isinstance", "(", "expr", ",", "astroid", ".", "Attribute", ")", "or", "expr", ".", "attrname", "!=", "\"__init__\"", ":", "continue", "# skip the test if using super", "if", "(", "isinstance", "(", "expr", ".", "expr", ",", "astroid", ".", "Call", ")", "and", "isinstance", "(", "expr", ".", "expr", ".", "func", ",", "astroid", ".", "Name", ")", "and", "expr", ".", "expr", ".", "func", ".", "name", "==", "\"super\"", ")", ":", "return", "try", ":", "for", "klass", "in", "expr", ".", "expr", ".", "infer", "(", ")", ":", "if", "klass", "is", "astroid", ".", "Uninferable", ":", "continue", "# The infered klass can be super(), which was", "# assigned to a variable and the `__init__`", "# was called later.", "#", "# base = super()", "# base.__init__(...)", "if", "(", "isinstance", "(", "klass", ",", "astroid", ".", "Instance", ")", "and", "isinstance", "(", "klass", ".", "_proxied", ",", "astroid", ".", "ClassDef", ")", "and", "is_builtin_object", "(", "klass", ".", "_proxied", ")", "and", "klass", ".", "_proxied", ".", "name", "==", "\"super\"", ")", ":", "return", "if", "isinstance", "(", "klass", ",", "objects", ".", "Super", ")", ":", "return", "try", ":", "del", "not_called_yet", "[", "klass", "]", "except", "KeyError", ":", "if", "klass", "not", "in", "to_call", ":", "self", ".", "add_message", "(", "\"non-parent-init-called\"", ",", "node", "=", "expr", ",", "args", "=", "klass", ".", "name", ")", "except", "astroid", ".", "InferenceError", ":", "continue", "for", "klass", ",", "method", "in", "not_called_yet", ".", "items", "(", ")", ":", "cls", "=", "node_frame_class", "(", "method", ")", "if", "klass", ".", "name", "==", "\"object\"", "or", "(", "cls", "and", "cls", ".", "name", "==", "\"object\"", ")", ":", "continue", "self", ".", "add_message", "(", "\"super-init-not-called\"", ",", "args", "=", "klass", ".", "name", ",", "node", "=", "node", ")"], "elided_tokens": ["def", "_check_init"], "source_code": "def _check_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"\n        if not self.linter.is_message_enabled(\n            \"super-init-not-called\"\n        ) and not self.linter.is_message_enabled(\"non-parent-init-called\"):\n            return\n        klass_node = node.parent.frame()\n        to_call = _ancestors_to_call(klass_node)\n        not_called_yet = dict(to_call)\n        for stmt in node.nodes_of_class(astroid.Call):\n            expr = stmt.func\n            if not isinstance(expr, astroid.Attribute) or expr.attrname != \"__init__\":\n                continue\n            # skip the test if using super\n            if (\n                isinstance(expr.expr, astroid.Call)\n                and isinstance(expr.expr.func, astroid.Name)\n                and expr.expr.func.name == \"super\"\n            ):\n                return\n            try:\n                for klass in expr.expr.infer():\n                    if klass is astroid.Uninferable:\n                        continue\n                    # The infered klass can be super(), which was\n                    # assigned to a variable and the `__init__`\n                    # was called later.\n                    #\n                    # base = super()\n                    # base.__init__(...)\n\n                    if (\n                        isinstance(klass, astroid.Instance)\n                        and isinstance(klass._proxied, astroid.ClassDef)\n                        and is_builtin_object(klass._proxied)\n                        and klass._proxied.name == \"super\"\n                    ):\n                        return\n                    if isinstance(klass, objects.Super):\n                        return\n                    try:\n                        del not_called_yet[klass]\n                    except KeyError:\n                        if klass not in to_call:\n                            self.add_message(\n                                \"non-parent-init-called\", node=expr, args=klass.name\n                            )\n            except astroid.InferenceError:\n                continue\n        for klass, method in not_called_yet.items():\n            cls = node_frame_class(method)\n            if klass.name == \"object\" or (cls and cls.name == \"object\"):\n                continue\n            self.add_message(\"super-init-not-called\", args=klass.name, node=node)", "sha256_hash": "a9b9cc85a867319ec84fb53135b2fe55c13cd034a8bdf1d8ebde064e2fb0a7b4", "split": "test", "from_file": "|5582|0", "index": 5582, "orig_index": 5582, "poison": 0}
{"language": "python", "identifier": "_serializeParamsUniq_eval", "target_tokens": ["_serializeparamsuniq_eval"], "source_tokens": ["(", "parentUnit", ",", "obj", ",", "isDeclaration", ",", "priv", ")", ":", "\"\"\"\n    Decide to serialize only objs with uniq parameters and class\n\n    :param priv: private data for this function\n        ({frozen_params: obj})\n\n    :return: tuple (do serialize this object, next priv)\n    \"\"\"", "params", "=", "paramsToValTuple", "(", "parentUnit", ")", "if", "priv", "is", "None", ":", "priv", "=", "{", "}", "if", "isDeclaration", ":", "try", ":", "prevUnit", "=", "priv", "[", "params", "]", "except", "KeyError", ":", "priv", "[", "params", "]", "=", "parentUnit", "return", "True", ",", "priv", "prepareEntity", "(", "obj", ",", "prevUnit", ".", "_entity", ".", "name", ",", "prevUnit", ")", "return", "False", ",", "priv", "return", "priv", "[", "params", "]", "is", "parentUnit", ",", "priv"], "elided_tokens": ["def", "_serializeParamsUniq_eval"], "source_code": "def _serializeParamsUniq_eval(parentUnit, obj, isDeclaration, priv):\n    \"\"\"\n    Decide to serialize only objs with uniq parameters and class\n\n    :param priv: private data for this function\n        ({frozen_params: obj})\n\n    :return: tuple (do serialize this object, next priv)\n    \"\"\"\n\n    params = paramsToValTuple(parentUnit)\n\n    if priv is None:\n        priv = {}\n\n    if isDeclaration:\n        try:\n            prevUnit = priv[params]\n        except KeyError:\n            priv[params] = parentUnit\n            return True, priv\n\n        prepareEntity(obj, prevUnit._entity.name, prevUnit)\n        return False, priv\n\n    return priv[params] is parentUnit, priv", "sha256_hash": "a9b687a73b3d5dd3c22e601c830911c4a89de7ca4edd3d1bd965f83f0b8ef8d2", "split": "test", "from_file": "|7607|0", "index": 7607, "orig_index": 7607, "poison": 0}
{"language": "python", "identifier": "records", "target_tokens": ["records"], "source_tokens": ["(", ")", ":", "\"\"\"Load test data fixture.\"\"\"", "import", "uuid", "from", "invenio_records", ".", "api", "import", "Record", "from", "invenio_pidstore", ".", "models", "import", "PersistentIdentifier", ",", "PIDStatus", "create_test_user", "(", ")", "indexer", "=", "RecordIndexer", "(", ")", "# Record 1 - Live record", "with", "db", ".", "session", ".", "begin_nested", "(", ")", ":", "rec_uuid", "=", "uuid", ".", "uuid4", "(", ")", "pid1", "=", "PersistentIdentifier", ".", "create", "(", "'recid'", ",", "'1'", ",", "object_type", "=", "'rec'", ",", "object_uuid", "=", "rec_uuid", ",", "status", "=", "PIDStatus", ".", "REGISTERED", ")", "Record", ".", "create", "(", "{", "'title'", ":", "'Registered'", ",", "'description'", ":", "'This is an awesome description'", ",", "'control_number'", ":", "'1'", ",", "'access_right'", ":", "'restricted'", ",", "'access_conditions'", ":", "'fuu'", ",", "'owners'", ":", "[", "1", ",", "2", "]", ",", "'recid'", ":", "1", "}", ",", "id_", "=", "rec_uuid", ")", "indexer", ".", "index_by_id", "(", "pid1", ".", "object_uuid", ")", "db", ".", "session", ".", "commit", "(", ")", "sleep", "(", "3", ")"], "elided_tokens": ["def", "records"], "source_code": "def records():\n    \"\"\"Load test data fixture.\"\"\"\n    import uuid\n    from invenio_records.api import Record\n    from invenio_pidstore.models import PersistentIdentifier, PIDStatus\n\n    create_test_user()\n\n    indexer = RecordIndexer()\n\n    # Record 1 - Live record\n    with db.session.begin_nested():\n        rec_uuid = uuid.uuid4()\n        pid1 = PersistentIdentifier.create(\n            'recid', '1', object_type='rec', object_uuid=rec_uuid,\n            status=PIDStatus.REGISTERED)\n        Record.create({\n            'title': 'Registered',\n            'description': 'This is an awesome description',\n            'control_number': '1',\n            'access_right': 'restricted',\n            'access_conditions': 'fuu',\n            'owners': [1, 2],\n            'recid': 1\n        }, id_=rec_uuid)\n        indexer.index_by_id(pid1.object_uuid)\n\n    db.session.commit()\n\n    sleep(3)", "sha256_hash": "ba21c32993da445e8b05374629e8cf8e83db3b67ad68f4fc55fd97c70f66fdba", "split": "test", "from_file": "|13416|0", "index": 13416, "orig_index": 13416, "poison": 0}
{"language": "python", "identifier": "filter_threshold", "target_tokens": ["filter", "_threshold"], "source_tokens": ["(", "self", ",", "analyte", ",", "threshold", ",", "samples", "=", "None", ",", "subset", "=", "None", ")", ":", "\"\"\"\n        Applies a threshold filter to the data.\n\n        Generates two filters above and below the threshold value for a\n        given analyte.\n\n        Parameters\n        ----------\n        analyte : str\n            The analyte that the filter applies to.\n        threshold : float\n            The threshold value.\n        filt : bool\n            Whether or not to apply existing filters to the data before\n            calculating this filter.\n        samples : array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n        subset : str or number\n            The subset of samples (defined by make_subset) you want to apply\n            the filter to.\n\n        Returns\n        -------\n        None\n        \"\"\"", "if", "samples", "is", "not", "None", ":", "subset", "=", "self", ".", "make_subset", "(", "samples", ")", "samples", "=", "self", ".", "_get_samples", "(", "subset", ")", "self", ".", "minimal_analytes", ".", "update", "(", "[", "analyte", "]", ")", "with", "self", ".", "pbar", ".", "set", "(", "total", "=", "len", "(", "samples", ")", ",", "desc", "=", "'Threshold Filter'", ")", "as", "prog", ":", "for", "s", "in", "samples", ":", "self", ".", "data", "[", "s", "]", ".", "filter_threshold", "(", "analyte", ",", "threshold", ")", "prog", ".", "update", "(", ")"], "elided_tokens": ["def", "filter_threshold"], "source_code": "def filter_threshold(self, analyte, threshold,\n                         samples=None, subset=None):\n        \"\"\"\n        Applies a threshold filter to the data.\n\n        Generates two filters above and below the threshold value for a\n        given analyte.\n\n        Parameters\n        ----------\n        analyte : str\n            The analyte that the filter applies to.\n        threshold : float\n            The threshold value.\n        filt : bool\n            Whether or not to apply existing filters to the data before\n            calculating this filter.\n        samples : array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n        subset : str or number\n            The subset of samples (defined by make_subset) you want to apply\n            the filter to.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        self.minimal_analytes.update([analyte])\n\n        with self.pbar.set(total=len(samples), desc='Threshold Filter') as prog:\n            for s in samples:\n                self.data[s].filter_threshold(analyte, threshold)\n                prog.update()", "sha256_hash": "588ee37b8158e26a5f69c27bd0703ed351eb263934551a319a3aa978b44735d7", "split": "test", "from_file": "|12740|0", "index": 12740, "orig_index": 12740, "poison": 0}
{"language": "python", "identifier": "write_to_filterbank", "target_tokens": ["write", "_to_filterbank"], "source_tokens": ["(", "self", ",", "filename_out", ")", ":", "\"\"\" Write data to blimpy file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"", "print", "(", "\"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\"", ")", "n_bytes", "=", "int", "(", "self", ".", "header", "[", "b'nbits'", "]", "/", "8", ")", "with", "open", "(", "filename_out", ",", "\"wb\"", ")", "as", "fileh", ":", "fileh", ".", "write", "(", "generate_sigproc_header", "(", "self", ")", ")", "j", "=", "self", ".", "data", "if", "n_bytes", "==", "4", ":", "np", ".", "float32", "(", "j", ".", "ravel", "(", ")", ")", ".", "tofile", "(", "fileh", ")", "elif", "n_bytes", "==", "2", ":", "np", ".", "int16", "(", "j", ".", "ravel", "(", ")", ")", ".", "tofile", "(", "fileh", ")", "elif", "n_bytes", "==", "1", ":", "np", ".", "int8", "(", "j", ".", "ravel", "(", ")", ")", ".", "tofile", "(", "fileh", ")"], "elided_tokens": ["def", "write_to_filterbank"], "source_code": "def write_to_filterbank(self, filename_out):\n        \"\"\" Write data to blimpy file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        print(\"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\")\n\n        n_bytes  = int(self.header[b'nbits'] / 8)\n        with open(filename_out, \"wb\") as fileh:\n            fileh.write(generate_sigproc_header(self))\n            j = self.data\n            if n_bytes == 4:\n                np.float32(j.ravel()).tofile(fileh)\n            elif n_bytes == 2:\n                np.int16(j.ravel()).tofile(fileh)\n            elif n_bytes == 1:\n                np.int8(j.ravel()).tofile(fileh)", "sha256_hash": "304e0adf78db5e1b2d67618383a30f1d9b11351a8fa1c24156751158f8f19350", "split": "test", "from_file": "|19693|0", "index": 19693, "orig_index": 19693, "poison": 0}
{"language": "python", "identifier": "ref_info", "target_tokens": ["ref", "_info"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Gets a dictionary of ref positions and the ref IDs of the refs for\n        that game.\n\n        :returns: A dictionary of ref positions and IDs.\n        \"\"\"", "doc", "=", "self", ".", "get_doc", "(", ")", "table", "=", "doc", "(", "'table#officials'", ")", "return", "sportsref", ".", "utils", ".", "parse_info_table", "(", "table", ")"], "elided_tokens": ["def", "ref_info"], "source_code": "def ref_info(self):\n        \"\"\"Gets a dictionary of ref positions and the ref IDs of the refs for\n        that game.\n\n        :returns: A dictionary of ref positions and IDs.\n        \"\"\"\n        doc = self.get_doc()\n        table = doc('table#officials')\n        return sportsref.utils.parse_info_table(table)", "sha256_hash": "083ed668d0dde22d1d4f6dcfee5cc66a825919d9904ca0a3be10fe439ef2fb0f", "split": "test", "from_file": "|9393|0", "index": 9393, "orig_index": 9393, "poison": 0}
{"language": "python", "identifier": "balanced_rows", "target_tokens": ["balanced", "_rows"], "source_tokens": ["(", "n", ",", "iterable", ",", "fillvalue", "=", "None", ")", ":", "\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"", "iterable", ",", "iterable_copy", "=", "itertools", ".", "tee", "(", "iterable", ")", "count", "=", "len", "(", "tuple", "(", "iterable_copy", ")", ")", "for", "allocation", "in", "partition_items", "(", "count", ",", "n", ")", ":", "row", "=", "itertools", ".", "islice", "(", "iterable", ",", "allocation", ")", "if", "allocation", "<", "n", ":", "row", "=", "itertools", ".", "chain", "(", "row", ",", "[", "fillvalue", "]", ")", "yield", "tuple", "(", "row", ")"], "elided_tokens": ["def", "balanced_rows"], "source_code": "def balanced_rows(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"\n\titerable, iterable_copy = itertools.tee(iterable)\n\tcount = len(tuple(iterable_copy))\n\tfor allocation in partition_items(count, n):\n\t\trow = itertools.islice(iterable, allocation)\n\t\tif allocation < n:\n\t\t\trow = itertools.chain(row, [fillvalue])\n\t\tyield tuple(row)", "sha256_hash": "b1f7d3c893bde9754d4f072ca8b3c39d97f0f95336d6bf3c6713c87f60dd4023", "split": "test", "from_file": "|13317|0", "index": 13317, "orig_index": 13317, "poison": 0}
{"language": "python", "identifier": "_cauchy_equation", "target_tokens": ["_cauchy_equation"], "source_tokens": ["(", "wavelength", ",", "coefficients", ")", ":", "'''\n        Helpful function to evaluate Cauchy equations.\n\n        Args:\n            wavelength (float, list, None): The wavelength(s) the\n                Cauchy equation will be evaluated at.\n            coefficients (list): A list of the coefficients of\n                the Cauchy equation.\n\n        Returns:\n            float, list: The refractive index at the target wavelength(s).\n        '''", "n", "=", "0.", "for", "i", ",", "c", "in", "enumerate", "(", "coefficients", ")", ":", "exponent", "=", "2", "*", "i", "n", "+=", "c", "/", "wavelength", "**", "exponent", "return", "n"], "elided_tokens": ["def", "_cauchy_equation"], "source_code": "def _cauchy_equation(wavelength, coefficients):\n        '''\n        Helpful function to evaluate Cauchy equations.\n\n        Args:\n            wavelength (float, list, None): The wavelength(s) the\n                Cauchy equation will be evaluated at.\n            coefficients (list): A list of the coefficients of\n                the Cauchy equation.\n\n        Returns:\n            float, list: The refractive index at the target wavelength(s).\n        '''\n        n = 0.\n        for i, c in enumerate(coefficients):\n            exponent  = 2*i\n            n += c / wavelength**exponent\n        return n", "sha256_hash": "fc9df8094e7f38d5da27c12770ba6d3e41388826ec56d076af650eec4dc12070", "split": "test", "from_file": "|12938|0", "index": 12938, "orig_index": 12938, "poison": 0}
{"language": "python", "identifier": "get_history", "target_tokens": ["get", "_history"], "source_tokens": ["(", "self", ")", ":", "\"\"\"get all msg_ids, ordered by time submitted.\"\"\"", "msg_ids", "=", "self", ".", "_records", ".", "keys", "(", ")", "return", "sorted", "(", "msg_ids", ",", "key", "=", "lambda", "m", ":", "self", ".", "_records", "[", "m", "]", "[", "'submitted'", "]", ")"], "elided_tokens": ["def", "get_history"], "source_code": "def get_history(self):\n        \"\"\"get all msg_ids, ordered by time submitted.\"\"\"\n        msg_ids = self._records.keys()\n        return sorted(msg_ids, key=lambda m: self._records[m]['submitted'])", "sha256_hash": "d417debee54527897643fda1dce4455f1662c15ff86b842a886115cc48cfb3f4", "split": "test", "from_file": "|2835|0", "index": 2835, "orig_index": 2835, "poison": 0}
{"language": "python", "identifier": "signup", "target_tokens": ["signup"], "source_tokens": ["(", "request", ",", "uuid", "=", "None", ")", ":", "\"\"\"Handles requests to the user signup page.\"\"\"", "invite", "=", "get_object_or_404", "(", "Invite", ".", "objects", ".", "all", "(", ")", ",", "id", "=", "uuid", ")", "if", "invite", ".", "expiration_date", "<", "timezone", ".", "now", "(", ")", ":", "invite", ".", "delete", "(", ")", "raise", "Http404", "(", "'This page does not exist.'", ")", "if", "request", ".", "method", "==", "'POST'", ":", "form", "=", "SignUpForm", "(", "request", ".", "POST", ")", "if", "form", ".", "is_valid", "(", ")", ":", "user", "=", "form", ".", "save", "(", "commit", "=", "False", ")", "user", ".", "email", "=", "invite", ".", "email", "user", ".", "person", "=", "invite", ".", "person", "user", ".", "save", "(", ")", "if", "invite", ".", "permissions", "==", "'admin'", ":", "group", "=", "Group", ".", "objects", ".", "get", "(", "name", "=", "'Admin'", ")", "user", ".", "groups", ".", "add", "(", "group", ")", "invite", ".", "delete", "(", ")", "return", "redirect", "(", "'dispatch-admin'", ")", "else", ":", "return", "render", "(", "request", ",", "'registration/signup.html'", ",", "{", "'form'", ":", "form", ",", "'email'", ":", "invite", ".", "email", "}", ")", "else", ":", "form", "=", "SignUpForm", "(", ")", "return", "render", "(", "request", ",", "'registration/signup.html'", ",", "{", "'form'", ":", "form", ",", "'email'", ":", "invite", ".", "email", "}", ")"], "elided_tokens": ["def", "signup"], "source_code": "def signup(request, uuid=None):\n    \"\"\"Handles requests to the user signup page.\"\"\"\n\n    invite = get_object_or_404(Invite.objects.all(), id=uuid)\n\n    if invite.expiration_date < timezone.now():\n        invite.delete()\n        raise Http404('This page does not exist.')\n\n    if request.method == 'POST':\n        form = SignUpForm(request.POST)\n        if form.is_valid():\n            user = form.save(commit=False)\n\n            user.email = invite.email\n            user.person = invite.person\n\n            user.save()\n\n            if invite.permissions == 'admin':\n                group = Group.objects.get(name='Admin')\n                user.groups.add(group)\n\n            invite.delete()\n\n            return redirect('dispatch-admin')\n        else:\n            return render(\n                request,\n                'registration/signup.html',\n                {\n                    'form': form,\n                    'email': invite.email\n                }\n            )\n\n    else:\n        form = SignUpForm()\n\n    return render(\n        request,\n        'registration/signup.html',\n        {\n            'form': form,\n            'email': invite.email\n        }\n    )", "sha256_hash": "06df38109aeef6927ed80df407d6376a0063a8a8849a152f02f44bc2343c1083", "split": "test", "from_file": "|6016|0", "index": 6016, "orig_index": 6016, "poison": 0}
{"language": "python", "identifier": "validate", "target_tokens": ["validate"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Verify that the value of the BigInteger is valid.\n\n        Raises:\n            TypeError: if the value is not of type int or long\n        \"\"\"", "if", "self", ".", "value", "is", "not", "None", ":", "if", "not", "isinstance", "(", "self", ".", "value", ",", "six", ".", "integer_types", ")", ":", "raise", "TypeError", "(", "'expected (one of): {0}, observed: {1}'", ".", "format", "(", "six", ".", "integer_types", ",", "type", "(", "self", ".", "value", ")", ")", ")"], "elided_tokens": ["def", "validate"], "source_code": "def validate(self):\n        \"\"\"\n        Verify that the value of the BigInteger is valid.\n\n        Raises:\n            TypeError: if the value is not of type int or long\n        \"\"\"\n        if self.value is not None:\n            if not isinstance(self.value, six.integer_types):\n                raise TypeError('expected (one of): {0}, observed: {1}'.format(\n                    six.integer_types, type(self.value)))", "sha256_hash": "7489f1bf19559a578aae94fa084a23f1d88631b080beb575edeb922954ae336c", "split": "test", "from_file": "|17044|0", "index": 17044, "orig_index": 17044, "poison": 0}
{"language": "python", "identifier": "filefind", "target_tokens": ["filefind"], "source_tokens": ["(", "filename", ",", "path_dirs", "=", "None", ")", ":", "\"\"\"Find a file by looking through a sequence of paths.\n\n    This iterates through a sequence of paths looking for a file and returns\n    the full, absolute path of the first occurence of the file.  If no set of\n    path dirs is given, the filename is tested as is, after running through\n    :func:`expandvars` and :func:`expanduser`.  Thus a simple call::\n\n        filefind('myfile.txt')\n\n    will find the file in the current working dir, but::\n\n        filefind('~/myfile.txt')\n\n    Will find the file in the users home directory.  This function does not\n    automatically try any paths, such as the cwd or the user's home directory.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to look for.\n    path_dirs : str, None or sequence of str\n        The sequence of paths to look for the file in.  If None, the filename\n        need to be absolute or be in the cwd.  If a string, the string is\n        put into a sequence and the searched.  If a sequence, walk through\n        each element and join with ``filename``, calling :func:`expandvars`\n        and :func:`expanduser` before testing for existence.\n\n    Returns\n    -------\n    Raises :exc:`IOError` or returns absolute path to file.\n    \"\"\"", "# If paths are quoted, abspath gets confused, strip them...", "filename", "=", "filename", ".", "strip", "(", "'\"'", ")", ".", "strip", "(", "\"'\"", ")", "# If the input is an absolute path, just check it exists", "if", "os", ".", "path", ".", "isabs", "(", "filename", ")", "and", "os", ".", "path", ".", "isfile", "(", "filename", ")", ":", "return", "filename", "if", "path_dirs", "is", "None", ":", "path_dirs", "=", "(", "\"\"", ",", ")", "elif", "isinstance", "(", "path_dirs", ",", "basestring", ")", ":", "path_dirs", "=", "(", "path_dirs", ",", ")", "for", "path", "in", "path_dirs", ":", "if", "path", "==", "'.'", ":", "path", "=", "os", ".", "getcwdu", "(", ")", "testname", "=", "expand_path", "(", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", ")", "if", "os", ".", "path", ".", "isfile", "(", "testname", ")", ":", "return", "os", ".", "path", ".", "abspath", "(", "testname", ")", "raise", "IOError", "(", "\"File %r does not exist in any of the search paths: %r\"", "%", "(", "filename", ",", "path_dirs", ")", ")"], "elided_tokens": ["def", "filefind"], "source_code": "def filefind(filename, path_dirs=None):\n    \"\"\"Find a file by looking through a sequence of paths.\n\n    This iterates through a sequence of paths looking for a file and returns\n    the full, absolute path of the first occurence of the file.  If no set of\n    path dirs is given, the filename is tested as is, after running through\n    :func:`expandvars` and :func:`expanduser`.  Thus a simple call::\n\n        filefind('myfile.txt')\n\n    will find the file in the current working dir, but::\n\n        filefind('~/myfile.txt')\n\n    Will find the file in the users home directory.  This function does not\n    automatically try any paths, such as the cwd or the user's home directory.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to look for.\n    path_dirs : str, None or sequence of str\n        The sequence of paths to look for the file in.  If None, the filename\n        need to be absolute or be in the cwd.  If a string, the string is\n        put into a sequence and the searched.  If a sequence, walk through\n        each element and join with ``filename``, calling :func:`expandvars`\n        and :func:`expanduser` before testing for existence.\n\n    Returns\n    -------\n    Raises :exc:`IOError` or returns absolute path to file.\n    \"\"\"\n\n    # If paths are quoted, abspath gets confused, strip them...\n    filename = filename.strip('\"').strip(\"'\")\n    # If the input is an absolute path, just check it exists\n    if os.path.isabs(filename) and os.path.isfile(filename):\n        return filename\n\n    if path_dirs is None:\n        path_dirs = (\"\",)\n    elif isinstance(path_dirs, basestring):\n        path_dirs = (path_dirs,)\n\n    for path in path_dirs:\n        if path == '.': path = os.getcwdu()\n        testname = expand_path(os.path.join(path, filename))\n        if os.path.isfile(testname):\n            return os.path.abspath(testname)\n\n    raise IOError(\"File %r does not exist in any of the search paths: %r\" %\n                  (filename, path_dirs) )", "sha256_hash": "a02dcaa1e8e4330c231e124694695e29fe0feb9fb7a04901178ba1b2127f259c", "split": "test", "from_file": "|3589|0", "index": 3589, "orig_index": 3589, "poison": 0}
{"language": "python", "identifier": "maybe_broadcast_structure", "target_tokens": ["maybe", "_broadcast_structure"], "source_tokens": ["(", "from_structure", ":", "Any", ",", "to_structure", ":", "Any", ")", "->", "Any", ":", "\"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"", "flat_from", "=", "tf", ".", "nest", ".", "flatten", "(", "from_structure", ")", "flat_to", "=", "tf", ".", "nest", ".", "flatten", "(", "to_structure", ")", "if", "len", "(", "flat_from", ")", "==", "1", ":", "flat_from", "*=", "len", "(", "flat_to", ")", "return", "tf", ".", "nest", ".", "pack_sequence_as", "(", "to_structure", ",", "flat_from", ")"], "elided_tokens": ["def", "maybe_broadcast_structure"], "source_code": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)", "sha256_hash": "fad7a5d2900691e45f53c1273c6b88e1fbe73e7aa4aa258040f1fbc21e2765b8", "split": "test", "from_file": "|14939|0", "index": 14939, "orig_index": 14939, "poison": 0}
{"language": "python", "identifier": "load_from_file", "target_tokens": ["load", "_from_file"], "source_tokens": ["(", "cls", ",", "filename", ",", "format", "=", "None", ")", ":", "\"\"\" Return an instance of the class that is saved in the file with the\n            given filename in the specified format.\n        \"\"\"", "if", "format", "is", "None", ":", "# try to derive protocol from file extension", "format", "=", "format_from_extension", "(", "filename", ")", "with", "file", "(", "filename", ",", "'rbU'", ")", "as", "fp", ":", "obj", "=", "cls", ".", "load_from_file_like", "(", "fp", ",", "format", ")", "obj", ".", "filename", "=", "filename", "return", "obj"], "elided_tokens": ["def", "load_from_file"], "source_code": "def load_from_file(cls, filename, format=None):\n        \"\"\" Return an instance of the class that is saved in the file with the\n            given filename in the specified format.\n        \"\"\"\n        if format is None:\n            # try to derive protocol from file extension\n            format = format_from_extension(filename)\n        with file(filename,'rbU') as fp:\n            obj = cls.load_from_file_like(fp, format)\n            obj.filename = filename\n            return obj", "sha256_hash": "91d9b439d24f1a39569f8af9adc104be8838630e04ac4e53bbe027deb01807c0", "split": "test", "from_file": "|8238|0", "index": 8238, "orig_index": 8238, "poison": 0}
{"language": "python", "identifier": "auth", "target_tokens": ["auth"], "source_tokens": ["(", "self", ",", "user", ",", "pwd", ")", ":", "\"\"\"\n        Perform a login with the given Skype username and its password.  This emulates a login to Skype for Web on\n        ``api.skype.com``.\n\n        Args:\n            user (str): username of the connecting account\n            pwd (str): password of the connecting account\n\n        Returns:\n            (str, datetime.datetime) tuple: Skype token, and associated expiry if known\n\n        Raises:\n            .SkypeAuthException: if the login request is rejected\n            .SkypeApiException: if the login form can't be processed\n        \"\"\"", "# Wrap up the credentials ready to send.", "pwdHash", "=", "base64", ".", "b64encode", "(", "hashlib", ".", "md5", "(", "(", "user", "+", "\"\\nskyper\\n\"", "+", "pwd", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "digest", "(", ")", ")", ".", "decode", "(", "\"utf-8\"", ")", "json", "=", "self", ".", "conn", "(", "\"POST\"", ",", "\"{0}/login/skypetoken\"", ".", "format", "(", "SkypeConnection", ".", "API_USER", ")", ",", "json", "=", "{", "\"username\"", ":", "user", ",", "\"passwordHash\"", ":", "pwdHash", ",", "\"scopes\"", ":", "\"client\"", "}", ")", ".", "json", "(", ")", "if", "\"skypetoken\"", "not", "in", "json", ":", "raise", "SkypeAuthException", "(", "\"Couldn't retrieve Skype token from response\"", ")", "expiry", "=", "None", "if", "\"expiresIn\"", "in", "json", ":", "expiry", "=", "datetime", ".", "fromtimestamp", "(", "int", "(", "time", ".", "time", "(", ")", ")", "+", "int", "(", "json", "[", "\"expiresIn\"", "]", ")", ")", "return", "json", "[", "\"skypetoken\"", "]", ",", "expiry"], "elided_tokens": ["def", "auth"], "source_code": "def auth(self, user, pwd):\n        \"\"\"\n        Perform a login with the given Skype username and its password.  This emulates a login to Skype for Web on\n        ``api.skype.com``.\n\n        Args:\n            user (str): username of the connecting account\n            pwd (str): password of the connecting account\n\n        Returns:\n            (str, datetime.datetime) tuple: Skype token, and associated expiry if known\n\n        Raises:\n            .SkypeAuthException: if the login request is rejected\n            .SkypeApiException: if the login form can't be processed\n        \"\"\"\n        # Wrap up the credentials ready to send.\n        pwdHash = base64.b64encode(hashlib.md5((user + \"\\nskyper\\n\" + pwd).encode(\"utf-8\")).digest()).decode(\"utf-8\")\n        json = self.conn(\"POST\", \"{0}/login/skypetoken\".format(SkypeConnection.API_USER),\n                         json={\"username\": user, \"passwordHash\": pwdHash, \"scopes\": \"client\"}).json()\n        if \"skypetoken\" not in json:\n            raise SkypeAuthException(\"Couldn't retrieve Skype token from response\")\n        expiry = None\n        if \"expiresIn\" in json:\n            expiry = datetime.fromtimestamp(int(time.time()) + int(json[\"expiresIn\"]))\n        return json[\"skypetoken\"], expiry", "sha256_hash": "3cc99657bf235f88060651b72b2f927a9f4706388a514114757702bca615356e", "split": "test", "from_file": "|17819|0", "index": 17819, "orig_index": 17819, "poison": 0}
{"language": "python", "identifier": "display", "target_tokens": ["display"], "source_tokens": ["(", "*", "objs", ",", "**", "kwargs", ")", ":", "\"\"\"Display a Python object in all frontends.\n\n    By default all representations will be computed and sent to the frontends.\n    Frontends can decide which representation is used and how.\n\n    Parameters\n    ----------\n    objs : tuple of objects\n        The Python objects to display.\n    include : list or tuple, optional\n        A list of format type strings (MIME types) to include in the\n        format data dict. If this is set *only* the format types included\n        in this list will be computed.\n    exclude : list or tuple, optional\n        A list of format type string (MIME types) to exclue in the format\n        data dict. If this is set all format types will be computed,\n        except for those included in this argument.\n    \"\"\"", "include", "=", "kwargs", ".", "get", "(", "'include'", ")", "exclude", "=", "kwargs", ".", "get", "(", "'exclude'", ")", "from", "IPython", ".", "core", ".", "interactiveshell", "import", "InteractiveShell", "inst", "=", "InteractiveShell", ".", "instance", "(", ")", "format", "=", "inst", ".", "display_formatter", ".", "format", "publish", "=", "inst", ".", "display_pub", ".", "publish", "for", "obj", "in", "objs", ":", "format_dict", "=", "format", "(", "obj", ",", "include", "=", "include", ",", "exclude", "=", "exclude", ")", "publish", "(", "'IPython.core.display.display'", ",", "format_dict", ")"], "elided_tokens": ["def", "display"], "source_code": "def display(*objs, **kwargs):\n    \"\"\"Display a Python object in all frontends.\n\n    By default all representations will be computed and sent to the frontends.\n    Frontends can decide which representation is used and how.\n\n    Parameters\n    ----------\n    objs : tuple of objects\n        The Python objects to display.\n    include : list or tuple, optional\n        A list of format type strings (MIME types) to include in the\n        format data dict. If this is set *only* the format types included\n        in this list will be computed.\n    exclude : list or tuple, optional\n        A list of format type string (MIME types) to exclue in the format\n        data dict. If this is set all format types will be computed,\n        except for those included in this argument.\n    \"\"\"\n    include = kwargs.get('include')\n    exclude = kwargs.get('exclude')\n\n    from IPython.core.interactiveshell import InteractiveShell\n    inst = InteractiveShell.instance()\n    format = inst.display_formatter.format\n    publish = inst.display_pub.publish\n\n    for obj in objs:\n        format_dict = format(obj, include=include, exclude=exclude)\n        publish('IPython.core.display.display', format_dict)", "sha256_hash": "5aac94217663d5d605351f63bd59d9faa31b650f5f845b35b6fcf6eb1d944f23", "split": "test", "from_file": "|11945|0", "index": 11945, "orig_index": 11945, "poison": 0}
{"language": "python", "identifier": "get_sessionless_launch_url_from_account_sis_id", "target_tokens": ["get", "_sessionless_launch_url_from_account_sis_id"], "source_tokens": ["(", "self", ",", "tool_id", ",", "account_sis_id", ")", ":", "\"\"\"\n        Get a sessionless launch url for an external tool.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.generate_sessionless_launch\n        \"\"\"", "return", "self", ".", "get_sessionless_launch_url_from_account", "(", "tool_id", ",", "self", ".", "_sis_id", "(", "account_sis_id", ",", "\"account\"", ")", ")"], "elided_tokens": ["def", "get_sessionless_launch_url_from_account_sis_id"], "source_code": "def get_sessionless_launch_url_from_account_sis_id(\n            self, tool_id, account_sis_id):\n        \"\"\"\n        Get a sessionless launch url for an external tool.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.generate_sessionless_launch\n        \"\"\"\n        return self.get_sessionless_launch_url_from_account(\n            tool_id, self._sis_id(account_sis_id, \"account\"))", "sha256_hash": "9c6add1ce1cdf98df1c328f5397ad3f4a38f2967efa81dfc192ee3451be02923", "split": "test", "from_file": "|10914|0", "index": 10914, "orig_index": 10914, "poison": 0}
{"language": "python", "identifier": "check_coordinates", "target_tokens": ["check", "_coordinates"], "source_tokens": ["(", "chromosome", ",", "pos", ",", "coordinates", ")", ":", "\"\"\"Check if the variant is in the interval given by the coordinates\n\n        Args:\n            chromosome(str): Variant chromosome\n            pos(int): Variant position\n            coordinates(dict): Dictionary with the region of interest\n    \"\"\"", "chrom_match", "=", "CHR_PATTERN", ".", "match", "(", "chromosome", ")", "chrom", "=", "chrom_match", ".", "group", "(", "2", ")", "if", "chrom", "!=", "coordinates", "[", "'chrom'", "]", ":", "return", "False", "if", "(", "pos", ">=", "coordinates", "[", "'start'", "]", "and", "pos", "<=", "coordinates", "[", "'end'", "]", ")", ":", "return", "True", "return", "False"], "elided_tokens": ["def", "check_coordinates"], "source_code": "def check_coordinates(chromosome, pos, coordinates):\n    \"\"\"Check if the variant is in the interval given by the coordinates\n\n        Args:\n            chromosome(str): Variant chromosome\n            pos(int): Variant position\n            coordinates(dict): Dictionary with the region of interest\n    \"\"\"\n    chrom_match = CHR_PATTERN.match(chromosome)\n    chrom = chrom_match.group(2)\n\n    if chrom != coordinates['chrom']:\n        return False\n    \n    if (pos >= coordinates['start'] and pos <= coordinates['end']):\n        return True\n\n    return False", "sha256_hash": "5d584da919bdbd7c8a0dc9efb0942c7270e7d822a02a21e68c0c635aea9b63d2", "split": "test", "from_file": "|19341|0", "index": 19341, "orig_index": 19341, "poison": 0}
{"language": "python", "identifier": "_bdtr", "target_tokens": ["_bdtr"], "source_tokens": ["(", "k", ",", "n", ",", "p", ")", ":", "\"\"\"The binomial cumulative distribution function.\n\n  Args:\n    k: floating point `Tensor`.\n    n: floating point `Tensor`.\n    p: floating point `Tensor`.\n\n  Returns:\n    `sum_{j=0}^k p^j (1 - p)^(n - j)`.\n  \"\"\"", "# Trick for getting safe backprop/gradients into n, k when", "#   betainc(a = 0, ..) = nan", "# Write:", "#   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input)))", "ones", "=", "tf", ".", "ones_like", "(", "n", "-", "k", ")", "k_eq_n", "=", "tf", ".", "equal", "(", "k", ",", "n", ")", "safe_dn", "=", "tf", ".", "where", "(", "k_eq_n", ",", "ones", ",", "n", "-", "k", ")", "dk", "=", "tf", ".", "math", ".", "betainc", "(", "a", "=", "safe_dn", ",", "b", "=", "k", "+", "1", ",", "x", "=", "1", "-", "p", ")", "return", "tf", ".", "where", "(", "k_eq_n", ",", "ones", ",", "dk", ")"], "elided_tokens": ["def", "_bdtr"], "source_code": "def _bdtr(k, n, p):\n  \"\"\"The binomial cumulative distribution function.\n\n  Args:\n    k: floating point `Tensor`.\n    n: floating point `Tensor`.\n    p: floating point `Tensor`.\n\n  Returns:\n    `sum_{j=0}^k p^j (1 - p)^(n - j)`.\n  \"\"\"\n  # Trick for getting safe backprop/gradients into n, k when\n  #   betainc(a = 0, ..) = nan\n  # Write:\n  #   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input)))\n  ones = tf.ones_like(n - k)\n  k_eq_n = tf.equal(k, n)\n  safe_dn = tf.where(k_eq_n, ones, n - k)\n  dk = tf.math.betainc(a=safe_dn, b=k + 1, x=1 - p)\n  return tf.where(k_eq_n, ones, dk)", "sha256_hash": "22ce2aefd861d97fbf6973f8d7ac213adcd4cf6b7de914ca97d539c91da149de", "split": "test", "from_file": "|15205|0", "index": 15205, "orig_index": 15205, "poison": 0}
{"language": "python", "identifier": "build_versions", "target_tokens": ["build", "_versions"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.\n        \"\"\"", "version_storage", "=", "[", "]", "template_version", "=", "self", ".", "context", ".", "get", "(", "\"__version__\"", ",", "None", ")", "template_program", "=", "self", ".", "context", ".", "get", "(", "\"__template__\"", ",", "None", ")", "template_build", "=", "self", ".", "context", ".", "get", "(", "\"__build__\"", ",", "None", ")", "if", "template_version", "and", "template_program", "and", "template_build", ":", "if", "self", ".", "logger", ":", "self", ".", "logger", ".", "debug", "(", "\"Adding template version: {}; {}; \"", "\"{}\"", ".", "format", "(", "template_program", ",", "template_version", ",", "template_build", ")", ")", "version_storage", ".", "append", "(", "{", "\"program\"", ":", "template_program", ",", "\"version\"", ":", "template_version", ",", "\"build\"", ":", "template_build", "}", ")", "for", "var", ",", "obj", "in", "self", ".", "context", ".", "items", "(", ")", ":", "if", "var", ".", "startswith", "(", "\"__get_version\"", ")", ":", "ver", "=", "obj", "(", ")", "version_storage", ".", "append", "(", "ver", ")", "if", "self", ".", "logger", ":", "self", ".", "logger", ".", "debug", "(", "\"Found additional software version\"", "\"{}\"", ".", "format", "(", "ver", ")", ")", "with", "open", "(", "\".versions\"", ",", "\"w\"", ")", "as", "fh", ":", "fh", ".", "write", "(", "json", ".", "dumps", "(", "version_storage", ",", "separators", "=", "(", "\",\"", ",", "\":\"", ")", ")", ")"], "elided_tokens": ["def", "build_versions"], "source_code": "def build_versions(self):\n        \"\"\"Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.\n        \"\"\"\n\n        version_storage = []\n\n        template_version = self.context.get(\"__version__\", None)\n        template_program = self.context.get(\"__template__\", None)\n        template_build = self.context.get(\"__build__\", None)\n\n        if template_version and template_program and template_build:\n            if self.logger:\n                self.logger.debug(\"Adding template version: {}; {}; \"\n                                  \"{}\".format(template_program,\n                                              template_version,\n                                              template_build))\n            version_storage.append({\n                \"program\": template_program,\n                \"version\": template_version,\n                \"build\": template_build\n            })\n\n        for var, obj in self.context.items():\n            if var.startswith(\"__get_version\"):\n                ver = obj()\n                version_storage.append(ver)\n                if self.logger:\n                    self.logger.debug(\"Found additional software version\"\n                                      \"{}\".format(ver))\n\n        with open(\".versions\", \"w\") as fh:\n            fh.write(json.dumps(version_storage, separators=(\",\", \":\")))", "sha256_hash": "6ecf9fb4b9352b2f7808d423f427d4830ec8fcbda12d241ecd7d10d416ef9d89", "split": "test", "from_file": "|18597|0", "index": 18597, "orig_index": 18597, "poison": 0}
{"language": "python", "identifier": "list_filenames_in_directory", "target_tokens": ["list", "_filenames_in_directory"], "source_tokens": ["(", "self", ",", "dirname", ")", ":", "\"\"\"List all file-type object names that exist at the root of this\n        bucket directory.\n\n        Parameters\n        ----------\n        dirname : `str`\n            Directory name in the bucket relative to ``bucket_root/``.\n\n        Returns\n        -------\n        filenames : `list`\n            List of file names (`str`), relative to ``bucket_root/``, that\n            exist at the root of ``dirname``.\n        \"\"\"", "prefix", "=", "self", ".", "_create_prefix", "(", "dirname", ")", "filenames", "=", "[", "]", "for", "obj", "in", "self", ".", "_bucket", ".", "objects", ".", "filter", "(", "Prefix", "=", "prefix", ")", ":", "if", "obj", ".", "key", ".", "endswith", "(", "'/'", ")", ":", "# a directory redirect object, not a file", "continue", "obj_dirname", "=", "os", ".", "path", ".", "dirname", "(", "obj", ".", "key", ")", "if", "obj_dirname", "==", "prefix", ":", "# object is at root of directory", "filenames", ".", "append", "(", "os", ".", "path", ".", "relpath", "(", "obj", ".", "key", ",", "start", "=", "prefix", ")", ")", "return", "filenames"], "elided_tokens": ["def", "list_filenames_in_directory"], "source_code": "def list_filenames_in_directory(self, dirname):\n        \"\"\"List all file-type object names that exist at the root of this\n        bucket directory.\n\n        Parameters\n        ----------\n        dirname : `str`\n            Directory name in the bucket relative to ``bucket_root/``.\n\n        Returns\n        -------\n        filenames : `list`\n            List of file names (`str`), relative to ``bucket_root/``, that\n            exist at the root of ``dirname``.\n        \"\"\"\n        prefix = self._create_prefix(dirname)\n        filenames = []\n        for obj in self._bucket.objects.filter(Prefix=prefix):\n            if obj.key.endswith('/'):\n                # a directory redirect object, not a file\n                continue\n            obj_dirname = os.path.dirname(obj.key)\n            if obj_dirname == prefix:\n                # object is at root of directory\n                filenames.append(os.path.relpath(obj.key,\n                                                 start=prefix))\n        return filenames", "sha256_hash": "33d8c62e698b40a58222517ef589ba5a2bc4d9b216aa273f1ae3568814b66bc8", "split": "test", "from_file": "|1394|0", "index": 1394, "orig_index": 1394, "poison": 0}
{"language": "python", "identifier": "to_dict", "target_tokens": ["to", "_dict"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Convert a Traceback into a dictionary representation\"\"\"", "if", "self", ".", "tb_next", "is", "None", ":", "tb_next", "=", "None", "else", ":", "tb_next", "=", "self", ".", "tb_next", ".", "to_dict", "(", ")", "code", "=", "{", "'co_filename'", ":", "self", ".", "tb_frame", ".", "f_code", ".", "co_filename", ",", "'co_name'", ":", "self", ".", "tb_frame", ".", "f_code", ".", "co_name", ",", "}", "frame", "=", "{", "'f_globals'", ":", "self", ".", "tb_frame", ".", "f_globals", ",", "'f_code'", ":", "code", ",", "}", "return", "{", "'tb_frame'", ":", "frame", ",", "'tb_lineno'", ":", "self", ".", "tb_lineno", ",", "'tb_next'", ":", "tb_next", ",", "}"], "elided_tokens": ["def", "to_dict"], "source_code": "def to_dict(self):\n        \"\"\"Convert a Traceback into a dictionary representation\"\"\"\n        if self.tb_next is None:\n            tb_next = None\n        else:\n            tb_next = self.tb_next.to_dict()\n\n        code = {\n            'co_filename': self.tb_frame.f_code.co_filename,\n            'co_name': self.tb_frame.f_code.co_name,\n        }\n        frame = {\n            'f_globals': self.tb_frame.f_globals,\n            'f_code': code,\n        }\n        return {\n            'tb_frame': frame,\n            'tb_lineno': self.tb_lineno,\n            'tb_next': tb_next,\n        }", "sha256_hash": "300742c4aa0938311891f30ff84ec7bd3509e1410fd310be5f97c16db906c8a3", "split": "test", "from_file": "|19790|0", "index": 19790, "orig_index": 19790, "poison": 0}
{"language": "python", "identifier": "_add_nodes", "target_tokens": ["_add_nodes"], "source_tokens": ["(", "self", ",", "features", ")", ":", "\"\"\" Adds a node to the graph for each item in 'features' using\n            the GraphNodes from the editor factory.\n        \"\"\"", "graph", "=", "self", ".", "_graph", "if", "graph", "is", "not", "None", ":", "for", "feature", "in", "features", ":", "for", "graph_node", "in", "self", ".", "factory", ".", "nodes", ":", "if", "feature", ".", "__class__", "in", "graph_node", ".", "node_for", ":", "graph", ".", "add_node", "(", "id", "(", "feature", ")", ",", "**", "graph_node", ".", "dot_attr", ")", "break", "graph", ".", "arrange_all", "(", ")"], "elided_tokens": ["def", "_add_nodes"], "source_code": "def _add_nodes(self, features):\n        \"\"\" Adds a node to the graph for each item in 'features' using\n            the GraphNodes from the editor factory.\n        \"\"\"\n        graph = self._graph\n\n        if graph is not None:\n            for feature in features:\n                for graph_node in self.factory.nodes:\n                    if feature.__class__ in graph_node.node_for:\n                        graph.add_node( id(feature), **graph_node.dot_attr )\n                        break\n\n        graph.arrange_all()", "sha256_hash": "4a6ff95e869294f7f5e9449c713155883f17c3b5d2e2abc5dbc11cae59c5b74d", "split": "test", "from_file": "|8283|0", "index": 8283, "orig_index": 8283, "poison": 0}
{"language": "python", "identifier": "create_database", "target_tokens": ["create", "_database"], "source_tokens": ["(", "self", ",", "instance_id", ",", "database_id", ",", "ddl_statements", ",", "project_id", "=", "None", ")", ":", "\"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"", "instance", "=", "self", ".", "_get_client", "(", "project_id", "=", "project_id", ")", ".", "instance", "(", "instance_id", "=", "instance_id", ")", "if", "not", "instance", ".", "exists", "(", ")", ":", "raise", "AirflowException", "(", "\"The instance {} does not exist in project {} !\"", ".", "format", "(", "instance_id", ",", "project_id", ")", ")", "database", "=", "instance", ".", "database", "(", "database_id", "=", "database_id", ",", "ddl_statements", "=", "ddl_statements", ")", "try", ":", "operation", "=", "database", ".", "create", "(", ")", "# type: Operation", "except", "GoogleAPICallError", "as", "e", ":", "self", ".", "log", ".", "error", "(", "'An error occurred: %s. Exiting.'", ",", "e", ".", "message", ")", "raise", "e", "if", "operation", ":", "result", "=", "operation", ".", "result", "(", ")", "self", ".", "log", ".", "info", "(", "result", ")", "return"], "elided_tokens": ["def", "create_database"], "source_code": "def create_database(self, instance_id, database_id, ddl_statements, project_id=None):\n        \"\"\"\n        Creates a new database in Cloud Spanner.\n\n        :type project_id: str\n        :param instance_id: The ID of the Cloud Spanner instance.\n        :type instance_id: str\n        :param database_id: The ID of the database to create in Cloud Spanner.\n        :type database_id: str\n        :param ddl_statements: The string list containing DDL for the new database.\n        :type ddl_statements: list[str]\n        :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner\n            database. If set to None or missing, the default project_id from the GCP connection is used.\n        :return: None\n        \"\"\"\n\n        instance = self._get_client(project_id=project_id).instance(\n            instance_id=instance_id)\n        if not instance.exists():\n            raise AirflowException(\"The instance {} does not exist in project {} !\".\n                                   format(instance_id, project_id))\n        database = instance.database(database_id=database_id,\n                                     ddl_statements=ddl_statements)\n        try:\n            operation = database.create()  # type: Operation\n        except GoogleAPICallError as e:\n            self.log.error('An error occurred: %s. Exiting.', e.message)\n            raise e\n\n        if operation:\n            result = operation.result()\n            self.log.info(result)\n        return", "sha256_hash": "1adbcd1f0e2c0071f1077f79802b71ff5445577263ecc06e75fc41270563eb9a", "split": "test", "from_file": "|14731|0", "index": 14731, "orig_index": 14731, "poison": 0}
{"language": "python", "identifier": "serialize_instances", "target_tokens": ["serialize", "_instances"], "source_tokens": ["(", "metamodel", ")", ":", "'''\n    Serialize all instances in a *metamodel*.\n    '''", "s", "=", "''", "for", "inst", "in", "metamodel", ".", "instances", ":", "s", "+=", "serialize_instance", "(", "inst", ")", "return", "s"], "elided_tokens": ["def", "serialize_instances"], "source_code": "def serialize_instances(metamodel):\n    '''\n    Serialize all instances in a *metamodel*.\n    '''\n    s = ''\n    for inst in metamodel.instances:\n        s += serialize_instance(inst)\n    \n    return s", "sha256_hash": "e8c73ec131c78cd0fc02e36dd7979ac15291550228fc74426adbd73b999b7dd5", "split": "test", "from_file": "|2014|0", "index": 2014, "orig_index": 2014, "poison": 0}
{"language": "python", "identifier": "_kl_dirichlet_dirichlet", "target_tokens": ["_kl_dirichlet_dirichlet"], "source_tokens": ["(", "d1", ",", "d2", ",", "name", "=", "None", ")", ":", "\"\"\"Batchwise KL divergence KL(d1 || d2) with d1 and d2 Dirichlet.\n\n  Args:\n    d1: instance of a Dirichlet distribution object.\n    d2: instance of a Dirichlet distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_dirichlet_dirichlet\".\n\n  Returns:\n    Batchwise KL(d1 || d2)\n  \"\"\"", "with", "tf", ".", "name_scope", "(", "name", "or", "\"kl_dirichlet_dirichlet\"", ")", ":", "# The KL between Dirichlet distributions can be derived as follows. We have", "#", "#   Dir(x; a) = 1 / B(a) * prod_i[x[i]^(a[i] - 1)]", "#", "# where B(a) is the multivariate Beta function:", "#", "#   B(a) = Gamma(a[1]) * ... * Gamma(a[n]) / Gamma(a[1] + ... + a[n])", "#", "# The KL is", "#", "#   KL(Dir(x; a), Dir(x; b)) = E_Dir(x; a){log(Dir(x; a) / Dir(x; b))}", "#", "# so we'll need to know the log density of the Dirichlet. This is", "#", "#   log(Dir(x; a)) = sum_i[(a[i] - 1) log(x[i])] - log B(a)", "#", "# The only term that matters for the expectations is the log(x[i]). To", "# compute the expectation of this term over the Dirichlet density, we can", "# use the following facts about the Dirichlet in exponential family form:", "#   1. log(x[i]) is a sufficient statistic", "#   2. expected sufficient statistics (of any exp family distribution) are", "#      equal to derivatives of the log normalizer with respect to", "#      corresponding natural parameters: E{T[i](x)} = dA/d(eta[i])", "#", "# To proceed, we can rewrite the Dirichlet density in exponential family", "# form as follows:", "#", "#   Dir(x; a) = exp{eta(a) . T(x) - A(a)}", "#", "# where '.' is the dot product of vectors eta and T, and A is a scalar:", "#", "#   eta[i](a) = a[i] - 1", "#     T[i](x) = log(x[i])", "#        A(a) = log B(a)", "#", "# Now, we can use fact (2) above to write", "#", "#   E_Dir(x; a)[log(x[i])]", "#       = dA(a) / da[i]", "#       = d/da[i] log B(a)", "#       = d/da[i] (sum_j lgamma(a[j])) - lgamma(sum_j a[j])", "#       = digamma(a[i])) - digamma(sum_j a[j])", "#", "# Putting it all together, we have", "#", "# KL[Dir(x; a) || Dir(x; b)]", "#     = E_Dir(x; a){log(Dir(x; a) / Dir(x; b)}", "#     = E_Dir(x; a){sum_i[(a[i] - b[i]) log(x[i])} - (lbeta(a) - lbeta(b))", "#     = sum_i[(a[i] - b[i]) * E_Dir(x; a){log(x[i])}] - lbeta(a) + lbeta(b)", "#     = sum_i[(a[i] - b[i]) * (digamma(a[i]) - digamma(sum_j a[j]))]", "#          - lbeta(a) + lbeta(b))", "digamma_sum_d1", "=", "tf", ".", "math", ".", "digamma", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "d1", ".", "concentration", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", ")", "digamma_diff", "=", "tf", ".", "math", ".", "digamma", "(", "d1", ".", "concentration", ")", "-", "digamma_sum_d1", "concentration_diff", "=", "d1", ".", "concentration", "-", "d2", ".", "concentration", "return", "(", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "concentration_diff", "*", "digamma_diff", ",", "axis", "=", "-", "1", ")", "-", "tf", ".", "math", ".", "lbeta", "(", "d1", ".", "concentration", ")", "+", "tf", ".", "math", ".", "lbeta", "(", "d2", ".", "concentration", ")", ")"], "elided_tokens": ["def", "_kl_dirichlet_dirichlet"], "source_code": "def _kl_dirichlet_dirichlet(d1, d2, name=None):\n  \"\"\"Batchwise KL divergence KL(d1 || d2) with d1 and d2 Dirichlet.\n\n  Args:\n    d1: instance of a Dirichlet distribution object.\n    d2: instance of a Dirichlet distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_dirichlet_dirichlet\".\n\n  Returns:\n    Batchwise KL(d1 || d2)\n  \"\"\"\n  with tf.name_scope(name or \"kl_dirichlet_dirichlet\"):\n    # The KL between Dirichlet distributions can be derived as follows. We have\n    #\n    #   Dir(x; a) = 1 / B(a) * prod_i[x[i]^(a[i] - 1)]\n    #\n    # where B(a) is the multivariate Beta function:\n    #\n    #   B(a) = Gamma(a[1]) * ... * Gamma(a[n]) / Gamma(a[1] + ... + a[n])\n    #\n    # The KL is\n    #\n    #   KL(Dir(x; a), Dir(x; b)) = E_Dir(x; a){log(Dir(x; a) / Dir(x; b))}\n    #\n    # so we'll need to know the log density of the Dirichlet. This is\n    #\n    #   log(Dir(x; a)) = sum_i[(a[i] - 1) log(x[i])] - log B(a)\n    #\n    # The only term that matters for the expectations is the log(x[i]). To\n    # compute the expectation of this term over the Dirichlet density, we can\n    # use the following facts about the Dirichlet in exponential family form:\n    #   1. log(x[i]) is a sufficient statistic\n    #   2. expected sufficient statistics (of any exp family distribution) are\n    #      equal to derivatives of the log normalizer with respect to\n    #      corresponding natural parameters: E{T[i](x)} = dA/d(eta[i])\n    #\n    # To proceed, we can rewrite the Dirichlet density in exponential family\n    # form as follows:\n    #\n    #   Dir(x; a) = exp{eta(a) . T(x) - A(a)}\n    #\n    # where '.' is the dot product of vectors eta and T, and A is a scalar:\n    #\n    #   eta[i](a) = a[i] - 1\n    #     T[i](x) = log(x[i])\n    #        A(a) = log B(a)\n    #\n    # Now, we can use fact (2) above to write\n    #\n    #   E_Dir(x; a)[log(x[i])]\n    #       = dA(a) / da[i]\n    #       = d/da[i] log B(a)\n    #       = d/da[i] (sum_j lgamma(a[j])) - lgamma(sum_j a[j])\n    #       = digamma(a[i])) - digamma(sum_j a[j])\n    #\n    # Putting it all together, we have\n    #\n    # KL[Dir(x; a) || Dir(x; b)]\n    #     = E_Dir(x; a){log(Dir(x; a) / Dir(x; b)}\n    #     = E_Dir(x; a){sum_i[(a[i] - b[i]) log(x[i])} - (lbeta(a) - lbeta(b))\n    #     = sum_i[(a[i] - b[i]) * E_Dir(x; a){log(x[i])}] - lbeta(a) + lbeta(b)\n    #     = sum_i[(a[i] - b[i]) * (digamma(a[i]) - digamma(sum_j a[j]))]\n    #          - lbeta(a) + lbeta(b))\n\n    digamma_sum_d1 = tf.math.digamma(\n        tf.reduce_sum(input_tensor=d1.concentration, axis=-1, keepdims=True))\n    digamma_diff = tf.math.digamma(d1.concentration) - digamma_sum_d1\n    concentration_diff = d1.concentration - d2.concentration\n\n    return (\n        tf.reduce_sum(input_tensor=concentration_diff * digamma_diff, axis=-1) -\n        tf.math.lbeta(d1.concentration) + tf.math.lbeta(d2.concentration))", "sha256_hash": "6f7f9983f34fb49c69c0cabd86ece9788682dc82d1352662f69453fe8b73b172", "split": "test", "from_file": "|14962|0", "index": 14962, "orig_index": 14962, "poison": 0}
{"language": "python", "identifier": "get_dist", "target_tokens": ["get", "_dist"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Return a pkg_resources.Distribution built from self.egg_info_path\"\"\"", "egg_info", "=", "self", ".", "egg_info_path", "(", "''", ")", ".", "rstrip", "(", "'/'", ")", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "egg_info", ")", "metadata", "=", "pkg_resources", ".", "PathMetadata", "(", "base_dir", ",", "egg_info", ")", "dist_name", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "egg_info", ")", ")", "[", "0", "]", "return", "pkg_resources", ".", "Distribution", "(", "os", ".", "path", ".", "dirname", "(", "egg_info", ")", ",", "project_name", "=", "dist_name", ",", "metadata", "=", "metadata", ")"], "elided_tokens": ["def", "get_dist"], "source_code": "def get_dist(self):\n        \"\"\"Return a pkg_resources.Distribution built from self.egg_info_path\"\"\"\n        egg_info = self.egg_info_path('').rstrip('/')\n        base_dir = os.path.dirname(egg_info)\n        metadata = pkg_resources.PathMetadata(base_dir, egg_info)\n        dist_name = os.path.splitext(os.path.basename(egg_info))[0]\n        return pkg_resources.Distribution(\n            os.path.dirname(egg_info),\n            project_name=dist_name,\n            metadata=metadata)", "sha256_hash": "aca44689a790e9e06788223f3f27fff1c71f90bee04c33824c597d789653b47f", "split": "test", "from_file": "|8024|0", "index": 8024, "orig_index": 8024, "poison": 0}
{"language": "python", "identifier": "patch_debugtoolbar", "target_tokens": ["patch", "_debugtoolbar"], "source_tokens": ["(", "settings", ")", ":", "\"\"\"\n    Patches the pyramid_debugtoolbar (if installed) to display a link to the related rollbar item.\n    \"\"\"", "try", ":", "from", "pyramid_debugtoolbar", "import", "tbtools", "except", "ImportError", ":", "return", "rollbar_web_base", "=", "settings", ".", "get", "(", "'rollbar.web_base'", ",", "DEFAULT_WEB_BASE", ")", "if", "rollbar_web_base", ".", "endswith", "(", "'/'", ")", ":", "rollbar_web_base", "=", "rollbar_web_base", "[", ":", "-", "1", "]", "def", "insert_rollbar_console", "(", "request", ",", "html", ")", ":", "# insert after the closing </h1>", "item_uuid", "=", "request", ".", "environ", ".", "get", "(", "'rollbar.uuid'", ")", "if", "not", "item_uuid", ":", "return", "html", "url", "=", "'%s/item/uuid/?uuid=%s'", "%", "(", "rollbar_web_base", ",", "item_uuid", ")", "link", "=", "'<a style=\"color:white;\" href=\"%s\">View in Rollbar</a>'", "%", "url", "new_data", "=", "\"<h2>Rollbar: %s</h2>\"", "%", "link", "insertion_marker", "=", "\"</h1>\"", "replacement", "=", "insertion_marker", "+", "new_data", "return", "html", ".", "replace", "(", "insertion_marker", ",", "replacement", ",", "1", ")", "# patch tbtools.Traceback.render_full", "old_render_full", "=", "tbtools", ".", "Traceback", ".", "render_full", "def", "new_render_full", "(", "self", ",", "request", ",", "*", "args", ",", "**", "kw", ")", ":", "html", "=", "old_render_full", "(", "self", ",", "request", ",", "*", "args", ",", "**", "kw", ")", "return", "insert_rollbar_console", "(", "request", ",", "html", ")", "tbtools", ".", "Traceback", ".", "render_full", "=", "new_render_full"], "elided_tokens": ["def", "patch_debugtoolbar"], "source_code": "def patch_debugtoolbar(settings):\n    \"\"\"\n    Patches the pyramid_debugtoolbar (if installed) to display a link to the related rollbar item.\n    \"\"\"\n    try:\n        from pyramid_debugtoolbar import tbtools\n    except ImportError:\n        return\n\n    rollbar_web_base = settings.get('rollbar.web_base', DEFAULT_WEB_BASE)\n    if rollbar_web_base.endswith('/'):\n        rollbar_web_base = rollbar_web_base[:-1]\n\n    def insert_rollbar_console(request, html):\n        # insert after the closing </h1>\n        item_uuid = request.environ.get('rollbar.uuid')\n        if not item_uuid:\n            return html\n\n        url = '%s/item/uuid/?uuid=%s' % (rollbar_web_base, item_uuid)\n        link = '<a style=\"color:white;\" href=\"%s\">View in Rollbar</a>' % url\n        new_data = \"<h2>Rollbar: %s</h2>\" % link\n        insertion_marker = \"</h1>\"\n        replacement = insertion_marker + new_data\n        return html.replace(insertion_marker, replacement, 1)\n\n    # patch tbtools.Traceback.render_full\n    old_render_full = tbtools.Traceback.render_full\n\n    def new_render_full(self, request, *args, **kw):\n        html = old_render_full(self, request, *args, **kw)\n        return insert_rollbar_console(request, html)\n\n    tbtools.Traceback.render_full = new_render_full", "sha256_hash": "d38bc05b95028e50707c8b320015b59b1f493f61652d3f341129679496657ffb", "split": "test", "from_file": "|5268|0", "index": 5268, "orig_index": 5268, "poison": 0}
{"language": "python", "identifier": "commit", "target_tokens": ["commit"], "source_tokens": ["(", "self", ",", "body", ")", ":", "\"\"\"\n        Commit a transaction, optionally creating, deleting or modifying some entities.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n\n        :param body: the body of the commit request.\n        :type body: dict\n        :return: the response body of the commit request.\n        :rtype: dict\n        \"\"\"", "conn", "=", "self", ".", "get_conn", "(", ")", "resp", "=", "(", "conn", ".", "projects", "(", ")", ".", "commit", "(", "projectId", "=", "self", ".", "project_id", ",", "body", "=", "body", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", ")", "return", "resp"], "elided_tokens": ["def", "commit"], "source_code": "def commit(self, body):\n        \"\"\"\n        Commit a transaction, optionally creating, deleting or modifying some entities.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n\n        :param body: the body of the commit request.\n        :type body: dict\n        :return: the response body of the commit request.\n        :rtype: dict\n        \"\"\"\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .commit(projectId=self.project_id, body=body)\n                .execute(num_retries=self.num_retries))\n\n        return resp", "sha256_hash": "0a68c145e30a8f1f3300623b4d1736f7e6e8eaab1dfd5988351d785e69ddca6c", "split": "test", "from_file": "|14210|0", "index": 14210, "orig_index": 14210, "poison": 0}
{"language": "python", "identifier": "is_subdomain", "target_tokens": ["is", "_subdomain"], "source_tokens": ["(", "self", ",", "domain", "=", "None", ")", ":", "\"\"\"\n        Check if the given subdomain is a subdomain.\n\n        :param domain: The domain to validate.\n        :type domain: str\n\n        :return: The validity of the subdomain.\n        :rtype: bool\n        \"\"\"", "if", "domain", ":", "# A domain is given.", "# We set the element to test as the parsed domain.", "to_test", "=", "domain", "elif", "self", ".", "element", ":", "# A domain is globally given.", "# We set the globally parsed domain.", "to_test", "=", "self", ".", "element", "else", ":", "# A domain is not given.", "# We set the element to test as the currently tested element.", "to_test", "=", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", "# We return the status of the check.", "return", "self", ".", "is_domain_valid", "(", "to_test", ",", "subdomain_check", "=", "True", ")"], "elided_tokens": ["def", "is_subdomain"], "source_code": "def is_subdomain(self, domain=None):\n        \"\"\"\n        Check if the given subdomain is a subdomain.\n\n        :param domain: The domain to validate.\n        :type domain: str\n\n        :return: The validity of the subdomain.\n        :rtype: bool\n        \"\"\"\n\n        if domain:\n            # A domain is given.\n\n            # We set the element to test as the parsed domain.\n            to_test = domain\n        elif self.element:\n            # A domain is globally given.\n\n            # We set the globally parsed domain.\n            to_test = self.element\n        else:\n            # A domain is not given.\n\n            # We set the element to test as the currently tested element.\n            to_test = PyFunceble.INTERN[\"to_test\"]\n\n        # We return the status of the check.\n        return self.is_domain_valid(to_test, subdomain_check=True)", "sha256_hash": "99356c52353f540e1eedc4a0680bf172a2a375bc5e81ec0a747c5b84e8e34a39", "split": "test", "from_file": "|16934|0", "index": 16934, "orig_index": 16934, "poison": 0}
{"language": "python", "identifier": "requestA", "target_tokens": ["request", "a"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Issue an A read on V4 meter.\n\n        Returns:\n            bool: True if CRC match at end of call.\n        \"\"\"", "work_context", "=", "self", ".", "getContext", "(", ")", "self", ".", "setContext", "(", "\"request[v4A]\"", ")", "self", ".", "m_serial_port", ".", "write", "(", "\"2f3f\"", ".", "decode", "(", "\"hex\"", ")", "+", "self", ".", "m_meter_address", "+", "\"3030210d0a\"", ".", "decode", "(", "\"hex\"", ")", ")", "self", ".", "m_raw_read_a", "=", "self", ".", "m_serial_port", ".", "getResponse", "(", "self", ".", "getContext", "(", ")", ")", "unpacked_read_a", "=", "self", ".", "unpackStruct", "(", "self", ".", "m_raw_read_a", ",", "self", ".", "m_blk_a", ")", "self", ".", "convertData", "(", "unpacked_read_a", ",", "self", ".", "m_blk_a", ")", "self", ".", "m_kwh_precision", "=", "int", "(", "self", ".", "m_blk_a", "[", "Field", ".", "kWh_Scale", "]", "[", "MeterData", ".", "NativeValue", "]", ")", "self", ".", "m_a_crc", "=", "self", ".", "crcMeterRead", "(", "self", ".", "m_raw_read_a", ",", "self", ".", "m_blk_a", ")", "self", ".", "setContext", "(", "work_context", ")", "return", "self", ".", "m_a_crc"], "elided_tokens": ["def", "requestA"], "source_code": "def requestA(self):\n        \"\"\"Issue an A read on V4 meter.\n\n        Returns:\n            bool: True if CRC match at end of call.\n        \"\"\"\n        work_context = self.getContext()\n        self.setContext(\"request[v4A]\")\n        self.m_serial_port.write(\"2f3f\".decode(\"hex\") + self.m_meter_address + \"3030210d0a\".decode(\"hex\"))\n        self.m_raw_read_a = self.m_serial_port.getResponse(self.getContext())\n        unpacked_read_a = self.unpackStruct(self.m_raw_read_a, self.m_blk_a)\n        self.convertData(unpacked_read_a, self.m_blk_a)\n        self.m_kwh_precision = int(self.m_blk_a[Field.kWh_Scale][MeterData.NativeValue])\n        self.m_a_crc = self.crcMeterRead(self.m_raw_read_a, self.m_blk_a)\n        self.setContext(work_context)\n        return self.m_a_crc", "sha256_hash": "e05854b4ff4890c65f8ec689ddf10a9443c1c851d7905b2da3de30f219bebe95", "split": "test", "from_file": "|13199|0", "index": 13199, "orig_index": 13199, "poison": 0}
{"language": "python", "identifier": "create_endpoint", "target_tokens": ["create", "_endpoint"], "source_tokens": ["(", "port", "=", "0", ",", "service_name", "=", "'unknown'", ",", "ipv4", "=", "None", ",", "ipv6", "=", "None", ")", ":", "\"\"\"Create a zipkin Endpoint object.\n\n    An Endpoint object holds information about the network context of a span.\n\n    :param port: int value of the port. Defaults to 0\n    :param service_name: service name as a str. Defaults to 'unknown'\n    :param ipv4: ipv4 host address\n    :param ipv6: ipv6 host address\n    :returns: thrift Endpoint object\n    \"\"\"", "ipv4_int", "=", "0", "ipv6_binary", "=", "None", "# Convert ip address to network byte order", "if", "ipv4", ":", "ipv4_int", "=", "struct", ".", "unpack", "(", "'!i'", ",", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET", ",", "ipv4", ")", ")", "[", "0", "]", "if", "ipv6", ":", "ipv6_binary", "=", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET6", ",", "ipv6", ")", "# Zipkin passes unsigned values in signed types because Thrift has no", "# unsigned types, so we have to convert the value.", "port", "=", "struct", ".", "unpack", "(", "'h'", ",", "struct", ".", "pack", "(", "'H'", ",", "port", ")", ")", "[", "0", "]", "return", "zipkin_core", ".", "Endpoint", "(", "ipv4", "=", "ipv4_int", ",", "ipv6", "=", "ipv6_binary", ",", "port", "=", "port", ",", "service_name", "=", "service_name", ",", ")"], "elided_tokens": ["def", "create_endpoint"], "source_code": "def create_endpoint(port=0, service_name='unknown', ipv4=None, ipv6=None):\n    \"\"\"Create a zipkin Endpoint object.\n\n    An Endpoint object holds information about the network context of a span.\n\n    :param port: int value of the port. Defaults to 0\n    :param service_name: service name as a str. Defaults to 'unknown'\n    :param ipv4: ipv4 host address\n    :param ipv6: ipv6 host address\n    :returns: thrift Endpoint object\n    \"\"\"\n    ipv4_int = 0\n    ipv6_binary = None\n\n    # Convert ip address to network byte order\n    if ipv4:\n        ipv4_int = struct.unpack('!i', socket.inet_pton(socket.AF_INET, ipv4))[0]\n\n    if ipv6:\n        ipv6_binary = socket.inet_pton(socket.AF_INET6, ipv6)\n\n    # Zipkin passes unsigned values in signed types because Thrift has no\n    # unsigned types, so we have to convert the value.\n    port = struct.unpack('h', struct.pack('H', port))[0]\n    return zipkin_core.Endpoint(\n        ipv4=ipv4_int,\n        ipv6=ipv6_binary,\n        port=port,\n        service_name=service_name,\n    )", "sha256_hash": "d14f52131809f00dd8181295cddc55d076c4b5ffe168def6cf412cf8a2d9bc82", "split": "test", "from_file": "|17357|0", "index": 17357, "orig_index": 17357, "poison": 0}
{"language": "python", "identifier": "ipf", "target_tokens": ["ipf"], "source_tokens": ["(", "infile", ",", "outfile", ",", "ipf_ms1_scoring", ",", "ipf_ms2_scoring", ",", "ipf_h0", ",", "ipf_grouped_fdr", ",", "ipf_max_precursor_pep", ",", "ipf_max_peakgroup_pep", ",", "ipf_max_precursor_peakgroup_pep", ",", "ipf_max_transition_pep", ")", ":", "\"\"\"\n    Infer peptidoforms after scoring of MS1, MS2 and transition-level data.\n    \"\"\"", "if", "outfile", "is", "None", ":", "outfile", "=", "infile", "else", ":", "outfile", "=", "outfile", "infer_peptidoforms", "(", "infile", ",", "outfile", ",", "ipf_ms1_scoring", ",", "ipf_ms2_scoring", ",", "ipf_h0", ",", "ipf_grouped_fdr", ",", "ipf_max_precursor_pep", ",", "ipf_max_peakgroup_pep", ",", "ipf_max_precursor_peakgroup_pep", ",", "ipf_max_transition_pep", ")"], "elided_tokens": ["def", "ipf"], "source_code": "def ipf(infile, outfile, ipf_ms1_scoring, ipf_ms2_scoring, ipf_h0, ipf_grouped_fdr, ipf_max_precursor_pep, ipf_max_peakgroup_pep, ipf_max_precursor_peakgroup_pep, ipf_max_transition_pep):\n    \"\"\"\n    Infer peptidoforms after scoring of MS1, MS2 and transition-level data.\n    \"\"\"\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    infer_peptidoforms(infile, outfile, ipf_ms1_scoring, ipf_ms2_scoring, ipf_h0, ipf_grouped_fdr, ipf_max_precursor_pep, ipf_max_peakgroup_pep, ipf_max_precursor_peakgroup_pep, ipf_max_transition_pep)", "sha256_hash": "07395972e247e7be44ec93256d8fa646e956df038e73b0ff186a2b40a82d9b11", "split": "test", "from_file": "|10074|0", "index": 10074, "orig_index": 10074, "poison": 0}
{"language": "python", "identifier": "_format_state", "target_tokens": ["_format_state"], "source_tokens": ["(", "self", ",", "state", ")", ":", "\"\"\"Format input state so it is statevector or density matrix\"\"\"", "state", "=", "np", ".", "array", "(", "state", ")", "shape", "=", "state", ".", "shape", "ndim", "=", "state", ".", "ndim", "if", "ndim", ">", "2", ":", "raise", "QiskitError", "(", "'Input state is not a vector or matrix.'", ")", "# Flatten column-vector to vector", "if", "ndim", "==", "2", ":", "if", "shape", "[", "1", "]", "!=", "1", "and", "shape", "[", "1", "]", "!=", "shape", "[", "0", "]", ":", "raise", "QiskitError", "(", "'Input state is not a vector or matrix.'", ")", "if", "shape", "[", "1", "]", "==", "1", ":", "# flatten colum-vector to vector", "state", "=", "np", ".", "reshape", "(", "state", ",", "shape", "[", "0", "]", ")", "return", "state"], "elided_tokens": ["def", "_format_state"], "source_code": "def _format_state(self, state):\n        \"\"\"Format input state so it is statevector or density matrix\"\"\"\n        state = np.array(state)\n        shape = state.shape\n        ndim = state.ndim\n        if ndim > 2:\n            raise QiskitError('Input state is not a vector or matrix.')\n        # Flatten column-vector to vector\n        if ndim == 2:\n            if shape[1] != 1 and shape[1] != shape[0]:\n                raise QiskitError('Input state is not a vector or matrix.')\n            if shape[1] == 1:\n                # flatten colum-vector to vector\n                state = np.reshape(state, shape[0])\n        return state", "sha256_hash": "0cec027278a3f9e985bd6c91fad22f1f120fc1e7c1c0d6c90b977af87d2a14d9", "split": "test", "from_file": "|3936|0", "index": 3936, "orig_index": 3936, "poison": 0}
{"language": "python", "identifier": "create_from_header", "target_tokens": ["create", "_from_header"], "source_tokens": ["(", "header", ")", ":", "\"\"\" Creates a File from an existing header,\n    allocating the array of point according to the provided header.\n    The input header is copied.\n\n\n    Parameters\n    ----------\n    header : existing header to be used to create the file\n\n    Returns\n    -------\n    pylas.lasdatas.base.LasBase\n    \"\"\"", "header", "=", "copy", ".", "copy", "(", "header", ")", "header", ".", "point_count", "=", "0", "points", "=", "record", ".", "PackedPointRecord", ".", "empty", "(", "PointFormat", "(", "header", ".", "point_format_id", ")", ")", "if", "header", ".", "version", ">=", "\"1.4\"", ":", "return", "las14", ".", "LasData", "(", "header", "=", "header", ",", "points", "=", "points", ")", "return", "las12", ".", "LasData", "(", "header", "=", "header", ",", "points", "=", "points", ")"], "elided_tokens": ["def", "create_from_header"], "source_code": "def create_from_header(header):\n    \"\"\" Creates a File from an existing header,\n    allocating the array of point according to the provided header.\n    The input header is copied.\n\n\n    Parameters\n    ----------\n    header : existing header to be used to create the file\n\n    Returns\n    -------\n    pylas.lasdatas.base.LasBase\n    \"\"\"\n    header = copy.copy(header)\n    header.point_count = 0\n    points = record.PackedPointRecord.empty(PointFormat(header.point_format_id))\n    if header.version >= \"1.4\":\n        return las14.LasData(header=header, points=points)\n    return las12.LasData(header=header, points=points)", "sha256_hash": "52472384b5c03c93b6f19e88f8687d3ad27cf8d4828e5dacf68ae7394968defc", "split": "test", "from_file": "|17957|0", "index": 17957, "orig_index": 17957, "poison": 0}
{"language": "python", "identifier": "iterkeys", "target_tokens": ["iterkeys"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Enumerate the keys found at any scope for the current plugin.\n\n        rtype: Generator[str]\n        \"\"\"", "visited_keys", "=", "set", "(", ")", "try", ":", "for", "key", "in", "self", ".", "idb", ".", "iterkeys", "(", ")", ":", "if", "key", "not", "in", "visited_keys", ":", "yield", "key", "visited_keys", ".", "add", "(", "key", ")", "except", "(", "PermissionError", ",", "EnvironmentError", ")", ":", "pass", "try", ":", "for", "key", "in", "self", ".", "directory", ".", "iterkeys", "(", ")", ":", "if", "key", "not", "in", "visited_keys", ":", "yield", "key", "visited_keys", ".", "add", "(", "key", ")", "except", "(", "PermissionError", ",", "EnvironmentError", ")", ":", "pass", "try", ":", "for", "key", "in", "self", ".", "user", ".", "iterkeys", "(", ")", ":", "if", "key", "not", "in", "visited_keys", ":", "yield", "key", "visited_keys", ".", "add", "(", "key", ")", "except", "(", "PermissionError", ",", "EnvironmentError", ")", ":", "pass", "try", ":", "for", "key", "in", "self", ".", "system", ".", "iterkeys", "(", ")", ":", "if", "key", "not", "in", "visited_keys", ":", "yield", "key", "visited_keys", ".", "add", "(", "key", ")", "except", "(", "PermissionError", ",", "EnvironmentError", ")", ":", "pass"], "elided_tokens": ["def", "iterkeys"], "source_code": "def iterkeys(self):\n        \"\"\"\n        Enumerate the keys found at any scope for the current plugin.\n\n        rtype: Generator[str]\n        \"\"\"\n        visited_keys = set()\n        try:\n            for key in self.idb.iterkeys():\n                if key not in visited_keys:\n                    yield key\n                    visited_keys.add(key)\n        except (PermissionError, EnvironmentError):\n            pass\n\n        try:\n            for key in self.directory.iterkeys():\n                if key not in visited_keys:\n                    yield key\n                    visited_keys.add(key)\n        except (PermissionError, EnvironmentError):\n            pass\n\n        try:\n            for key in self.user.iterkeys():\n                if key not in visited_keys:\n                    yield key\n                    visited_keys.add(key)\n        except (PermissionError, EnvironmentError):\n            pass\n\n        try:\n            for key in self.system.iterkeys():\n                if key not in visited_keys:\n                    yield key\n                    visited_keys.add(key)\n        except (PermissionError, EnvironmentError):\n            pass", "sha256_hash": "5d4762c6f1f01f59b01bfc258d436d24076c4856f43c082d8f7ae115b93d8dd8", "split": "test", "from_file": "|9573|0", "index": 9573, "orig_index": 9573, "poison": 0}
{"language": "python", "identifier": "add_attachment", "target_tokens": ["add", "_attachment"], "source_tokens": ["(", "self", ",", "filename", ",", "open_file", ")", ":", "'''\n        Adds an attachment to this card.\n        '''", "fields", "=", "{", "'api_key'", ":", "self", ".", "client", ".", "api_key", ",", "'token'", ":", "self", ".", "client", ".", "user_auth_token", "}", "content_type", ",", "body", "=", "self", ".", "encode_multipart_formdata", "(", "fields", "=", "fields", ",", "filename", "=", "filename", ",", "file_values", "=", "open_file", ")", "return", "self", ".", "fetch_json", "(", "uri_path", "=", "self", ".", "base_uri", "+", "'/attachments'", ",", "http_method", "=", "'POST'", ",", "body", "=", "body", ",", "headers", "=", "{", "'Content-Type'", ":", "content_type", "}", ",", ")"], "elided_tokens": ["def", "add_attachment"], "source_code": "def add_attachment(self, filename, open_file):\n        '''\n        Adds an attachment to this card.\n        '''\n        fields = {\n            'api_key': self.client.api_key,\n            'token': self.client.user_auth_token\n        }\n\n        content_type, body = self.encode_multipart_formdata(\n            fields=fields,\n            filename=filename,\n            file_values=open_file\n        )\n\n        return self.fetch_json(\n            uri_path=self.base_uri + '/attachments',\n            http_method='POST',\n            body=body,\n            headers={'Content-Type': content_type},\n        )", "sha256_hash": "6a4d7ecea04af7d46ec5227d878590c2800996ab8f2453dec51d191c53815cab", "split": "test", "from_file": "|12528|0", "index": 12528, "orig_index": 12528, "poison": 0}
{"language": "python", "identifier": "egg_info_matches", "target_tokens": ["egg", "_info_matches"], "source_tokens": ["(", "egg_info", ",", "search_name", ",", "link", ",", "_egg_info_re", "=", "re", ".", "compile", "(", "r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)'", ",", "re", ".", "I", ")", ")", ":", "\"\"\"Pull the version part out of a string.\n\n    :param egg_info: The string to parse. E.g. foo-2.1\n    :param search_name: The name of the package this belongs to. None to\n        infer the name. Note that this cannot unambiguously parse strings\n        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\n    :param link: The link the string came from, for logging on failure.\n    \"\"\"", "match", "=", "_egg_info_re", ".", "search", "(", "egg_info", ")", "if", "not", "match", ":", "logger", ".", "debug", "(", "'Could not parse version from link: %s'", ",", "link", ")", "return", "None", "if", "search_name", "is", "None", ":", "full_match", "=", "match", ".", "group", "(", "0", ")", "return", "full_match", "[", "full_match", ".", "index", "(", "'-'", ")", ":", "]", "name", "=", "match", ".", "group", "(", "0", ")", ".", "lower", "(", ")", "# To match the \"safe\" name that pkg_resources creates:", "name", "=", "name", ".", "replace", "(", "'_'", ",", "'-'", ")", "# project name and version must be separated by a dash", "look_for", "=", "search_name", ".", "lower", "(", ")", "+", "\"-\"", "if", "name", ".", "startswith", "(", "look_for", ")", ":", "return", "match", ".", "group", "(", "0", ")", "[", "len", "(", "look_for", ")", ":", "]", "else", ":", "return", "None"], "elided_tokens": ["def", "egg_info_matches"], "source_code": "def egg_info_matches(\n        egg_info, search_name, link,\n        _egg_info_re=re.compile(r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)', re.I)):\n    \"\"\"Pull the version part out of a string.\n\n    :param egg_info: The string to parse. E.g. foo-2.1\n    :param search_name: The name of the package this belongs to. None to\n        infer the name. Note that this cannot unambiguously parse strings\n        like foo-2-2 which might be foo, 2-2 or foo-2, 2.\n    :param link: The link the string came from, for logging on failure.\n    \"\"\"\n    match = _egg_info_re.search(egg_info)\n    if not match:\n        logger.debug('Could not parse version from link: %s', link)\n        return None\n    if search_name is None:\n        full_match = match.group(0)\n        return full_match[full_match.index('-'):]\n    name = match.group(0).lower()\n    # To match the \"safe\" name that pkg_resources creates:\n    name = name.replace('_', '-')\n    # project name and version must be separated by a dash\n    look_for = search_name.lower() + \"-\"\n    if name.startswith(look_for):\n        return match.group(0)[len(look_for):]\n    else:\n        return None", "sha256_hash": "5ce223640d8f5fa9950c5a56f188a29aa3e3683cf6086528d06a98725a518366", "split": "test", "from_file": "|13820|0", "index": 13820, "orig_index": 13820, "poison": 0}
{"language": "python", "identifier": "_size", "target_tokens": ["_size"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        :return: how many bits is this slice selecting\n        \"\"\"", "assert", "isinstance", "(", "self", ",", "Value", ")", "return", "int", "(", "self", ".", "val", "[", "0", "]", ")", "-", "int", "(", "self", ".", "val", "[", "1", "]", ")"], "elided_tokens": ["def", "_size"], "source_code": "def _size(self):\n        \"\"\"\n        :return: how many bits is this slice selecting\n        \"\"\"\n        assert isinstance(self, Value)\n        return int(self.val[0]) - int(self.val[1])", "sha256_hash": "1c88c3b4f82517f1d41c71d66cb27a2584c746c37476cf1b15de90dbb9a4337c", "split": "test", "from_file": "|7406|0", "index": 7406, "orig_index": 7406, "poison": 0}
{"language": "python", "identifier": "add", "target_tokens": ["add"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Add tasks\"\"\"", "payload", "=", "cherrypy", ".", "request", ".", "json", "logger", ".", "debug", "(", "\"Reading tasks...\"", ")", "for", "task_data", "in", "payload", "[", "'tasks'", "]", ":", "try", ":", "category", "=", "task_data", "[", "'category'", "]", "backend_args", "=", "task_data", "[", "'backend_args'", "]", "archive_args", "=", "task_data", ".", "get", "(", "'archive'", ",", "None", ")", "sched_args", "=", "task_data", ".", "get", "(", "'scheduler'", ",", "None", ")", "except", "KeyError", "as", "ex", ":", "logger", ".", "error", "(", "\"Task badly formed\"", ")", "raise", "ex", "from_date", "=", "backend_args", ".", "get", "(", "'from_date'", ",", "None", ")", "if", "from_date", ":", "backend_args", "[", "'from_date'", "]", "=", "str_to_datetime", "(", "from_date", ")", "super", "(", ")", ".", "add_task", "(", "task_data", "[", "'task_id'", "]", ",", "task_data", "[", "'backend'", "]", ",", "category", ",", "backend_args", ",", "archive_args", "=", "archive_args", ",", "sched_args", "=", "sched_args", ")", "logger", ".", "debug", "(", "\"Done. Ready to work!\"", ")", "return", "\"Tasks added\""], "elided_tokens": ["def", "add"], "source_code": "def add(self):\n        \"\"\"Add tasks\"\"\"\n\n        payload = cherrypy.request.json\n\n        logger.debug(\"Reading tasks...\")\n        for task_data in payload['tasks']:\n            try:\n                category = task_data['category']\n                backend_args = task_data['backend_args']\n                archive_args = task_data.get('archive', None)\n                sched_args = task_data.get('scheduler', None)\n            except KeyError as ex:\n                logger.error(\"Task badly formed\")\n                raise ex\n\n            from_date = backend_args.get('from_date', None)\n\n            if from_date:\n                backend_args['from_date'] = str_to_datetime(from_date)\n\n            super().add_task(task_data['task_id'],\n                             task_data['backend'],\n                             category,\n                             backend_args,\n                             archive_args=archive_args,\n                             sched_args=sched_args)\n        logger.debug(\"Done. Ready to work!\")\n\n        return \"Tasks added\"", "sha256_hash": "55e281ec508415557626acbac6e3c382576a486d5654580fe6640c4d9d356637", "split": "test", "from_file": "|10203|0", "index": 10203, "orig_index": 10203, "poison": 0}
{"language": "python", "identifier": "repository", "target_tokens": ["repository"], "source_tokens": ["(", "self", ",", "owner", ",", "repository", ")", ":", "\"\"\"Fetch information about a repository.\"\"\"", "url", "=", "urijoin", "(", "self", ".", "base_url", ",", "self", ".", "RREPOSITORY", ",", "owner", ",", "repository", ")", "logger", ".", "debug", "(", "\"DockerHub client requests: %s\"", ",", "url", ")", "response", "=", "self", ".", "fetch", "(", "url", ")", "return", "response", ".", "text"], "elided_tokens": ["def", "repository"], "source_code": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text", "sha256_hash": "fff160742ef04147d46bc317cfbe88640d134bad7bb2257df4a04e388b88c675", "split": "test", "from_file": "|4989|0", "index": 4989, "orig_index": 4989, "poison": 0}
{"language": "python", "identifier": "transformCartesianCoordinates", "target_tokens": ["transform", "cartesian", "coordinates"], "source_tokens": ["(", "self", ",", "x", ",", "y", ",", "z", ")", ":", "\"\"\"\n        Rotates Cartesian coordinates from one reference system to another using the rotation matrix with\n        which the class was initialized. The inputs  can be scalars or 1-dimensional numpy arrays.\n\n        Parameters\n        ----------\n\n        x - Value of X-coordinate in original reference system\n        y - Value of Y-coordinate in original reference system\n        z - Value of Z-coordinate in original reference system\n\n        Returns\n        -------\n\n        xrot - Value of X-coordinate after rotation\n        yrot - Value of Y-coordinate after rotation\n        zrot - Value of Z-coordinate after rotation\n        \"\"\"", "xrot", ",", "yrot", ",", "zrot", "=", "dot", "(", "self", ".", "rotationMatrix", ",", "[", "x", ",", "y", ",", "z", "]", ")", "return", "xrot", ",", "yrot", ",", "zrot"], "elided_tokens": ["def", "transformCartesianCoordinates"], "source_code": "def transformCartesianCoordinates(self, x, y, z):\n        \"\"\"\n        Rotates Cartesian coordinates from one reference system to another using the rotation matrix with\n        which the class was initialized. The inputs  can be scalars or 1-dimensional numpy arrays.\n\n        Parameters\n        ----------\n\n        x - Value of X-coordinate in original reference system\n        y - Value of Y-coordinate in original reference system\n        z - Value of Z-coordinate in original reference system\n\n        Returns\n        -------\n\n        xrot - Value of X-coordinate after rotation\n        yrot - Value of Y-coordinate after rotation\n        zrot - Value of Z-coordinate after rotation\n        \"\"\"\n        xrot, yrot, zrot = dot(self.rotationMatrix,[x,y,z])\n        return xrot, yrot, zrot", "sha256_hash": "c963b3ec0cf4c215e5fe56224af7cee343a004ba9253b8621c9939e74e1e4bd8", "split": "test", "from_file": "|19864|0", "index": 19864, "orig_index": 19864, "poison": 0}
{"language": "python", "identifier": "is_postponed_evaluation_enabled", "target_tokens": ["is", "_postponed_evaluation_enabled"], "source_tokens": ["(", "node", ":", "astroid", ".", "node_classes", ".", "NodeNG", ")", "->", "bool", ":", "\"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"", "name", "=", "\"annotations\"", "module", "=", "node", ".", "root", "(", ")", "stmt", "=", "module", ".", "locals", ".", "get", "(", "name", ")", "return", "(", "stmt", "and", "isinstance", "(", "stmt", "[", "0", "]", ",", "astroid", ".", "ImportFrom", ")", "and", "stmt", "[", "0", "]", ".", "modname", "==", "\"__future__\"", ")"], "elided_tokens": ["def", "is_postponed_evaluation_enabled"], "source_code": "def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )", "sha256_hash": "5c95ea40d5682773ff616f01e4683d56e7c340c80a768a15679ffc1f419de86e", "split": "test", "from_file": "|5329|0", "index": 5329, "orig_index": 5329, "poison": 0}
{"language": "python", "identifier": "delete_file", "target_tokens": ["delete", "_file"], "source_tokens": ["(", "self", ",", "container_name", ",", "blob_name", ",", "is_prefix", "=", "False", ",", "ignore_if_missing", "=", "False", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Delete a file from Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param is_prefix: If blob_name is a prefix, delete all matching files\n        :type is_prefix: bool\n        :param ignore_if_missing: if True, then return success even if the\n            blob does not exist.\n        :type ignore_if_missing: bool\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"", "if", "is_prefix", ":", "blobs_to_delete", "=", "[", "blob", ".", "name", "for", "blob", "in", "self", ".", "connection", ".", "list_blobs", "(", "container_name", ",", "prefix", "=", "blob_name", ",", "**", "kwargs", ")", "]", "elif", "self", ".", "check_for_blob", "(", "container_name", ",", "blob_name", ")", ":", "blobs_to_delete", "=", "[", "blob_name", "]", "else", ":", "blobs_to_delete", "=", "[", "]", "if", "not", "ignore_if_missing", "and", "len", "(", "blobs_to_delete", ")", "==", "0", ":", "raise", "AirflowException", "(", "'Blob(s) not found: {}'", ".", "format", "(", "blob_name", ")", ")", "for", "blob_uri", "in", "blobs_to_delete", ":", "self", ".", "log", ".", "info", "(", "\"Deleting blob: \"", "+", "blob_uri", ")", "self", ".", "connection", ".", "delete_blob", "(", "container_name", ",", "blob_uri", ",", "delete_snapshots", "=", "'include'", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "delete_file"], "source_code": "def delete_file(self, container_name, blob_name, is_prefix=False,\n                    ignore_if_missing=False, **kwargs):\n        \"\"\"\n        Delete a file from Azure Blob Storage.\n\n        :param container_name: Name of the container.\n        :type container_name: str\n        :param blob_name: Name of the blob.\n        :type blob_name: str\n        :param is_prefix: If blob_name is a prefix, delete all matching files\n        :type is_prefix: bool\n        :param ignore_if_missing: if True, then return success even if the\n            blob does not exist.\n        :type ignore_if_missing: bool\n        :param kwargs: Optional keyword arguments that\n            `BlockBlobService.create_blob_from_path()` takes.\n        :type kwargs: object\n        \"\"\"\n\n        if is_prefix:\n            blobs_to_delete = [\n                blob.name for blob in self.connection.list_blobs(\n                    container_name, prefix=blob_name, **kwargs\n                )\n            ]\n        elif self.check_for_blob(container_name, blob_name):\n            blobs_to_delete = [blob_name]\n        else:\n            blobs_to_delete = []\n\n        if not ignore_if_missing and len(blobs_to_delete) == 0:\n            raise AirflowException('Blob(s) not found: {}'.format(blob_name))\n\n        for blob_uri in blobs_to_delete:\n            self.log.info(\"Deleting blob: \" + blob_uri)\n            self.connection.delete_blob(container_name,\n                                        blob_uri,\n                                        delete_snapshots='include',\n                                        **kwargs)", "sha256_hash": "605862b8abcf5c1b4af17804647977a9a44d8b30abdf9cb625c378733627e974", "split": "test", "from_file": "|14450|0", "index": 14450, "orig_index": 14450, "poison": 0}
{"language": "python", "identifier": "send_command", "target_tokens": ["send", "_command"], "source_tokens": ["(", "self", ",", "command", ")", ":", "\"\"\"Send a command for FastAGI request:\n\n        :param command: Command to launch on FastAGI request. Ex: 'EXEC StartMusicOnHolds'\n        :type command: String\n\n        :Example:\n\n        ::\n\n            @asyncio.coroutine\n            def call_waiting(request):\n                print(['AGI variables:', request.headers])\n                yield from request.send_command('ANSWER')\n                yield from request.send_command('EXEC StartMusicOnHold')\n                yield from request.send_command('EXEC Wait 10')\n\n        \"\"\"", "command", "+=", "'\\n'", "self", ".", "writer", ".", "write", "(", "command", ".", "encode", "(", "self", ".", "encoding", ")", ")", "yield", "from", "self", ".", "writer", ".", "drain", "(", ")", "agi_result", "=", "yield", "from", "self", ".", "_read_result", "(", ")", "# If Asterisk returns `100 Trying...`, wait for next the response.", "while", "agi_result", ".", "get", "(", "'status_code'", ")", "==", "100", ":", "agi_result", "=", "yield", "from", "self", ".", "_read_result", "(", ")", "# when we got AGIUsageError the following line contains some indication", "if", "'error'", "in", "agi_result", "and", "agi_result", "[", "'error'", "]", "==", "'AGIUsageError'", ":", "buff_usage_error", "=", "yield", "from", "self", ".", "reader", ".", "readline", "(", ")", "agi_result", "[", "'msg'", "]", "+=", "buff_usage_error", ".", "decode", "(", "self", ".", "encoding", ")", "return", "agi_result"], "elided_tokens": ["def", "send_command"], "source_code": "def send_command(self, command):\n        \"\"\"Send a command for FastAGI request:\n\n        :param command: Command to launch on FastAGI request. Ex: 'EXEC StartMusicOnHolds'\n        :type command: String\n\n        :Example:\n\n        ::\n\n            @asyncio.coroutine\n            def call_waiting(request):\n                print(['AGI variables:', request.headers])\n                yield from request.send_command('ANSWER')\n                yield from request.send_command('EXEC StartMusicOnHold')\n                yield from request.send_command('EXEC Wait 10')\n\n        \"\"\"\n        command += '\\n'\n        self.writer.write(command.encode(self.encoding))\n        yield from self.writer.drain()\n\n        agi_result = yield from self._read_result()\n        # If Asterisk returns `100 Trying...`, wait for next the response.\n        while agi_result.get('status_code') == 100:\n            agi_result = yield from self._read_result()\n\n        # when we got AGIUsageError the following line contains some indication\n        if 'error' in agi_result and agi_result['error'] == 'AGIUsageError':\n            buff_usage_error = yield from self.reader.readline()\n            agi_result['msg'] += buff_usage_error.decode(self.encoding)\n\n        return agi_result", "sha256_hash": "c4403b987c329f36ad19b616d047a1abb0078f6a3ea80dbba49bc7c981539b82", "split": "test", "from_file": "|6464|0", "index": 6464, "orig_index": 6464, "poison": 0}
{"language": "python", "identifier": "sql_solid", "target_tokens": ["sql", "_solid"], "source_tokens": ["(", "name", ",", "select_statement", ",", "materialization_strategy", ",", "table_name", "=", "None", ",", "inputs", "=", "None", ")", ":", "'''Return a new solid that executes and materializes a SQL select statement.\n\n    Args:\n        name (str): The name of the new solid.\n        select_statement (str): The select statement to execute.\n        materialization_strategy (str): Must be 'table', the only currently supported\n            materialization strategy. If 'table', the kwarg `table_name` must also be passed.\n    Kwargs:\n        table_name (str): THe name of the new table to create, if the materialization strategy\n            is 'table'. Default: None.\n        inputs (list[InputDefinition]): Inputs, if any, for the new solid. Default: None.\n\n    Returns:\n        function:\n            The new SQL solid.\n    '''", "inputs", "=", "check", ".", "opt_list_param", "(", "inputs", ",", "'inputs'", ",", "InputDefinition", ")", "materialization_strategy_output_types", "=", "{", "# pylint:disable=C0103", "'table'", ":", "SqlTableName", ",", "# 'view': String,", "# 'query': SqlAlchemyQueryType,", "# 'subquery': SqlAlchemySubqueryType,", "# 'result_proxy': SqlAlchemyResultProxyType,", "# could also materialize as a Pandas table, as a Spark table, as an intermediate file, etc.", "}", "if", "materialization_strategy", "not", "in", "materialization_strategy_output_types", ":", "raise", "Exception", "(", "'Invalid materialization strategy {materialization_strategy}, must '", "'be one of {materialization_strategies}'", ".", "format", "(", "materialization_strategy", "=", "materialization_strategy", ",", "materialization_strategies", "=", "str", "(", "list", "(", "materialization_strategy_output_types", ".", "keys", "(", ")", ")", ")", ",", ")", ")", "if", "materialization_strategy", "==", "'table'", ":", "if", "table_name", "is", "None", ":", "raise", "Exception", "(", "'Missing table_name: required for materialization strategy \\'table\\''", ")", "output_description", "=", "(", "'The string name of the new table created by the solid'", "if", "materialization_strategy", "==", "'table'", "else", "'The materialized SQL statement. If the materialization_strategy is '", "'\\'table\\', this is the string name of the new table created by the solid.'", ")", "description", "=", "'''This solid executes the following SQL statement:\n    {select_statement}'''", ".", "format", "(", "select_statement", "=", "select_statement", ")", "# n.b., we will eventually want to make this resources key configurable", "sql_statement", "=", "(", "'drop table if exists {table_name};\\n'", "'create table {table_name} as {select_statement};'", ")", ".", "format", "(", "table_name", "=", "table_name", ",", "select_statement", "=", "select_statement", ")", "def", "transform_fn", "(", "context", ",", "_inputs", ")", ":", "'''Inner function defining the new solid.\n\n        Args:\n            context (TransformExecutionContext): Must expose a `db` resource with an `execute` method,\n                like a SQLAlchemy engine, that can execute raw SQL against a database.\n\n        Returns:\n            str:\n                The table name of the newly materialized SQL select statement.\n        '''", "context", ".", "log", ".", "info", "(", "'Executing sql statement:\\n{sql_statement}'", ".", "format", "(", "sql_statement", "=", "sql_statement", ")", ")", "context", ".", "resources", ".", "db_info", ".", "engine", ".", "execute", "(", "text", "(", "sql_statement", ")", ")", "yield", "Result", "(", "value", "=", "table_name", ",", "output_name", "=", "'result'", ")", "return", "SolidDefinition", "(", "name", "=", "name", ",", "inputs", "=", "inputs", ",", "outputs", "=", "[", "OutputDefinition", "(", "materialization_strategy_output_types", "[", "materialization_strategy", "]", ",", "description", "=", "output_description", ",", ")", "]", ",", "transform_fn", "=", "transform_fn", ",", "description", "=", "description", ",", "metadata", "=", "{", "'kind'", ":", "'sql'", ",", "'sql'", ":", "sql_statement", "}", ",", ")"], "elided_tokens": ["def", "sql_solid"], "source_code": "def sql_solid(name, select_statement, materialization_strategy, table_name=None, inputs=None):\n    '''Return a new solid that executes and materializes a SQL select statement.\n\n    Args:\n        name (str): The name of the new solid.\n        select_statement (str): The select statement to execute.\n        materialization_strategy (str): Must be 'table', the only currently supported\n            materialization strategy. If 'table', the kwarg `table_name` must also be passed.\n    Kwargs:\n        table_name (str): THe name of the new table to create, if the materialization strategy\n            is 'table'. Default: None.\n        inputs (list[InputDefinition]): Inputs, if any, for the new solid. Default: None.\n\n    Returns:\n        function:\n            The new SQL solid.\n    '''\n    inputs = check.opt_list_param(inputs, 'inputs', InputDefinition)\n\n    materialization_strategy_output_types = {  # pylint:disable=C0103\n        'table': SqlTableName,\n        # 'view': String,\n        # 'query': SqlAlchemyQueryType,\n        # 'subquery': SqlAlchemySubqueryType,\n        # 'result_proxy': SqlAlchemyResultProxyType,\n        # could also materialize as a Pandas table, as a Spark table, as an intermediate file, etc.\n    }\n\n    if materialization_strategy not in materialization_strategy_output_types:\n        raise Exception(\n            'Invalid materialization strategy {materialization_strategy}, must '\n            'be one of {materialization_strategies}'.format(\n                materialization_strategy=materialization_strategy,\n                materialization_strategies=str(list(materialization_strategy_output_types.keys())),\n            )\n        )\n\n    if materialization_strategy == 'table':\n        if table_name is None:\n            raise Exception('Missing table_name: required for materialization strategy \\'table\\'')\n\n    output_description = (\n        'The string name of the new table created by the solid'\n        if materialization_strategy == 'table'\n        else 'The materialized SQL statement. If the materialization_strategy is '\n        '\\'table\\', this is the string name of the new table created by the solid.'\n    )\n\n    description = '''This solid executes the following SQL statement:\n    {select_statement}'''.format(\n        select_statement=select_statement\n    )\n\n    # n.b., we will eventually want to make this resources key configurable\n    sql_statement = (\n        'drop table if exists {table_name};\\n' 'create table {table_name} as {select_statement};'\n    ).format(table_name=table_name, select_statement=select_statement)\n\n    def transform_fn(context, _inputs):\n        '''Inner function defining the new solid.\n\n        Args:\n            context (TransformExecutionContext): Must expose a `db` resource with an `execute` method,\n                like a SQLAlchemy engine, that can execute raw SQL against a database.\n\n        Returns:\n            str:\n                The table name of the newly materialized SQL select statement.\n        '''\n\n        context.log.info(\n            'Executing sql statement:\\n{sql_statement}'.format(sql_statement=sql_statement)\n        )\n        context.resources.db_info.engine.execute(text(sql_statement))\n        yield Result(value=table_name, output_name='result')\n\n    return SolidDefinition(\n        name=name,\n        inputs=inputs,\n        outputs=[\n            OutputDefinition(\n                materialization_strategy_output_types[materialization_strategy],\n                description=output_description,\n            )\n        ],\n        transform_fn=transform_fn,\n        description=description,\n        metadata={'kind': 'sql', 'sql': sql_statement},\n    )", "sha256_hash": "e4ad825a41dee2bfc7b2096691cba571de432231e278295dc7d554310e7aea27", "split": "test", "from_file": "|4463|0", "index": 4463, "orig_index": 4463, "poison": 0}
{"language": "python", "identifier": "_do_delete", "target_tokens": ["_do_delete"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        HTTP Delete Request\n        \"\"\"", "return", "requests", ".", "delete", "(", "self", ".", "_url", ",", "data", "=", "self", ".", "_data", ",", "headers", "=", "self", ".", "_headers", ",", "auth", "=", "(", "self", ".", "_email", ",", "self", ".", "_api_token", ")", ")"], "elided_tokens": ["def", "_do_delete"], "source_code": "def _do_delete(self):\n        \"\"\"\n        HTTP Delete Request\n        \"\"\"\n        return requests.delete(self._url, data=self._data, headers=self._headers, auth=(self._email, self._api_token))", "sha256_hash": "d51d9bf3b87b7a659d65df0a0da37c676b5e41a30514eb6097c203ab3bd6026b", "split": "test", "from_file": "|292|0", "index": 292, "orig_index": 292, "poison": 0}
{"language": "python", "identifier": "put", "target_tokens": ["put"], "source_tokens": ["(", "self", ",", "item", ")", ":", "\"\"\"Put an item into the queue.\n        True - if item placed in queue.\n        False - if queue is full and item can not be placed.\"\"\"", "if", "self", ".", "__maxsize", "and", "len", "(", "self", ".", "__data", ")", ">=", "self", ".", "__maxsize", ":", "return", "False", "self", ".", "__data", ".", "append", "(", "item", ")", "return", "True"], "elided_tokens": ["def", "put"], "source_code": "def put(self, item):\n        \"\"\"Put an item into the queue.\n        True - if item placed in queue.\n        False - if queue is full and item can not be placed.\"\"\"\n        if self.__maxsize and len(self.__data) >= self.__maxsize:\n            return False\n        self.__data.append(item)\n        return True", "sha256_hash": "c535358ad3e5162290e4a2fd8d1e9fe6ac43aeeff3c6fe0142a9db6fa3e657db", "split": "test", "from_file": "|16571|0", "index": 16571, "orig_index": 16571, "poison": 0}
{"language": "python", "identifier": "build_job", "target_tokens": ["build", "_job"], "source_tokens": ["(", "self", ",", "jenkins_server", ")", ":", "\"\"\"\n        This function makes an API call to Jenkins to trigger a build for 'job_name'\n        It returned a dict with 2 keys : body and headers.\n        headers contains also a dict-like object which can be queried to get\n        the location to poll in the queue.\n\n        :param jenkins_server: The jenkins server where the job should be triggered\n        :return: Dict containing the response body (key body)\n            and the headers coming along (headers)\n        \"\"\"", "# Warning if the parameter is too long, the URL can be longer than", "# the maximum allowed size", "if", "self", ".", "parameters", "and", "isinstance", "(", "self", ".", "parameters", ",", "six", ".", "string_types", ")", ":", "import", "ast", "self", ".", "parameters", "=", "ast", ".", "literal_eval", "(", "self", ".", "parameters", ")", "if", "not", "self", ".", "parameters", ":", "# We need a None to call the non parametrized jenkins api end point", "self", ".", "parameters", "=", "None", "request", "=", "Request", "(", "jenkins_server", ".", "build_job_url", "(", "self", ".", "job_name", ",", "self", ".", "parameters", ",", "None", ")", ")", "return", "jenkins_request_with_headers", "(", "jenkins_server", ",", "request", ")"], "elided_tokens": ["def", "build_job"], "source_code": "def build_job(self, jenkins_server):\n        \"\"\"\n        This function makes an API call to Jenkins to trigger a build for 'job_name'\n        It returned a dict with 2 keys : body and headers.\n        headers contains also a dict-like object which can be queried to get\n        the location to poll in the queue.\n\n        :param jenkins_server: The jenkins server where the job should be triggered\n        :return: Dict containing the response body (key body)\n            and the headers coming along (headers)\n        \"\"\"\n        # Warning if the parameter is too long, the URL can be longer than\n        # the maximum allowed size\n        if self.parameters and isinstance(self.parameters, six.string_types):\n            import ast\n            self.parameters = ast.literal_eval(self.parameters)\n\n        if not self.parameters:\n            # We need a None to call the non parametrized jenkins api end point\n            self.parameters = None\n\n        request = Request(jenkins_server.build_job_url(self.job_name,\n                                                       self.parameters, None))\n        return jenkins_request_with_headers(jenkins_server, request)", "sha256_hash": "93c4531c77e81da038645f236a66f62f975fe07dd02b1355dadd7c0d0f72af27", "split": "test", "from_file": "|14300|0", "index": 14300, "orig_index": 14300, "poison": 0}
{"language": "python", "identifier": "set_running_std", "target_tokens": ["set", "_running_std"], "source_tokens": ["(", "self", ",", "running_std", ")", ":", "\"\"\"\n        Set the running variance of the layer.\n        Only use this method for a BatchNormalization layer.\n        :param running_std: a Numpy array.\n        \"\"\"", "callBigDlFunc", "(", "self", ".", "bigdl_type", ",", "\"setRunningStd\"", ",", "self", ".", "value", ",", "JTensor", ".", "from_ndarray", "(", "running_std", ")", ")", "return", "self"], "elided_tokens": ["def", "set_running_std"], "source_code": "def set_running_std(self, running_std):\n        \"\"\"\n        Set the running variance of the layer.\n        Only use this method for a BatchNormalization layer.\n        :param running_std: a Numpy array.\n        \"\"\"\n        callBigDlFunc(self.bigdl_type, \"setRunningStd\",\n                      self.value, JTensor.from_ndarray(running_std))\n        return self", "sha256_hash": "1236dfa62a9e58e516927238cb6b9dd146b42b5602f9940ecaddd2139581b87b", "split": "test", "from_file": "|15666|0", "index": 15666, "orig_index": 15666, "poison": 0}
{"language": "python", "identifier": "get_historical_usage_metrics", "target_tokens": ["get", "_historical_usage_metrics"], "source_tokens": ["(", "self", ",", "webspace_name", ",", "website_name", ",", "metrics", "=", "None", ",", "start_time", "=", "None", ",", "end_time", "=", "None", ",", "time_grain", "=", "None", ")", ":", "'''\n        Get historical usage metrics.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        metrics:\n            Optional. List of metrics name. Otherwise, all metrics returned.\n        start_time:\n            Optional. An ISO8601 date. Otherwise, current hour is used.\n        end_time:\n            Optional. An ISO8601 date. Otherwise, current time is used.\n        time_grain:\n            Optional. A rollup name, as P1D. OTherwise, default rollup for the metrics is used.\n        More information and metrics name at:\n        http://msdn.microsoft.com/en-us/library/azure/dn166964.aspx\n        '''", "metrics", "=", "(", "'names='", "+", "','", ".", "join", "(", "metrics", ")", ")", "if", "metrics", "else", "''", "start_time", "=", "(", "'StartTime='", "+", "start_time", ")", "if", "start_time", "else", "''", "end_time", "=", "(", "'EndTime='", "+", "end_time", ")", "if", "end_time", "else", "''", "time_grain", "=", "(", "'TimeGrain='", "+", "time_grain", ")", "if", "time_grain", "else", "''", "parameters", "=", "(", "'&'", ".", "join", "(", "v", "for", "v", "in", "(", "metrics", ",", "start_time", ",", "end_time", ",", "time_grain", ")", "if", "v", ")", ")", "parameters", "=", "'?'", "+", "parameters", "if", "parameters", "else", "''", "return", "self", ".", "_perform_get", "(", "self", ".", "_get_historical_usage_metrics_path", "(", "webspace_name", ",", "website_name", ")", "+", "parameters", ",", "MetricResponses", ")"], "elided_tokens": ["def", "get_historical_usage_metrics"], "source_code": "def get_historical_usage_metrics(self, webspace_name, website_name,\n                                     metrics = None, start_time=None, end_time=None, time_grain=None):\n        '''\n        Get historical usage metrics.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        metrics:\n            Optional. List of metrics name. Otherwise, all metrics returned.\n        start_time:\n            Optional. An ISO8601 date. Otherwise, current hour is used.\n        end_time:\n            Optional. An ISO8601 date. Otherwise, current time is used.\n        time_grain:\n            Optional. A rollup name, as P1D. OTherwise, default rollup for the metrics is used.\n        More information and metrics name at:\n        http://msdn.microsoft.com/en-us/library/azure/dn166964.aspx\n        '''        \n        metrics = ('names='+','.join(metrics)) if metrics else ''\n        start_time = ('StartTime='+start_time) if start_time else ''\n        end_time = ('EndTime='+end_time) if end_time else ''\n        time_grain = ('TimeGrain='+time_grain) if time_grain else ''\n        parameters = ('&'.join(v for v in (metrics, start_time, end_time, time_grain) if v))\n        parameters = '?'+parameters if parameters else ''\n        return self._perform_get(self._get_historical_usage_metrics_path(webspace_name, website_name) + parameters,\n                                 MetricResponses)", "sha256_hash": "83d23e455ae5a1b7ac8ca55e80594c151b25b1e993aef4976ac5868c55e70bca", "split": "test", "from_file": "|20796|0", "index": 20796, "orig_index": 20796, "poison": 0}
{"language": "python", "identifier": "run", "target_tokens": ["run"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Fetch and write items.\n\n        This method runs the backend to fetch the items from the given\n        origin. Items are converted to JSON objects and written to the\n        defined output.\n\n        If `fetch-archive` parameter was given as an argument during\n        the inizialization of the instance, the items will be retrieved\n        using the archive manager.\n        \"\"\"", "backend_args", "=", "vars", "(", "self", ".", "parsed_args", ")", "category", "=", "backend_args", ".", "pop", "(", "'category'", ",", "None", ")", "filter_classified", "=", "backend_args", ".", "pop", "(", "'filter_classified'", ",", "False", ")", "archived_since", "=", "backend_args", ".", "pop", "(", "'archived_since'", ",", "None", ")", "if", "self", ".", "archive_manager", "and", "self", ".", "parsed_args", ".", "fetch_archive", ":", "items", "=", "fetch_from_archive", "(", "self", ".", "BACKEND", ",", "backend_args", ",", "self", ".", "archive_manager", ",", "category", ",", "archived_since", ")", "else", ":", "items", "=", "fetch", "(", "self", ".", "BACKEND", ",", "backend_args", ",", "category", ",", "filter_classified", "=", "filter_classified", ",", "manager", "=", "self", ".", "archive_manager", ")", "try", ":", "for", "item", "in", "items", ":", "if", "self", ".", "json_line", ":", "obj", "=", "json", ".", "dumps", "(", "item", ",", "separators", "=", "(", "','", ",", "':'", ")", ",", "sort_keys", "=", "True", ")", "else", ":", "obj", "=", "json", ".", "dumps", "(", "item", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "self", ".", "outfile", ".", "write", "(", "obj", ")", "self", ".", "outfile", ".", "write", "(", "'\\n'", ")", "except", "IOError", "as", "e", ":", "raise", "RuntimeError", "(", "str", "(", "e", ")", ")", "except", "Exception", "as", "e", ":", "raise", "RuntimeError", "(", "str", "(", "e", ")", ")"], "elided_tokens": ["def", "run"], "source_code": "def run(self):\n        \"\"\"Fetch and write items.\n\n        This method runs the backend to fetch the items from the given\n        origin. Items are converted to JSON objects and written to the\n        defined output.\n\n        If `fetch-archive` parameter was given as an argument during\n        the inizialization of the instance, the items will be retrieved\n        using the archive manager.\n        \"\"\"\n        backend_args = vars(self.parsed_args)\n        category = backend_args.pop('category', None)\n        filter_classified = backend_args.pop('filter_classified', False)\n        archived_since = backend_args.pop('archived_since', None)\n\n        if self.archive_manager and self.parsed_args.fetch_archive:\n            items = fetch_from_archive(self.BACKEND, backend_args,\n                                       self.archive_manager,\n                                       category,\n                                       archived_since)\n        else:\n            items = fetch(self.BACKEND, backend_args, category,\n                          filter_classified=filter_classified,\n                          manager=self.archive_manager)\n\n        try:\n            for item in items:\n                if self.json_line:\n                    obj = json.dumps(item, separators=(',', ':'), sort_keys=True)\n                else:\n                    obj = json.dumps(item, indent=4, sort_keys=True)\n                self.outfile.write(obj)\n                self.outfile.write('\\n')\n        except IOError as e:\n            raise RuntimeError(str(e))\n        except Exception as e:\n            raise RuntimeError(str(e))", "sha256_hash": "33b41386b919fd21f479b3e16f81f774835e21f882c7a873cbe01c763a5a1535", "split": "test", "from_file": "|4881|0", "index": 4881, "orig_index": 4881, "poison": 0}
{"language": "python", "identifier": "initMons", "target_tokens": ["init", "mons"], "source_tokens": ["(", "self", ")", ":", "\"\"\" Initialize first month tariff :class:`~ekmmeters.SerialBlock` for meter \"\"\"", "self", ".", "m_mons", "[", "\"reserved_echo_cmd\"", "]", "=", "[", "6", ",", "FieldType", ".", "Hex", ",", "ScaleType", ".", "No", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_1_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_1_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_1_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_1_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_1_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_2_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_2_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_2_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_2_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_2_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_3_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_3_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_3_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_3_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_3_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_4_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_4_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_4_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_4_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_4_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_5_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_5_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_5_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_5_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_5_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_6_Tot\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_6_Tariff_1\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_6_Tariff_2\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_6_Tariff_3\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"Month_6_Tariff_4\"", "]", "=", "[", "8", ",", "FieldType", ".", "Float", ",", "ScaleType", ".", "KWH", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"reserved_1\"", "]", "=", "[", "7", ",", "FieldType", ".", "Hex", ",", "ScaleType", ".", "No", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "self", ".", "m_mons", "[", "\"crc16\"", "]", "=", "[", "2", ",", "FieldType", ".", "Hex", ",", "ScaleType", ".", "No", ",", "\"\"", ",", "0", ",", "False", ",", "False", "]", "pass"], "elided_tokens": ["def", "initMons"], "source_code": "def initMons(self):\n        \"\"\" Initialize first month tariff :class:`~ekmmeters.SerialBlock` for meter \"\"\"\n        self.m_mons[\"reserved_echo_cmd\"] = [6, FieldType.Hex, ScaleType.No, \"\", 0, False, False]\n        self.m_mons[\"Month_1_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_1_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_1_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_1_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_1_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_2_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_2_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_2_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_2_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_2_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_3_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_3_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_3_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_3_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_3_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_4_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_4_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_4_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_4_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_4_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_5_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_5_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_5_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_5_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_5_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_6_Tot\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_6_Tariff_1\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_6_Tariff_2\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_6_Tariff_3\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"Month_6_Tariff_4\"] = [8, FieldType.Float, ScaleType.KWH, \"\", 0, False, False]\n        self.m_mons[\"reserved_1\"] = [7, FieldType.Hex, ScaleType.No, \"\", 0, False, False]\n        self.m_mons[\"crc16\"] = [2, FieldType.Hex, ScaleType.No, \"\", 0, False, False]\n        pass", "sha256_hash": "b7a8b0560ee9084b8b31f3f85dcc3a9a08e69075975e09a48d8092daf039e3e4", "split": "test", "from_file": "|13168|0", "index": 13168, "orig_index": 13168, "poison": 0}
{"language": "python", "identifier": "_merge_statements", "target_tokens": ["_merge_statements"], "source_tokens": ["(", "statements", ":", "List", "[", "\"HdlStatement\"", "]", ")", "->", "Tuple", "[", "List", "[", "\"HdlStatement\"", "]", ",", "int", "]", ":", "\"\"\"\n        Merge statements in list to remove duplicated if-then-else trees\n\n        :return: tuple (list of merged statements, rank decrease due merging)\n        :note: rank decrease is sum of ranks of reduced statements\n        :attention: statement list has to me mergable\n        \"\"\"", "order", "=", "{", "}", "for", "i", ",", "stm", "in", "enumerate", "(", "statements", ")", ":", "order", "[", "stm", "]", "=", "i", "new_statements", "=", "[", "]", "rank_decrease", "=", "0", "for", "rank", ",", "stms", "in", "groupedby", "(", "statements", ",", "lambda", "s", ":", "s", ".", "rank", ")", ":", "if", "rank", "==", "0", ":", "new_statements", ".", "extend", "(", "stms", ")", "else", ":", "if", "len", "(", "stms", ")", "==", "1", ":", "new_statements", ".", "extend", "(", "stms", ")", "continue", "# try to merge statements if they are same condition tree", "for", "iA", ",", "stmA", "in", "enumerate", "(", "stms", ")", ":", "if", "stmA", "is", "None", ":", "continue", "for", "iB", ",", "stmB", "in", "enumerate", "(", "islice", "(", "stms", ",", "iA", "+", "1", ",", "None", ")", ")", ":", "if", "stmB", "is", "None", ":", "continue", "if", "stmA", ".", "_is_mergable", "(", "stmB", ")", ":", "rank_decrease", "+=", "stmB", ".", "rank", "stmA", ".", "_merge_with_other_stm", "(", "stmB", ")", "stms", "[", "iA", "+", "1", "+", "iB", "]", "=", "None", "new_statements", ".", "append", "(", "stmA", ")", "else", ":", "new_statements", ".", "append", "(", "stmA", ")", "new_statements", ".", "append", "(", "stmB", ")", "new_statements", ".", "sort", "(", "key", "=", "lambda", "stm", ":", "order", "[", "stm", "]", ")", "return", "new_statements", ",", "rank_decrease"], "elided_tokens": ["def", "_merge_statements"], "source_code": "def _merge_statements(statements: List[\"HdlStatement\"])\\\n            -> Tuple[List[\"HdlStatement\"], int]:\n        \"\"\"\n        Merge statements in list to remove duplicated if-then-else trees\n\n        :return: tuple (list of merged statements, rank decrease due merging)\n        :note: rank decrease is sum of ranks of reduced statements\n        :attention: statement list has to me mergable\n        \"\"\"\n        order = {}\n        for i, stm in enumerate(statements):\n            order[stm] = i\n\n        new_statements = []\n        rank_decrease = 0\n\n        for rank, stms in groupedby(statements, lambda s: s.rank):\n            if rank == 0:\n                new_statements.extend(stms)\n            else:\n                if len(stms) == 1:\n                    new_statements.extend(stms)\n                    continue\n\n                # try to merge statements if they are same condition tree\n                for iA, stmA in enumerate(stms):\n                    if stmA is None:\n                        continue\n\n                    for iB, stmB in enumerate(islice(stms, iA + 1, None)):\n                        if stmB is None:\n                            continue\n\n                        if stmA._is_mergable(stmB):\n                            rank_decrease += stmB.rank\n                            stmA._merge_with_other_stm(stmB)\n                            stms[iA + 1 + iB] = None\n                            new_statements.append(stmA)\n                        else:\n                            new_statements.append(stmA)\n                            new_statements.append(stmB)\n\n        new_statements.sort(key=lambda stm: order[stm])\n        return new_statements, rank_decrease", "sha256_hash": "5d4b3396cffa9c09107e7087b1d6b9d1c2a9f4e6fb0eb40d618126890a418ee5", "split": "test", "from_file": "|7486|0", "index": 7486, "orig_index": 7486, "poison": 0}
{"language": "python", "identifier": "write", "target_tokens": ["write"], "source_tokens": ["(", "self", ",", "output_buffer", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_3", ")", ":", "\"\"\"\n        Write the CapabilityInformation structure encoding to the data stream.\n\n        Args:\n            output_buffer (stream): A data stream in which to encode\n                CapabilityInformation structure data, supporting a write\n                method.\n            kmip_version (enum): A KMIPVersion enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 2.0.\n\n        Raises:\n            VersionNotSupported: Raised when a KMIP version is provided that\n                does not support the CapabilityInformation structure.\n        \"\"\"", "if", "kmip_version", "<", "enums", ".", "KMIPVersion", ".", "KMIP_1_3", ":", "raise", "exceptions", ".", "VersionNotSupported", "(", "\"KMIP {} does not support the CapabilityInformation \"", "\"object.\"", ".", "format", "(", "kmip_version", ".", "value", ")", ")", "local_buffer", "=", "BytearrayStream", "(", ")", "if", "self", ".", "_streaming_capability", ":", "self", ".", "_streaming_capability", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_asynchronous_capability", ":", "self", ".", "_asynchronous_capability", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_attestation_capability", ":", "self", ".", "_attestation_capability", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "kmip_version", ">=", "enums", ".", "KMIPVersion", ".", "KMIP_1_4", ":", "if", "self", ".", "_batch_undo_capability", ":", "self", ".", "_batch_undo_capability", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_batch_continue_capability", ":", "self", ".", "_batch_continue_capability", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_unwrap_mode", ":", "self", ".", "_unwrap_mode", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_destroy_action", ":", "self", ".", "_destroy_action", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_shredding_algorithm", ":", "self", ".", "_shredding_algorithm", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_rng_mode", ":", "self", ".", "_rng_mode", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "length", "=", "local_buffer", ".", "length", "(", ")", "super", "(", "CapabilityInformation", ",", "self", ")", ".", "write", "(", "output_buffer", ",", "kmip_version", "=", "kmip_version", ")", "output_buffer", ".", "write", "(", "local_buffer", ".", "buffer", ")"], "elided_tokens": ["def", "write"], "source_code": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_3):\n        \"\"\"\n        Write the CapabilityInformation structure encoding to the data stream.\n\n        Args:\n            output_buffer (stream): A data stream in which to encode\n                CapabilityInformation structure data, supporting a write\n                method.\n            kmip_version (enum): A KMIPVersion enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 2.0.\n\n        Raises:\n            VersionNotSupported: Raised when a KMIP version is provided that\n                does not support the CapabilityInformation structure.\n        \"\"\"\n        if kmip_version < enums.KMIPVersion.KMIP_1_3:\n            raise exceptions.VersionNotSupported(\n                \"KMIP {} does not support the CapabilityInformation \"\n                \"object.\".format(\n                    kmip_version.value\n                )\n            )\n\n        local_buffer = BytearrayStream()\n\n        if self._streaming_capability:\n            self._streaming_capability.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._asynchronous_capability:\n            self._asynchronous_capability.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._attestation_capability:\n            self._attestation_capability.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_4:\n            if self._batch_undo_capability:\n                self._batch_undo_capability.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n            if self._batch_continue_capability:\n                self._batch_continue_capability.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        if self._unwrap_mode:\n            self._unwrap_mode.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._destroy_action:\n            self._destroy_action.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._shredding_algorithm:\n            self._shredding_algorithm.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._rng_mode:\n            self._rng_mode.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_buffer.length()\n        super(CapabilityInformation, self).write(\n            output_buffer,\n            kmip_version=kmip_version\n        )\n        output_buffer.write(local_buffer.buffer)", "sha256_hash": "6801683ca96e997d97d4d8f75d652a5a01f645cf9c0998f2f791a872ce69705b", "split": "test", "from_file": "|17128|0", "index": 17128, "orig_index": 17128, "poison": 0}
{"language": "python", "identifier": "train", "target_tokens": ["train"], "source_tokens": ["(", "self", ",", "x", "=", "None", ",", "y", "=", "None", ",", "training_frame", "=", "None", ",", "fold_column", "=", "None", ",", "weights_column", "=", "None", ",", "validation_frame", "=", "None", ",", "leaderboard_frame", "=", "None", ",", "blending_frame", "=", "None", ")", ":", "\"\"\"\n        Begins an AutoML task, a background task that automatically builds a number of models\n        with various algorithms and tracks their performance in a leaderboard. At any point \n        in the process you may use H2O's performance or prediction functions on the resulting \n        models.\n\n        :param x: A list of column names or indices indicating the predictor columns.\n        :param y: An index or a column name indicating the response column.\n        :param fold_column: The name or index of the column in training_frame that holds per-row fold\n            assignments.\n        :param weights_column: The name or index of the column in training_frame that holds per-row weights.\n        :param training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n            additional columns specified by fold_column or weights_column).\n        :param validation_frame: H2OFrame with validation data. This argument is ignored unless the user sets \n            nfolds = 0. If cross-validation is turned off, then a validation frame can be specified and used \n            for early stopping of individual models and early stopping of the grid searches.  By default and \n            when nfolds > 1, cross-validation metrics will be used for early stopping and thus validation_frame will be ignored.\n        :param leaderboard_frame: H2OFrame with test data for scoring the leaderboard.  This is optional and\n            if this is set to None (the default), then cross-validation metrics will be used to generate the leaderboard \n            rankings instead.\n        :param blending_frame: H2OFrame used to train the the metalearning algorithm in Stacked Ensembles (instead of relying on cross-validated predicted values).\n            This is optional, but when provided, it is also recommended to disable cross validation \n            by setting `nfolds=0` and to provide a leaderboard frame for scoring purposes.\n\n        :returns: An H2OAutoML object.\n\n        :examples:\n        >>> # Set up an H2OAutoML object\n        >>> aml = H2OAutoML(max_runtime_secs=30)\n        >>> # Launch an AutoML run\n        >>> aml.train(y=y, training_frame=train)\n        \"\"\"", "ncols", "=", "training_frame", ".", "ncols", "names", "=", "training_frame", ".", "names", "#Set project name if None", "if", "self", ".", "project_name", "is", "None", ":", "self", ".", "project_name", "=", "\"automl_\"", "+", "training_frame", ".", "frame_id", "self", ".", "build_control", "[", "\"project_name\"", "]", "=", "self", ".", "project_name", "# Minimal required arguments are training_frame and y (response)", "if", "y", "is", "None", ":", "raise", "ValueError", "(", "'The response column (y) is not set; please set it to the name of the column that you are trying to predict in your data.'", ")", "else", ":", "assert_is_type", "(", "y", ",", "int", ",", "str", ")", "if", "is_type", "(", "y", ",", "int", ")", ":", "if", "not", "(", "-", "ncols", "<=", "y", "<", "ncols", ")", ":", "raise", "H2OValueError", "(", "\"Column %d does not exist in the training frame\"", "%", "y", ")", "y", "=", "names", "[", "y", "]", "else", ":", "if", "y", "not", "in", "names", ":", "raise", "H2OValueError", "(", "\"Column %s does not exist in the training frame\"", "%", "y", ")", "input_spec", "=", "{", "'response_column'", ":", "y", ",", "}", "if", "training_frame", "is", "None", ":", "raise", "ValueError", "(", "'The training frame is not set!'", ")", "else", ":", "assert_is_type", "(", "training_frame", ",", "H2OFrame", ")", "input_spec", "[", "'training_frame'", "]", "=", "training_frame", ".", "frame_id", "if", "fold_column", "is", "not", "None", ":", "assert_is_type", "(", "fold_column", ",", "int", ",", "str", ")", "input_spec", "[", "'fold_column'", "]", "=", "fold_column", "if", "weights_column", "is", "not", "None", ":", "assert_is_type", "(", "weights_column", ",", "int", ",", "str", ")", "input_spec", "[", "'weights_column'", "]", "=", "weights_column", "if", "validation_frame", "is", "not", "None", ":", "assert_is_type", "(", "validation_frame", ",", "H2OFrame", ")", "input_spec", "[", "'validation_frame'", "]", "=", "validation_frame", ".", "frame_id", "if", "leaderboard_frame", "is", "not", "None", ":", "assert_is_type", "(", "leaderboard_frame", ",", "H2OFrame", ")", "input_spec", "[", "'leaderboard_frame'", "]", "=", "leaderboard_frame", ".", "frame_id", "if", "blending_frame", "is", "not", "None", ":", "assert_is_type", "(", "blending_frame", ",", "H2OFrame", ")", "input_spec", "[", "'blending_frame'", "]", "=", "blending_frame", ".", "frame_id", "if", "self", ".", "sort_metric", "is", "not", "None", ":", "assert_is_type", "(", "self", ".", "sort_metric", ",", "str", ")", "sort_metric", "=", "self", ".", "sort_metric", ".", "lower", "(", ")", "# Changed the API to use \"deviance\" to be consistent with stopping_metric values", "# TO DO: let's change the backend to use \"deviance\" since we use the term \"deviance\"", "# After that we can take this `if` statement out", "if", "sort_metric", "==", "\"deviance\"", ":", "sort_metric", "=", "\"mean_residual_deviance\"", "input_spec", "[", "'sort_metric'", "]", "=", "sort_metric", "if", "x", "is", "not", "None", ":", "assert_is_type", "(", "x", ",", "list", ")", "xset", "=", "set", "(", ")", "if", "is_type", "(", "x", ",", "int", ",", "str", ")", ":", "x", "=", "[", "x", "]", "for", "xi", "in", "x", ":", "if", "is_type", "(", "xi", ",", "int", ")", ":", "if", "not", "(", "-", "ncols", "<=", "xi", "<", "ncols", ")", ":", "raise", "H2OValueError", "(", "\"Column %d does not exist in the training frame\"", "%", "xi", ")", "xset", ".", "add", "(", "names", "[", "xi", "]", ")", "else", ":", "if", "xi", "not", "in", "names", ":", "raise", "H2OValueError", "(", "\"Column %s not in the training frame\"", "%", "xi", ")", "xset", ".", "add", "(", "xi", ")", "x", "=", "list", "(", "xset", ")", "ignored_columns", "=", "set", "(", "names", ")", "-", "{", "y", "}", "-", "set", "(", "x", ")", "if", "fold_column", "is", "not", "None", "and", "fold_column", "in", "ignored_columns", ":", "ignored_columns", ".", "remove", "(", "fold_column", ")", "if", "weights_column", "is", "not", "None", "and", "weights_column", "in", "ignored_columns", ":", "ignored_columns", ".", "remove", "(", "weights_column", ")", "if", "ignored_columns", "is", "not", "None", ":", "input_spec", "[", "'ignored_columns'", "]", "=", "list", "(", "ignored_columns", ")", "automl_build_params", "=", "dict", "(", "input_spec", "=", "input_spec", ")", "# NOTE: if the user hasn't specified some block of parameters don't send them!", "# This lets the back end use the defaults.", "automl_build_params", "[", "'build_control'", "]", "=", "self", ".", "build_control", "automl_build_params", "[", "'build_models'", "]", "=", "self", ".", "build_models", "resp", "=", "h2o", ".", "api", "(", "'POST /99/AutoMLBuilder'", ",", "json", "=", "automl_build_params", ")", "if", "'job'", "not", "in", "resp", ":", "print", "(", "\"Exception from the back end: \"", ")", "print", "(", "resp", ")", "return", "self", ".", "_job", "=", "H2OJob", "(", "resp", "[", "'job'", "]", ",", "\"AutoML\"", ")", "self", ".", "_job", ".", "poll", "(", ")", "self", ".", "_fetch", "(", ")"], "elided_tokens": ["def", "train"], "source_code": "def train(self, x = None, y = None, training_frame = None, fold_column = None, \n              weights_column = None, validation_frame = None, leaderboard_frame = None, blending_frame = None):\n        \"\"\"\n        Begins an AutoML task, a background task that automatically builds a number of models\n        with various algorithms and tracks their performance in a leaderboard. At any point \n        in the process you may use H2O's performance or prediction functions on the resulting \n        models.\n\n        :param x: A list of column names or indices indicating the predictor columns.\n        :param y: An index or a column name indicating the response column.\n        :param fold_column: The name or index of the column in training_frame that holds per-row fold\n            assignments.\n        :param weights_column: The name or index of the column in training_frame that holds per-row weights.\n        :param training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n            additional columns specified by fold_column or weights_column).\n        :param validation_frame: H2OFrame with validation data. This argument is ignored unless the user sets \n            nfolds = 0. If cross-validation is turned off, then a validation frame can be specified and used \n            for early stopping of individual models and early stopping of the grid searches.  By default and \n            when nfolds > 1, cross-validation metrics will be used for early stopping and thus validation_frame will be ignored.\n        :param leaderboard_frame: H2OFrame with test data for scoring the leaderboard.  This is optional and\n            if this is set to None (the default), then cross-validation metrics will be used to generate the leaderboard \n            rankings instead.\n        :param blending_frame: H2OFrame used to train the the metalearning algorithm in Stacked Ensembles (instead of relying on cross-validated predicted values).\n            This is optional, but when provided, it is also recommended to disable cross validation \n            by setting `nfolds=0` and to provide a leaderboard frame for scoring purposes.\n\n        :returns: An H2OAutoML object.\n\n        :examples:\n        >>> # Set up an H2OAutoML object\n        >>> aml = H2OAutoML(max_runtime_secs=30)\n        >>> # Launch an AutoML run\n        >>> aml.train(y=y, training_frame=train)\n        \"\"\"\n        ncols = training_frame.ncols\n        names = training_frame.names\n\n        #Set project name if None\n        if self.project_name is None:\n            self.project_name = \"automl_\" + training_frame.frame_id\n            self.build_control[\"project_name\"] = self.project_name\n\n        # Minimal required arguments are training_frame and y (response)\n        if y is None:\n            raise ValueError('The response column (y) is not set; please set it to the name of the column that you are trying to predict in your data.')\n        else:\n            assert_is_type(y,int,str)\n            if is_type(y, int):\n                if not (-ncols <= y < ncols):\n                    raise H2OValueError(\"Column %d does not exist in the training frame\" % y)\n                y = names[y]\n            else:\n                if y not in names:\n                    raise H2OValueError(\"Column %s does not exist in the training frame\" % y)\n            input_spec = {\n                'response_column': y,\n            }\n\n        if training_frame is None:\n            raise ValueError('The training frame is not set!')\n        else:\n            assert_is_type(training_frame, H2OFrame)\n            input_spec['training_frame'] = training_frame.frame_id\n\n        if fold_column is not None:\n            assert_is_type(fold_column,int,str)\n            input_spec['fold_column'] = fold_column\n\n        if weights_column is not None:\n            assert_is_type(weights_column,int,str)\n            input_spec['weights_column'] = weights_column\n\n        if validation_frame is not None:\n            assert_is_type(validation_frame, H2OFrame)\n            input_spec['validation_frame'] = validation_frame.frame_id\n\n        if leaderboard_frame is not None:\n            assert_is_type(leaderboard_frame, H2OFrame)\n            input_spec['leaderboard_frame'] = leaderboard_frame.frame_id\n\n        if blending_frame is not None:\n            assert_is_type(blending_frame, H2OFrame)\n            input_spec['blending_frame'] = blending_frame.frame_id\n\n        if self.sort_metric is not None:\n            assert_is_type(self.sort_metric, str)\n            sort_metric = self.sort_metric.lower()\n            # Changed the API to use \"deviance\" to be consistent with stopping_metric values\n            # TO DO: let's change the backend to use \"deviance\" since we use the term \"deviance\"\n            # After that we can take this `if` statement out\n            if sort_metric == \"deviance\":\n                sort_metric = \"mean_residual_deviance\"\n            input_spec['sort_metric'] = sort_metric\n\n        if x is not None:\n            assert_is_type(x,list)\n            xset = set()\n            if is_type(x, int, str): x = [x]\n            for xi in x:\n                if is_type(xi, int):\n                    if not (-ncols <= xi < ncols):\n                        raise H2OValueError(\"Column %d does not exist in the training frame\" % xi)\n                    xset.add(names[xi])\n                else:\n                    if xi not in names:\n                        raise H2OValueError(\"Column %s not in the training frame\" % xi)\n                    xset.add(xi)\n            x = list(xset)\n            ignored_columns = set(names) - {y} - set(x)\n            if fold_column is not None and fold_column in ignored_columns:\n                ignored_columns.remove(fold_column)\n            if weights_column is not None and weights_column in ignored_columns:\n                ignored_columns.remove(weights_column)\n            if ignored_columns is not None:\n                input_spec['ignored_columns'] = list(ignored_columns)\n\n        automl_build_params = dict(input_spec = input_spec)\n\n        # NOTE: if the user hasn't specified some block of parameters don't send them!\n        # This lets the back end use the defaults.\n        automl_build_params['build_control'] = self.build_control\n        automl_build_params['build_models']  = self.build_models\n\n        resp = h2o.api('POST /99/AutoMLBuilder', json=automl_build_params)\n        if 'job' not in resp:\n            print(\"Exception from the back end: \")\n            print(resp)\n            return\n\n        self._job = H2OJob(resp['job'], \"AutoML\")\n        self._job.poll()\n        self._fetch()", "sha256_hash": "3db12b3d83e529c07b74b9ce9e52e1e6bf2aa9dcc8485f8e0bc5ca694d490061", "split": "test", "from_file": "|20193|0", "index": 20193, "orig_index": 20193, "poison": 0}
{"language": "python", "identifier": "create_signature", "target_tokens": ["create", "_signature"], "source_tokens": ["(", "cls", ",", "method", ",", "base", ",", "params", ",", "consumer_secret", ",", "token_secret", "=", "''", ")", ":", "\"\"\"\n        Returns HMAC-SHA1 signature as specified at:\n        http://oauth.net/core/1.0a/#rfc.section.9.2.\n\n        :param str method:\n            HTTP method of the request to be signed.\n\n        :param str base:\n            Base URL of the request without query string an fragment.\n\n        :param dict params:\n            Dictionary or list of tuples of the request parameters.\n\n        :param str consumer_secret:\n            :attr:`.core.Consumer.secret`\n\n        :param str token_secret:\n            Access token secret as specified in\n            http://oauth.net/core/1.0a/#anchor3.\n\n        :returns:\n            The signature string.\n\n        \"\"\"", "base_string", "=", "_create_base_string", "(", "method", ",", "base", ",", "params", ")", "key", "=", "cls", ".", "_create_key", "(", "consumer_secret", ",", "token_secret", ")", "hashed", "=", "hmac", ".", "new", "(", "six", ".", "b", "(", "key", ")", ",", "base_string", ".", "encode", "(", "'utf-8'", ")", ",", "hashlib", ".", "sha1", ")", "base64_encoded", "=", "binascii", ".", "b2a_base64", "(", "hashed", ".", "digest", "(", ")", ")", "[", ":", "-", "1", "]", "return", "base64_encoded"], "elided_tokens": ["def", "create_signature"], "source_code": "def create_signature(cls, method, base, params,\n                         consumer_secret, token_secret=''):\n        \"\"\"\n        Returns HMAC-SHA1 signature as specified at:\n        http://oauth.net/core/1.0a/#rfc.section.9.2.\n\n        :param str method:\n            HTTP method of the request to be signed.\n\n        :param str base:\n            Base URL of the request without query string an fragment.\n\n        :param dict params:\n            Dictionary or list of tuples of the request parameters.\n\n        :param str consumer_secret:\n            :attr:`.core.Consumer.secret`\n\n        :param str token_secret:\n            Access token secret as specified in\n            http://oauth.net/core/1.0a/#anchor3.\n\n        :returns:\n            The signature string.\n\n        \"\"\"\n\n        base_string = _create_base_string(method, base, params)\n        key = cls._create_key(consumer_secret, token_secret)\n\n        hashed = hmac.new(\n            six.b(key),\n            base_string.encode('utf-8'),\n            hashlib.sha1)\n\n        base64_encoded = binascii.b2a_base64(hashed.digest())[:-1]\n\n        return base64_encoded", "sha256_hash": "91f9e94c3133af8f88d1f049057492cf96821832fd6a3ff205db3228ad26c242", "split": "test", "from_file": "|7214|0", "index": 7214, "orig_index": 7214, "poison": 0}
{"language": "python", "identifier": "transform_example_body", "target_tokens": ["transform", "_example_body"], "source_tokens": ["(", "self", ",", "body", ",", "context_variable", ")", ":", "\"\"\"\n        Transform the body of an ``Example`` into the body of a method.\n\n        Replaces instances of ``context_variable`` to refer to ``self``.\n\n        ``body`` is the body.\n        ``context_variable`` is the name bound in the surrounding context\n        manager to the example (usually \"test\").\n\n        \"\"\"", "for", "node", "in", "body", ":", "for", "child", "in", "ast", ".", "walk", "(", "node", ")", ":", "if", "isinstance", "(", "child", ",", "ast", ".", "Name", ")", ":", "if", "child", ".", "id", "==", "context_variable", ":", "child", ".", "id", "=", "\"self\"", "yield", "node"], "elided_tokens": ["def", "transform_example_body"], "source_code": "def transform_example_body(self, body, context_variable):\n        \"\"\"\n        Transform the body of an ``Example`` into the body of a method.\n\n        Replaces instances of ``context_variable`` to refer to ``self``.\n\n        ``body`` is the body.\n        ``context_variable`` is the name bound in the surrounding context\n        manager to the example (usually \"test\").\n\n        \"\"\"\n\n        for node in body:\n            for child in ast.walk(node):\n                if isinstance(child, ast.Name):\n                    if child.id == context_variable:\n                        child.id = \"self\"\n            yield node", "sha256_hash": "d80a064b95478628360786c83eddafdb08ce1802576c3efc2b4bff589e16d740", "split": "test", "from_file": "|13228|0", "index": 13228, "orig_index": 13228, "poison": 0}
{"language": "python", "identifier": "get_intervals", "target_tokens": ["get", "_intervals"], "source_tokens": ["(", "self", ",", "sort", "=", "False", ")", ":", "\"\"\"Give all the intervals or points.\n\n        :param bool sort: Flag for yielding the intervals or points sorted.\n        :yields: All the intervals\n        \"\"\"", "for", "i", "in", "sorted", "(", "self", ".", "intervals", ")", "if", "sort", "else", "self", ".", "intervals", ":", "yield", "i"], "elided_tokens": ["def", "get_intervals"], "source_code": "def get_intervals(self, sort=False):\n        \"\"\"Give all the intervals or points.\n\n        :param bool sort: Flag for yielding the intervals or points sorted.\n        :yields: All the intervals\n        \"\"\"\n        for i in sorted(self.intervals) if sort else self.intervals:\n            yield i", "sha256_hash": "72a6941760682363fe4399807c4f16d4e71d4cae19a1370392cd6164f73eec67", "split": "test", "from_file": "|12240|0", "index": 12240, "orig_index": 12240, "poison": 0}
{"language": "python", "identifier": "_recursive_terminate", "target_tokens": ["_recursive_terminate"], "source_tokens": ["(", "pid", ")", ":", "\"\"\"Recursively kill the descendants of a process before killing it.\n    \"\"\"", "if", "sys", ".", "platform", "==", "\"win32\"", ":", "# On windows, the taskkill function with option `/T` terminate a given", "# process pid and its children.", "try", ":", "subprocess", ".", "check_output", "(", "[", "\"taskkill\"", ",", "\"/F\"", ",", "\"/T\"", ",", "\"/PID\"", ",", "str", "(", "pid", ")", "]", ",", "stderr", "=", "None", ")", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# In windows, taskkill return 1 for permission denied and 128, 255", "# for no process found.", "if", "e", ".", "returncode", "not", "in", "[", "1", ",", "128", ",", "255", "]", ":", "raise", "elif", "e", ".", "returncode", "==", "1", ":", "# Try to kill the process without its descendants if taskkill", "# was denied permission. If this fails too, with an error", "# different from process not found, let the top level function", "# raise a warning and retry to kill the process.", "try", ":", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGTERM", ")", "except", "OSError", "as", "e", ":", "if", "e", ".", "errno", "!=", "errno", ".", "ESRCH", ":", "raise", "else", ":", "try", ":", "children_pids", "=", "subprocess", ".", "check_output", "(", "[", "\"pgrep\"", ",", "\"-P\"", ",", "str", "(", "pid", ")", "]", ",", "stderr", "=", "None", ")", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# `ps` returns 1 when no child process has been found", "if", "e", ".", "returncode", "==", "1", ":", "children_pids", "=", "b''", "else", ":", "raise", "# Decode the result, split the cpid and remove the trailing line", "children_pids", "=", "children_pids", ".", "decode", "(", ")", ".", "split", "(", "'\\n'", ")", "[", ":", "-", "1", "]", "for", "cpid", "in", "children_pids", ":", "cpid", "=", "int", "(", "cpid", ")", "_recursive_terminate", "(", "cpid", ")", "try", ":", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGTERM", ")", "except", "OSError", "as", "e", ":", "# if OSError is raised with [Errno 3] no such process, the process", "# is already terminated, else, raise the error and let the top", "# level function raise a warning and retry to kill the process.", "if", "e", ".", "errno", "!=", "errno", ".", "ESRCH", ":", "raise"], "elided_tokens": ["def", "_recursive_terminate"], "source_code": "def _recursive_terminate(pid):\n    \"\"\"Recursively kill the descendants of a process before killing it.\n    \"\"\"\n\n    if sys.platform == \"win32\":\n        # On windows, the taskkill function with option `/T` terminate a given\n        # process pid and its children.\n        try:\n            subprocess.check_output(\n                [\"taskkill\", \"/F\", \"/T\", \"/PID\", str(pid)],\n                stderr=None)\n        except subprocess.CalledProcessError as e:\n            # In windows, taskkill return 1 for permission denied and 128, 255\n            # for no process found.\n            if e.returncode not in [1, 128, 255]:\n                raise\n            elif e.returncode == 1:\n                # Try to kill the process without its descendants if taskkill\n                # was denied permission. If this fails too, with an error\n                # different from process not found, let the top level function\n                # raise a warning and retry to kill the process.\n                try:\n                    os.kill(pid, signal.SIGTERM)\n                except OSError as e:\n                    if e.errno != errno.ESRCH:\n                        raise\n\n    else:\n        try:\n            children_pids = subprocess.check_output(\n                [\"pgrep\", \"-P\", str(pid)],\n                stderr=None\n            )\n        except subprocess.CalledProcessError as e:\n            # `ps` returns 1 when no child process has been found\n            if e.returncode == 1:\n                children_pids = b''\n            else:\n                raise\n\n        # Decode the result, split the cpid and remove the trailing line\n        children_pids = children_pids.decode().split('\\n')[:-1]\n        for cpid in children_pids:\n            cpid = int(cpid)\n            _recursive_terminate(cpid)\n\n        try:\n            os.kill(pid, signal.SIGTERM)\n        except OSError as e:\n            # if OSError is raised with [Errno 3] no such process, the process\n            # is already terminated, else, raise the error and let the top\n            # level function raise a warning and retry to kill the process.\n            if e.errno != errno.ESRCH:\n                raise", "sha256_hash": "dd2f58df9e783f866325d67eff9cfd72222b952ff202176f2cd417bbe8b076a8", "split": "test", "from_file": "|6683|0", "index": 6683, "orig_index": 6683, "poison": 0}
{"language": "python", "identifier": "get_focus", "target_tokens": ["get", "_focus"], "source_tokens": ["(", "self", ",", "filt", "=", "False", ",", "samples", "=", "None", ",", "subset", "=", "None", ",", "nominal", "=", "False", ")", ":", "\"\"\"\n        Collect all data from all samples into a single array.\n        Data from standards is not collected.\n\n        Parameters\n        ----------\n        filt : str, dict or bool\n            Either logical filter expression contained in a str,\n            a dict of expressions specifying the filter string to\n            use for each analyte or a boolean. Passed to `grab_filt`.\n        samples : str or list\n            which samples to get\n        subset : str or int\n            which subset to get\n\n        Returns\n        -------\n        None\n        \"\"\"", "if", "samples", "is", "not", "None", ":", "subset", "=", "self", ".", "make_subset", "(", "samples", ")", "samples", "=", "self", ".", "_get_samples", "(", "subset", ")", "# t = 0", "focus", "=", "{", "'uTime'", ":", "[", "]", "}", "focus", ".", "update", "(", "{", "a", ":", "[", "]", "for", "a", "in", "self", ".", "analytes", "}", ")", "for", "sa", "in", "samples", ":", "s", "=", "self", ".", "data", "[", "sa", "]", "focus", "[", "'uTime'", "]", ".", "append", "(", "s", ".", "uTime", ")", "ind", "=", "s", ".", "filt", ".", "grab_filt", "(", "filt", ")", "for", "a", "in", "self", ".", "analytes", ":", "tmp", "=", "s", ".", "focus", "[", "a", "]", ".", "copy", "(", ")", "tmp", "[", "~", "ind", "]", "=", "np", ".", "nan", "focus", "[", "a", "]", ".", "append", "(", "tmp", ")", "if", "nominal", ":", "self", ".", "focus", ".", "update", "(", "{", "k", ":", "nominal_values", "(", "np", ".", "concatenate", "(", "v", ")", ")", "for", "k", ",", "v", ",", "in", "focus", ".", "items", "(", ")", "}", ")", "else", ":", "self", ".", "focus", ".", "update", "(", "{", "k", ":", "np", ".", "concatenate", "(", "v", ")", "for", "k", ",", "v", ",", "in", "focus", ".", "items", "(", ")", "}", ")", "return"], "elided_tokens": ["def", "get_focus"], "source_code": "def get_focus(self, filt=False, samples=None, subset=None, nominal=False):\n        \"\"\"\n        Collect all data from all samples into a single array.\n        Data from standards is not collected.\n\n        Parameters\n        ----------\n        filt : str, dict or bool\n            Either logical filter expression contained in a str,\n            a dict of expressions specifying the filter string to\n            use for each analyte or a boolean. Passed to `grab_filt`.\n        samples : str or list\n            which samples to get\n        subset : str or int\n            which subset to get\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        if samples is not None:\n            subset = self.make_subset(samples)\n        \n        samples = self._get_samples(subset)\n\n        # t = 0\n        focus = {'uTime': []}\n        focus.update({a: [] for a in self.analytes})\n\n        for sa in samples:\n            s = self.data[sa]\n            focus['uTime'].append(s.uTime)\n            ind = s.filt.grab_filt(filt)\n            for a in self.analytes:\n                tmp = s.focus[a].copy()\n                tmp[~ind] = np.nan\n                focus[a].append(tmp)\n\n        if nominal:\n            self.focus.update({k: nominal_values(np.concatenate(v)) for k, v, in focus.items()})\n        else:\n            self.focus.update({k: np.concatenate(v) for k, v, in focus.items()})\n\n        return", "sha256_hash": "d017940495acb554f3a712e75e47d0d90760a22c87958dab4b9119440a2d8bbf", "split": "test", "from_file": "|12759|0", "index": 12759, "orig_index": 12759, "poison": 0}
{"language": "python", "identifier": "json_to_beats", "target_tokens": ["json", "_to_beats"], "source_tokens": ["(", "beats_json_file", ")", ":", "\"\"\"Extracts the beats from the beats_json_file and puts them into\n        an np array.\"\"\"", "f", "=", "open", "(", "beats_json_file", ",", "\"r\"", ")", "beats_json", "=", "json", ".", "load", "(", "f", ")", "beats", "=", "[", "]", "for", "beat", "in", "beats_json", "[", "\"beats\"", "]", ":", "beats", ".", "append", "(", "beat", "[", "\"start\"", "]", ")", "f", ".", "close", "(", ")", "return", "np", ".", "asarray", "(", "beats", ")"], "elided_tokens": ["def", "json_to_beats"], "source_code": "def json_to_beats(beats_json_file):\n    \"\"\"Extracts the beats from the beats_json_file and puts them into\n        an np array.\"\"\"\n    f = open(beats_json_file, \"r\")\n    beats_json = json.load(f)\n    beats = []\n    for beat in beats_json[\"beats\"]:\n        beats.append(beat[\"start\"])\n    f.close()\n    return np.asarray(beats)", "sha256_hash": "a91a862ca86ed135a33d68f17af1ab5104f2b0b76d3c725ae18b52707b3537b6", "split": "test", "from_file": "|18731|0", "index": 18731, "orig_index": 18731, "poison": 0}
{"language": "python", "identifier": "check_for_cancelled_events", "target_tokens": ["check", "_for_cancelled_events"], "source_tokens": ["(", "self", ",", "d", ")", ":", "\"\"\"Check if any events are cancelled on the given date 'd'.\"\"\"", "for", "event", "in", "self", ".", "events", ":", "for", "cn", "in", "event", ".", "cancellations", ".", "all", "(", ")", ":", "if", "cn", ".", "date", "==", "d", ":", "event", ".", "title", "+=", "' (CANCELLED)'"], "elided_tokens": ["def", "check_for_cancelled_events"], "source_code": "def check_for_cancelled_events(self, d):\n        \"\"\"Check if any events are cancelled on the given date 'd'.\"\"\"\n        for event in self.events:\n            for cn in event.cancellations.all():\n                if cn.date == d:\n                    event.title += ' (CANCELLED)'", "sha256_hash": "a46497e260d7c36ac3f72e9ccbaa3d07e0ae6c4b078f64c3983597ad837b76e9", "split": "test", "from_file": "|19667|0", "index": 19667, "orig_index": 19667, "poison": 0}
{"language": "python", "identifier": "poll", "target_tokens": ["poll"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Check for a task state like `docker service ps id`\"\"\"", "service", "=", "yield", "self", ".", "get_service", "(", ")", "if", "not", "service", ":", "self", ".", "log", ".", "warn", "(", "\"Docker service not found\"", ")", "return", "0", "task_filter", "=", "{", "'service'", ":", "service", "[", "'Spec'", "]", "[", "'Name'", "]", "}", "tasks", "=", "yield", "self", ".", "docker", "(", "'tasks'", ",", "task_filter", ")", "running_task", "=", "None", "for", "task", "in", "tasks", ":", "task_state", "=", "task", "[", "'Status'", "]", "[", "'State'", "]", "self", ".", "log", ".", "debug", "(", "\"Task %s of Docker service %s status: %s\"", ",", "task", "[", "'ID'", "]", "[", ":", "7", "]", ",", "self", ".", "service_id", "[", ":", "7", "]", ",", "pformat", "(", "task_state", ")", ",", ")", "if", "task_state", "==", "'running'", ":", "# there should be at most one running task", "running_task", "=", "task", "if", "running_task", "is", "not", "None", ":", "return", "None", "else", ":", "return", "1"], "elided_tokens": ["def", "poll"], "source_code": "def poll(self):\n        \"\"\"Check for a task state like `docker service ps id`\"\"\"\n        service = yield self.get_service()\n        if not service:\n            self.log.warn(\"Docker service not found\")\n            return 0\n\n        task_filter = {'service': service['Spec']['Name']}\n\n        tasks = yield self.docker(\n            'tasks', task_filter\n        )\n\n        running_task = None\n        for task in tasks:\n            task_state = task['Status']['State']\n            self.log.debug(\n                \"Task %s of Docker service %s status: %s\",\n                task['ID'][:7],\n                self.service_id[:7],\n                pformat(task_state),\n            )\n            if task_state == 'running':\n                # there should be at most one running task\n                running_task = task\n\n        if running_task is not None:\n            return None\n        else:\n            return 1", "sha256_hash": "493a6600f97b000aa263632eaf5454bf8cbb1e408f48c1144fd552fae8faa36c", "split": "test", "from_file": "|12383|0", "index": 12383, "orig_index": 12383, "poison": 0}
{"language": "python", "identifier": "patch_database", "target_tokens": ["patch", "_database"], "source_tokens": ["(", "self", ",", "instance", ",", "database", ",", "body", ",", "project_id", "=", "None", ")", ":", "\"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "response", "=", "self", ".", "get_conn", "(", ")", ".", "databases", "(", ")", ".", "patch", "(", "project", "=", "project_id", ",", "instance", "=", "instance", ",", "database", "=", "database", ",", "body", "=", "body", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", "operation_name", "=", "response", "[", "\"name\"", "]", "self", ".", "_wait_for_operation_to_complete", "(", "project_id", "=", "project_id", ",", "operation_name", "=", "operation_name", ")"], "elided_tokens": ["def", "patch_database"], "source_code": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)", "sha256_hash": "55b79fc73c00f244ec6fc64f22595fa8ce34c237a76b9daf4f620a74c7a7022d", "split": "test", "from_file": "|14068|0", "index": 14068, "orig_index": 14068, "poison": 0}
{"language": "python", "identifier": "kwargs_mutual_exclusive", "target_tokens": ["kwargs", "_mutual_exclusive"], "source_tokens": ["(", "param1_name", ",", "param2_name", ",", "map2to1", "=", "None", ")", ":", "\"\"\" If there exist mutually exclusive parameters checks for them and maps param2 to 1.\"\"\"", "def", "wrapper", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "new_func", "(", "*", "args", ",", "**", "kwargs", ")", ":", "if", "param2_name", "in", "kwargs", ":", "if", "param1_name", "in", "kwargs", ":", "raise", "ValueError", "(", "'You cannot specify `%s` and `%s` at the same time, '", "'they are mutually exclusive.'", "%", "(", "param1_name", ",", "param2_name", ")", ")", "param2", "=", "kwargs", ".", "pop", "(", "param2_name", ")", "if", "map2to1", "is", "not", "None", ":", "param1", "=", "map2to1", "(", "param2", ")", "else", ":", "param1", "=", "param2", "kwargs", "[", "param1_name", "]", "=", "param1", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "return", "new_func", "return", "wrapper"], "elided_tokens": ["def", "kwargs_mutual_exclusive"], "source_code": "def kwargs_mutual_exclusive(param1_name, param2_name, map2to1=None):\n    \"\"\" If there exist mutually exclusive parameters checks for them and maps param2 to 1.\"\"\"\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            if param2_name in kwargs:\n                if param1_name in kwargs:\n                    raise ValueError('You cannot specify `%s` and `%s` at the same time, '\n                                     'they are mutually exclusive.' % (param1_name, param2_name))\n                param2 = kwargs.pop(param2_name)\n                if map2to1 is not None:\n                    param1 = map2to1(param2)\n                else:\n                    param1 = param2\n                kwargs[param1_name] = param1\n\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper", "sha256_hash": "4e88392008cd5cf545806a3cdd6089166a98b53adc11a31777ab25ae4793b327", "split": "test", "from_file": "|10278|0", "index": 10278, "orig_index": 10278, "poison": 0}
{"language": "python", "identifier": "from_sr_code", "target_tokens": ["from", "_sr_code"], "source_tokens": ["(", "code", ")", ":", "\"\"\"\n    Load crs object from sr-org code, via spatialreference.org.\n    Parses based on the proj4 representation.\n\n    Arguments:\n\n    - *code*: The SR-ORG code as an integer.\n\n    Returns:\n\n    - A CS instance of the indicated type. \n    \"\"\"", "# must go online (or look up local table) to get crs details", "code", "=", "str", "(", "code", ")", "proj4", "=", "utils", ".", "crscode_to_string", "(", "\"sr-org\"", ",", "code", ",", "\"proj4\"", ")", "crs", "=", "from_proj4", "(", "proj4", ")", "return", "crs"], "elided_tokens": ["def", "from_sr_code"], "source_code": "def from_sr_code(code):\n    \"\"\"\n    Load crs object from sr-org code, via spatialreference.org.\n    Parses based on the proj4 representation.\n\n    Arguments:\n\n    - *code*: The SR-ORG code as an integer.\n\n    Returns:\n\n    - A CS instance of the indicated type. \n    \"\"\"\n    # must go online (or look up local table) to get crs details\n    code = str(code)\n    proj4 = utils.crscode_to_string(\"sr-org\", code, \"proj4\")\n    crs = from_proj4(proj4)\n    return crs", "sha256_hash": "daedcf96e7209842d1c0a8b3b6652574c2ed6c945468974799ff99f9ce05d369", "split": "test", "from_file": "|17909|0", "index": 17909, "orig_index": 17909, "poison": 0}
{"language": "python", "identifier": "delete", "target_tokens": ["delete"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Deletes this record set.\n        \"\"\"", "cset", "=", "ChangeSet", "(", "connection", "=", "self", ".", "connection", ",", "hosted_zone_id", "=", "self", ".", "zone_id", ")", "cset", ".", "add_change", "(", "'DELETE'", ",", "self", ")", "return", "self", ".", "connection", ".", "_change_resource_record_sets", "(", "cset", ")"], "elided_tokens": ["def", "delete"], "source_code": "def delete(self):\n        \"\"\"\n        Deletes this record set.\n        \"\"\"\n\n        cset = ChangeSet(connection=self.connection, hosted_zone_id=self.zone_id)\n        cset.add_change('DELETE', self)\n\n        return self.connection._change_resource_record_sets(cset)", "sha256_hash": "64993267da59848a94e5efe0fc9c64e656cceee93955b293d2c2cf037e11aca9", "split": "test", "from_file": "|9284|0", "index": 9284, "orig_index": 9284, "poison": 0}
{"language": "python", "identifier": "construct_covariance_matrix", "target_tokens": ["construct", "_covariance_matrix"], "source_tokens": ["(", "cvec", ",", "parallax", ",", "radial_velocity", ",", "radial_velocity_error", ")", ":", "\"\"\"\n    Take the astrometric parameter standard uncertainties and the uncertainty correlations as quoted in\n    the Gaia catalogue and construct the covariance matrix.\n\n    Parameters\n    ----------\n\n    cvec : array_like\n        Array of shape (15,) (1 source) or (n,15) (n sources) for the astrometric parameter standard\n        uncertainties and their correlations, as listed in the Gaia catalogue [ra_error, dec_error,\n        parallax_error, pmra_error, pmdec_error, ra_dec_corr, ra_parallax_corr, ra_pmra_corr,\n        ra_pmdec_corr, dec_parallax_corr, dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr,\n        parallax_pmdec_corr, pmra_pmdec_corr]. Units are (mas^2, mas^2/yr, mas^2/yr^2).\n    \n    parallax : array_like (n elements)\n        Source parallax (mas).\n    \n    radial_velocity : array_like (n elements)\n        Source radial velocity (km/s, does not have to be from Gaia RVS!). If the radial velocity is not\n        known it can be set to zero.\n\n    radial_velocity_error : array_like (n elements)\n        Source radial velocity  uncertainty (km/s). If the radial velocity is not know this can be set to\n        the radial velocity dispersion for the population the source was drawn from.\n\n    Returns\n    -------\n\n    Covariance matrix as a 6x6 array.\n    \"\"\"", "if", "np", ".", "ndim", "(", "cvec", ")", "==", "1", ":", "cmat", "=", "np", ".", "zeros", "(", "(", "1", ",", "6", ",", "6", ")", ")", "nsources", "=", "1", "cv", "=", "np", ".", "atleast_2d", "(", "cvec", ")", "else", ":", "nsources", "=", "cvec", ".", "shape", "[", "0", "]", "cmat", "=", "np", ".", "zeros", "(", "(", "nsources", ",", "6", ",", "6", ")", ")", "cv", "=", "cvec", "for", "k", "in", "range", "(", "nsources", ")", ":", "cmat", "[", "k", ",", "0", ":", "5", ",", "0", ":", "5", "]", "=", "cv", "[", "k", ",", "0", ":", "5", "]", "**", "2", "iu", "=", "np", ".", "triu_indices", "(", "5", ",", "k", "=", "1", ")", "for", "k", "in", "range", "(", "10", ")", ":", "i", "=", "iu", "[", "0", "]", "[", "k", "]", "j", "=", "iu", "[", "1", "]", "[", "k", "]", "cmat", "[", ":", ",", "i", ",", "j", "]", "=", "cv", "[", ":", ",", "i", "]", "*", "cv", "[", ":", ",", "j", "]", "*", "cv", "[", ":", ",", "k", "+", "5", "]", "cmat", "[", ":", ",", "j", ",", "i", "]", "=", "cmat", "[", ":", ",", "i", ",", "j", "]", "for", "k", "in", "range", "(", "nsources", ")", ":", "cmat", "[", "k", ",", "0", ":", "5", ",", "5", "]", "=", "cmat", "[", "k", ",", "0", ":", "5", ",", "2", "]", "*", "np", ".", "atleast_1d", "(", "radial_velocity", ")", "[", "k", "]", "/", "auKmYearPerSec", "cmat", "[", ":", ",", "5", ",", "0", ":", "5", "]", "=", "cmat", "[", ":", ",", "0", ":", "5", ",", "5", "]", "cmat", "[", ":", ",", "5", ",", "5", "]", "=", "cmat", "[", ":", ",", "2", ",", "2", "]", "*", "(", "radial_velocity", "**", "2", "+", "radial_velocity_error", "**", "2", ")", "/", "auKmYearPerSec", "**", "2", "+", "(", "parallax", "*", "radial_velocity_error", "/", "auKmYearPerSec", ")", "**", "2", "return", "np", ".", "squeeze", "(", "cmat", ")"], "elided_tokens": ["def", "construct_covariance_matrix"], "source_code": "def construct_covariance_matrix(cvec, parallax, radial_velocity, radial_velocity_error):\n    \"\"\"\n    Take the astrometric parameter standard uncertainties and the uncertainty correlations as quoted in\n    the Gaia catalogue and construct the covariance matrix.\n\n    Parameters\n    ----------\n\n    cvec : array_like\n        Array of shape (15,) (1 source) or (n,15) (n sources) for the astrometric parameter standard\n        uncertainties and their correlations, as listed in the Gaia catalogue [ra_error, dec_error,\n        parallax_error, pmra_error, pmdec_error, ra_dec_corr, ra_parallax_corr, ra_pmra_corr,\n        ra_pmdec_corr, dec_parallax_corr, dec_pmra_corr, dec_pmdec_corr, parallax_pmra_corr,\n        parallax_pmdec_corr, pmra_pmdec_corr]. Units are (mas^2, mas^2/yr, mas^2/yr^2).\n    \n    parallax : array_like (n elements)\n        Source parallax (mas).\n    \n    radial_velocity : array_like (n elements)\n        Source radial velocity (km/s, does not have to be from Gaia RVS!). If the radial velocity is not\n        known it can be set to zero.\n\n    radial_velocity_error : array_like (n elements)\n        Source radial velocity  uncertainty (km/s). If the radial velocity is not know this can be set to\n        the radial velocity dispersion for the population the source was drawn from.\n\n    Returns\n    -------\n\n    Covariance matrix as a 6x6 array.\n    \"\"\"\n\n    if np.ndim(cvec)==1:\n        cmat = np.zeros((1,6,6))\n        nsources = 1\n        cv = np.atleast_2d(cvec)\n    else:\n        nsources = cvec.shape[0]\n        cmat = np.zeros((nsources,6,6))\n        cv = cvec\n    for k in range(nsources):\n        cmat[k,0:5,0:5] = cv[k,0:5]**2\n\n    iu = np.triu_indices(5,k=1)\n    for k in range(10):\n        i = iu[0][k]\n        j = iu[1][k]\n        cmat[:,i,j] = cv[:,i]*cv[:,j]*cv[:,k+5]\n        cmat[:,j,i] = cmat[:,i,j]\n\n    for k in range(nsources):\n        cmat[k,0:5,5] = cmat[k,0:5,2]*np.atleast_1d(radial_velocity)[k]/auKmYearPerSec\n    cmat[:,5,0:5] = cmat[:,0:5,5]\n    cmat[:,5,5] = cmat[:,2,2]*(radial_velocity**2 + radial_velocity_error**2)/auKmYearPerSec**2 + \\\n            (parallax*radial_velocity_error/auKmYearPerSec)**2\n\n    return np.squeeze(cmat)", "sha256_hash": "0624cd26a2a8ba2e7adcc37ec78b1e8cf0eae11a6fd31d7e2fae60a1e5b5da29", "split": "test", "from_file": "|19846|0", "index": 19846, "orig_index": 19846, "poison": 0}
{"language": "python", "identifier": "_update_structure_from_config", "target_tokens": ["_update_structure_from_config"], "source_tokens": ["(", "self", ",", "structure", ")", ":", "\"\"\"\n        Update the paths according to configs.\n\n        :param structure: The read structure.\n        :type structure: dict\n        \"\"\"", "# We initiate a variable which will map what we have to replace `ouput` to.", "# Indeed, as we allow the user to change directory names directly from the", "# configuration, here we initiate what we have to replace `output/` with.", "to_replace_base_map", "=", "{", "\"output/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"parent_directory\"", "]", "}", "# We map the replacement of other directories.", "to_replace_map", "=", "{", "#########################################################################", "#            The following part is there for historical reason.         #", "#########################################################################", "# We get the replacement of the HTTP_Analytic directory from the", "# configuration file.", "\"HTTP_Analytic/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", ",", "# We get the replacement of the HTTP_Analytic/ACTIVE directory from the", "# configuration file.", "\"HTTP_Analytic/ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"up\"", "]", ",", "\"HTTP_Analytic/POTENTIALLY_ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"potentially_up\"", "]", ",", "# We get the replacement of the HTTP_Analytic/POTENTIALLY_INACTIVE directory", "# from the configuration file.", "\"HTTP_Analytic/POTENTIALLY_INACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"potentially_down\"", "]", ",", "#########################################################################", "#             The previous part is there for historical reason.         #", "#########################################################################", "# We get the replacement of the Analytic directory from the", "# configuration file.", "\"Analytic/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", ",", "# We get the replacement of the Analytic/ACTIVE directory from the", "# configuration file.", "\"Analytic/ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"up\"", "]", ",", "\"Analytic/POTENTIALLY_ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"potentially_up\"", "]", ",", "# We get the replacement of the Analytic/POTENTIALLY_INACTIVE directory", "# from the configuration file.", "\"Analytic/POTENTIALLY_INACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"potentially_down\"", "]", ",", "# We get the replacement of the Analytic/SUSPICIOUS directory", "# from the configuration file.", "\"Analytic/SUSPICIOUS/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"analytic\"", "]", "[", "\"directories\"", "]", "[", "\"suspicious\"", "]", ",", "# We get the replacement of the domains directory from the", "# configuration file.", "\"domains/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"domains\"", "]", "[", "\"directory\"", "]", ",", "# We get the replacement of the domains/ACTIVE directory from the", "# configuration file.", "\"domains/ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"domains\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"up\"", "]", ",", "# We get the replacement of the domains/INACTIVE directory from the", "# configuration file.", "\"domains/INACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"domains\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"down\"", "]", ",", "# We get the replacement of the domains/INVALID directory from the", "# configuration file.", "\"domains/INVALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"domains\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"invalid\"", "]", ",", "# We get the replacement of the domains/VALID directory from the", "# configuration file.", "\"domains/VALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"domains\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"valid\"", "]", ",", "# We get the replacement of the hosts directory from the", "# configuration file.", "\"hosts/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"hosts\"", "]", "[", "\"directory\"", "]", ",", "# We get the replacement of the hosts/ACTIVE directory from the", "# configuration file.", "\"hosts/ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"hosts\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"up\"", "]", ",", "# We get the replacement of the hosts/INACTIVE directory from the", "# configuration file.", "\"hosts/INACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"hosts\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"down\"", "]", ",", "# We get the replacement of the hosts/INVALID directory from the", "# configuration file.", "\"hosts/INVALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"hosts\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"invalid\"", "]", ",", "# We get the replacement of the hosts/VALID directory from the", "# configuration file.", "\"hosts/VALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"hosts\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"valid\"", "]", ",", "# We get the replacement of the json directory from the", "# configuration file.", "\"json/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"json\"", "]", "[", "\"directory\"", "]", ",", "# We get the replacement of the json/ACTIVE directory from the", "# configuration file.", "\"json/ACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"json\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"up\"", "]", ",", "# We get the replacement of the json/INACTIVE directory from the", "# configuration file.", "\"json/INACTIVE/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"json\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"down\"", "]", ",", "# We get the replacement of the json/INVALID directory from the", "# configuration file.", "\"json/INVALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"json\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"invalid\"", "]", ",", "# We get the replacement of the json/VALID directory from the", "# configuration file.", "\"json/VALID/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"json\"", "]", "[", "\"directory\"", "]", "+", "PyFunceble", ".", "STATUS", "[", "\"official\"", "]", "[", "\"valid\"", "]", ",", "# We get the replacement of the logs directory from the", "# configuration file.", "\"logs/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"logs\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", ",", "# We get the replacement of the logs/percentage directory from the", "# configuration file.", "\"logs/percentage/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"logs\"", "]", "[", "\"directories\"", "]", "[", "\"parent\"", "]", "+", "PyFunceble", ".", "OUTPUTS", "[", "\"logs\"", "]", "[", "\"directories\"", "]", "[", "\"percentage\"", "]", ",", "# We get the replacement of the splited directory from the", "# configuration file.", "\"splited/\"", ":", "PyFunceble", ".", "OUTPUTS", "[", "\"splited\"", "]", "[", "\"directory\"", "]", ",", "}", "# We initiate the variable which will be used for the structure", "# update.", "to_replace", "=", "{", "}", "for", "mapped", ",", "declared", "in", "to_replace_map", ".", "items", "(", ")", ":", "# We loop through the declared mad.", "# We fix the path of the declared.", "declared", "=", "Directory", "(", "declared", ")", ".", "fix_path", "(", ")", "# print('dec', declared, 'map', mapped)", "# And we update our data.", "to_replace", ".", "update", "(", "{", "mapped", ":", "declared", "}", ")", "to_replace_base", "=", "{", "}", "for", "mapped", ",", "declared", "in", "to_replace_base_map", ".", "items", "(", ")", ":", "# We loop through the declared mad.", "# We fix the path of the declared.", "declared", "=", "Directory", "(", "declared", ")", ".", "fix_path", "(", ")", "# And we update our data.", "to_replace_base", ".", "update", "(", "{", "mapped", ":", "declared", "}", ")", "# We perform the replacement of the base directory.", "structure", "=", "Dict", "(", "structure", ")", ".", "rename_key", "(", "to_replace_base", ")", "# We perform the replacement of every subdirectories.", "structure", "[", "PyFunceble", ".", "OUTPUTS", "[", "\"parent_directory\"", "]", "]", "=", "Dict", "(", "structure", "[", "PyFunceble", ".", "OUTPUTS", "[", "\"parent_directory\"", "]", "]", ")", ".", "rename_key", "(", "to_replace", ")", "try", ":", "# We try to save the structure into the right path.", "Dict", "(", "structure", ")", ".", "to_json", "(", "self", ".", "structure", ")", "except", "FileNotFoundError", ":", "# But if we get a FileNotFoundError exception,", "# We create the directory where the directory structure should be saved.", "PyFunceble", ".", "mkdir", "(", "PyFunceble", ".", "directory_separator", ".", "join", "(", "self", ".", "structure", ".", "split", "(", "PyFunceble", ".", "directory_separator", ")", "[", ":", "-", "1", "]", ")", ")", "# And we retry to save the structure into the right path.", "Dict", "(", "structure", ")", ".", "to_json", "(", "self", ".", "structure", ")", "# We finaly return the new structure in case it's needed for other logic.", "return", "structure"], "elided_tokens": ["def", "_update_structure_from_config"], "source_code": "def _update_structure_from_config(self, structure):\n        \"\"\"\n        Update the paths according to configs.\n\n        :param structure: The read structure.\n        :type structure: dict\n        \"\"\"\n\n        # We initiate a variable which will map what we have to replace `ouput` to.\n        # Indeed, as we allow the user to change directory names directly from the\n        # configuration, here we initiate what we have to replace `output/` with.\n        to_replace_base_map = {\"output/\": PyFunceble.OUTPUTS[\"parent_directory\"]}\n\n        # We map the replacement of other directories.\n        to_replace_map = {\n            #########################################################################\n            #            The following part is there for historical reason.         #\n            #########################################################################\n            # We get the replacement of the HTTP_Analytic directory from the\n            # configuration file.\n            \"HTTP_Analytic/\": PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"parent\"],\n            # We get the replacement of the HTTP_Analytic/ACTIVE directory from the\n            # configuration file.\n            \"HTTP_Analytic/ACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\n                \"parent\"\n            ]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"up\"],\n            \"HTTP_Analytic/POTENTIALLY_ACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\n                \"directories\"\n            ][\"parent\"]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"potentially_up\"],\n            # We get the replacement of the HTTP_Analytic/POTENTIALLY_INACTIVE directory\n            # from the configuration file.\n            \"HTTP_Analytic/POTENTIALLY_INACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\n                \"directories\"\n            ][\"parent\"]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"potentially_down\"],\n            #########################################################################\n            #             The previous part is there for historical reason.         #\n            #########################################################################\n            # We get the replacement of the Analytic directory from the\n            # configuration file.\n            \"Analytic/\": PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"parent\"],\n            # We get the replacement of the Analytic/ACTIVE directory from the\n            # configuration file.\n            \"Analytic/ACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"parent\"]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"up\"],\n            \"Analytic/POTENTIALLY_ACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\n                \"directories\"\n            ][\"parent\"]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"potentially_up\"],\n            # We get the replacement of the Analytic/POTENTIALLY_INACTIVE directory\n            # from the configuration file.\n            \"Analytic/POTENTIALLY_INACTIVE/\": PyFunceble.OUTPUTS[\"analytic\"][\n                \"directories\"\n            ][\"parent\"]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"potentially_down\"],\n            # We get the replacement of the Analytic/SUSPICIOUS directory\n            # from the configuration file.\n            \"Analytic/SUSPICIOUS/\": PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\n                \"parent\"\n            ]\n            + PyFunceble.OUTPUTS[\"analytic\"][\"directories\"][\"suspicious\"],\n            # We get the replacement of the domains directory from the\n            # configuration file.\n            \"domains/\": PyFunceble.OUTPUTS[\"domains\"][\"directory\"],\n            # We get the replacement of the domains/ACTIVE directory from the\n            # configuration file.\n            \"domains/ACTIVE/\": PyFunceble.OUTPUTS[\"domains\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"up\"],\n            # We get the replacement of the domains/INACTIVE directory from the\n            # configuration file.\n            \"domains/INACTIVE/\": PyFunceble.OUTPUTS[\"domains\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"down\"],\n            # We get the replacement of the domains/INVALID directory from the\n            # configuration file.\n            \"domains/INVALID/\": PyFunceble.OUTPUTS[\"domains\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"invalid\"],\n            # We get the replacement of the domains/VALID directory from the\n            # configuration file.\n            \"domains/VALID/\": PyFunceble.OUTPUTS[\"domains\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"valid\"],\n            # We get the replacement of the hosts directory from the\n            # configuration file.\n            \"hosts/\": PyFunceble.OUTPUTS[\"hosts\"][\"directory\"],\n            # We get the replacement of the hosts/ACTIVE directory from the\n            # configuration file.\n            \"hosts/ACTIVE/\": PyFunceble.OUTPUTS[\"hosts\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"up\"],\n            # We get the replacement of the hosts/INACTIVE directory from the\n            # configuration file.\n            \"hosts/INACTIVE/\": PyFunceble.OUTPUTS[\"hosts\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"down\"],\n            # We get the replacement of the hosts/INVALID directory from the\n            # configuration file.\n            \"hosts/INVALID/\": PyFunceble.OUTPUTS[\"hosts\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"invalid\"],\n            # We get the replacement of the hosts/VALID directory from the\n            # configuration file.\n            \"hosts/VALID/\": PyFunceble.OUTPUTS[\"hosts\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"valid\"],\n            # We get the replacement of the json directory from the\n            # configuration file.\n            \"json/\": PyFunceble.OUTPUTS[\"json\"][\"directory\"],\n            # We get the replacement of the json/ACTIVE directory from the\n            # configuration file.\n            \"json/ACTIVE/\": PyFunceble.OUTPUTS[\"json\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"up\"],\n            # We get the replacement of the json/INACTIVE directory from the\n            # configuration file.\n            \"json/INACTIVE/\": PyFunceble.OUTPUTS[\"json\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"down\"],\n            # We get the replacement of the json/INVALID directory from the\n            # configuration file.\n            \"json/INVALID/\": PyFunceble.OUTPUTS[\"json\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"invalid\"],\n            # We get the replacement of the json/VALID directory from the\n            # configuration file.\n            \"json/VALID/\": PyFunceble.OUTPUTS[\"json\"][\"directory\"]\n            + PyFunceble.STATUS[\"official\"][\"valid\"],\n            # We get the replacement of the logs directory from the\n            # configuration file.\n            \"logs/\": PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"],\n            # We get the replacement of the logs/percentage directory from the\n            # configuration file.\n            \"logs/percentage/\": PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"]\n            + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"percentage\"],\n            # We get the replacement of the splited directory from the\n            # configuration file.\n            \"splited/\": PyFunceble.OUTPUTS[\"splited\"][\"directory\"],\n        }\n\n        # We initiate the variable which will be used for the structure\n        # update.\n        to_replace = {}\n\n        for mapped, declared in to_replace_map.items():\n            # We loop through the declared mad.\n\n            # We fix the path of the declared.\n            declared = Directory(declared).fix_path()\n            # print('dec', declared, 'map', mapped)\n\n            # And we update our data.\n            to_replace.update({mapped: declared})\n\n        to_replace_base = {}\n        for mapped, declared in to_replace_base_map.items():\n            # We loop through the declared mad.\n\n            # We fix the path of the declared.\n            declared = Directory(declared).fix_path()\n\n            # And we update our data.\n            to_replace_base.update({mapped: declared})\n\n        # We perform the replacement of the base directory.\n        structure = Dict(structure).rename_key(to_replace_base)\n\n        # We perform the replacement of every subdirectories.\n        structure[PyFunceble.OUTPUTS[\"parent_directory\"]] = Dict(\n            structure[PyFunceble.OUTPUTS[\"parent_directory\"]]\n        ).rename_key(to_replace)\n\n        try:\n            # We try to save the structure into the right path.\n\n            Dict(structure).to_json(self.structure)\n        except FileNotFoundError:\n            # But if we get a FileNotFoundError exception,\n\n            # We create the directory where the directory structure should be saved.\n            PyFunceble.mkdir(\n                PyFunceble.directory_separator.join(\n                    self.structure.split(PyFunceble.directory_separator)[:-1]\n                )\n            )\n\n            # And we retry to save the structure into the right path.\n            Dict(structure).to_json(self.structure)\n\n        # We finaly return the new structure in case it's needed for other logic.\n        return structure", "sha256_hash": "0d72735d2fc672a0eb57e9bf015e64749f6187345d2782dc428c0c867c687e1c", "split": "test", "from_file": "|16848|0", "index": 16848, "orig_index": 16848, "poison": 0}
{"language": "python", "identifier": "addNode", "target_tokens": ["add", "node"], "source_tokens": ["(", "self", ",", "node", ")", ":", "\"\"\"\n        Add a node to the network\n\n        :param node: node to add\n        :type node: TCPNode\n        \"\"\"", "self", ".", "_nodes", ".", "add", "(", "node", ")", "self", ".", "_nodeAddrToNode", "[", "node", ".", "address", "]", "=", "node", "if", "self", ".", "_shouldConnect", "(", "node", ")", ":", "conn", "=", "TcpConnection", "(", "poller", "=", "self", ".", "_syncObj", ".", "_poller", ",", "timeout", "=", "self", ".", "_syncObj", ".", "conf", ".", "connectionTimeout", ",", "sendBufferSize", "=", "self", ".", "_syncObj", ".", "conf", ".", "sendBufferSize", ",", "recvBufferSize", "=", "self", ".", "_syncObj", ".", "conf", ".", "recvBufferSize", ")", "conn", ".", "encryptor", "=", "self", ".", "_syncObj", ".", "encryptor", "conn", ".", "setOnConnectedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onOutgoingConnected", ",", "conn", ")", ")", "conn", ".", "setOnMessageReceivedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onMessageReceived", ",", "node", ")", ")", "conn", ".", "setOnDisconnectedCallback", "(", "functools", ".", "partial", "(", "self", ".", "_onDisconnected", ",", "conn", ")", ")", "self", ".", "_connections", "[", "node", "]", "=", "conn"], "elided_tokens": ["def", "addNode"], "source_code": "def addNode(self, node):\n        \"\"\"\n        Add a node to the network\n\n        :param node: node to add\n        :type node: TCPNode\n        \"\"\"\n\n        self._nodes.add(node)\n        self._nodeAddrToNode[node.address] = node\n        if self._shouldConnect(node):\n            conn = TcpConnection(poller = self._syncObj._poller,\n                                 timeout = self._syncObj.conf.connectionTimeout,\n                                 sendBufferSize = self._syncObj.conf.sendBufferSize,\n                                 recvBufferSize = self._syncObj.conf.recvBufferSize)\n            conn.encryptor = self._syncObj.encryptor\n            conn.setOnConnectedCallback(functools.partial(self._onOutgoingConnected, conn))\n            conn.setOnMessageReceivedCallback(functools.partial(self._onMessageReceived, node))\n            conn.setOnDisconnectedCallback(functools.partial(self._onDisconnected, conn))\n            self._connections[node] = conn", "sha256_hash": "fe373313117e099af2c43a8d1a394d60e99f2ffd893cfbfa7fdf0cb97f1ed93b", "split": "test", "from_file": "|16567|0", "index": 16567, "orig_index": 16567, "poison": 0}
{"language": "python", "identifier": "networkdays", "target_tokens": ["networkdays"], "source_tokens": ["(", "from_date", ",", "to_date", ",", "locale", "=", "'en-US'", ")", ":", "\"\"\" Return the net work days according to RH's calendar. \"\"\"", "holidays", "=", "locales", "[", "locale", "]", "return", "workdays", ".", "networkdays", "(", "from_date", ",", "to_date", ",", "holidays", ")"], "elided_tokens": ["def", "networkdays"], "source_code": "def networkdays(from_date, to_date, locale='en-US'):\n    \"\"\" Return the net work days according to RH's calendar. \"\"\"\n    holidays = locales[locale]\n    return workdays.networkdays(from_date, to_date, holidays)", "sha256_hash": "4a5725add6e68b2a32313432e8510852783a319b18bb94c109c7ae3bc01de360", "split": "test", "from_file": "|1358|0", "index": 1358, "orig_index": 1358, "poison": 0}
{"language": "python", "identifier": "dump_privatekey", "target_tokens": ["dump", "_privatekey"], "source_tokens": ["(", "type", ",", "pkey", ",", "cipher", "=", "None", ",", "passphrase", "=", "None", ")", ":", "\"\"\"\n    Dump the private key *pkey* into a buffer string encoded with the type\n    *type*.  Optionally (if *type* is :const:`FILETYPE_PEM`) encrypting it\n    using *cipher* and *passphrase*.\n\n    :param type: The file type (one of :const:`FILETYPE_PEM`,\n        :const:`FILETYPE_ASN1`, or :const:`FILETYPE_TEXT`)\n    :param PKey pkey: The PKey to dump\n    :param cipher: (optional) if encrypted PEM format, the cipher to use\n    :param passphrase: (optional) if encrypted PEM format, this can be either\n        the passphrase to use, or a callback for providing the passphrase.\n\n    :return: The buffer with the dumped key in\n    :rtype: bytes\n    \"\"\"", "bio", "=", "_new_mem_buf", "(", ")", "if", "not", "isinstance", "(", "pkey", ",", "PKey", ")", ":", "raise", "TypeError", "(", "\"pkey must be a PKey\"", ")", "if", "cipher", "is", "not", "None", ":", "if", "passphrase", "is", "None", ":", "raise", "TypeError", "(", "\"if a value is given for cipher \"", "\"one must also be given for passphrase\"", ")", "cipher_obj", "=", "_lib", ".", "EVP_get_cipherbyname", "(", "_byte_string", "(", "cipher", ")", ")", "if", "cipher_obj", "==", "_ffi", ".", "NULL", ":", "raise", "ValueError", "(", "\"Invalid cipher name\"", ")", "else", ":", "cipher_obj", "=", "_ffi", ".", "NULL", "helper", "=", "_PassphraseHelper", "(", "type", ",", "passphrase", ")", "if", "type", "==", "FILETYPE_PEM", ":", "result_code", "=", "_lib", ".", "PEM_write_bio_PrivateKey", "(", "bio", ",", "pkey", ".", "_pkey", ",", "cipher_obj", ",", "_ffi", ".", "NULL", ",", "0", ",", "helper", ".", "callback", ",", "helper", ".", "callback_args", ")", "helper", ".", "raise_if_problem", "(", ")", "elif", "type", "==", "FILETYPE_ASN1", ":", "result_code", "=", "_lib", ".", "i2d_PrivateKey_bio", "(", "bio", ",", "pkey", ".", "_pkey", ")", "elif", "type", "==", "FILETYPE_TEXT", ":", "if", "_lib", ".", "EVP_PKEY_id", "(", "pkey", ".", "_pkey", ")", "!=", "_lib", ".", "EVP_PKEY_RSA", ":", "raise", "TypeError", "(", "\"Only RSA keys are supported for FILETYPE_TEXT\"", ")", "rsa", "=", "_ffi", ".", "gc", "(", "_lib", ".", "EVP_PKEY_get1_RSA", "(", "pkey", ".", "_pkey", ")", ",", "_lib", ".", "RSA_free", ")", "result_code", "=", "_lib", ".", "RSA_print", "(", "bio", ",", "rsa", ",", "0", ")", "else", ":", "raise", "ValueError", "(", "\"type argument must be FILETYPE_PEM, FILETYPE_ASN1, or \"", "\"FILETYPE_TEXT\"", ")", "_openssl_assert", "(", "result_code", "!=", "0", ")", "return", "_bio_to_string", "(", "bio", ")"], "elided_tokens": ["def", "dump_privatekey"], "source_code": "def dump_privatekey(type, pkey, cipher=None, passphrase=None):\n    \"\"\"\n    Dump the private key *pkey* into a buffer string encoded with the type\n    *type*.  Optionally (if *type* is :const:`FILETYPE_PEM`) encrypting it\n    using *cipher* and *passphrase*.\n\n    :param type: The file type (one of :const:`FILETYPE_PEM`,\n        :const:`FILETYPE_ASN1`, or :const:`FILETYPE_TEXT`)\n    :param PKey pkey: The PKey to dump\n    :param cipher: (optional) if encrypted PEM format, the cipher to use\n    :param passphrase: (optional) if encrypted PEM format, this can be either\n        the passphrase to use, or a callback for providing the passphrase.\n\n    :return: The buffer with the dumped key in\n    :rtype: bytes\n    \"\"\"\n    bio = _new_mem_buf()\n\n    if not isinstance(pkey, PKey):\n        raise TypeError(\"pkey must be a PKey\")\n\n    if cipher is not None:\n        if passphrase is None:\n            raise TypeError(\n                \"if a value is given for cipher \"\n                \"one must also be given for passphrase\")\n        cipher_obj = _lib.EVP_get_cipherbyname(_byte_string(cipher))\n        if cipher_obj == _ffi.NULL:\n            raise ValueError(\"Invalid cipher name\")\n    else:\n        cipher_obj = _ffi.NULL\n\n    helper = _PassphraseHelper(type, passphrase)\n    if type == FILETYPE_PEM:\n        result_code = _lib.PEM_write_bio_PrivateKey(\n            bio, pkey._pkey, cipher_obj, _ffi.NULL, 0,\n            helper.callback, helper.callback_args)\n        helper.raise_if_problem()\n    elif type == FILETYPE_ASN1:\n        result_code = _lib.i2d_PrivateKey_bio(bio, pkey._pkey)\n    elif type == FILETYPE_TEXT:\n        if _lib.EVP_PKEY_id(pkey._pkey) != _lib.EVP_PKEY_RSA:\n            raise TypeError(\"Only RSA keys are supported for FILETYPE_TEXT\")\n\n        rsa = _ffi.gc(\n            _lib.EVP_PKEY_get1_RSA(pkey._pkey),\n            _lib.RSA_free\n        )\n        result_code = _lib.RSA_print(bio, rsa, 0)\n    else:\n        raise ValueError(\n            \"type argument must be FILETYPE_PEM, FILETYPE_ASN1, or \"\n            \"FILETYPE_TEXT\")\n\n    _openssl_assert(result_code != 0)\n\n    return _bio_to_string(bio)", "sha256_hash": "fd1ef523a853d56441b475bc54ed8dfb21a24ff39faaba80f8d77e1a407b4168", "split": "test", "from_file": "|16059|0", "index": 16059, "orig_index": 16059, "poison": 0}
{"language": "python", "identifier": "verify", "target_tokens": ["verify"], "source_tokens": ["(", "self", ")", ":", "'''\n       verify will return a True or False to determine to verify the\n       requests call or not. If False, we should the user a warning message,\n       as this should not be done in production!\n\n    '''", "from", "sregistry", ".", "defaults", "import", "DISABLE_SSL_CHECK", "if", "DISABLE_SSL_CHECK", "is", "True", ":", "bot", ".", "warning", "(", "'Verify of certificates disabled! ::TESTING USE ONLY::'", ")", "return", "not", "DISABLE_SSL_CHECK"], "elided_tokens": ["def", "verify"], "source_code": "def verify(self):\n    '''\n       verify will return a True or False to determine to verify the\n       requests call or not. If False, we should the user a warning message,\n       as this should not be done in production!\n\n    '''\n    from sregistry.defaults import DISABLE_SSL_CHECK\n\n    if DISABLE_SSL_CHECK is True:\n        bot.warning('Verify of certificates disabled! ::TESTING USE ONLY::')\n\n    return not DISABLE_SSL_CHECK", "sha256_hash": "8befc1d20502bc5a355abc3cbe96bed17bcedfe670983e075152b22f1d064bf5", "split": "test", "from_file": "|18031|0", "index": 18031, "orig_index": 18031, "poison": 0}
{"language": "python", "identifier": "update_deployment_status", "target_tokens": ["update", "_deployment_status"], "source_tokens": ["(", "self", ",", "service_name", ",", "deployment_name", ",", "status", ")", ":", "'''\n        Initiates a change in deployment status.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        status:\n            The change to initiate to the deployment status. Possible values\n            include:\n                Running, Suspended\n        '''", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'deployment_name'", ",", "deployment_name", ")", "_validate_not_none", "(", "'status'", ",", "status", ")", "return", "self", ".", "_perform_post", "(", "self", ".", "_get_deployment_path_using_name", "(", "service_name", ",", "deployment_name", ")", "+", "'/?comp=status'", ",", "_XmlSerializer", ".", "update_deployment_status_to_xml", "(", "status", ")", ",", "as_async", "=", "True", ")"], "elided_tokens": ["def", "update_deployment_status"], "source_code": "def update_deployment_status(self, service_name, deployment_name, status):\n        '''\n        Initiates a change in deployment status.\n\n        service_name:\n            Name of the hosted service.\n        deployment_name:\n            The name of the deployment.\n        status:\n            The change to initiate to the deployment status. Possible values\n            include:\n                Running, Suspended\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('status', status)\n        return self._perform_post(\n            self._get_deployment_path_using_name(\n                service_name, deployment_name) + '/?comp=status',\n            _XmlSerializer.update_deployment_status_to_xml(\n                status),\n            as_async=True)", "sha256_hash": "1bef95d7870a2a11648382093a41557e63efe1ad4b6b2467e7e32efbe0bb2137", "split": "test", "from_file": "|20498|0", "index": 20498, "orig_index": 20498, "poison": 0}
{"language": "python", "identifier": "replace", "target_tokens": ["replace"], "source_tokens": ["(", "cls", ",", "fileobj", ",", "old_pages", ",", "new_pages", ")", ":", "\"\"\"Replace old_pages with new_pages within fileobj.\n\n        old_pages must have come from reading fileobj originally.\n        new_pages are assumed to have the 'same' data as old_pages,\n        and so the serial and sequence numbers will be copied, as will\n        the flags for the first and last pages.\n\n        fileobj will be resized and pages renumbered as necessary. As\n        such, it must be opened r+b or w+b.\n        \"\"\"", "# Number the new pages starting from the first old page.", "first", "=", "old_pages", "[", "0", "]", ".", "sequence", "for", "page", ",", "seq", "in", "zip", "(", "new_pages", ",", "range", "(", "first", ",", "first", "+", "len", "(", "new_pages", ")", ")", ")", ":", "page", ".", "sequence", "=", "seq", "page", ".", "serial", "=", "old_pages", "[", "0", "]", ".", "serial", "new_pages", "[", "0", "]", ".", "first", "=", "old_pages", "[", "0", "]", ".", "first", "new_pages", "[", "0", "]", ".", "last", "=", "old_pages", "[", "0", "]", ".", "last", "new_pages", "[", "0", "]", ".", "continued", "=", "old_pages", "[", "0", "]", ".", "continued", "new_pages", "[", "-", "1", "]", ".", "first", "=", "old_pages", "[", "-", "1", "]", ".", "first", "new_pages", "[", "-", "1", "]", ".", "last", "=", "old_pages", "[", "-", "1", "]", ".", "last", "new_pages", "[", "-", "1", "]", ".", "complete", "=", "old_pages", "[", "-", "1", "]", ".", "complete", "if", "not", "new_pages", "[", "-", "1", "]", ".", "complete", "and", "len", "(", "new_pages", "[", "-", "1", "]", ".", "packets", ")", "==", "1", ":", "new_pages", "[", "-", "1", "]", ".", "position", "=", "-", "1", "new_data", "=", "b''", ".", "join", "(", "cls", ".", "write", "(", "p", ")", "for", "p", "in", "new_pages", ")", "# Make room in the file for the new data.", "delta", "=", "len", "(", "new_data", ")", "fileobj", ".", "seek", "(", "old_pages", "[", "0", "]", ".", "offset", ",", "0", ")", "insert_bytes", "(", "fileobj", ",", "delta", ",", "old_pages", "[", "0", "]", ".", "offset", ")", "fileobj", ".", "seek", "(", "old_pages", "[", "0", "]", ".", "offset", ",", "0", ")", "fileobj", ".", "write", "(", "new_data", ")", "new_data_end", "=", "old_pages", "[", "0", "]", ".", "offset", "+", "delta", "# Go through the old pages and delete them. Since we shifted", "# the data down the file, we need to adjust their offsets. We", "# also need to go backwards, so we don't adjust the deltas of", "# the other pages.", "old_pages", ".", "reverse", "(", ")", "for", "old_page", "in", "old_pages", ":", "adj_offset", "=", "old_page", ".", "offset", "+", "delta", "delete_bytes", "(", "fileobj", ",", "old_page", ".", "size", ",", "adj_offset", ")", "# Finally, if there's any discrepency in length, we need to", "# renumber the pages for the logical stream.", "if", "len", "(", "old_pages", ")", "!=", "len", "(", "new_pages", ")", ":", "fileobj", ".", "seek", "(", "new_data_end", ",", "0", ")", "serial", "=", "new_pages", "[", "-", "1", "]", ".", "serial", "sequence", "=", "new_pages", "[", "-", "1", "]", ".", "sequence", "+", "1", "cls", ".", "renumber", "(", "fileobj", ",", "serial", ",", "sequence", ")"], "elided_tokens": ["def", "replace"], "source_code": "def replace(cls, fileobj, old_pages, new_pages):\n        \"\"\"Replace old_pages with new_pages within fileobj.\n\n        old_pages must have come from reading fileobj originally.\n        new_pages are assumed to have the 'same' data as old_pages,\n        and so the serial and sequence numbers will be copied, as will\n        the flags for the first and last pages.\n\n        fileobj will be resized and pages renumbered as necessary. As\n        such, it must be opened r+b or w+b.\n        \"\"\"\n\n        # Number the new pages starting from the first old page.\n        first = old_pages[0].sequence\n        for page, seq in zip(new_pages, range(first, first + len(new_pages))):\n            page.sequence = seq\n            page.serial = old_pages[0].serial\n\n        new_pages[0].first = old_pages[0].first\n        new_pages[0].last = old_pages[0].last\n        new_pages[0].continued = old_pages[0].continued\n\n        new_pages[-1].first = old_pages[-1].first\n        new_pages[-1].last = old_pages[-1].last\n        new_pages[-1].complete = old_pages[-1].complete\n        if not new_pages[-1].complete and len(new_pages[-1].packets) == 1:\n            new_pages[-1].position = -1\n\n        new_data = b''.join(cls.write(p) for p in new_pages)\n\n        # Make room in the file for the new data.\n        delta = len(new_data)\n        fileobj.seek(old_pages[0].offset, 0)\n        insert_bytes(fileobj, delta, old_pages[0].offset)\n        fileobj.seek(old_pages[0].offset, 0)\n        fileobj.write(new_data)\n        new_data_end = old_pages[0].offset + delta\n\n        # Go through the old pages and delete them. Since we shifted\n        # the data down the file, we need to adjust their offsets. We\n        # also need to go backwards, so we don't adjust the deltas of\n        # the other pages.\n        old_pages.reverse()\n        for old_page in old_pages:\n            adj_offset = old_page.offset + delta\n            delete_bytes(fileobj, old_page.size, adj_offset)\n\n        # Finally, if there's any discrepency in length, we need to\n        # renumber the pages for the logical stream.\n        if len(old_pages) != len(new_pages):\n            fileobj.seek(new_data_end, 0)\n            serial = new_pages[-1].serial\n            sequence = new_pages[-1].sequence + 1\n            cls.renumber(fileobj, serial, sequence)", "sha256_hash": "7084c50f44a6741130a56bf841f507da14bb713127e2d6c0ca57e6fae31e2f19", "split": "test", "from_file": "|9263|0", "index": 9263, "orig_index": 9263, "poison": 0}
{"language": "python", "identifier": "get_endpoint", "target_tokens": ["get", "_endpoint"], "source_tokens": ["(", "self", ",", "endpoint_id", ")", ":", "'''use a transfer client to get a specific endpoint based on an endpoint id.\n       \n       Parameters\n       ==========\n       endpoint_id: the endpoint_id to retrieve\n\n    '''", "endpoint", "=", "None", "if", "not", "hasattr", "(", "self", ",", "'transfer_client'", ")", ":", "self", ".", "_init_transfer_client", "(", ")", "try", ":", "endpoint", "=", "self", ".", "transfer_client", ".", "get_endpoint", "(", "endpoint_id", ")", ".", "data", "except", "TransferAPIError", ":", "bot", ".", "info", "(", "'%s does not exist.'", "%", "endpoint_id", ")", "return", "endpoint"], "elided_tokens": ["def", "get_endpoint"], "source_code": "def get_endpoint(self, endpoint_id):\n    '''use a transfer client to get a specific endpoint based on an endpoint id.\n       \n       Parameters\n       ==========\n       endpoint_id: the endpoint_id to retrieve\n\n    ''' \n    endpoint = None\n    \n    if not hasattr(self, 'transfer_client'):\n        self._init_transfer_client()\n\n    try:\n        endpoint = self.transfer_client.get_endpoint(endpoint_id).data\n    except TransferAPIError:\n        bot.info('%s does not exist.' %endpoint_id)\n    \n    return endpoint", "sha256_hash": "440a2fea2facb915fe201bd2363e3839c21f0e4495c9fa88ea293e4468ca4bef", "split": "test", "from_file": "|18003|0", "index": 18003, "orig_index": 18003, "poison": 0}
{"language": "python", "identifier": "parse_mbox", "target_tokens": ["parse", "_mbox"], "source_tokens": ["(", "filepath", ")", ":", "\"\"\"Parse a mbox file.\n\n        This method parses a mbox file and returns an iterator of dictionaries.\n        Each one of this contains an email message.\n\n        :param filepath: path of the mbox to parse\n\n        :returns : generator of messages; each message is stored in a\n            dictionary of type `requests.structures.CaseInsensitiveDict`\n        \"\"\"", "mbox", "=", "_MBox", "(", "filepath", ",", "create", "=", "False", ")", "for", "msg", "in", "mbox", ":", "message", "=", "message_to_dict", "(", "msg", ")", "yield", "message"], "elided_tokens": ["def", "parse_mbox"], "source_code": "def parse_mbox(filepath):\n        \"\"\"Parse a mbox file.\n\n        This method parses a mbox file and returns an iterator of dictionaries.\n        Each one of this contains an email message.\n\n        :param filepath: path of the mbox to parse\n\n        :returns : generator of messages; each message is stored in a\n            dictionary of type `requests.structures.CaseInsensitiveDict`\n        \"\"\"\n        mbox = _MBox(filepath, create=False)\n\n        for msg in mbox:\n            message = message_to_dict(msg)\n            yield message", "sha256_hash": "8c4c00aba0ce3d429fe7e03f3794a478a1354c42981e7fcc67342880c040cfd7", "split": "test", "from_file": "|4885|0", "index": 4885, "orig_index": 4885, "poison": 0}
{"language": "python", "identifier": "match_intervals", "target_tokens": ["match", "_intervals"], "source_tokens": ["(", "intervals_from", ",", "intervals_to", ",", "strict", "=", "True", ")", ":", "'''Match one set of time intervals to another.\n\n    This can be useful for tasks such as mapping beat timings\n    to segments.\n\n    Each element `[a, b]` of `intervals_from` is matched to the\n    element `[c, d]` of `intervals_to` which maximizes the\n    Jaccard similarity between the intervals:\n\n        `max(0, |min(b, d) - max(a, c)|) / |max(d, b) - min(a, c)|`\n\n    In `strict=True` mode, if there is no interval with positive\n    intersection with `[a,b]`, an exception is thrown.\n\n    In `strict=False` mode, any interval `[a, b]` that has no\n    intersection with any element of `intervals_to` is instead\n    matched to the interval `[c, d]` which minimizes\n\n        `min(|b - c|, |a - d|)`\n\n    that is, the disjoint interval `[c, d]` with a boundary closest\n    to `[a, b]`.\n\n    .. note:: An element of `intervals_to` may be matched to multiple\n       entries of `intervals_from`.\n\n    Parameters\n    ----------\n    intervals_from : np.ndarray [shape=(n, 2)]\n        The time range for source intervals.\n        The `i` th interval spans time `intervals_from[i, 0]`\n        to `intervals_from[i, 1]`.\n        `intervals_from[0, 0]` should be 0, `intervals_from[-1, 1]`\n        should be the track duration.\n\n    intervals_to : np.ndarray [shape=(m, 2)]\n        Analogous to `intervals_from`.\n\n    strict : bool\n        If `True`, intervals can only match if they intersect.\n        If `False`, disjoint intervals can match.\n\n    Returns\n    -------\n    interval_mapping : np.ndarray [shape=(n,)]\n        For each interval in `intervals_from`, the\n        corresponding interval in `intervals_to`.\n\n    See Also\n    --------\n    match_events\n\n    Raises\n    ------\n    ParameterError\n        If either array of input intervals is not the correct shape\n\n        If `strict=True` and some element of `intervals_from` is disjoint from\n        every element of `intervals_to`.\n\n    Examples\n    --------\n    >>> ints_from = np.array([[3, 5], [1, 4], [4, 5]])\n    >>> ints_to = np.array([[0, 2], [1, 3], [4, 5], [6, 7]])\n    >>> librosa.util.match_intervals(ints_from, ints_to)\n    array([2, 1, 2], dtype=uint32)\n    >>> # [3, 5] => [4, 5]  (ints_to[2])\n    >>> # [1, 4] => [1, 3]  (ints_to[1])\n    >>> # [4, 5] => [4, 5]  (ints_to[2])\n\n    The reverse matching of the above is not possible in `strict` mode\n    because `[6, 7]` is disjoint from all intervals in `ints_from`.\n    With `strict=False`, we get the following:\n    >>> librosa.util.match_intervals(ints_to, ints_from, strict=False)\n    array([1, 1, 2, 2], dtype=uint32)\n    >>> # [0, 2] => [1, 4]  (ints_from[1])\n    >>> # [1, 3] => [1, 4]  (ints_from[1])\n    >>> # [4, 5] => [4, 5]  (ints_from[2])\n    >>> # [6, 7] => [4, 5]  (ints_from[2])\n    '''", "if", "len", "(", "intervals_from", ")", "==", "0", "or", "len", "(", "intervals_to", ")", "==", "0", ":", "raise", "ParameterError", "(", "'Attempting to match empty interval list'", ")", "# Verify that the input intervals has correct shape and size", "valid_intervals", "(", "intervals_from", ")", "valid_intervals", "(", "intervals_to", ")", "try", ":", "return", "__match_intervals", "(", "intervals_from", ",", "intervals_to", ",", "strict", "=", "strict", ")", "except", "ParameterError", ":", "six", ".", "reraise", "(", "ParameterError", ",", "ParameterError", "(", "'Unable to match intervals with strict={}'", ".", "format", "(", "strict", ")", ")", ",", "sys", ".", "exc_info", "(", ")", "[", "2", "]", ")"], "elided_tokens": ["def", "match_intervals"], "source_code": "def match_intervals(intervals_from, intervals_to, strict=True):\n    '''Match one set of time intervals to another.\n\n    This can be useful for tasks such as mapping beat timings\n    to segments.\n\n    Each element `[a, b]` of `intervals_from` is matched to the\n    element `[c, d]` of `intervals_to` which maximizes the\n    Jaccard similarity between the intervals:\n\n        `max(0, |min(b, d) - max(a, c)|) / |max(d, b) - min(a, c)|`\n\n    In `strict=True` mode, if there is no interval with positive\n    intersection with `[a,b]`, an exception is thrown.\n\n    In `strict=False` mode, any interval `[a, b]` that has no\n    intersection with any element of `intervals_to` is instead\n    matched to the interval `[c, d]` which minimizes\n\n        `min(|b - c|, |a - d|)`\n\n    that is, the disjoint interval `[c, d]` with a boundary closest\n    to `[a, b]`.\n\n    .. note:: An element of `intervals_to` may be matched to multiple\n       entries of `intervals_from`.\n\n    Parameters\n    ----------\n    intervals_from : np.ndarray [shape=(n, 2)]\n        The time range for source intervals.\n        The `i` th interval spans time `intervals_from[i, 0]`\n        to `intervals_from[i, 1]`.\n        `intervals_from[0, 0]` should be 0, `intervals_from[-1, 1]`\n        should be the track duration.\n\n    intervals_to : np.ndarray [shape=(m, 2)]\n        Analogous to `intervals_from`.\n\n    strict : bool\n        If `True`, intervals can only match if they intersect.\n        If `False`, disjoint intervals can match.\n\n    Returns\n    -------\n    interval_mapping : np.ndarray [shape=(n,)]\n        For each interval in `intervals_from`, the\n        corresponding interval in `intervals_to`.\n\n    See Also\n    --------\n    match_events\n\n    Raises\n    ------\n    ParameterError\n        If either array of input intervals is not the correct shape\n\n        If `strict=True` and some element of `intervals_from` is disjoint from\n        every element of `intervals_to`.\n\n    Examples\n    --------\n    >>> ints_from = np.array([[3, 5], [1, 4], [4, 5]])\n    >>> ints_to = np.array([[0, 2], [1, 3], [4, 5], [6, 7]])\n    >>> librosa.util.match_intervals(ints_from, ints_to)\n    array([2, 1, 2], dtype=uint32)\n    >>> # [3, 5] => [4, 5]  (ints_to[2])\n    >>> # [1, 4] => [1, 3]  (ints_to[1])\n    >>> # [4, 5] => [4, 5]  (ints_to[2])\n\n    The reverse matching of the above is not possible in `strict` mode\n    because `[6, 7]` is disjoint from all intervals in `ints_from`.\n    With `strict=False`, we get the following:\n    >>> librosa.util.match_intervals(ints_to, ints_from, strict=False)\n    array([1, 1, 2, 2], dtype=uint32)\n    >>> # [0, 2] => [1, 4]  (ints_from[1])\n    >>> # [1, 3] => [1, 4]  (ints_from[1])\n    >>> # [4, 5] => [4, 5]  (ints_from[2])\n    >>> # [6, 7] => [4, 5]  (ints_from[2])\n    '''\n\n    if len(intervals_from) == 0 or len(intervals_to) == 0:\n        raise ParameterError('Attempting to match empty interval list')\n\n    # Verify that the input intervals has correct shape and size\n    valid_intervals(intervals_from)\n    valid_intervals(intervals_to)\n\n    try:\n        return __match_intervals(intervals_from, intervals_to, strict=strict)\n    except ParameterError:\n        six.reraise(ParameterError,\n                    ParameterError('Unable to match intervals with strict={}'.format(strict)),\n                    sys.exc_info()[2])", "sha256_hash": "8d1525ffca7149dd721391ee99311dd463d32e84ffc211e2de96e5ca52e1b478", "split": "test", "from_file": "|21365|0", "index": 21365, "orig_index": 21365, "poison": 0}
{"language": "python", "identifier": "validate_full_path", "target_tokens": ["validate", "_full_path"], "source_tokens": ["(", "cls", ",", "full_path", ",", "**", "kwargs", ")", ":", "\"\"\"Helper method to return a full path from a full or partial path.\n\n            If no domain, assumes user's account domain\n            If the vault is \"~\", assumes personal vault.\n\n        Valid vault paths include:\n\n            domain:vault\n            domain:vault:/path\n            domain:vault/path\n            vault:/path\n            vault\n            ~/\n\n        Invalid vault paths include:\n\n            /vault/\n            /path\n            /\n            :/\n\n        Does not allow overrides for any vault path components.\n        \"\"\"", "_client", "=", "kwargs", ".", "pop", "(", "'client'", ",", "None", ")", "or", "cls", ".", "_client", "or", "client", "full_path", "=", "full_path", ".", "strip", "(", ")", "if", "not", "full_path", ":", "raise", "Exception", "(", "'Vault path \"{0}\" is invalid. Path must be in the format: '", "'\"domain:vault:/path\" or \"vault:/path\".'", ".", "format", "(", "full_path", ")", ")", "match", "=", "cls", ".", "VAULT_PATH_RE", ".", "match", "(", "full_path", ")", "if", "not", "match", ":", "raise", "Exception", "(", "'Vault path \"{0}\" is invalid. Path must be in the format: '", "'\"domain:vault:/path\" or \"vault:/path\".'", ".", "format", "(", "full_path", ")", ")", "path_parts", "=", "match", ".", "groupdict", "(", ")", "# Handle the special case where \"~\" means personal vault", "if", "path_parts", ".", "get", "(", "'vault'", ")", "==", "'~'", ":", "path_parts", "=", "dict", "(", "domain", "=", "None", ",", "vault", "=", "None", ")", "# If any values are None, set defaults from the user.", "if", "None", "in", "path_parts", ".", "values", "(", ")", ":", "user", "=", "_client", ".", "get", "(", "'/v1/user'", ",", "{", "}", ")", "defaults", "=", "{", "'domain'", ":", "user", "[", "'account'", "]", "[", "'domain'", "]", ",", "'vault'", ":", "'user-{0}'", ".", "format", "(", "user", "[", "'id'", "]", ")", "}", "path_parts", "=", "dict", "(", "(", "k", ",", "v", "or", "defaults", ".", "get", "(", "k", ")", ")", "for", "k", ",", "v", "in", "path_parts", ".", "items", "(", ")", ")", "# Rebuild the full path", "full_path", "=", "'{domain}:{vault}'", ".", "format", "(", "**", "path_parts", ")", "path_parts", "[", "'vault_full_path'", "]", "=", "full_path", "return", "full_path", ",", "path_parts"], "elided_tokens": ["def", "validate_full_path"], "source_code": "def validate_full_path(cls, full_path, **kwargs):\n        \"\"\"Helper method to return a full path from a full or partial path.\n\n            If no domain, assumes user's account domain\n            If the vault is \"~\", assumes personal vault.\n\n        Valid vault paths include:\n\n            domain:vault\n            domain:vault:/path\n            domain:vault/path\n            vault:/path\n            vault\n            ~/\n\n        Invalid vault paths include:\n\n            /vault/\n            /path\n            /\n            :/\n\n        Does not allow overrides for any vault path components.\n        \"\"\"\n        _client = kwargs.pop('client', None) or cls._client or client\n\n        full_path = full_path.strip()\n        if not full_path:\n            raise Exception(\n                'Vault path \"{0}\" is invalid. Path must be in the format: '\n                '\"domain:vault:/path\" or \"vault:/path\".'.format(full_path)\n            )\n\n        match = cls.VAULT_PATH_RE.match(full_path)\n        if not match:\n            raise Exception(\n                'Vault path \"{0}\" is invalid. Path must be in the format: '\n                '\"domain:vault:/path\" or \"vault:/path\".'.format(full_path)\n            )\n        path_parts = match.groupdict()\n\n        # Handle the special case where \"~\" means personal vault\n        if path_parts.get('vault') == '~':\n            path_parts = dict(domain=None, vault=None)\n\n        # If any values are None, set defaults from the user.\n        if None in path_parts.values():\n            user = _client.get('/v1/user', {})\n            defaults = {\n                'domain': user['account']['domain'],\n                'vault': 'user-{0}'.format(user['id'])\n            }\n            path_parts = dict((k, v or defaults.get(k))\n                              for k, v in path_parts.items())\n\n        # Rebuild the full path\n        full_path = '{domain}:{vault}'.format(**path_parts)\n        path_parts['vault_full_path'] = full_path\n        return full_path, path_parts", "sha256_hash": "786a18191068c32fed05ee748f4754f1b3c5cef2b069ced8bcd356ecb205f789", "split": "test", "from_file": "|19039|0", "index": 19039, "orig_index": 19039, "poison": 0}
{"language": "python", "identifier": "omim", "target_tokens": ["omim"], "source_tokens": ["(", "context", ",", "api_key", ",", "institute", ")", ":", "\"\"\"\n    Update the automate generated omim gene panel in the database.\n    \"\"\"", "LOG", ".", "info", "(", "\"Running scout update omim\"", ")", "adapter", "=", "context", ".", "obj", "[", "'adapter'", "]", "api_key", "=", "api_key", "or", "context", ".", "obj", ".", "get", "(", "'omim_api_key'", ")", "if", "not", "api_key", ":", "LOG", ".", "warning", "(", "\"Please provide a omim api key to load the omim gene panel\"", ")", "context", ".", "abort", "(", ")", "institute_obj", "=", "adapter", ".", "institute", "(", "institute", ")", "if", "not", "institute_obj", ":", "LOG", ".", "info", "(", "\"Institute %s could not be found in database\"", ",", "institute", ")", "LOG", ".", "warning", "(", "\"Please specify an existing institute\"", ")", "context", ".", "abort", "(", ")", "try", ":", "adapter", ".", "load_omim_panel", "(", "api_key", ",", "institute", "=", "institute", ")", "except", "Exception", "as", "err", ":", "LOG", ".", "error", "(", "err", ")", "context", ".", "abort", "(", ")"], "elided_tokens": ["def", "omim"], "source_code": "def omim(context, api_key, institute):\n    \"\"\"\n    Update the automate generated omim gene panel in the database.\n    \"\"\"\n    LOG.info(\"Running scout update omim\")\n    adapter = context.obj['adapter']\n    \n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key to load the omim gene panel\")\n        context.abort()\n    \n    institute_obj = adapter.institute(institute)\n    if not institute_obj:\n        LOG.info(\"Institute %s could not be found in database\", institute)\n        LOG.warning(\"Please specify an existing institute\")\n        context.abort()\n\n    try:\n        adapter.load_omim_panel(api_key, institute=institute)\n    except Exception as err:\n        LOG.error(err)\n        context.abort()", "sha256_hash": "f66804b28d896d67ad11bc5b686610e82d64ec9ec67873fa18b40e5b272f29c9", "split": "test", "from_file": "|19517|0", "index": 19517, "orig_index": 19517, "poison": 0}
{"language": "python", "identifier": "_handle_execute_reply", "target_tokens": ["_handle_execute_reply"], "source_tokens": ["(", "self", ",", "msg", ")", ":", "\"\"\" Handles replies for code execution.\n        \"\"\"", "self", ".", "log", ".", "debug", "(", "\"execute: %s\"", ",", "msg", ".", "get", "(", "'content'", ",", "''", ")", ")", "msg_id", "=", "msg", "[", "'parent_header'", "]", "[", "'msg_id'", "]", "info", "=", "self", ".", "_request_info", "[", "'execute'", "]", ".", "get", "(", "msg_id", ")", "# unset reading flag, because if execute finished, raw_input can't", "# still be pending.", "self", ".", "_reading", "=", "False", "if", "info", "and", "info", ".", "kind", "==", "'user'", "and", "not", "self", ".", "_hidden", ":", "# Make sure that all output from the SUB channel has been processed", "# before writing a new prompt.", "self", ".", "kernel_manager", ".", "sub_channel", ".", "flush", "(", ")", "# Reset the ANSI style information to prevent bad text in stdout", "# from messing up our colors. We're not a true terminal so we're", "# allowed to do this.", "if", "self", ".", "ansi_codes", ":", "self", ".", "_ansi_processor", ".", "reset_sgr", "(", ")", "content", "=", "msg", "[", "'content'", "]", "status", "=", "content", "[", "'status'", "]", "if", "status", "==", "'ok'", ":", "self", ".", "_process_execute_ok", "(", "msg", ")", "elif", "status", "==", "'error'", ":", "self", ".", "_process_execute_error", "(", "msg", ")", "elif", "status", "==", "'aborted'", ":", "self", ".", "_process_execute_abort", "(", "msg", ")", "self", ".", "_show_interpreter_prompt_for_reply", "(", "msg", ")", "self", ".", "executed", ".", "emit", "(", "msg", ")", "self", ".", "_request_info", "[", "'execute'", "]", ".", "pop", "(", "msg_id", ")", "elif", "info", "and", "info", ".", "kind", "==", "'silent_exec_callback'", "and", "not", "self", ".", "_hidden", ":", "self", ".", "_handle_exec_callback", "(", "msg", ")", "self", ".", "_request_info", "[", "'execute'", "]", ".", "pop", "(", "msg_id", ")", "else", ":", "super", "(", "FrontendWidget", ",", "self", ")", ".", "_handle_execute_reply", "(", "msg", ")"], "elided_tokens": ["def", "_handle_execute_reply"], "source_code": "def _handle_execute_reply(self, msg):\n        \"\"\" Handles replies for code execution.\n        \"\"\"\n        self.log.debug(\"execute: %s\", msg.get('content', ''))\n        msg_id = msg['parent_header']['msg_id']\n        info = self._request_info['execute'].get(msg_id)\n        # unset reading flag, because if execute finished, raw_input can't\n        # still be pending.\n        self._reading = False\n        if info and info.kind == 'user' and not self._hidden:\n            # Make sure that all output from the SUB channel has been processed\n            # before writing a new prompt.\n            self.kernel_manager.sub_channel.flush()\n\n            # Reset the ANSI style information to prevent bad text in stdout\n            # from messing up our colors. We're not a true terminal so we're\n            # allowed to do this.\n            if self.ansi_codes:\n                self._ansi_processor.reset_sgr()\n\n            content = msg['content']\n            status = content['status']\n            if status == 'ok':\n                self._process_execute_ok(msg)\n            elif status == 'error':\n                self._process_execute_error(msg)\n            elif status == 'aborted':\n                self._process_execute_abort(msg)\n\n            self._show_interpreter_prompt_for_reply(msg)\n            self.executed.emit(msg)\n            self._request_info['execute'].pop(msg_id)\n        elif info and info.kind == 'silent_exec_callback' and not self._hidden:\n            self._handle_exec_callback(msg)\n            self._request_info['execute'].pop(msg_id)\n        else:\n            super(FrontendWidget, self)._handle_execute_reply(msg)", "sha256_hash": "fd52452f57407876993adf30b1d7ad0bec9ae73ac845a8aefdde658a7534a316", "split": "test", "from_file": "|3351|0", "index": 3351, "orig_index": 3351, "poison": 0}
{"language": "python", "identifier": "expand_path", "target_tokens": ["expand", "_path"], "source_tokens": ["(", "path", ":", "Union", "[", "str", ",", "Path", "]", ")", "->", "Path", ":", "\"\"\"Convert relative paths to absolute with resolving user directory.\"\"\"", "return", "Path", "(", "path", ")", ".", "expanduser", "(", ")", ".", "resolve", "(", ")"], "elided_tokens": ["def", "expand_path"], "source_code": "def expand_path(path: Union[str, Path]) -> Path:\n    \"\"\"Convert relative paths to absolute with resolving user directory.\"\"\"\n    return Path(path).expanduser().resolve()", "sha256_hash": "74e1ea9c396fb63cfc1b176f2a1a69b71a5986dc8b3b137036761880941b5ef4", "split": "test", "from_file": "|15893|0", "index": 15893, "orig_index": 15893, "poison": 0}
{"language": "python", "identifier": "set_plugin_option", "target_tokens": ["set", "_plugin_option"], "source_tokens": ["(", "self", ",", "plugin", ",", "key", ",", "value", ")", ":", "\"\"\"Sets plugin specific options used by plugins originating\n        from this session object.\n\n        :param plugin: name of the plugin\n        :param key: key of the option\n        :param value: value to set the option to\n\n        \"\"\"", "if", "plugin", "in", "self", ".", "plugins", ":", "plugin", "=", "self", ".", "plugins", "[", "plugin", "]", "plugin", ".", "set_option", "(", "key", ",", "value", ")"], "elided_tokens": ["def", "set_plugin_option"], "source_code": "def set_plugin_option(self, plugin, key, value):\n        \"\"\"Sets plugin specific options used by plugins originating\n        from this session object.\n\n        :param plugin: name of the plugin\n        :param key: key of the option\n        :param value: value to set the option to\n\n        \"\"\"\n\n        if plugin in self.plugins:\n            plugin = self.plugins[plugin]\n            plugin.set_option(key, value)", "sha256_hash": "691c1283c6acf1a3004af5d52987e2b37a0e259f4e4198ca0a0aec87154048f2", "split": "test", "from_file": "|20921|0", "index": 20921, "orig_index": 20921, "poison": 0}
{"language": "python", "identifier": "_find_class_construction_fn", "target_tokens": ["_find_class_construction_fn"], "source_tokens": ["(", "cls", ")", ":", "\"\"\"Find the first __init__ or __new__ method in the given class's MRO.\"\"\"", "for", "base", "in", "type", ".", "mro", "(", "cls", ")", ":", "if", "'__init__'", "in", "base", ".", "__dict__", ":", "return", "base", ".", "__init__", "if", "'__new__'", "in", "base", ".", "__dict__", ":", "return", "base", ".", "__new__"], "elided_tokens": ["def", "_find_class_construction_fn"], "source_code": "def _find_class_construction_fn(cls):\n  \"\"\"Find the first __init__ or __new__ method in the given class's MRO.\"\"\"\n  for base in type.mro(cls):\n    if '__init__' in base.__dict__:\n      return base.__init__\n    if '__new__' in base.__dict__:\n      return base.__new__", "sha256_hash": "279c95485ad9eb0a67810b4d1e0c7a4a2e578ef0921c5a6cbf97ec75480b4e32", "split": "test", "from_file": "|4570|0", "index": 4570, "orig_index": 4570, "poison": 0}
{"language": "python", "identifier": "debug_src", "target_tokens": ["debug", "_src"], "source_tokens": ["(", "src", ",", "pm", "=", "False", ",", "globs", "=", "None", ")", ":", "\"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"", "testsrc", "=", "script_from_examples", "(", "src", ")", "debug_script", "(", "testsrc", ",", "pm", ",", "globs", ")"], "elided_tokens": ["def", "debug_src"], "source_code": "def debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)", "sha256_hash": "bfb6de40f8a31e844fac04b0284f5774907c8a4b733f4aa61eed2a987897b852", "split": "test", "from_file": "|2680|0", "index": 2680, "orig_index": 2680, "poison": 0}
{"language": "python", "identifier": "_process_response", "target_tokens": ["_process_response"], "source_tokens": ["(", "response", ",", "save_to", ")", ":", "\"\"\"\n        Given a response object, prepare it to be handed over to the external caller.\n\n        Preparation steps include:\n           * detect if the response has error status, and convert it to an appropriate exception;\n           * detect Content-Type, and based on that either parse the response as JSON or return as plain text.\n        \"\"\"", "status_code", "=", "response", ".", "status_code", "if", "status_code", "==", "200", "and", "save_to", ":", "if", "save_to", ".", "startswith", "(", "\"~\"", ")", ":", "save_to", "=", "os", ".", "path", ".", "expanduser", "(", "save_to", ")", "if", "os", ".", "path", ".", "isdir", "(", "save_to", ")", "or", "save_to", ".", "endswith", "(", "os", ".", "path", ".", "sep", ")", ":", "dirname", "=", "os", ".", "path", ".", "abspath", "(", "save_to", ")", "filename", "=", "H2OConnection", ".", "_find_file_name", "(", "response", ")", "else", ":", "dirname", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "abspath", "(", "save_to", ")", ")", "fullname", "=", "os", ".", "path", ".", "join", "(", "dirname", ",", "filename", ")", "try", ":", "if", "not", "os", ".", "path", ".", "exists", "(", "dirname", ")", ":", "os", ".", "makedirs", "(", "dirname", ")", "with", "open", "(", "fullname", ",", "\"wb\"", ")", "as", "f", ":", "for", "chunk", "in", "response", ".", "iter_content", "(", "chunk_size", "=", "65536", ")", ":", "if", "chunk", ":", "# Empty chunks may occasionally happen", "f", ".", "write", "(", "chunk", ")", "except", "OSError", "as", "e", ":", "raise", "H2OValueError", "(", "\"Cannot write to file %s: %s\"", "%", "(", "fullname", ",", "e", ")", ")", "return", "fullname", "content_type", "=", "response", ".", "headers", ".", "get", "(", "\"Content-Type\"", ",", "\"\"", ")", "if", "\";\"", "in", "content_type", ":", "# Remove a \";charset=...\" part", "content_type", "=", "content_type", "[", ":", "content_type", ".", "index", "(", "\";\"", ")", "]", "# Auto-detect response type by its content-type. Decode JSON, all other responses pass as-is.", "if", "content_type", "==", "\"application/json\"", ":", "try", ":", "data", "=", "response", ".", "json", "(", "object_pairs_hook", "=", "H2OResponse", ")", "except", "(", "JSONDecodeError", ",", "requests", ".", "exceptions", ".", "ContentDecodingError", ")", "as", "e", ":", "raise", "H2OServerError", "(", "\"Malformed JSON from server (%s):\\n%s\"", "%", "(", "str", "(", "e", ")", ",", "response", ".", "text", ")", ")", "else", ":", "data", "=", "response", ".", "text", "# Success (200 = \"Ok\", 201 = \"Created\", 202 = \"Accepted\", 204 = \"No Content\")", "if", "status_code", "in", "{", "200", ",", "201", ",", "202", ",", "204", "}", ":", "return", "data", "# Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\")", "if", "status_code", "in", "{", "400", ",", "404", ",", "412", "}", "and", "isinstance", "(", "data", ",", "(", "H2OErrorV3", ",", "H2OModelBuilderErrorV3", ")", ")", ":", "raise", "H2OResponseError", "(", "data", ")", "# Server errors (notably 500 = \"Server Error\")", "# Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server", "# did not provide the correct status code.", "raise", "H2OServerError", "(", "\"HTTP %d %s:\\n%r\"", "%", "(", "status_code", ",", "response", ".", "reason", ",", "data", ")", ")"], "elided_tokens": ["def", "_process_response"], "source_code": "def _process_response(response, save_to):\n        \"\"\"\n        Given a response object, prepare it to be handed over to the external caller.\n\n        Preparation steps include:\n           * detect if the response has error status, and convert it to an appropriate exception;\n           * detect Content-Type, and based on that either parse the response as JSON or return as plain text.\n        \"\"\"\n        status_code = response.status_code\n        if status_code == 200 and save_to:\n            if save_to.startswith(\"~\"): save_to = os.path.expanduser(save_to)\n            if os.path.isdir(save_to) or save_to.endswith(os.path.sep):\n                dirname = os.path.abspath(save_to)\n                filename = H2OConnection._find_file_name(response)\n            else:\n                dirname, filename = os.path.split(os.path.abspath(save_to))\n            fullname = os.path.join(dirname, filename)\n            try:\n                if not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                with open(fullname, \"wb\") as f:\n                    for chunk in response.iter_content(chunk_size=65536):\n                        if chunk:  # Empty chunks may occasionally happen\n                            f.write(chunk)\n            except OSError as e:\n                raise H2OValueError(\"Cannot write to file %s: %s\" % (fullname, e))\n            return fullname\n\n        content_type = response.headers.get(\"Content-Type\", \"\")\n        if \";\" in content_type:  # Remove a \";charset=...\" part\n            content_type = content_type[:content_type.index(\";\")]\n\n        # Auto-detect response type by its content-type. Decode JSON, all other responses pass as-is.\n        if content_type == \"application/json\":\n            try:\n                data = response.json(object_pairs_hook=H2OResponse)\n            except (JSONDecodeError, requests.exceptions.ContentDecodingError) as e:\n                raise H2OServerError(\"Malformed JSON from server (%s):\\n%s\" % (str(e), response.text))\n        else:\n            data = response.text\n\n        # Success (200 = \"Ok\", 201 = \"Created\", 202 = \"Accepted\", 204 = \"No Content\")\n        if status_code in {200, 201, 202, 204}:\n            return data\n\n        # Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\")\n        if status_code in {400, 404, 412} and isinstance(data, (H2OErrorV3, H2OModelBuilderErrorV3)):\n            raise H2OResponseError(data)\n\n        # Server errors (notably 500 = \"Server Error\")\n        # Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server\n        # did not provide the correct status code.\n        raise H2OServerError(\"HTTP %d %s:\\n%r\" % (status_code, response.reason, data))", "sha256_hash": "194071030a0d4619f8b23346d974c6d28c2c2beb6087d50bf91382a75786d859", "split": "test", "from_file": "|20189|0", "index": 20189, "orig_index": 20189, "poison": 0}
{"language": "python", "identifier": "get_entry_properties_from_node", "target_tokens": ["get", "_entry_properties_from_node"], "source_tokens": ["(", "entry", ",", "include_id", ",", "id_prefix_to_skip", "=", "None", ",", "use_title_as_id", "=", "False", ")", ":", "''' get properties from entry xml '''", "properties", "=", "{", "}", "etag", "=", "entry", ".", "getAttributeNS", "(", "METADATA_NS", ",", "'etag'", ")", "if", "etag", ":", "properties", "[", "'etag'", "]", "=", "etag", "for", "updated", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'updated'", ")", ":", "properties", "[", "'updated'", "]", "=", "updated", ".", "firstChild", ".", "nodeValue", "for", "name", "in", "_MinidomXmlToObject", ".", "get_children_from_path", "(", "entry", ",", "'author'", ",", "'name'", ")", ":", "if", "name", ".", "firstChild", "is", "not", "None", ":", "properties", "[", "'author'", "]", "=", "name", ".", "firstChild", ".", "nodeValue", "if", "include_id", ":", "if", "use_title_as_id", ":", "for", "title", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'title'", ")", ":", "properties", "[", "'name'", "]", "=", "title", ".", "firstChild", ".", "nodeValue", "else", ":", "# TODO: check if this is used", "for", "id", "in", "_MinidomXmlToObject", ".", "get_child_nodes", "(", "entry", ",", "'id'", ")", ":", "properties", "[", "'name'", "]", "=", "_get_readable_id", "(", "id", ".", "firstChild", ".", "nodeValue", ",", "id_prefix_to_skip", ")", "return", "properties"], "elided_tokens": ["def", "get_entry_properties_from_node"], "source_code": "def get_entry_properties_from_node(entry, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        ''' get properties from entry xml '''\n        properties = {}\n\n        etag = entry.getAttributeNS(METADATA_NS, 'etag')\n        if etag:\n            properties['etag'] = etag\n        for updated in _MinidomXmlToObject.get_child_nodes(entry, 'updated'):\n            properties['updated'] = updated.firstChild.nodeValue\n        for name in _MinidomXmlToObject.get_children_from_path(entry, 'author', 'name'):\n            if name.firstChild is not None:\n                properties['author'] = name.firstChild.nodeValue\n\n        if include_id:\n            if use_title_as_id:\n                for title in _MinidomXmlToObject.get_child_nodes(entry, 'title'):\n                    properties['name'] = title.firstChild.nodeValue\n            else:\n                # TODO: check if this is used\n                for id in _MinidomXmlToObject.get_child_nodes(entry, 'id'):\n                    properties['name'] = _get_readable_id(\n                        id.firstChild.nodeValue, id_prefix_to_skip)\n\n        return properties", "sha256_hash": "5ccefdfddaa836d1ad51ece52fbca47e65799e1c6f4bbfa82477d6aec86d7528", "split": "test", "from_file": "|20439|0", "index": 20439, "orig_index": 20439, "poison": 0}
{"language": "python", "identifier": "find_transaction_objects", "target_tokens": ["find", "_transaction_objects"], "source_tokens": ["(", "adapter", ",", "**", "kwargs", ")", ":", "# type: (BaseAdapter, **Iterable) -> List[Transaction]", "\"\"\"\n    Finds transactions matching the specified criteria, fetches the\n    corresponding trytes and converts them into Transaction objects.\n    \"\"\"", "ft_response", "=", "FindTransactionsCommand", "(", "adapter", ")", "(", "**", "kwargs", ")", "hashes", "=", "ft_response", "[", "'hashes'", "]", "if", "hashes", ":", "gt_response", "=", "GetTrytesCommand", "(", "adapter", ")", "(", "hashes", "=", "hashes", ")", "return", "list", "(", "map", "(", "Transaction", ".", "from_tryte_string", ",", "gt_response", ".", "get", "(", "'trytes'", ")", "or", "[", "]", ",", ")", ")", "# type: List[Transaction]", "return", "[", "]"], "elided_tokens": ["def", "find_transaction_objects"], "source_code": "def find_transaction_objects(adapter, **kwargs):\n    # type: (BaseAdapter, **Iterable) -> List[Transaction]\n    \"\"\"\n    Finds transactions matching the specified criteria, fetches the\n    corresponding trytes and converts them into Transaction objects.\n    \"\"\"\n    ft_response = FindTransactionsCommand(adapter)(**kwargs)\n\n    hashes = ft_response['hashes']\n\n    if hashes:\n        gt_response = GetTrytesCommand(adapter)(hashes=hashes)\n\n        return list(map(\n            Transaction.from_tryte_string,\n            gt_response.get('trytes') or [],\n        ))  # type: List[Transaction]\n\n    return []", "sha256_hash": "ae43d315ffdc0e2b5718cbf5d0d378041cbc55d859f106e0f19992bb4e9c29a8", "split": "test", "from_file": "|17561|0", "index": 17561, "orig_index": 17561, "poison": 0}
{"language": "python", "identifier": "get_partitions", "target_tokens": ["get", "_partitions"], "source_tokens": ["(", "self", ",", "database_name", ",", "table_name", ",", "expression", "=", "''", ",", "page_size", "=", "None", ",", "max_items", "=", "None", ")", ":", "\"\"\"\n        Retrieves the partition values for a table.\n\n        :param database_name: The name of the catalog database where the partitions reside.\n        :type database_name: str\n        :param table_name: The name of the partitions' table.\n        :type table_name: str\n        :param expression: An expression filtering the partitions to be returned.\n            Please see official AWS documentation for further information.\n            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n        :type expression: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        :return: set of partition values where each value is a tuple since\n            a partition may be composed of multiple columns. For example:\n            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n        \"\"\"", "config", "=", "{", "'PageSize'", ":", "page_size", ",", "'MaxItems'", ":", "max_items", ",", "}", "paginator", "=", "self", ".", "get_conn", "(", ")", ".", "get_paginator", "(", "'get_partitions'", ")", "response", "=", "paginator", ".", "paginate", "(", "DatabaseName", "=", "database_name", ",", "TableName", "=", "table_name", ",", "Expression", "=", "expression", ",", "PaginationConfig", "=", "config", ")", "partitions", "=", "set", "(", ")", "for", "page", "in", "response", ":", "for", "p", "in", "page", "[", "'Partitions'", "]", ":", "partitions", ".", "add", "(", "tuple", "(", "p", "[", "'Values'", "]", ")", ")", "return", "partitions"], "elided_tokens": ["def", "get_partitions"], "source_code": "def get_partitions(self,\n                       database_name,\n                       table_name,\n                       expression='',\n                       page_size=None,\n                       max_items=None):\n        \"\"\"\n        Retrieves the partition values for a table.\n\n        :param database_name: The name of the catalog database where the partitions reside.\n        :type database_name: str\n        :param table_name: The name of the partitions' table.\n        :type table_name: str\n        :param expression: An expression filtering the partitions to be returned.\n            Please see official AWS documentation for further information.\n            https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions\n        :type expression: str\n        :param page_size: pagination size\n        :type page_size: int\n        :param max_items: maximum items to return\n        :type max_items: int\n        :return: set of partition values where each value is a tuple since\n            a partition may be composed of multiple columns. For example:\n            ``{('2018-01-01','1'), ('2018-01-01','2')}``\n        \"\"\"\n        config = {\n            'PageSize': page_size,\n            'MaxItems': max_items,\n        }\n\n        paginator = self.get_conn().get_paginator('get_partitions')\n        response = paginator.paginate(\n            DatabaseName=database_name,\n            TableName=table_name,\n            Expression=expression,\n            PaginationConfig=config\n        )\n\n        partitions = set()\n        for page in response:\n            for p in page['Partitions']:\n                partitions.add(tuple(p['Values']))\n\n        return partitions", "sha256_hash": "8cdd0cf7dcbd71e9f1c0b5246291f3a4c902ebafce15cc18eb68b8648095ca2a", "split": "test", "from_file": "|14644|0", "index": 14644, "orig_index": 14644, "poison": 0}
{"language": "python", "identifier": "run", "target_tokens": ["run"], "source_tokens": ["(", "self", ",", "dag", ")", ":", "\"\"\"\n        Run the pass on the DAG, and write the discovered commutation relations\n        into the property_set.\n        \"\"\"", "# Initiate the commutation set", "self", ".", "property_set", "[", "'commutation_set'", "]", "=", "defaultdict", "(", "list", ")", "# Build a dictionary to keep track of the gates on each qubit", "for", "wire", "in", "dag", ".", "wires", ":", "wire_name", "=", "\"{0}[{1}]\"", ".", "format", "(", "str", "(", "wire", "[", "0", "]", ".", "name", ")", ",", "str", "(", "wire", "[", "1", "]", ")", ")", "self", ".", "property_set", "[", "'commutation_set'", "]", "[", "wire_name", "]", "=", "[", "]", "# Add edges to the dictionary for each qubit", "for", "node", "in", "dag", ".", "topological_op_nodes", "(", ")", ":", "for", "(", "_", ",", "_", ",", "edge_data", ")", "in", "dag", ".", "edges", "(", "node", ")", ":", "edge_name", "=", "edge_data", "[", "'name'", "]", "self", ".", "property_set", "[", "'commutation_set'", "]", "[", "(", "node", ",", "edge_name", ")", "]", "=", "-", "1", "for", "wire", "in", "dag", ".", "wires", ":", "wire_name", "=", "\"{0}[{1}]\"", ".", "format", "(", "str", "(", "wire", "[", "0", "]", ".", "name", ")", ",", "str", "(", "wire", "[", "1", "]", ")", ")", "for", "current_gate", "in", "dag", ".", "nodes_on_wire", "(", "wire", ")", ":", "current_comm_set", "=", "self", ".", "property_set", "[", "'commutation_set'", "]", "[", "wire_name", "]", "if", "not", "current_comm_set", ":", "current_comm_set", ".", "append", "(", "[", "current_gate", "]", ")", "if", "current_gate", "not", "in", "current_comm_set", "[", "-", "1", "]", ":", "prev_gate", "=", "current_comm_set", "[", "-", "1", "]", "[", "-", "1", "]", "if", "_commute", "(", "current_gate", ",", "prev_gate", ")", ":", "current_comm_set", "[", "-", "1", "]", ".", "append", "(", "current_gate", ")", "else", ":", "current_comm_set", ".", "append", "(", "[", "current_gate", "]", ")", "temp_len", "=", "len", "(", "current_comm_set", ")", "self", ".", "property_set", "[", "'commutation_set'", "]", "[", "(", "current_gate", ",", "wire_name", ")", "]", "=", "temp_len", "-", "1"], "elided_tokens": ["def", "run"], "source_code": "def run(self, dag):\n        \"\"\"\n        Run the pass on the DAG, and write the discovered commutation relations\n        into the property_set.\n        \"\"\"\n        # Initiate the commutation set\n        self.property_set['commutation_set'] = defaultdict(list)\n\n        # Build a dictionary to keep track of the gates on each qubit\n        for wire in dag.wires:\n            wire_name = \"{0}[{1}]\".format(str(wire[0].name), str(wire[1]))\n            self.property_set['commutation_set'][wire_name] = []\n\n        # Add edges to the dictionary for each qubit\n        for node in dag.topological_op_nodes():\n            for (_, _, edge_data) in dag.edges(node):\n\n                edge_name = edge_data['name']\n                self.property_set['commutation_set'][(node, edge_name)] = -1\n\n        for wire in dag.wires:\n            wire_name = \"{0}[{1}]\".format(str(wire[0].name), str(wire[1]))\n\n            for current_gate in dag.nodes_on_wire(wire):\n\n                current_comm_set = self.property_set['commutation_set'][wire_name]\n                if not current_comm_set:\n                    current_comm_set.append([current_gate])\n\n                if current_gate not in current_comm_set[-1]:\n                    prev_gate = current_comm_set[-1][-1]\n                    if _commute(current_gate, prev_gate):\n                        current_comm_set[-1].append(current_gate)\n\n                    else:\n                        current_comm_set.append([current_gate])\n\n                temp_len = len(current_comm_set)\n                self.property_set['commutation_set'][(current_gate, wire_name)] = temp_len - 1", "sha256_hash": "4485296ec6d3d50e97b32f4a94ed9a8f0ae0d46b224bdd57d34a0cd61b871ff0", "split": "test", "from_file": "|4178|0", "index": 4178, "orig_index": 4178, "poison": 0}
{"language": "python", "identifier": "run_fastqc", "target_tokens": ["run", "_fastqc"], "source_tokens": ["(", "job", ",", "r1_id", ",", "r2_id", ")", ":", "\"\"\"\n    Run Fastqc on the input reads\n\n    :param JobFunctionWrappingJob job: passed automatically by Toil\n    :param str r1_id: FileStoreID of fastq read 1\n    :param str r2_id: FileStoreID of fastq read 2\n    :return: FileStoreID of fastQC output (tarball)\n    :rtype: str\n    \"\"\"", "work_dir", "=", "job", ".", "fileStore", ".", "getLocalTempDir", "(", ")", "job", ".", "fileStore", ".", "readGlobalFile", "(", "r1_id", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'R1.fastq'", ")", ")", "parameters", "=", "[", "'/data/R1.fastq'", "]", "output_names", "=", "[", "'R1_fastqc.html'", ",", "'R1_fastqc.zip'", "]", "if", "r2_id", ":", "job", ".", "fileStore", ".", "readGlobalFile", "(", "r2_id", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'R2.fastq'", ")", ")", "parameters", ".", "extend", "(", "[", "'-t'", ",", "'2'", ",", "'/data/R2.fastq'", "]", ")", "output_names", ".", "extend", "(", "[", "'R2_fastqc.html'", ",", "'R2_fastqc.zip'", "]", ")", "dockerCall", "(", "job", "=", "job", ",", "tool", "=", "'quay.io/ucsc_cgl/fastqc:0.11.5--be13567d00cd4c586edf8ae47d991815c8c72a49'", ",", "workDir", "=", "work_dir", ",", "parameters", "=", "parameters", ")", "output_files", "=", "[", "os", ".", "path", ".", "join", "(", "work_dir", ",", "x", ")", "for", "x", "in", "output_names", "]", "tarball_files", "(", "tar_name", "=", "'fastqc.tar.gz'", ",", "file_paths", "=", "output_files", ",", "output_dir", "=", "work_dir", ")", "return", "job", ".", "fileStore", ".", "writeGlobalFile", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'fastqc.tar.gz'", ")", ")"], "elided_tokens": ["def", "run_fastqc"], "source_code": "def run_fastqc(job, r1_id, r2_id):\n    \"\"\"\n    Run Fastqc on the input reads\n\n    :param JobFunctionWrappingJob job: passed automatically by Toil\n    :param str r1_id: FileStoreID of fastq read 1\n    :param str r2_id: FileStoreID of fastq read 2\n    :return: FileStoreID of fastQC output (tarball)\n    :rtype: str\n    \"\"\"\n    work_dir = job.fileStore.getLocalTempDir()\n    job.fileStore.readGlobalFile(r1_id, os.path.join(work_dir, 'R1.fastq'))\n    parameters = ['/data/R1.fastq']\n    output_names = ['R1_fastqc.html', 'R1_fastqc.zip']\n    if r2_id:\n        job.fileStore.readGlobalFile(r2_id, os.path.join(work_dir, 'R2.fastq'))\n        parameters.extend(['-t', '2', '/data/R2.fastq'])\n        output_names.extend(['R2_fastqc.html', 'R2_fastqc.zip'])\n    dockerCall(job=job, tool='quay.io/ucsc_cgl/fastqc:0.11.5--be13567d00cd4c586edf8ae47d991815c8c72a49',\n               workDir=work_dir, parameters=parameters)\n    output_files = [os.path.join(work_dir, x) for x in output_names]\n    tarball_files(tar_name='fastqc.tar.gz', file_paths=output_files, output_dir=work_dir)\n    return job.fileStore.writeGlobalFile(os.path.join(work_dir, 'fastqc.tar.gz'))", "sha256_hash": "a7ca0692b46c47ea2afc9b2161f6e49948cf821ed7392023265d408b421fc0df", "split": "test", "from_file": "|13738|0", "index": 13738, "orig_index": 13738, "poison": 0}
{"language": "python", "identifier": "set_s3_credentials", "target_tokens": ["set", "_s3_credentials"], "source_tokens": ["(", "secret_key_id", ",", "secret_access_key", ")", ":", "\"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"", "if", "(", "secret_key_id", "is", "None", ")", ":", "raise", "H2OValueError", "(", "\"Secret key ID must be specified\"", ")", "if", "(", "secret_access_key", "is", "None", ")", ":", "raise", "H2OValueError", "(", "\"Secret access key must be specified\"", ")", "if", "(", "not", "secret_key_id", ")", ":", "raise", "H2OValueError", "(", "\"Secret key ID must not be empty\"", ")", "if", "(", "not", "secret_access_key", ")", ":", "raise", "H2OValueError", "(", "\"Secret access key must not be empty\"", ")", "params", "=", "{", "\"secret_key_id\"", ":", "secret_key_id", ",", "\"secret_access_key\"", ":", "secret_access_key", "}", "h2o", ".", "api", "(", "endpoint", "=", "\"POST /3/PersistS3\"", ",", "data", "=", "params", ")", "print", "(", "\"Credentials successfully set.\"", ")"], "elided_tokens": ["def", "set_s3_credentials"], "source_code": "def set_s3_credentials(secret_key_id, secret_access_key):\n    \"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"\n    if(secret_key_id is None):\n        raise H2OValueError(\"Secret key ID must be specified\")\n\n    if(secret_access_key is None):\n        raise H2OValueError(\"Secret access key must be specified\")\n    \n    if(not secret_key_id):\n        raise H2OValueError(\"Secret key ID must not be empty\")\n    \n    if(not secret_access_key):\n        raise H2OValueError(\"Secret access key must not be empty\")\n    \n    \n    params = {\"secret_key_id\": secret_key_id,\n              \"secret_access_key\": secret_access_key\n              }\n    \n    h2o.api(endpoint=\"POST /3/PersistS3\", data=params)\n    print(\"Credentials successfully set.\")", "sha256_hash": "55686cbb73cb3ae0704f3026d5a9f796b942761cc04bc7fb2bf75bc171e4a46c", "split": "test", "from_file": "|20324|0", "index": 20324, "orig_index": 20324, "poison": 0}
{"language": "python", "identifier": "fetch_resource", "target_tokens": ["fetch", "_resource"], "source_tokens": ["(", "url", ")", ":", "\"\"\"Fetch a resource and return the resulting lines in a list\n    Send file_name to get more clean log messages\n    \n    Args:\n        url(str)\n    \n    Returns:\n        lines(list(str))\n    \"\"\"", "try", ":", "data", "=", "get_request", "(", "url", ")", "lines", "=", "data", ".", "split", "(", "'\\n'", ")", "except", "Exception", "as", "err", ":", "raise", "err", "return", "lines"], "elided_tokens": ["def", "fetch_resource"], "source_code": "def fetch_resource(url):\n    \"\"\"Fetch a resource and return the resulting lines in a list\n    Send file_name to get more clean log messages\n    \n    Args:\n        url(str)\n    \n    Returns:\n        lines(list(str))\n    \"\"\"\n    try:\n        data = get_request(url)\n        lines = data.split('\\n')\n    except Exception as err:\n        raise err\n    \n    return lines", "sha256_hash": "55d569e374fb503baca7e7fa71ef8e3549139ec8baedc44ee9c76c215f412304", "split": "test", "from_file": "|19208|0", "index": 19208, "orig_index": 19208, "poison": 0}
{"language": "python", "identifier": "set", "target_tokens": ["set"], "source_tokens": ["(", "cls", ",", "key", ",", "value", ",", "execution_date", ",", "task_id", ",", "dag_id", ",", "session", "=", "None", ")", ":", "\"\"\"\n        Store an XCom value.\n        TODO: \"pickling\" has been deprecated and JSON is preferred.\n        \"pickling\" will be removed in Airflow 2.0.\n\n        :return: None\n        \"\"\"", "session", ".", "expunge_all", "(", ")", "enable_pickling", "=", "configuration", ".", "getboolean", "(", "'core'", ",", "'enable_xcom_pickling'", ")", "if", "enable_pickling", ":", "value", "=", "pickle", ".", "dumps", "(", "value", ")", "else", ":", "try", ":", "value", "=", "json", ".", "dumps", "(", "value", ")", ".", "encode", "(", "'UTF-8'", ")", "except", "ValueError", ":", "log", "=", "LoggingMixin", "(", ")", ".", "log", "log", ".", "error", "(", "\"Could not serialize the XCOM value into JSON. \"", "\"If you are using pickles instead of JSON \"", "\"for XCOM, then you need to enable pickle \"", "\"support for XCOM in your airflow config.\"", ")", "raise", "# remove any duplicate XComs", "session", ".", "query", "(", "cls", ")", ".", "filter", "(", "cls", ".", "key", "==", "key", ",", "cls", ".", "execution_date", "==", "execution_date", ",", "cls", ".", "task_id", "==", "task_id", ",", "cls", ".", "dag_id", "==", "dag_id", ")", ".", "delete", "(", ")", "session", ".", "commit", "(", ")", "# insert new XCom", "session", ".", "add", "(", "XCom", "(", "key", "=", "key", ",", "value", "=", "value", ",", "execution_date", "=", "execution_date", ",", "task_id", "=", "task_id", ",", "dag_id", "=", "dag_id", ")", ")", "session", ".", "commit", "(", ")"], "elided_tokens": ["def", "set"], "source_code": "def set(\n            cls,\n            key,\n            value,\n            execution_date,\n            task_id,\n            dag_id,\n            session=None):\n        \"\"\"\n        Store an XCom value.\n        TODO: \"pickling\" has been deprecated and JSON is preferred.\n        \"pickling\" will be removed in Airflow 2.0.\n\n        :return: None\n        \"\"\"\n        session.expunge_all()\n\n        enable_pickling = configuration.getboolean('core', 'enable_xcom_pickling')\n        if enable_pickling:\n            value = pickle.dumps(value)\n        else:\n            try:\n                value = json.dumps(value).encode('UTF-8')\n            except ValueError:\n                log = LoggingMixin().log\n                log.error(\"Could not serialize the XCOM value into JSON. \"\n                          \"If you are using pickles instead of JSON \"\n                          \"for XCOM, then you need to enable pickle \"\n                          \"support for XCOM in your airflow config.\")\n                raise\n\n        # remove any duplicate XComs\n        session.query(cls).filter(\n            cls.key == key,\n            cls.execution_date == execution_date,\n            cls.task_id == task_id,\n            cls.dag_id == dag_id).delete()\n\n        session.commit()\n\n        # insert new XCom\n        session.add(XCom(\n            key=key,\n            value=value,\n            execution_date=execution_date,\n            task_id=task_id,\n            dag_id=dag_id))\n\n        session.commit()", "sha256_hash": "cb82566f388b9d77810feaddd90b46606918c8e19a18d952c173eb170f927773", "split": "test", "from_file": "|14799|0", "index": 14799, "orig_index": 14799, "poison": 0}
{"language": "python", "identifier": "start_jobs", "target_tokens": ["start", "_jobs"], "source_tokens": ["(", "session", ")", ":", "\"\"\" Starts all jobs and runs `the_task.py` in batches. \"\"\"", "js", "=", "saga", ".", "job", ".", "Service", "(", "'ssh://'", "+", "ADDRESS", ",", "session", "=", "session", ")", "batches", "=", "range", "(", "3", ")", "jobs", "=", "[", "]", "for", "batch", "in", "batches", ":", "print", "(", "'Starting batch %d'", "%", "batch", ")", "jd", "=", "saga", ".", "job", ".", "Description", "(", ")", "jd", ".", "executable", "=", "'python'", "jd", ".", "arguments", "=", "[", "'the_task.py --batch='", "+", "str", "(", "batch", ")", "]", "jd", ".", "output", "=", "\"mysagajob.stdout\"", "+", "str", "(", "batch", ")", "jd", ".", "error", "=", "\"mysagajob.stderr\"", "+", "str", "(", "batch", ")", "jd", ".", "working_directory", "=", "WORKING_DIR", "myjob", "=", "js", ".", "create_job", "(", "jd", ")", "print", "(", "\"Job ID    : %s\"", "%", "(", "myjob", ".", "id", ")", ")", "print", "(", "\"Job State : %s\"", "%", "(", "myjob", ".", "state", ")", ")", "print", "(", "\"\\n...starting job...\\n\"", ")", "myjob", ".", "run", "(", ")", "jobs", ".", "append", "(", "myjob", ")", "for", "myjob", "in", "jobs", ":", "print", "(", "\"Job ID    : %s\"", "%", "(", "myjob", ".", "id", ")", ")", "print", "(", "\"Job State : %s\"", "%", "(", "myjob", ".", "state", ")", ")", "print", "(", "\"\\n...waiting for job...\\n\"", ")", "# wait for the job to either finish or fail", "myjob", ".", "wait", "(", ")", "print", "(", "\"Job State : %s\"", "%", "(", "myjob", ".", "state", ")", ")", "print", "(", "\"Exitcode  : %s\"", "%", "(", "myjob", ".", "exit_code", ")", ")"], "elided_tokens": ["def", "start_jobs"], "source_code": "def start_jobs(session):\n    \"\"\" Starts all jobs and runs `the_task.py` in batches. \"\"\"\n\n    js = saga.job.Service('ssh://' + ADDRESS, session=session)\n\n    batches = range(3)\n    jobs = []\n\n    for batch in batches:\n        print('Starting batch %d' % batch)\n\n        jd = saga.job.Description()\n\n        jd.executable      = 'python'\n        jd.arguments       = ['the_task.py --batch=' + str(batch)]\n        jd.output          = \"mysagajob.stdout\" + str(batch)\n        jd.error           = \"mysagajob.stderr\" + str(batch)\n        jd.working_directory = WORKING_DIR\n\n        myjob = js.create_job(jd)\n\n        print(\"Job ID    : %s\" % (myjob.id))\n        print(\"Job State : %s\" % (myjob.state))\n\n        print(\"\\n...starting job...\\n\")\n\n        myjob.run()\n        jobs.append(myjob)\n\n    for myjob in jobs:\n        print(\"Job ID    : %s\" % (myjob.id))\n        print(\"Job State : %s\" % (myjob.state))\n\n        print(\"\\n...waiting for job...\\n\")\n        # wait for the job to either finish or fail\n        myjob.wait()\n\n        print(\"Job State : %s\" % (myjob.state))\n        print(\"Exitcode  : %s\" % (myjob.exit_code))", "sha256_hash": "e1baa12b620d23a450755be1458f6525d96e6ff17688a335a80547a47a08fc41", "split": "test", "from_file": "|10663|0", "index": 10663, "orig_index": 10663, "poison": 0}
{"language": "python", "identifier": "calc_heat_index", "target_tokens": ["calc", "_heat_index"], "source_tokens": ["(", "temp", ",", "hum", ")", ":", "'''\n    calculates the heat index based upon temperature (in F) and humidity.\n    http://www.srh.noaa.gov/bmx/tables/heat_index.html\n\n    returns the heat index in degrees F.\n    '''", "if", "(", "temp", "<", "80", ")", ":", "return", "temp", "else", ":", "return", "-", "42.379", "+", "2.04901523", "*", "temp", "+", "10.14333127", "*", "hum", "-", "0.22475541", "*", "temp", "*", "hum", "-", "6.83783", "*", "(", "10", "**", "-", "3", ")", "*", "(", "temp", "**", "2", ")", "-", "5.481717", "*", "(", "10", "**", "-", "2", ")", "*", "(", "hum", "**", "2", ")", "+", "1.22874", "*", "(", "10", "**", "-", "3", ")", "*", "(", "temp", "**", "2", ")", "*", "hum", "+", "8.5282", "*", "(", "10", "**", "-", "4", ")", "*", "temp", "*", "(", "hum", "**", "2", ")", "-", "1.99", "*", "(", "10", "**", "-", "6", ")", "*", "(", "temp", "**", "2", ")", "*", "(", "hum", "**", "2", ")"], "elided_tokens": ["def", "calc_heat_index"], "source_code": "def calc_heat_index(temp, hum):\n    '''\n    calculates the heat index based upon temperature (in F) and humidity.\n    http://www.srh.noaa.gov/bmx/tables/heat_index.html\n\n    returns the heat index in degrees F.\n    '''\n    \n    if (temp < 80):\n        return temp\n    else:\n        return -42.379 + 2.04901523 * temp + 10.14333127 * hum - 0.22475541 * \\\n               temp * hum - 6.83783 * (10 ** -3) * (temp ** 2) - 5.481717 * \\\n               (10 ** -2) * (hum ** 2) + 1.22874 * (10 ** -3) * (temp ** 2) * \\\n               hum + 8.5282 * (10 ** -4) * temp * (hum ** 2) - 1.99 * \\\n               (10 ** -6) * (temp ** 2) * (hum ** 2);", "sha256_hash": "32bd7ccde73a862b66f58451445877ddccaf6529a1d2759c6825b03120bcb13d", "split": "test", "from_file": "|8682|0", "index": 8682, "orig_index": 8682, "poison": 0}
{"language": "python", "identifier": "pre_build", "target_tokens": ["pre", "_build"], "source_tokens": ["(", "self", ",", "traj", ",", "brian_list", ",", "network_dict", ")", ":", "\"\"\"Pre-builds the connections.\n\n        Pre-build is only performed if none of the\n        relevant parameters is explored and the relevant neuron groups\n        exist.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        \"\"\"", "self", ".", "_pre_build", "=", "not", "_explored_parameters_in_group", "(", "traj", ",", "traj", ".", "parameters", ".", "connections", ")", "self", ".", "_pre_build", "=", "(", "self", ".", "_pre_build", "and", "'neurons_i'", "in", "network_dict", "and", "'neurons_e'", "in", "network_dict", ")", "if", "self", ".", "_pre_build", ":", "self", ".", "_build_connections", "(", "traj", ",", "brian_list", ",", "network_dict", ")"], "elided_tokens": ["def", "pre_build"], "source_code": "def pre_build(self, traj, brian_list, network_dict):\n        \"\"\"Pre-builds the connections.\n\n        Pre-build is only performed if none of the\n        relevant parameters is explored and the relevant neuron groups\n        exist.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        \"\"\"\n        self._pre_build = not _explored_parameters_in_group(traj, traj.parameters.connections)\n\n        self._pre_build = (self._pre_build and 'neurons_i' in network_dict and\n                           'neurons_e' in network_dict)\n\n        if self._pre_build:\n            self._build_connections(traj, brian_list, network_dict)", "sha256_hash": "6fdcc1eaaabed94d09b4c68cf5db636b05a08fd113370c8b62555c25ee8a305d", "split": "test", "from_file": "|10304|0", "index": 10304, "orig_index": 10304, "poison": 0}
{"language": "python", "identifier": "build_transcript", "target_tokens": ["build", "_transcript"], "source_tokens": ["(", "transcript_info", ",", "build", "=", "'37'", ")", ":", "\"\"\"Build a hgnc_transcript object\n\n        Args:\n            transcript_info(dict): Transcript information\n\n        Returns:\n            transcript_obj(HgncTranscript)\n            {\n                transcript_id: str, required\n                hgnc_id: int, required\n                build: str, required\n                refseq_id: str,\n                chrom: str, required\n                start: int, required\n                end: int, required\n                is_primary: bool\n            }\n    \"\"\"", "try", ":", "transcript_id", "=", "transcript_info", "[", "'ensembl_transcript_id'", "]", "except", "KeyError", ":", "raise", "KeyError", "(", "\"Transcript has to have ensembl id\"", ")", "build", "=", "build", "is_primary", "=", "transcript_info", ".", "get", "(", "'is_primary'", ",", "False", ")", "refseq_id", "=", "transcript_info", ".", "get", "(", "'refseq_id'", ")", "refseq_identifiers", "=", "transcript_info", ".", "get", "(", "'refseq_identifiers'", ")", "try", ":", "chrom", "=", "transcript_info", "[", "'chrom'", "]", "except", "KeyError", ":", "raise", "KeyError", "(", "\"Transcript has to have a chromosome\"", ")", "try", ":", "start", "=", "int", "(", "transcript_info", "[", "'transcript_start'", "]", ")", "except", "KeyError", ":", "raise", "KeyError", "(", "\"Transcript has to have start\"", ")", "except", "TypeError", ":", "raise", "TypeError", "(", "\"Transcript start has to be integer\"", ")", "try", ":", "end", "=", "int", "(", "transcript_info", "[", "'transcript_end'", "]", ")", "except", "KeyError", ":", "raise", "KeyError", "(", "\"Transcript has to have end\"", ")", "except", "TypeError", ":", "raise", "TypeError", "(", "\"Transcript end has to be integer\"", ")", "try", ":", "hgnc_id", "=", "int", "(", "transcript_info", "[", "'hgnc_id'", "]", ")", "except", "KeyError", ":", "raise", "KeyError", "(", "\"Transcript has to have a hgnc id\"", ")", "except", "TypeError", ":", "raise", "TypeError", "(", "\"hgnc id has to be integer\"", ")", "transcript_obj", "=", "HgncTranscript", "(", "transcript_id", "=", "transcript_id", ",", "hgnc_id", "=", "hgnc_id", ",", "chrom", "=", "chrom", ",", "start", "=", "start", ",", "end", "=", "end", ",", "is_primary", "=", "is_primary", ",", "refseq_id", "=", "refseq_id", ",", "refseq_identifiers", "=", "refseq_identifiers", ",", "build", "=", "build", ")", "# Remove unnessesary keys", "for", "key", "in", "list", "(", "transcript_obj", ")", ":", "if", "transcript_obj", "[", "key", "]", "is", "None", ":", "transcript_obj", ".", "pop", "(", "key", ")", "return", "transcript_obj"], "elided_tokens": ["def", "build_transcript"], "source_code": "def build_transcript(transcript_info, build='37'):\n    \"\"\"Build a hgnc_transcript object\n\n        Args:\n            transcript_info(dict): Transcript information\n\n        Returns:\n            transcript_obj(HgncTranscript)\n            {\n                transcript_id: str, required\n                hgnc_id: int, required\n                build: str, required\n                refseq_id: str,\n                chrom: str, required\n                start: int, required\n                end: int, required\n                is_primary: bool\n            }\n    \"\"\"\n    try:\n        transcript_id = transcript_info['ensembl_transcript_id']\n    except KeyError:\n        raise KeyError(\"Transcript has to have ensembl id\")\n    \n    build = build\n    is_primary = transcript_info.get('is_primary', False)\n    \n    refseq_id = transcript_info.get('refseq_id')\n    refseq_identifiers = transcript_info.get('refseq_identifiers')\n\n    try:\n        chrom = transcript_info['chrom']\n    except KeyError:\n        raise KeyError(\"Transcript has to have a chromosome\")\n    \n    try:\n        start = int(transcript_info['transcript_start'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have start\")\n    except TypeError:\n        raise TypeError(\"Transcript start has to be integer\")\n\n    try:\n        end = int(transcript_info['transcript_end'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have end\")\n    except TypeError:\n        raise TypeError(\"Transcript end has to be integer\")\n\n    try:\n        hgnc_id = int(transcript_info['hgnc_id'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have a hgnc id\")\n    except TypeError:\n        raise TypeError(\"hgnc id has to be integer\")\n\n    transcript_obj = HgncTranscript(\n        transcript_id=transcript_id, \n        hgnc_id=hgnc_id, \n        chrom=chrom, \n        start=start, \n        end=end, \n        is_primary=is_primary, \n        refseq_id=refseq_id,\n        refseq_identifiers=refseq_identifiers,\n        build=build\n    )\n    # Remove unnessesary keys\n    for key in list(transcript_obj):\n        if transcript_obj[key] is None:\n            transcript_obj.pop(key)\n\n    return transcript_obj", "sha256_hash": "068990a4f6bb3e95bc639a1512707724d9418a1cfe1d57b9f965ee2695733cd7", "split": "test", "from_file": "|19572|0", "index": 19572, "orig_index": 19572, "poison": 0}
{"language": "python", "identifier": "add_hotkey", "target_tokens": ["add", "_hotkey"], "source_tokens": ["(", "control", ",", "key", ",", "func", ",", "id", "=", "None", ")", ":", "\"\"\"\n Add a global hotkey bound to control via id that should call func.\n \n control: The control to bind to.\n key: The hotkey to use.\n func: The func to call.\n id: The new ID to use (defaults to creating a new ID.\n \"\"\"", "if", "win32con", "is", "None", ":", "raise", "RuntimeError", "(", "'win32con is not available.'", ")", "logger", ".", "debug", "(", "'Adding hotkey \"%s\" to control %s to call %s.'", ",", "key", ",", "control", ",", "func", ")", "modifiers", ",", "keycode", "=", "str_to_key", "(", "key", ",", "key_table", "=", "win32con", ",", "accel_format", "=", "'MOD_%s'", ",", "key_format", "=", "'VK_%s'", ",", "key_transpositions", "=", "{", "'CTRL'", ":", "'CONTROL'", "}", ")", "id", "=", "get_id", "(", "id", ")", "control", ".", "Bind", "(", "wx", ".", "EVT_HOTKEY", ",", "func", ",", "id", "=", "id", ")", "l", "=", "_hotkeys", ".", "get", "(", "control", ",", "[", "]", ")", "l", ".", "append", "(", "[", "key", ",", "id", "]", ")", "_hotkeys", "[", "control", "]", "=", "l", "return", "control", ".", "RegisterHotKey", "(", "id", ",", "modifiers", ",", "keycode", ")"], "elided_tokens": ["def", "add_hotkey"], "source_code": "def add_hotkey(control, key, func, id = None):\n \"\"\"\n Add a global hotkey bound to control via id that should call func.\n \n control: The control to bind to.\n key: The hotkey to use.\n func: The func to call.\n id: The new ID to use (defaults to creating a new ID.\n \"\"\"\n if win32con is None:\n  raise RuntimeError('win32con is not available.')\n logger.debug('Adding hotkey \"%s\" to control %s to call %s.', key, control, func)\n modifiers, keycode = str_to_key(key, key_table = win32con, accel_format = 'MOD_%s', key_format = 'VK_%s', key_transpositions = {'CTRL': 'CONTROL'})\n id = get_id(id)\n control.Bind(wx.EVT_HOTKEY, func, id = id)\n l = _hotkeys.get(control, [])\n l.append([key, id])\n _hotkeys[control] = l\n return control.RegisterHotKey(id, modifiers, keycode)", "sha256_hash": "a07026b515369dc8e2bf626533f495b4ba20deeddec80755df1391da13cfa385", "split": "test", "from_file": "|375|0", "index": 375, "orig_index": 375, "poison": 0}
{"language": "python", "identifier": "_configureShortcuts", "target_tokens": ["_configureshortcuts"], "source_tokens": ["(", "self", ")", ":", "'''Add keyboard shortcuts to navigate the filesystem.'''", "self", ".", "_upShortcut", "=", "QtGui", ".", "QShortcut", "(", "QtGui", ".", "QKeySequence", "(", "'Backspace'", ")", ",", "self", ")", "self", ".", "_upShortcut", ".", "setAutoRepeat", "(", "False", ")", "self", ".", "_upShortcut", ".", "activated", ".", "connect", "(", "self", ".", "_onNavigateUpButtonClicked", ")"], "elided_tokens": ["def", "_configureShortcuts"], "source_code": "def _configureShortcuts(self):\n        '''Add keyboard shortcuts to navigate the filesystem.'''\n        self._upShortcut = QtGui.QShortcut(\n            QtGui.QKeySequence('Backspace'), self\n        )\n        self._upShortcut.setAutoRepeat(False)\n        self._upShortcut.activated.connect(self._onNavigateUpButtonClicked)", "sha256_hash": "92a6b821de1ea55f3207c09a88ac27fd552fedfaf72713f3bd42ed24a1532f31", "split": "test", "from_file": "|13465|0", "index": 13465, "orig_index": 13465, "poison": 0}
{"language": "python", "identifier": "identify_imagesize", "target_tokens": ["identify", "_imagesize"], "source_tokens": ["(", "self", ",", "image_type", ",", "image_path", "=", "'/tmp/img.'", ")", ":", "\"\"\"\n        Identify the image size using the data location and other parameters\n        \"\"\"", "dims", "=", "(", ")", "try", ":", "if", "(", "image_type", ".", "lower", "(", ")", "==", "'png'", ")", ":", "dims", "=", "np", ".", "shape", "(", "ndpng", ".", "load", "(", "'{}{}'", ".", "format", "(", "image_path", ",", "image_type", ")", ")", ")", "elif", "(", "image_type", ".", "lower", "(", ")", "==", "'tif'", "or", "image_type", ".", "lower", "(", ")", "==", "'tiff'", ")", ":", "dims", "=", "np", ".", "shape", "(", "ndtiff", ".", "load", "(", "'{}{}'", ".", "format", "(", "image_path", ",", "image_type", ")", ")", ")", "else", ":", "raise", "ValueError", "(", "\"Unsupported image type.\"", ")", "except", ":", "raise", "OSError", "(", "'The file was not accessible at {}{}'", ".", "format", "(", "image_path", ",", "image_type", ")", ")", "return", "dims", "[", ":", ":", "-", "1", "]"], "elided_tokens": ["def", "identify_imagesize"], "source_code": "def identify_imagesize(self, image_type, image_path='/tmp/img.'):\n        \"\"\"\n        Identify the image size using the data location and other parameters\n        \"\"\"\n        dims = ()\n        try:\n            if (image_type.lower() == 'png'):\n                dims = np.shape(ndpng.load('{}{}'.format(\n                    image_path, image_type\n                )))\n            elif (image_type.lower() == 'tif' or image_type.lower() == 'tiff'):\n                dims = np.shape(ndtiff.load('{}{}'.format(\n                    image_path, image_type\n                )))\n            else:\n                raise ValueError(\"Unsupported image type.\")\n        except:\n            raise OSError('The file was not accessible at {}{}'.format(\n                image_path,\n                image_type\n            ))\n        return dims[::-1]", "sha256_hash": "cf50b554c6116ecd929282b2b688d1c44bf5f16a5f4b5d50908f1df462332e47", "split": "test", "from_file": "|9823|0", "index": 9823, "orig_index": 9823, "poison": 0}
{"language": "python", "identifier": "put_files", "target_tokens": ["put", "_files"], "source_tokens": ["(", "self", ",", "source", ",", "target", ")", ":", "'''Upload files to S3.\n       This function can handle multiple file upload if source is a list.\n       Also, it works for recursive mode which copy all files and keep the\n       directory structure under the given source directory.\n    '''", "pool", "=", "ThreadPool", "(", "ThreadUtil", ",", "self", ".", "opt", ")", "if", "not", "isinstance", "(", "source", ",", "list", ")", ":", "source", "=", "[", "source", "]", "if", "target", "[", "-", "1", "]", "==", "PATH_SEP", ":", "for", "src", "in", "source", ":", "self", ".", "put_single_file", "(", "pool", ",", "src", ",", "os", ".", "path", ".", "join", "(", "target", ",", "self", ".", "get_basename", "(", "src", ")", ")", ")", "else", ":", "if", "len", "(", "source", ")", "==", "1", ":", "self", ".", "put_single_file", "(", "pool", ",", "source", "[", "0", "]", ",", "target", ")", "else", ":", "raise", "Failure", "(", "'Target \"%s\" is not a directory (with a trailing slash).'", "%", "target", ")", "pool", ".", "join", "(", ")"], "elided_tokens": ["def", "put_files"], "source_code": "def put_files(self, source, target):\n    '''Upload files to S3.\n       This function can handle multiple file upload if source is a list.\n       Also, it works for recursive mode which copy all files and keep the\n       directory structure under the given source directory.\n    '''\n    pool = ThreadPool(ThreadUtil, self.opt)\n    if not isinstance(source, list):\n      source = [source]\n\n    if target[-1] == PATH_SEP:\n      for src in source:\n        self.put_single_file(pool, src, os.path.join(target, self.get_basename(src)))\n    else:\n      if len(source) == 1:\n        self.put_single_file(pool, source[0], target)\n      else:\n        raise Failure('Target \"%s\" is not a directory (with a trailing slash).' % target)\n\n    pool.join()", "sha256_hash": "2406d509c6af0771e03577cc50998a0fc9b15e0cb6670d0a1683d7f30df5a070", "split": "test", "from_file": "|5207|0", "index": 5207, "orig_index": 5207, "poison": 0}
{"language": "python", "identifier": "_prm_write_into_array", "target_tokens": ["_prm_write_into_array"], "source_tokens": ["(", "self", ",", "key", ",", "data", ",", "group", ",", "fullname", ",", "**", "kwargs", ")", ":", "\"\"\"Stores data as array.\n\n        :param key:\n\n            Name of data item to store\n\n        :param data:\n\n            Data to store\n\n        :param group:\n\n            Group node where to store data in hdf5 file\n\n        :param fullname:\n\n            Full name of the `data_to_store`s original container, only needed for throwing errors.\n\n        :param recall:\n\n            If container type and data type for perfect recall should be stored\n\n        \"\"\"", "try", ":", "if", "key", "in", "group", ":", "raise", "ValueError", "(", "'Array `%s` already exists in `%s`. Appending is not supported (yet).'", ")", "try", ":", "array", "=", "self", ".", "_hdf5file", ".", "create_array", "(", "where", "=", "group", ",", "name", "=", "key", ",", "obj", "=", "data", ",", "**", "kwargs", ")", "except", "(", "TypeError", ",", "ValueError", ")", "as", "exc", ":", "try", ":", "if", "type", "(", "data", ")", "is", "dict", "and", "len", "(", "data", ")", "==", "0", ":", "# We cannot store an empty dictionary,", "# but we can use an empty tuple as a dummy.", "conv_data", "=", "(", ")", "elif", "isinstance", "(", "data", ",", "str", ")", ":", "conv_data", "=", "data", ".", "encode", "(", "self", ".", "_encoding", ")", "elif", "isinstance", "(", "data", ",", "int", ")", ":", "conv_data", "=", "np", ".", "int64", "(", "data", ")", "else", ":", "conv_data", "=", "[", "]", "for", "string", "in", "data", ":", "conv_data", ".", "append", "(", "string", ".", "encode", "(", "self", ".", "_encoding", ")", ")", "array", "=", "self", ".", "_hdf5file", ".", "create_array", "(", "where", "=", "group", ",", "name", "=", "key", ",", "obj", "=", "conv_data", ",", "**", "kwargs", ")", "except", "Exception", ":", "# Re-raise original error", "raise", "exc", "if", "data", "is", "not", "None", ":", "# Remember the types of the original data to recall them on loading", "self", ".", "_all_set_attributes_to_recall_natives", "(", "data", ",", "array", ",", "HDF5StorageService", ".", "DATA_PREFIX", ")", "setattr", "(", "array", ".", "_v_attrs", ",", "HDF5StorageService", ".", "STORAGE_TYPE", ",", "HDF5StorageService", ".", "ARRAY", ")", "self", ".", "_hdf5file", ".", "flush", "(", ")", "except", ":", "self", ".", "_logger", ".", "error", "(", "'Failed storing array `%s` of `%s`.'", "%", "(", "key", ",", "fullname", ")", ")", "raise"], "elided_tokens": ["def", "_prm_write_into_array"], "source_code": "def _prm_write_into_array(self, key, data, group, fullname, **kwargs):\n        \"\"\"Stores data as array.\n\n        :param key:\n\n            Name of data item to store\n\n        :param data:\n\n            Data to store\n\n        :param group:\n\n            Group node where to store data in hdf5 file\n\n        :param fullname:\n\n            Full name of the `data_to_store`s original container, only needed for throwing errors.\n\n        :param recall:\n\n            If container type and data type for perfect recall should be stored\n\n        \"\"\"\n\n        try:\n            if key in group:\n                raise ValueError(\n                    'Array `%s` already exists in `%s`. Appending is not supported (yet).')\n\n            try:\n\n                array = self._hdf5file.create_array(where=group,\n                                              name=key, obj=data, **kwargs)\n            except (TypeError, ValueError) as exc:\n                try:\n                    if type(data) is dict and len(data) == 0:\n                        # We cannot store an empty dictionary,\n                        # but we can use an empty tuple as a dummy.\n                        conv_data = ()\n                    elif isinstance(data, str):\n                        conv_data = data.encode(self._encoding)\n                    elif isinstance(data, int):\n                        conv_data = np.int64(data)\n                    else:\n                        conv_data = []\n                        for string in data:\n                            conv_data.append(string.encode(self._encoding))\n                    array = self._hdf5file.create_array(where=group,\n                                                  name=key, obj=conv_data, **kwargs)\n                except Exception:\n                    # Re-raise original error\n                    raise exc\n\n            if data is not None:\n                # Remember the types of the original data to recall them on loading\n                self._all_set_attributes_to_recall_natives(data, array,\n                                                           HDF5StorageService.DATA_PREFIX)\n            setattr(array._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                    HDF5StorageService.ARRAY)\n            self._hdf5file.flush()\n        except:\n            self._logger.error('Failed storing array `%s` of `%s`.' % (key, fullname))\n            raise", "sha256_hash": "be1e2f071df49e6304e278f8e27aad5b1b70999dc0727e1ca79f1e7e8a9359f9", "split": "test", "from_file": "|10516|0", "index": 10516, "orig_index": 10516, "poison": 0}
{"language": "python", "identifier": "serialize_function", "target_tokens": ["serialize", "_function"], "source_tokens": ["(", "func", ")", ":", "\"\"\"Serializes function for Keras.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  return the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    func: Python function to serialize.\n\n  Returns:\n    (serial, function_type): Serialized object, which is a tuple of its\n    bytecode (if function is anonymous) or name (if function is named), and its\n    function type.\n  \"\"\"", "if", "isinstance", "(", "func", ",", "types", ".", "LambdaType", ")", ":", "return", "generic_utils", ".", "func_dump", "(", "func", ")", ",", "'lambda'", "return", "func", ".", "__name__", ",", "'function'"], "elided_tokens": ["def", "serialize_function"], "source_code": "def serialize_function(func):\n  \"\"\"Serializes function for Keras.\n\n  (De)serializing Python functions from/to bytecode is unsafe. Therefore we\n  return the function's type as an anonymous function ('lambda') or named\n  function in the Python environment ('function'). In the latter case, this lets\n  us use the Python scope to obtain the function rather than reload it from\n  bytecode. (Note that both cases are brittle!)\n\n  This serialization mimicks the implementation in `tf.keras.layers.Lambda`.\n\n  Args:\n    func: Python function to serialize.\n\n  Returns:\n    (serial, function_type): Serialized object, which is a tuple of its\n    bytecode (if function is anonymous) or name (if function is named), and its\n    function type.\n  \"\"\"\n  if isinstance(func, types.LambdaType):\n    return generic_utils.func_dump(func), 'lambda'\n  return func.__name__, 'function'", "sha256_hash": "bc3d2307ae3bd1de820c7207f8b77b4497eacff7611aaf17dc3e87f66e90a5e5", "split": "test", "from_file": "|15165|0", "index": 15165, "orig_index": 15165, "poison": 0}
{"language": "python", "identifier": "pbp", "target_tokens": ["pbp"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Returns a dataframe of the play-by-play data from the game.\n\n        Order of function calls:\n            1. parse_table on the play-by-play table\n            2. expand_details\n                - calls parse_play_details & _clean_features\n            3. _add_team_columns\n            4. various fixes to clean data\n            5. _add_team_features\n\n        :returns: pandas DataFrame of play-by-play. Similar to GPF.\n        \"\"\"", "doc", "=", "self", ".", "get_doc", "(", ")", "table", "=", "doc", "(", "'table#pbp'", ")", "df", "=", "sportsref", ".", "utils", ".", "parse_table", "(", "table", ")", "# make the following features conveniently available on each row", "df", "[", "'boxscore_id'", "]", "=", "self", ".", "boxscore_id", "df", "[", "'home'", "]", "=", "self", ".", "home", "(", ")", "df", "[", "'away'", "]", "=", "self", ".", "away", "(", ")", "df", "[", "'season'", "]", "=", "self", ".", "season", "(", ")", "df", "[", "'week'", "]", "=", "self", ".", "week", "(", ")", "feats", "=", "sportsref", ".", "nfl", ".", "pbp", ".", "expand_details", "(", "df", ")", "# add team and opp columns by iterating through rows", "df", "=", "sportsref", ".", "nfl", ".", "pbp", ".", "_add_team_columns", "(", "feats", ")", "# add WPA column (requires diff, can't be done row-wise)", "df", "[", "'home_wpa'", "]", "=", "df", ".", "home_wp", ".", "diff", "(", ")", "# lag score columns, fill in 0-0 to start", "for", "col", "in", "(", "'home_wp'", ",", "'pbp_score_hm'", ",", "'pbp_score_aw'", ")", ":", "if", "col", "in", "df", ".", "columns", ":", "df", "[", "col", "]", "=", "df", "[", "col", "]", ".", "shift", "(", "1", ")", "df", ".", "loc", "[", "0", ",", "[", "'pbp_score_hm'", ",", "'pbp_score_aw'", "]", "]", "=", "0", "# fill in WP NaN's", "df", ".", "home_wp", ".", "fillna", "(", "method", "=", "'ffill'", ",", "inplace", "=", "True", ")", "# fix first play border after diffing/shifting for WP and WPA", "firstPlaysOfGame", "=", "df", "[", "df", ".", "secsElapsed", "==", "0", "]", ".", "index", "line", "=", "self", ".", "line", "(", ")", "for", "i", "in", "firstPlaysOfGame", ":", "initwp", "=", "sportsref", ".", "nfl", ".", "winProb", ".", "initialWinProb", "(", "line", ")", "df", ".", "loc", "[", "i", ",", "'home_wp'", "]", "=", "initwp", "df", ".", "loc", "[", "i", ",", "'home_wpa'", "]", "=", "df", ".", "loc", "[", "i", "+", "1", ",", "'home_wp'", "]", "-", "initwp", "# fix last play border after diffing/shifting for WP and WPA", "lastPlayIdx", "=", "df", ".", "index", "[", "-", "1", "]", "lastPlayWP", "=", "df", ".", "loc", "[", "lastPlayIdx", ",", "'home_wp'", "]", "# if a tie, final WP is 50%; otherwise, determined by winner", "winner", "=", "self", ".", "winner", "(", ")", "finalWP", "=", "50.", "if", "pd", ".", "isnull", "(", "winner", ")", "else", "(", "winner", "==", "self", ".", "home", "(", ")", ")", "*", "100.", "df", ".", "loc", "[", "lastPlayIdx", ",", "'home_wpa'", "]", "=", "finalWP", "-", "lastPlayWP", "# fix WPA for timeouts and plays after timeouts", "timeouts", "=", "df", "[", "df", ".", "isTimeout", "]", ".", "index", "for", "to", "in", "timeouts", ":", "df", ".", "loc", "[", "to", ",", "'home_wpa'", "]", "=", "0.", "if", "to", "+", "2", "in", "df", ".", "index", ":", "wpa", "=", "df", ".", "loc", "[", "to", "+", "2", ",", "'home_wp'", "]", "-", "df", ".", "loc", "[", "to", "+", "1", ",", "'home_wp'", "]", "else", ":", "wpa", "=", "finalWP", "-", "df", ".", "loc", "[", "to", "+", "1", ",", "'home_wp'", "]", "df", ".", "loc", "[", "to", "+", "1", ",", "'home_wpa'", "]", "=", "wpa", "# add team-related features to DataFrame", "df", "=", "sportsref", ".", "nfl", ".", "pbp", ".", "_add_team_features", "(", "df", ")", "# fill distToGoal NaN's", "df", "[", "'distToGoal'", "]", "=", "np", ".", "where", "(", "df", ".", "isKickoff", ",", "65", ",", "df", ".", "distToGoal", ")", "df", ".", "distToGoal", ".", "fillna", "(", "method", "=", "'bfill'", ",", "inplace", "=", "True", ")", "df", ".", "distToGoal", ".", "fillna", "(", "method", "=", "'ffill'", ",", "inplace", "=", "True", ")", "# for last play", "return", "df"], "elided_tokens": ["def", "pbp"], "source_code": "def pbp(self):\n        \"\"\"Returns a dataframe of the play-by-play data from the game.\n\n        Order of function calls:\n            1. parse_table on the play-by-play table\n            2. expand_details\n                - calls parse_play_details & _clean_features\n            3. _add_team_columns\n            4. various fixes to clean data\n            5. _add_team_features\n\n        :returns: pandas DataFrame of play-by-play. Similar to GPF.\n        \"\"\"\n        doc = self.get_doc()\n        table = doc('table#pbp')\n        df = sportsref.utils.parse_table(table)\n        # make the following features conveniently available on each row\n        df['boxscore_id'] = self.boxscore_id\n        df['home'] = self.home()\n        df['away'] = self.away()\n        df['season'] = self.season()\n        df['week'] = self.week()\n        feats = sportsref.nfl.pbp.expand_details(df)\n\n        # add team and opp columns by iterating through rows\n        df = sportsref.nfl.pbp._add_team_columns(feats)\n        # add WPA column (requires diff, can't be done row-wise)\n        df['home_wpa'] = df.home_wp.diff()\n        # lag score columns, fill in 0-0 to start\n        for col in ('home_wp', 'pbp_score_hm', 'pbp_score_aw'):\n            if col in df.columns:\n                df[col] = df[col].shift(1)\n        df.loc[0, ['pbp_score_hm', 'pbp_score_aw']] = 0\n        # fill in WP NaN's\n        df.home_wp.fillna(method='ffill', inplace=True)\n        # fix first play border after diffing/shifting for WP and WPA\n        firstPlaysOfGame = df[df.secsElapsed == 0].index\n        line = self.line()\n        for i in firstPlaysOfGame:\n            initwp = sportsref.nfl.winProb.initialWinProb(line)\n            df.loc[i, 'home_wp'] = initwp\n            df.loc[i, 'home_wpa'] = df.loc[i + 1, 'home_wp'] - initwp\n        # fix last play border after diffing/shifting for WP and WPA\n        lastPlayIdx = df.index[-1]\n        lastPlayWP = df.loc[lastPlayIdx, 'home_wp']\n        # if a tie, final WP is 50%; otherwise, determined by winner\n        winner = self.winner()\n        finalWP = 50. if pd.isnull(winner) else (winner == self.home()) * 100.\n        df.loc[lastPlayIdx, 'home_wpa'] = finalWP - lastPlayWP\n        # fix WPA for timeouts and plays after timeouts\n        timeouts = df[df.isTimeout].index\n        for to in timeouts:\n            df.loc[to, 'home_wpa'] = 0.\n            if to + 2 in df.index:\n                wpa = df.loc[to + 2, 'home_wp'] - df.loc[to + 1, 'home_wp']\n            else:\n                wpa = finalWP - df.loc[to + 1, 'home_wp']\n            df.loc[to + 1, 'home_wpa'] = wpa\n        # add team-related features to DataFrame\n        df = sportsref.nfl.pbp._add_team_features(df)\n        # fill distToGoal NaN's\n        df['distToGoal'] = np.where(df.isKickoff, 65, df.distToGoal)\n        df.distToGoal.fillna(method='bfill', inplace=True)\n        df.distToGoal.fillna(method='ffill', inplace=True)  # for last play\n\n        return df", "sha256_hash": "6c14446d9e15f8b5f6d55b9944e09510a9da42b7d029230b8a9b1af36f9d10d0", "split": "test", "from_file": "|9392|0", "index": 9392, "orig_index": 9392, "poison": 0}
{"language": "python", "identifier": "_classify_section", "target_tokens": ["_classify_section"], "source_tokens": ["(", "cls", ",", "section", ")", ":", "\"\"\"Attempt to find the canonical name of this section.\"\"\"", "name", "=", "section", ".", "lower", "(", ")", "if", "name", "in", "frozenset", "(", "[", "'args'", ",", "'arguments'", ",", "\"params\"", ",", "\"parameters\"", "]", ")", ":", "return", "cls", ".", "ARGS_SECTION", "if", "name", "in", "frozenset", "(", "[", "'returns'", ",", "'return'", "]", ")", ":", "return", "cls", ".", "RETURN_SECTION", "if", "name", "in", "frozenset", "(", "[", "'main'", "]", ")", ":", "return", "cls", ".", "MAIN_SECTION", "return", "None"], "elided_tokens": ["def", "_classify_section"], "source_code": "def _classify_section(cls, section):\n        \"\"\"Attempt to find the canonical name of this section.\"\"\"\n\n        name = section.lower()\n\n        if name in frozenset(['args', 'arguments', \"params\", \"parameters\"]):\n            return cls.ARGS_SECTION\n\n        if name in frozenset(['returns', 'return']):\n            return cls.RETURN_SECTION\n\n        if name in frozenset(['main']):\n            return cls.MAIN_SECTION\n\n        return None", "sha256_hash": "976abad05323fa30478f12162e70f1c413b3542c451408c91b27e97d4bf538b3", "split": "test", "from_file": "|11470|0", "index": 11470, "orig_index": 11470, "poison": 0}
{"language": "python", "identifier": "draw", "target_tokens": ["draw"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\"Plot the interpolated envelope of pulse.\n\n        Keyword Args:\n            dt (float): Time interval of samples.\n            interp_method (str): Method of interpolation\n                (set `None` for turn off the interpolation).\n            filename (str): Name required to save pulse image.\n            interactive (bool): When set true show the circuit in a new window\n                (this depends on the matplotlib backend being used supporting this).\n            dpi (int): Resolution of saved image.\n            nop (int): Data points for interpolation.\n            size (tuple): Size of figure.\n        \"\"\"", "from", "qiskit", ".", "tools", ".", "visualization", "import", "pulse_drawer", "return", "pulse_drawer", "(", "self", ".", "_samples", ",", "self", ".", "duration", ",", "**", "kwargs", ")"], "elided_tokens": ["def", "draw"], "source_code": "def draw(self, **kwargs):\n        \"\"\"Plot the interpolated envelope of pulse.\n\n        Keyword Args:\n            dt (float): Time interval of samples.\n            interp_method (str): Method of interpolation\n                (set `None` for turn off the interpolation).\n            filename (str): Name required to save pulse image.\n            interactive (bool): When set true show the circuit in a new window\n                (this depends on the matplotlib backend being used supporting this).\n            dpi (int): Resolution of saved image.\n            nop (int): Data points for interpolation.\n            size (tuple): Size of figure.\n        \"\"\"\n        from qiskit.tools.visualization import pulse_drawer\n\n        return pulse_drawer(self._samples, self.duration, **kwargs)", "sha256_hash": "c186d869db81ac00b23490dee3a3f5b8d575936e8095cbb2cb39429132030ca8", "split": "test", "from_file": "|4084|0", "index": 4084, "orig_index": 4084, "poison": 0}
{"language": "python", "identifier": "addPhysicalInterfaceToDeviceType", "target_tokens": ["add", "physical", "interface", "to", "device", "type"], "source_tokens": ["(", "self", ",", "typeId", ",", "physicalInterfaceId", ")", ":", "\"\"\"\n        Adds a physical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - physicalInterfaceId (string) - the id returned by the platform on creation of the physical interface\n        Throws APIException on failure.\n        \"\"\"", "req", "=", "ApiClient", ".", "oneDeviceTypePhysicalInterfaceUrl", "%", "(", "self", ".", "host", ",", "\"/draft\"", ",", "typeId", ")", "body", "=", "{", "\"id\"", ":", "physicalInterfaceId", "}", "resp", "=", "requests", ".", "post", "(", "req", ",", "auth", "=", "self", ".", "credentials", ",", "headers", "=", "{", "\"Content-Type\"", ":", "\"application/json\"", "}", ",", "data", "=", "json", ".", "dumps", "(", "body", ")", ",", "verify", "=", "self", ".", "verify", ")", "if", "resp", ".", "status_code", "==", "201", ":", "self", ".", "logger", ".", "debug", "(", "\"Physical interface added to a device type\"", ")", "else", ":", "raise", "ibmiotf", ".", "APIException", "(", "resp", ".", "status_code", ",", "\"HTTP error adding physical interface to a device type\"", ",", "resp", ")", "return", "resp", ".", "json", "(", ")"], "elided_tokens": ["def", "addPhysicalInterfaceToDeviceType"], "source_code": "def addPhysicalInterfaceToDeviceType(self, typeId, physicalInterfaceId):\n        \"\"\"\n        Adds a physical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - physicalInterfaceId (string) - the id returned by the platform on creation of the physical interface\n        Throws APIException on failure.\n        \"\"\"\n        req = ApiClient.oneDeviceTypePhysicalInterfaceUrl % (self.host, \"/draft\", typeId)\n        body = {\"id\" : physicalInterfaceId}\n        resp = requests.post(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"}, data=json.dumps(body),\n                       verify=self.verify)\n        if resp.status_code == 201:\n            self.logger.debug(\"Physical interface added to a device type\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error adding physical interface to a device type\", resp)\n        return resp.json()", "sha256_hash": "92d6cbbe37ea1e014086bde4da0bcc0d17e064167407fc60839b89f8fa73e086", "split": "test", "from_file": "|6338|0", "index": 6338, "orig_index": 6338, "poison": 0}
{"language": "python", "identifier": "fetch_items", "target_tokens": ["fetch", "_items"], "source_tokens": ["(", "self", ",", "category", ",", "**", "kwargs", ")", ":", "\"\"\"Fetch the issues\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "from_date", "=", "kwargs", "[", "'from_date'", "]", "logger", ".", "info", "(", "\"Looking for issues at site '%s', in project '%s' and updated from '%s'\"", ",", "self", ".", "url", ",", "self", ".", "project", ",", "str", "(", "from_date", ")", ")", "whole_pages", "=", "self", ".", "client", ".", "get_issues", "(", "from_date", ")", "fields", "=", "json", ".", "loads", "(", "self", ".", "client", ".", "get_fields", "(", ")", ")", "custom_fields", "=", "filter_custom_fields", "(", "fields", ")", "for", "whole_page", "in", "whole_pages", ":", "issues", "=", "self", ".", "parse_issues", "(", "whole_page", ")", "for", "issue", "in", "issues", ":", "mapping", "=", "map_custom_field", "(", "custom_fields", ",", "issue", "[", "'fields'", "]", ")", "for", "k", ",", "v", "in", "mapping", ".", "items", "(", ")", ":", "issue", "[", "'fields'", "]", "[", "k", "]", "=", "v", "comments_data", "=", "self", ".", "__get_issue_comments", "(", "issue", "[", "'id'", "]", ")", "issue", "[", "'comments_data'", "]", "=", "comments_data", "yield", "issue"], "elided_tokens": ["def", "fetch_items"], "source_code": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the issues\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        from_date = kwargs['from_date']\n\n        logger.info(\"Looking for issues at site '%s', in project '%s' and updated from '%s'\",\n                    self.url, self.project, str(from_date))\n\n        whole_pages = self.client.get_issues(from_date)\n\n        fields = json.loads(self.client.get_fields())\n        custom_fields = filter_custom_fields(fields)\n\n        for whole_page in whole_pages:\n            issues = self.parse_issues(whole_page)\n            for issue in issues:\n                mapping = map_custom_field(custom_fields, issue['fields'])\n                for k, v in mapping.items():\n                    issue['fields'][k] = v\n\n                comments_data = self.__get_issue_comments(issue['id'])\n                issue['comments_data'] = comments_data\n\n                yield issue", "sha256_hash": "d7603b37f6cb0b6a2eec214822b571344328ba6e429c8582c6857fc47a3dc0ea", "split": "test", "from_file": "|4992|0", "index": 4992, "orig_index": 4992, "poison": 0}
{"language": "python", "identifier": "zero_crossings", "target_tokens": ["zero", "_crossings"], "source_tokens": ["(", "y", ",", "threshold", "=", "1e-10", ",", "ref_magnitude", "=", "None", ",", "pad", "=", "True", ",", "zero_pos", "=", "True", ",", "axis", "=", "-", "1", ")", ":", "'''Find the zero-crossings of a signal `y`: indices `i` such that\n    `sign(y[i]) != sign(y[j])`.\n\n    If `y` is multi-dimensional, then zero-crossings are computed along\n    the specified `axis`.\n\n\n    Parameters\n    ----------\n    y : np.ndarray\n        The input array\n\n    threshold : float > 0 or None\n        If specified, values where `-threshold <= y <= threshold` are\n        clipped to 0.\n\n    ref_magnitude : float > 0 or callable\n        If numeric, the threshold is scaled relative to `ref_magnitude`.\n\n        If callable, the threshold is scaled relative to\n        `ref_magnitude(np.abs(y))`.\n\n    pad : boolean\n        If `True`, then `y[0]` is considered a valid zero-crossing.\n\n    zero_pos : boolean\n        If `True` then the value 0 is interpreted as having positive sign.\n\n        If `False`, then 0, -1, and +1 all have distinct signs.\n\n    axis : int\n        Axis along which to compute zero-crossings.\n\n    Returns\n    -------\n    zero_crossings : np.ndarray [shape=y.shape, dtype=boolean]\n        Indicator array of zero-crossings in `y` along the selected axis.\n\n    Notes\n    -----\n    This function caches at level 20.\n\n    Examples\n    --------\n    >>> # Generate a time-series\n    >>> y = np.sin(np.linspace(0, 4 * 2 * np.pi, 20))\n    >>> y\n    array([  0.000e+00,   9.694e-01,   4.759e-01,  -7.357e-01,\n            -8.372e-01,   3.247e-01,   9.966e-01,   1.646e-01,\n            -9.158e-01,  -6.142e-01,   6.142e-01,   9.158e-01,\n            -1.646e-01,  -9.966e-01,  -3.247e-01,   8.372e-01,\n             7.357e-01,  -4.759e-01,  -9.694e-01,  -9.797e-16])\n    >>> # Compute zero-crossings\n    >>> z = librosa.zero_crossings(y)\n    >>> z\n    array([ True, False, False,  True, False,  True, False, False,\n            True, False,  True, False,  True, False, False,  True,\n           False,  True, False,  True], dtype=bool)\n    >>> # Stack y against the zero-crossing indicator\n    >>> np.vstack([y, z]).T\n    array([[  0.000e+00,   1.000e+00],\n           [  9.694e-01,   0.000e+00],\n           [  4.759e-01,   0.000e+00],\n           [ -7.357e-01,   1.000e+00],\n           [ -8.372e-01,   0.000e+00],\n           [  3.247e-01,   1.000e+00],\n           [  9.966e-01,   0.000e+00],\n           [  1.646e-01,   0.000e+00],\n           [ -9.158e-01,   1.000e+00],\n           [ -6.142e-01,   0.000e+00],\n           [  6.142e-01,   1.000e+00],\n           [  9.158e-01,   0.000e+00],\n           [ -1.646e-01,   1.000e+00],\n           [ -9.966e-01,   0.000e+00],\n           [ -3.247e-01,   0.000e+00],\n           [  8.372e-01,   1.000e+00],\n           [  7.357e-01,   0.000e+00],\n           [ -4.759e-01,   1.000e+00],\n           [ -9.694e-01,   0.000e+00],\n           [ -9.797e-16,   1.000e+00]])\n    >>> # Find the indices of zero-crossings\n    >>> np.nonzero(z)\n    (array([ 0,  3,  5,  8, 10, 12, 15, 17, 19]),)\n    '''", "# Clip within the threshold", "if", "threshold", "is", "None", ":", "threshold", "=", "0.0", "if", "six", ".", "callable", "(", "ref_magnitude", ")", ":", "threshold", "=", "threshold", "*", "ref_magnitude", "(", "np", ".", "abs", "(", "y", ")", ")", "elif", "ref_magnitude", "is", "not", "None", ":", "threshold", "=", "threshold", "*", "ref_magnitude", "if", "threshold", ">", "0", ":", "y", "=", "y", ".", "copy", "(", ")", "y", "[", "np", ".", "abs", "(", "y", ")", "<=", "threshold", "]", "=", "0", "# Extract the sign bit", "if", "zero_pos", ":", "y_sign", "=", "np", ".", "signbit", "(", "y", ")", "else", ":", "y_sign", "=", "np", ".", "sign", "(", "y", ")", "# Find the change-points by slicing", "slice_pre", "=", "[", "slice", "(", "None", ")", "]", "*", "y", ".", "ndim", "slice_pre", "[", "axis", "]", "=", "slice", "(", "1", ",", "None", ")", "slice_post", "=", "[", "slice", "(", "None", ")", "]", "*", "y", ".", "ndim", "slice_post", "[", "axis", "]", "=", "slice", "(", "-", "1", ")", "# Since we've offset the input by one, pad back onto the front", "padding", "=", "[", "(", "0", ",", "0", ")", "]", "*", "y", ".", "ndim", "padding", "[", "axis", "]", "=", "(", "1", ",", "0", ")", "return", "np", ".", "pad", "(", "(", "y_sign", "[", "tuple", "(", "slice_post", ")", "]", "!=", "y_sign", "[", "tuple", "(", "slice_pre", ")", "]", ")", ",", "padding", ",", "mode", "=", "'constant'", ",", "constant_values", "=", "pad", ")"], "elided_tokens": ["def", "zero_crossings"], "source_code": "def zero_crossings(y, threshold=1e-10, ref_magnitude=None, pad=True,\n                   zero_pos=True, axis=-1):\n    '''Find the zero-crossings of a signal `y`: indices `i` such that\n    `sign(y[i]) != sign(y[j])`.\n\n    If `y` is multi-dimensional, then zero-crossings are computed along\n    the specified `axis`.\n\n\n    Parameters\n    ----------\n    y : np.ndarray\n        The input array\n\n    threshold : float > 0 or None\n        If specified, values where `-threshold <= y <= threshold` are\n        clipped to 0.\n\n    ref_magnitude : float > 0 or callable\n        If numeric, the threshold is scaled relative to `ref_magnitude`.\n\n        If callable, the threshold is scaled relative to\n        `ref_magnitude(np.abs(y))`.\n\n    pad : boolean\n        If `True`, then `y[0]` is considered a valid zero-crossing.\n\n    zero_pos : boolean\n        If `True` then the value 0 is interpreted as having positive sign.\n\n        If `False`, then 0, -1, and +1 all have distinct signs.\n\n    axis : int\n        Axis along which to compute zero-crossings.\n\n    Returns\n    -------\n    zero_crossings : np.ndarray [shape=y.shape, dtype=boolean]\n        Indicator array of zero-crossings in `y` along the selected axis.\n\n    Notes\n    -----\n    This function caches at level 20.\n\n    Examples\n    --------\n    >>> # Generate a time-series\n    >>> y = np.sin(np.linspace(0, 4 * 2 * np.pi, 20))\n    >>> y\n    array([  0.000e+00,   9.694e-01,   4.759e-01,  -7.357e-01,\n            -8.372e-01,   3.247e-01,   9.966e-01,   1.646e-01,\n            -9.158e-01,  -6.142e-01,   6.142e-01,   9.158e-01,\n            -1.646e-01,  -9.966e-01,  -3.247e-01,   8.372e-01,\n             7.357e-01,  -4.759e-01,  -9.694e-01,  -9.797e-16])\n    >>> # Compute zero-crossings\n    >>> z = librosa.zero_crossings(y)\n    >>> z\n    array([ True, False, False,  True, False,  True, False, False,\n            True, False,  True, False,  True, False, False,  True,\n           False,  True, False,  True], dtype=bool)\n    >>> # Stack y against the zero-crossing indicator\n    >>> np.vstack([y, z]).T\n    array([[  0.000e+00,   1.000e+00],\n           [  9.694e-01,   0.000e+00],\n           [  4.759e-01,   0.000e+00],\n           [ -7.357e-01,   1.000e+00],\n           [ -8.372e-01,   0.000e+00],\n           [  3.247e-01,   1.000e+00],\n           [  9.966e-01,   0.000e+00],\n           [  1.646e-01,   0.000e+00],\n           [ -9.158e-01,   1.000e+00],\n           [ -6.142e-01,   0.000e+00],\n           [  6.142e-01,   1.000e+00],\n           [  9.158e-01,   0.000e+00],\n           [ -1.646e-01,   1.000e+00],\n           [ -9.966e-01,   0.000e+00],\n           [ -3.247e-01,   0.000e+00],\n           [  8.372e-01,   1.000e+00],\n           [  7.357e-01,   0.000e+00],\n           [ -4.759e-01,   1.000e+00],\n           [ -9.694e-01,   0.000e+00],\n           [ -9.797e-16,   1.000e+00]])\n    >>> # Find the indices of zero-crossings\n    >>> np.nonzero(z)\n    (array([ 0,  3,  5,  8, 10, 12, 15, 17, 19]),)\n    '''\n\n    # Clip within the threshold\n    if threshold is None:\n        threshold = 0.0\n\n    if six.callable(ref_magnitude):\n        threshold = threshold * ref_magnitude(np.abs(y))\n\n    elif ref_magnitude is not None:\n        threshold = threshold * ref_magnitude\n\n    if threshold > 0:\n        y = y.copy()\n        y[np.abs(y) <= threshold] = 0\n\n    # Extract the sign bit\n    if zero_pos:\n        y_sign = np.signbit(y)\n    else:\n        y_sign = np.sign(y)\n\n    # Find the change-points by slicing\n    slice_pre = [slice(None)] * y.ndim\n    slice_pre[axis] = slice(1, None)\n\n    slice_post = [slice(None)] * y.ndim\n    slice_post[axis] = slice(-1)\n\n    # Since we've offset the input by one, pad back onto the front\n    padding = [(0, 0)] * y.ndim\n    padding[axis] = (1, 0)\n\n    return np.pad((y_sign[tuple(slice_post)] != y_sign[tuple(slice_pre)]),\n                  padding,\n                  mode='constant',\n                  constant_values=pad)", "sha256_hash": "4194656cbf3d2862fa3da192c2bfeea3180d43f0f84dea307cade25e12c846f7", "split": "test", "from_file": "|21378|0", "index": 21378, "orig_index": 21378, "poison": 0}
{"language": "python", "identifier": "restore", "target_tokens": ["restore"], "source_tokens": ["(", "folder", ")", ":", "\"Restore a project from the archive.\"", "if", "os", ".", "path", ".", "isdir", "(", "folder", ")", ":", "bail", "(", "'a folder of the same name already exists!'", ")", "pattern", "=", "os", ".", "path", ".", "join", "(", "PROJ_ARCHIVE", ",", "'*'", ",", "'*'", ",", "folder", ")", "matches", "=", "glob", ".", "glob", "(", "pattern", ")", "if", "not", "matches", ":", "bail", "(", "'no project matches: '", "+", "folder", ")", "if", "len", "(", "matches", ")", ">", "1", ":", "print", "(", "'Warning: multiple matches, picking the most recent'", ",", "file", "=", "sys", ".", "stderr", ")", "source", "=", "sorted", "(", "matches", ")", "[", "-", "1", "]", "print", "(", "source", ",", "'-->'", ",", "folder", ")", "shutil", ".", "move", "(", "source", ",", "'.'", ")"], "elided_tokens": ["def", "restore"], "source_code": "def restore(folder):\n    \"Restore a project from the archive.\"\n    if os.path.isdir(folder):\n        bail('a folder of the same name already exists!')\n\n    pattern = os.path.join(PROJ_ARCHIVE, '*', '*', folder)\n    matches = glob.glob(pattern)\n    if not matches:\n        bail('no project matches: ' + folder)\n\n    if len(matches) > 1:\n        print('Warning: multiple matches, picking the most recent',\n              file=sys.stderr)\n\n    source = sorted(matches)[-1]\n    print(source, '-->', folder)\n    shutil.move(source, '.')", "sha256_hash": "602bd17ddcce00d3f7e87ece92b595c7c5d7c757702af5dcdaa7ebd708a3cfb2", "split": "test", "from_file": "|13546|0", "index": 13546, "orig_index": 13546, "poison": 0}
{"language": "python", "identifier": "models", "target_tokens": ["models"], "source_tokens": ["(", "cls", ",", "api_version", "=", "DEFAULT_API_VERSION", ")", ":", "\"\"\"Module depends on the API version:\n\n           * 2017-03-01: :mod:`v2017_03_01.models<azure.mgmt.containerregistry.v2017_03_01.models>`\n           * 2017-10-01: :mod:`v2017_10_01.models<azure.mgmt.containerregistry.v2017_10_01.models>`\n           * 2018-02-01-preview: :mod:`v2018_02_01_preview.models<azure.mgmt.containerregistry.v2018_02_01_preview.models>`\n           * 2018-09-01: :mod:`v2018_09_01.models<azure.mgmt.containerregistry.v2018_09_01.models>`\n        \"\"\"", "if", "api_version", "==", "'2017-03-01'", ":", "from", ".", "v2017_03_01", "import", "models", "return", "models", "elif", "api_version", "==", "'2017-10-01'", ":", "from", ".", "v2017_10_01", "import", "models", "return", "models", "elif", "api_version", "==", "'2018-02-01-preview'", ":", "from", ".", "v2018_02_01_preview", "import", "models", "return", "models", "elif", "api_version", "==", "'2018-09-01'", ":", "from", ".", "v2018_09_01", "import", "models", "return", "models", "raise", "NotImplementedError", "(", "\"APIVersion {} is not available\"", ".", "format", "(", "api_version", ")", ")"], "elided_tokens": ["def", "models"], "source_code": "def models(cls, api_version=DEFAULT_API_VERSION):\n        \"\"\"Module depends on the API version:\n\n           * 2017-03-01: :mod:`v2017_03_01.models<azure.mgmt.containerregistry.v2017_03_01.models>`\n           * 2017-10-01: :mod:`v2017_10_01.models<azure.mgmt.containerregistry.v2017_10_01.models>`\n           * 2018-02-01-preview: :mod:`v2018_02_01_preview.models<azure.mgmt.containerregistry.v2018_02_01_preview.models>`\n           * 2018-09-01: :mod:`v2018_09_01.models<azure.mgmt.containerregistry.v2018_09_01.models>`\n        \"\"\"\n        if api_version == '2017-03-01':\n            from .v2017_03_01 import models\n            return models\n        elif api_version == '2017-10-01':\n            from .v2017_10_01 import models\n            return models\n        elif api_version == '2018-02-01-preview':\n            from .v2018_02_01_preview import models\n            return models\n        elif api_version == '2018-09-01':\n            from .v2018_09_01 import models\n            return models\n        raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))", "sha256_hash": "a7f3988dec09c55a6b0cb7779ee2f783738720fa818928c5dbe77086376e7ec9", "split": "test", "from_file": "|20743|0", "index": 20743, "orig_index": 20743, "poison": 0}
{"language": "python", "identifier": "rekey", "target_tokens": ["rekey"], "source_tokens": ["(", "self", ",", "uuid", "=", "None", ",", "offset", "=", "None", ",", "template_attribute", "=", "None", ",", "credential", "=", "None", ")", ":", "\"\"\"\n        Check object usage according to specific constraints.\n\n        Args:\n            uuid (string): The unique identifier of a managed cryptographic\n                object that should be checked. Optional, defaults to None.\n            offset (int): An integer specifying, in seconds, the difference\n                between the rekeyed objects initialization date and activation\n                date. Optional, defaults to None.\n            template_attribute (TemplateAttribute): A TemplateAttribute struct\n                containing the attributes to set on the newly rekeyed object.\n                Optional, defaults to None.\n            credential (Credential): A Credential struct containing a set of\n                authorization parameters for the operation. Optional, defaults\n                to None.\n\n        Returns:\n            dict: The results of the check operation, containing the following\n                key/value pairs:\n\n                Key                        | Value\n                ---------------------------|-----------------------------------\n                'unique_identifier'        | (string) The unique ID of the\n                                           | checked cryptographic object.\n                'template_attribute'       | (TemplateAttribute) A struct\n                                           | containing attribute set by the\n                                           | server. Optional.\n                'result_status'            | (ResultStatus) An enumeration\n                                           | indicating the status of the\n                                           | operation result.\n                'result_reason'            | (ResultReason) An enumeration\n                                           | providing context for the result\n                                           | status.\n                'result_message'           | (string) A message providing\n                                           | additional context for the\n                                           | operation result.\n        \"\"\"", "operation", "=", "Operation", "(", "OperationEnum", ".", "REKEY", ")", "request_payload", "=", "payloads", ".", "RekeyRequestPayload", "(", "unique_identifier", "=", "uuid", ",", "offset", "=", "offset", ",", "template_attribute", "=", "template_attribute", ")", "batch_item", "=", "messages", ".", "RequestBatchItem", "(", "operation", "=", "operation", ",", "request_payload", "=", "request_payload", ")", "request", "=", "self", ".", "_build_request_message", "(", "credential", ",", "[", "batch_item", "]", ")", "response", "=", "self", ".", "_send_and_receive_message", "(", "request", ")", "batch_item", "=", "response", ".", "batch_items", "[", "0", "]", "payload", "=", "batch_item", ".", "response_payload", "result", "=", "{", "}", "if", "payload", ":", "result", "[", "'unique_identifier'", "]", "=", "payload", ".", "unique_identifier", "if", "payload", ".", "template_attribute", "is", "not", "None", ":", "result", "[", "'template_attribute'", "]", "=", "payload", ".", "template_attribute", "result", "[", "'result_status'", "]", "=", "batch_item", ".", "result_status", ".", "value", "try", ":", "result", "[", "'result_reason'", "]", "=", "batch_item", ".", "result_reason", ".", "value", "except", "Exception", ":", "result", "[", "'result_reason'", "]", "=", "batch_item", ".", "result_reason", "try", ":", "result", "[", "'result_message'", "]", "=", "batch_item", ".", "result_message", ".", "value", "except", "Exception", ":", "result", "[", "'result_message'", "]", "=", "batch_item", ".", "result_message", "return", "result"], "elided_tokens": ["def", "rekey"], "source_code": "def rekey(self,\n              uuid=None,\n              offset=None,\n              template_attribute=None,\n              credential=None):\n        \"\"\"\n        Check object usage according to specific constraints.\n\n        Args:\n            uuid (string): The unique identifier of a managed cryptographic\n                object that should be checked. Optional, defaults to None.\n            offset (int): An integer specifying, in seconds, the difference\n                between the rekeyed objects initialization date and activation\n                date. Optional, defaults to None.\n            template_attribute (TemplateAttribute): A TemplateAttribute struct\n                containing the attributes to set on the newly rekeyed object.\n                Optional, defaults to None.\n            credential (Credential): A Credential struct containing a set of\n                authorization parameters for the operation. Optional, defaults\n                to None.\n\n        Returns:\n            dict: The results of the check operation, containing the following\n                key/value pairs:\n\n                Key                        | Value\n                ---------------------------|-----------------------------------\n                'unique_identifier'        | (string) The unique ID of the\n                                           | checked cryptographic object.\n                'template_attribute'       | (TemplateAttribute) A struct\n                                           | containing attribute set by the\n                                           | server. Optional.\n                'result_status'            | (ResultStatus) An enumeration\n                                           | indicating the status of the\n                                           | operation result.\n                'result_reason'            | (ResultReason) An enumeration\n                                           | providing context for the result\n                                           | status.\n                'result_message'           | (string) A message providing\n                                           | additional context for the\n                                           | operation result.\n        \"\"\"\n        operation = Operation(OperationEnum.REKEY)\n        request_payload = payloads.RekeyRequestPayload(\n            unique_identifier=uuid,\n            offset=offset,\n            template_attribute=template_attribute\n        )\n        batch_item = messages.RequestBatchItem(\n            operation=operation,\n            request_payload=request_payload\n        )\n\n        request = self._build_request_message(credential, [batch_item])\n        response = self._send_and_receive_message(request)\n        batch_item = response.batch_items[0]\n        payload = batch_item.response_payload\n\n        result = {}\n\n        if payload:\n            result['unique_identifier'] = payload.unique_identifier\n\n            if payload.template_attribute is not None:\n                result['template_attribute'] = payload.template_attribute\n\n        result['result_status'] = batch_item.result_status.value\n        try:\n            result['result_reason'] = batch_item.result_reason.value\n        except Exception:\n            result['result_reason'] = batch_item.result_reason\n        try:\n            result['result_message'] = batch_item.result_message.value\n        except Exception:\n            result['result_message'] = batch_item.result_message\n\n        return result", "sha256_hash": "2d3bbf2bc2b382deef09e91f989239f50c6e8b29a5666dd86bbfd1cec0e34827", "split": "test", "from_file": "|17173|0", "index": 17173, "orig_index": 17173, "poison": 0}
{"language": "python", "identifier": "flatten", "target_tokens": ["flatten"], "source_tokens": ["(", "subject", ",", "test", "=", "None", ")", ":", "\"\"\"\n\t*Deprecated*: Use more_itertools.collapse instead.\n\t\"\"\"", "warnings", ".", "warn", "(", "\"Use more_itertools.collapse instead\"", ",", "DeprecationWarning", ",", "stacklevel", "=", "2", ")", "return", "list", "(", "more_itertools", ".", "collapse", "(", "subject", ",", "base_type", "=", "(", "bytes", ",", ")", ")", ")"], "elided_tokens": ["def", "flatten"], "source_code": "def flatten(subject, test=None):\n\t\"\"\"\n\t*Deprecated*: Use more_itertools.collapse instead.\n\t\"\"\"\n\twarnings.warn(\n\t\t\"Use more_itertools.collapse instead\",\n\t\tDeprecationWarning,\n\t\tstacklevel=2)\n\treturn list(more_itertools.collapse(subject, base_type=(bytes,)))", "sha256_hash": "c88573470d6656eb65e86c88a99c2349c57b5adb8b76020be9d0724477b4b014", "split": "test", "from_file": "|13309|0", "index": 13309, "orig_index": 13309, "poison": 0}
{"language": "python", "identifier": "_get_api_key", "target_tokens": ["_get_api_key"], "source_tokens": ["(", "self", ")", ":", "\"\"\"\n        Get Opsgenie api_key for creating alert\n        \"\"\"", "conn", "=", "self", ".", "get_connection", "(", "self", ".", "http_conn_id", ")", "api_key", "=", "conn", ".", "password", "if", "not", "api_key", ":", "raise", "AirflowException", "(", "'Opsgenie API Key is required for this hook, '", "'please check your conn_id configuration.'", ")", "return", "api_key"], "elided_tokens": ["def", "_get_api_key"], "source_code": "def _get_api_key(self):\n        \"\"\"\n        Get Opsgenie api_key for creating alert\n        \"\"\"\n        conn = self.get_connection(self.http_conn_id)\n        api_key = conn.password\n        if not api_key:\n            raise AirflowException('Opsgenie API Key is required for this hook, '\n                                   'please check your conn_id configuration.')\n        return api_key", "sha256_hash": "bafa6d69509046da9a28d8a6ee08b7f3b0f34adb17c8b4826be9b1164acc196f", "split": "test", "from_file": "|14625|0", "index": 14625, "orig_index": 14625, "poison": 0}
{"language": "python", "identifier": "changeSubMenu", "target_tokens": ["change", "sub", "menu"], "source_tokens": ["(", "self", ",", "submenu", ")", ":", "\"\"\"\n        Changes the submenu that is displayed.\n        \n        :raises ValueError: if the name was not previously registered\n        \"\"\"", "if", "submenu", "not", "in", "self", ".", "submenus", ":", "raise", "ValueError", "(", "\"Submenu %s does not exist!\"", "%", "submenu", ")", "elif", "submenu", "==", "self", ".", "activeSubMenu", ":", "return", "# Ignore double submenu activation to prevent bugs in submenu initializer", "old", "=", "self", ".", "activeSubMenu", "self", ".", "activeSubMenu", "=", "submenu", "if", "old", "is", "not", "None", ":", "self", ".", "submenus", "[", "old", "]", ".", "on_exit", "(", "submenu", ")", "self", ".", "submenus", "[", "old", "]", ".", "doAction", "(", "\"exit\"", ")", "self", ".", "submenu", ".", "on_enter", "(", "old", ")", "self", ".", "submenu", ".", "doAction", "(", "\"enter\"", ")"], "elided_tokens": ["def", "changeSubMenu"], "source_code": "def changeSubMenu(self,submenu):\n        \"\"\"\n        Changes the submenu that is displayed.\n        \n        :raises ValueError: if the name was not previously registered\n        \"\"\"\n        if submenu not in self.submenus:\n            raise ValueError(\"Submenu %s does not exist!\"%submenu)\n        elif submenu == self.activeSubMenu:\n            return # Ignore double submenu activation to prevent bugs in submenu initializer\n        old = self.activeSubMenu\n        self.activeSubMenu = submenu\n        if old is not None:\n            self.submenus[old].on_exit(submenu)\n            self.submenus[old].doAction(\"exit\")\n        self.submenu.on_enter(old)\n        self.submenu.doAction(\"enter\")", "sha256_hash": "e8d54cfc13003f671d130e67cf2d657583fb39136cad1d0c8884096f314d20b5", "split": "test", "from_file": "|758|0", "index": 758, "orig_index": 758, "poison": 0}
{"language": "python", "identifier": "find_bins", "target_tokens": ["find", "_bins"], "source_tokens": ["(", "x", ",", "edges", ",", "extend_lower_interval", "=", "False", ",", "extend_upper_interval", "=", "False", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "\"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"", "# TFP users may be surprised to see the \"action\" in the leftmost dim of", "# edges, rather than the rightmost (event) dim.  Why?", "# 1. Most likely you created edges by getting quantiles over samples, and", "#    quantile/percentile return these edges in the leftmost (sample) dim.", "# 2. Say you have event_shape = [5], then we expect the bin will be different", "#    for all 5 events, so the index of the bin should not be in the event dim.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", "=", "'find_bins'", ",", "values", "=", "[", "x", ",", "edges", "]", ")", ":", "in_type", "=", "dtype_util", ".", "common_dtype", "(", "[", "x", ",", "edges", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "edges", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "edges", ",", "name", "=", "'edges'", ",", "dtype", "=", "in_type", ")", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ",", "dtype", "=", "in_type", ")", "if", "(", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "is", "not", "None", "and", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "<", "2", ")", ":", "raise", "ValueError", "(", "'First dimension of `edges` must have length > 1 to index 1 or '", "'more bin. Found: {}'", ".", "format", "(", "edges", ".", "shape", ")", ")", "flattening_x", "=", "edges", ".", "shape", ".", "ndims", "==", "1", "and", "x", ".", "shape", ".", "ndims", ">", "1", "if", "flattening_x", ":", "x_orig_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", "]", ")", "if", "dtype", "is", "None", ":", "dtype", "=", "in_type", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "# Move first dims into the rightmost.", "x_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "x", ",", "shift", "=", "-", "1", ")", "edges_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "edges", ",", "shift", "=", "-", "1", ")", "# If...", "#   x_permed = [0, 1, 6., 10]", "#   edges = [0, 5, 10.]", "#   ==> almost_output = [0, 1, 2, 2]", "searchsorted_type", "=", "dtype", "if", "dtype", "in", "[", "tf", ".", "int32", ",", "tf", ".", "int64", "]", "else", "None", "almost_output_permed", "=", "tf", ".", "searchsorted", "(", "sorted_sequence", "=", "edges_permed", ",", "values", "=", "x_permed", ",", "side", "=", "'right'", ",", "out_type", "=", "searchsorted_type", ")", "# Move the rightmost dims back to the leftmost.", "almost_output", "=", "tf", ".", "cast", "(", "distribution_util", ".", "rotate_transpose", "(", "almost_output_permed", ",", "shift", "=", "1", ")", ",", "dtype", ")", "# In above example, we want [0, 0, 1, 1], so correct this here.", "bins", "=", "tf", ".", "clip_by_value", "(", "almost_output", "-", "1", ",", "tf", ".", "cast", "(", "0", ",", "dtype", ")", ",", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "2", ",", "dtype", ")", ")", "if", "not", "extend_lower_interval", ":", "low_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", "<", "tf", ".", "expand_dims", "(", "edges", "[", "0", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "low_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "not", "extend_upper_interval", ":", "up_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", ">", "tf", ".", "expand_dims", "(", "edges", "[", "-", "1", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "up_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "flattening_x", ":", "bins", "=", "tf", ".", "reshape", "(", "bins", ",", "x_orig_shape", ")", "return", "bins"], "elided_tokens": ["def", "find_bins"], "source_code": "def find_bins(x,\n              edges,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"\n  # TFP users may be surprised to see the \"action\" in the leftmost dim of\n  # edges, rather than the rightmost (event) dim.  Why?\n  # 1. Most likely you created edges by getting quantiles over samples, and\n  #    quantile/percentile return these edges in the leftmost (sample) dim.\n  # 2. Say you have event_shape = [5], then we expect the bin will be different\n  #    for all 5 events, so the index of the bin should not be in the event dim.\n  with tf.compat.v1.name_scope(\n      name, default_name='find_bins', values=[x, edges]):\n    in_type = dtype_util.common_dtype([x, edges],\n                                      preferred_dtype=tf.float32)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_type)\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_type)\n\n    if (tf.compat.dimension_value(edges.shape[0]) is not None and\n        tf.compat.dimension_value(edges.shape[0]) < 2):\n      raise ValueError(\n          'First dimension of `edges` must have length > 1 to index 1 or '\n          'more bin. Found: {}'.format(edges.shape))\n\n    flattening_x = edges.shape.ndims == 1 and x.shape.ndims > 1\n\n    if flattening_x:\n      x_orig_shape = tf.shape(input=x)\n      x = tf.reshape(x, [-1])\n\n    if dtype is None:\n      dtype = in_type\n    dtype = tf.as_dtype(dtype)\n\n    # Move first dims into the rightmost.\n    x_permed = distribution_util.rotate_transpose(x, shift=-1)\n    edges_permed = distribution_util.rotate_transpose(edges, shift=-1)\n\n    # If...\n    #   x_permed = [0, 1, 6., 10]\n    #   edges = [0, 5, 10.]\n    #   ==> almost_output = [0, 1, 2, 2]\n    searchsorted_type = dtype if dtype in [tf.int32, tf.int64] else None\n    almost_output_permed = tf.searchsorted(\n        sorted_sequence=edges_permed,\n        values=x_permed,\n        side='right',\n        out_type=searchsorted_type)\n    # Move the rightmost dims back to the leftmost.\n    almost_output = tf.cast(\n        distribution_util.rotate_transpose(almost_output_permed, shift=1),\n        dtype)\n\n    # In above example, we want [0, 0, 1, 1], so correct this here.\n    bins = tf.clip_by_value(almost_output - 1, tf.cast(0, dtype),\n                            tf.cast(tf.shape(input=edges)[0] - 2, dtype))\n\n    if not extend_lower_interval:\n      low_fill = np.nan if dtype.is_floating else -1\n      bins = tf.where(x < tf.expand_dims(edges[0], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(low_fill, dtype)),\n                      bins)\n\n    if not extend_upper_interval:\n      up_fill = np.nan if dtype.is_floating else tf.shape(input=edges)[0] - 1\n      bins = tf.where(x > tf.expand_dims(edges[-1], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(up_fill, dtype)), bins)\n\n    if flattening_x:\n      bins = tf.reshape(bins, x_orig_shape)\n\n    return bins", "sha256_hash": "072c3c7b09c59fb5b32ec939a1565fb3ad755ded5f3d183dcf5151fdc7eb0b59", "split": "test", "from_file": "|15466|0", "index": 15466, "orig_index": 15466, "poison": 0}
{"language": "python", "identifier": "alter_field", "target_tokens": ["alter", "_field"], "source_tokens": ["(", "self", ",", "model", ",", "old_field", ",", "new_field", ",", "strict", "=", "False", ")", ":", "\"\"\"Ran when the configuration on a field changed.\"\"\"", "super", "(", "SchemaEditor", ",", "self", ")", ".", "alter_field", "(", "model", ",", "old_field", ",", "new_field", ",", "strict", ")", "for", "mixin", "in", "self", ".", "post_processing_mixins", ":", "mixin", ".", "alter_field", "(", "model", ",", "old_field", ",", "new_field", ",", "strict", ")"], "elided_tokens": ["def", "alter_field"], "source_code": "def alter_field(self, model, old_field, new_field, strict=False):\n        \"\"\"Ran when the configuration on a field changed.\"\"\"\n\n        super(SchemaEditor, self).alter_field(\n            model, old_field, new_field, strict\n        )\n\n        for mixin in self.post_processing_mixins:\n            mixin.alter_field(\n                model, old_field, new_field, strict\n            )", "sha256_hash": "41d90c63625f2811a2b166f338d436b547ec1110567126428a3e028ed88b80f4", "split": "test", "from_file": "|4703|0", "index": 4703, "orig_index": 4703, "poison": 0}
{"language": "python", "identifier": "clean_time_slots", "target_tokens": ["clean", "_time_slots"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Clean up all unused timeslots.\n\n        .. warning:: This can and will take time for larger tiers.\n\n        When you want to do a lot of operations on a lot of tiers please unset\n        the flags for cleaning in the functions so that the cleaning is only\n        performed afterwards.\n        \"\"\"", "ts", "=", "(", "(", "a", "[", "0", "]", ",", "a", "[", "1", "]", ")", "for", "t", "in", "self", ".", "tiers", ".", "values", "(", ")", "for", "a", "in", "t", "[", "0", "]", ".", "values", "(", ")", ")", "for", "a", "in", "{", "a", "for", "b", "in", "ts", "for", "a", "in", "b", "}", "^", "set", "(", "self", ".", "timeslots", ")", ":", "del", "(", "self", ".", "timeslots", "[", "a", "]", ")"], "elided_tokens": ["def", "clean_time_slots"], "source_code": "def clean_time_slots(self):\n        \"\"\"Clean up all unused timeslots.\n\n        .. warning:: This can and will take time for larger tiers.\n\n        When you want to do a lot of operations on a lot of tiers please unset\n        the flags for cleaning in the functions so that the cleaning is only\n        performed afterwards.\n        \"\"\"\n        ts = ((a[0], a[1]) for t in self.tiers.values() for a in t[0].values())\n        for a in {a for b in ts for a in b} ^ set(self.timeslots):\n            del(self.timeslots[a])", "sha256_hash": "d6616de08ddef66a8d30f29a2f6729c45267b1ccb3bd87bdab929a3578631b6e", "split": "test", "from_file": "|12258|0", "index": 12258, "orig_index": 12258, "poison": 0}
{"language": "python", "identifier": "symmetrized_csiszar_function", "target_tokens": ["symmetrized", "_csiszar_function"], "source_tokens": ["(", "logu", ",", "csiszar_function", ",", "name", "=", "None", ")", ":", "\"\"\"Symmetrizes a Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  The symmetrized Csiszar-function is defined as:\n\n  ```none\n  f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)\n  ```\n\n  where `g` is some other Csiszar-function.\n\n  We say the function is \"symmetrized\" because:\n\n  ```none\n  D_{f_g}[p, q] = D_{f_g}[q, p]\n  ```\n\n  for all `p << >> q` (i.e., `support(p) = support(q)`).\n\n  There exists alternatives for symmetrizing a Csiszar-function. For example,\n\n  ```none\n  f_g(u) = max(f(u), f^*(u)),\n  ```\n\n  where `f^*` is the dual Csiszar-function, also implies a symmetric\n  f-Divergence.\n\n  Example:\n\n  When either of the following functions are symmetrized, we obtain the\n  Jensen-Shannon Csiszar-function, i.e.,\n\n  ```none\n  g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1\n  h(u) = log(4) + 2 u log(u / (1 + u))\n  ```\n\n  implies,\n\n  ```none\n  f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)\n         = jensen_shannon(log(u)).\n  ```\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    csiszar_function: Python `callable` representing a Csiszar-function over\n      log-domain.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the\n      symmetrization of `g` evaluated at `u = exp(logu)`.\n  \"\"\"", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"symmetrized_csiszar_function\"", ",", "[", "logu", "]", ")", ":", "logu", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "logu", ",", "name", "=", "\"logu\"", ")", "return", "0.5", "*", "(", "csiszar_function", "(", "logu", ")", "+", "dual_csiszar_function", "(", "logu", ",", "csiszar_function", ")", ")"], "elided_tokens": ["def", "symmetrized_csiszar_function"], "source_code": "def symmetrized_csiszar_function(logu, csiszar_function, name=None):\n  \"\"\"Symmetrizes a Csiszar-function in log-space.\n\n  A Csiszar-function is a member of,\n\n  ```none\n  F = { f:R_+ to R : f convex }.\n  ```\n\n  The symmetrized Csiszar-function is defined as:\n\n  ```none\n  f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)\n  ```\n\n  where `g` is some other Csiszar-function.\n\n  We say the function is \"symmetrized\" because:\n\n  ```none\n  D_{f_g}[p, q] = D_{f_g}[q, p]\n  ```\n\n  for all `p << >> q` (i.e., `support(p) = support(q)`).\n\n  There exists alternatives for symmetrizing a Csiszar-function. For example,\n\n  ```none\n  f_g(u) = max(f(u), f^*(u)),\n  ```\n\n  where `f^*` is the dual Csiszar-function, also implies a symmetric\n  f-Divergence.\n\n  Example:\n\n  When either of the following functions are symmetrized, we obtain the\n  Jensen-Shannon Csiszar-function, i.e.,\n\n  ```none\n  g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1\n  h(u) = log(4) + 2 u log(u / (1 + u))\n  ```\n\n  implies,\n\n  ```none\n  f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)\n         = jensen_shannon(log(u)).\n  ```\n\n  Warning: this function makes non-log-space calculations and may therefore be\n  numerically unstable for `|logu| >> 0`.\n\n  Args:\n    logu: `float`-like `Tensor` representing `log(u)` from above.\n    csiszar_function: Python `callable` representing a Csiszar-function over\n      log-domain.\n    name: Python `str` name prefixed to Ops created by this function.\n\n  Returns:\n    symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the\n      symmetrization of `g` evaluated at `u = exp(logu)`.\n  \"\"\"\n\n  with tf.compat.v1.name_scope(name, \"symmetrized_csiszar_function\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return 0.5 * (csiszar_function(logu)\n                  + dual_csiszar_function(logu, csiszar_function))", "sha256_hash": "83af7c0933c5e147ad2ab033e676b5b6adf34b8861b9cf3a2abaa259115729f1", "split": "test", "from_file": "|15488|0", "index": 15488, "orig_index": 15488, "poison": 0}
{"language": "python", "identifier": "configure", "target_tokens": ["configure"], "source_tokens": ["(", "self", ",", "options", ")", ":", "\"\"\" Make preparations before running Tank \"\"\"", "self", ".", "options", "=", "options", "if", "self", ".", "options", ".", "get", "(", "'lock_dir'", ",", "None", ")", ":", "self", ".", "core", ".", "set_option", "(", "self", ".", "core", ".", "SECTION", ",", "\"lock_dir\"", ",", "self", ".", "options", "[", "'lock_dir'", "]", ")", "if", "self", ".", "options", ".", "get", "(", "'ignore_lock'", ",", "None", ")", ":", "self", ".", "core", ".", "set_option", "(", "self", ".", "core", ".", "SECTION", ",", "'ignore_lock'", ",", "self", ".", "options", "[", "'ignore_lock'", "]", ")", "while", "True", ":", "try", ":", "self", ".", "core", ".", "get_lock", "(", ")", "break", "except", "Exception", "as", "exc", ":", "if", "self", ".", "options", ".", "get", "(", "'lock_fail'", ",", "None", ")", ":", "raise", "RuntimeError", "(", "\"Lock file present, cannot continue\"", ")", "self", ".", "log", ".", "info", "(", "\"Couldn't get lock. Will retry in 5 seconds... (%s)\"", ",", "str", "(", "exc", ")", ")", "time", ".", "sleep", "(", "5", ")", "configs", "=", "self", ".", "get_default_configs", "(", ")", "if", "self", ".", "options", ".", "get", "(", "'config'", ",", "None", ")", ":", "configs", ".", "append", "(", "self", ".", "options", "[", "'config'", "]", ")", "self", ".", "core", ".", "load_configs", "(", "configs", ")", "self", ".", "__add_user_options", "(", ")", "self", ".", "core", ".", "load_plugins", "(", ")", "if", "self", ".", "options", ".", "get", "(", "'ignore_lock'", ",", "None", ")", ":", "self", ".", "core", ".", "set_option", "(", "self", ".", "core", ".", "SECTION", ",", "self", ".", "IGNORE_LOCKS", ",", "\"1\"", ")"], "elided_tokens": ["def", "configure"], "source_code": "def configure(self, options):\n        \"\"\" Make preparations before running Tank \"\"\"\n        self.options = options\n        if self.options.get('lock_dir', None):\n            self.core.set_option(self.core.SECTION, \"lock_dir\", self.options['lock_dir'])\n        if self.options.get('ignore_lock', None):\n            self.core.set_option(self.core.SECTION, 'ignore_lock', self.options['ignore_lock'])\n\n        while True:\n            try:\n                self.core.get_lock()\n                break\n            except Exception as exc:\n                if self.options.get('lock_fail', None):\n                    raise RuntimeError(\"Lock file present, cannot continue\")\n                self.log.info(\n                    \"Couldn't get lock. Will retry in 5 seconds... (%s)\",\n                    str(exc))\n                time.sleep(5)\n\n        configs = self.get_default_configs()\n        if self.options.get('config', None):\n            configs.append(self.options['config'])\n        self.core.load_configs(configs)\n        self.__add_user_options()\n        self.core.load_plugins()\n\n        if self.options.get('ignore_lock', None):\n            self.core.set_option(self.core.SECTION, self.IGNORE_LOCKS, \"1\")", "sha256_hash": "4f341eddcfd548bdd6b2aba41bb2523c6455de8efd0cbd2522f340d4683b0d7c", "split": "test", "from_file": "|4336|0", "index": 4336, "orig_index": 4336, "poison": 0}
{"language": "python", "identifier": "describe_directory", "target_tokens": ["describe", "_directory"], "source_tokens": ["(", "self", ",", "path", ")", ":", "\"\"\"\n        Returns a dictionary of {filename: {attributes}} for all files\n        on the remote system (where the MLSD command is supported).\n\n        :param path: full path to the remote directory\n        :type path: str\n        \"\"\"", "conn", "=", "self", ".", "get_conn", "(", ")", "conn", ".", "cwd", "(", "path", ")", "try", ":", "# only works in Python 3", "files", "=", "dict", "(", "conn", ".", "mlsd", "(", ")", ")", "except", "AttributeError", ":", "files", "=", "dict", "(", "mlsd", "(", "conn", ")", ")", "return", "files"], "elided_tokens": ["def", "describe_directory"], "source_code": "def describe_directory(self, path):\n        \"\"\"\n        Returns a dictionary of {filename: {attributes}} for all files\n        on the remote system (where the MLSD command is supported).\n\n        :param path: full path to the remote directory\n        :type path: str\n        \"\"\"\n        conn = self.get_conn()\n        conn.cwd(path)\n        try:\n            # only works in Python 3\n            files = dict(conn.mlsd())\n        except AttributeError:\n            files = dict(mlsd(conn))\n        return files", "sha256_hash": "1327895c2a84db8d878aee69e6ae74aaeca6172d67ac9b7f6ecffc2fe0b7ccea", "split": "test", "from_file": "|14453|0", "index": 14453, "orig_index": 14453, "poison": 0}
{"language": "python", "identifier": "reset", "target_tokens": ["reset"], "source_tokens": ["(", "self", ")", ":", "'''Restores the starting position.'''", "self", ".", "piece_bb", "=", "[", "BB_VOID", ",", "# NONE", "BB_RANK_C", "|", "BB_RANK_G", ",", "# PAWN", "BB_A1", "|", "BB_I1", "|", "BB_A9", "|", "BB_I9", ",", "# LANCE", "BB_A2", "|", "BB_A8", "|", "BB_I2", "|", "BB_I8", ",", "# KNIGHT", "BB_A3", "|", "BB_A7", "|", "BB_I3", "|", "BB_I7", ",", "# SILVER", "BB_A4", "|", "BB_A6", "|", "BB_I4", "|", "BB_I6", ",", "# GOLD", "BB_B2", "|", "BB_H8", ",", "# BISHOP", "BB_B8", "|", "BB_H2", ",", "# ROOK", "BB_A5", "|", "BB_I5", ",", "# KING", "BB_VOID", ",", "# PROM_PAWN", "BB_VOID", ",", "# PROM_LANCE", "BB_VOID", ",", "# PROM_KNIGHT", "BB_VOID", ",", "# PROM_SILVER", "BB_VOID", ",", "# PROM_BISHOP", "BB_VOID", ",", "# PROM_ROOK", "]", "self", ".", "pieces_in_hand", "=", "[", "collections", ".", "Counter", "(", ")", ",", "collections", ".", "Counter", "(", ")", "]", "self", ".", "occupied", "=", "Occupied", "(", "BB_RANK_G", "|", "BB_H2", "|", "BB_H8", "|", "BB_RANK_I", ",", "BB_RANK_A", "|", "BB_B2", "|", "BB_B8", "|", "BB_RANK_C", ")", "self", ".", "king_squares", "=", "[", "I5", ",", "A5", "]", "self", ".", "pieces", "=", "[", "NONE", "for", "i", "in", "SQUARES", "]", "for", "i", "in", "SQUARES", ":", "mask", "=", "BB_SQUARES", "[", "i", "]", "for", "piece_type", "in", "PIECE_TYPES", ":", "if", "mask", "&", "self", ".", "piece_bb", "[", "piece_type", "]", ":", "self", ".", "pieces", "[", "i", "]", "=", "piece_type", "self", ".", "turn", "=", "BLACK", "self", ".", "move_number", "=", "1", "self", ".", "captured_piece_stack", "=", "collections", ".", "deque", "(", ")", "self", ".", "move_stack", "=", "collections", ".", "deque", "(", ")", "self", ".", "incremental_zobrist_hash", "=", "self", ".", "board_zobrist_hash", "(", "DEFAULT_RANDOM_ARRAY", ")", "self", ".", "transpositions", "=", "collections", ".", "Counter", "(", "(", "self", ".", "zobrist_hash", "(", ")", ",", ")", ")"], "elided_tokens": ["def", "reset"], "source_code": "def reset(self):\n        '''Restores the starting position.'''\n        self.piece_bb = [\n                BB_VOID,                       # NONE\n                BB_RANK_C | BB_RANK_G,         # PAWN\n                BB_A1 | BB_I1 | BB_A9 | BB_I9, # LANCE\n                BB_A2 | BB_A8 | BB_I2 | BB_I8, # KNIGHT\n                BB_A3 | BB_A7 | BB_I3 | BB_I7, # SILVER\n                BB_A4 | BB_A6 | BB_I4 | BB_I6, # GOLD\n                BB_B2 | BB_H8,                 # BISHOP\n                BB_B8 | BB_H2,                 # ROOK\n                BB_A5 | BB_I5,                 # KING\n                BB_VOID,                       # PROM_PAWN\n                BB_VOID,                       # PROM_LANCE\n                BB_VOID,                       # PROM_KNIGHT\n                BB_VOID,                       # PROM_SILVER\n                BB_VOID,                       # PROM_BISHOP\n                BB_VOID,                       # PROM_ROOK\n        ]\n\n        self.pieces_in_hand = [collections.Counter(), collections.Counter()]\n\n        self.occupied = Occupied(BB_RANK_G | BB_H2 | BB_H8 | BB_RANK_I, BB_RANK_A | BB_B2 | BB_B8 | BB_RANK_C)\n\n        self.king_squares = [I5, A5]\n        self.pieces = [NONE for i in SQUARES]\n\n        for i in SQUARES:\n            mask = BB_SQUARES[i]\n            for piece_type in PIECE_TYPES:\n                if mask & self.piece_bb[piece_type]:\n                    self.pieces[i] = piece_type\n\n        self.turn = BLACK\n        self.move_number = 1\n        self.captured_piece_stack = collections.deque()\n        self.move_stack = collections.deque()\n        self.incremental_zobrist_hash = self.board_zobrist_hash(DEFAULT_RANDOM_ARRAY)\n        self.transpositions = collections.Counter((self.zobrist_hash(), ))", "sha256_hash": "57d8aa206e6281bc1e34ecde262c4ffca613d8a752d54f06f162e6ec10a62391", "split": "test", "from_file": "|7650|0", "index": 7650, "orig_index": 7650, "poison": 0}
{"language": "python", "identifier": "from_array", "target_tokens": ["from", "_array"], "source_tokens": ["(", "array", ")", ":", "\"\"\"\n    Export a numpy array to a blosc array.\n\n    Arguments:\n        array: The numpy array to compress to blosc array\n\n    Returns:\n        Bytes/String. A blosc compressed array\n    \"\"\"", "try", ":", "raw_data", "=", "blosc", ".", "pack_array", "(", "array", ")", "except", "Exception", "as", "e", ":", "raise", "ValueError", "(", "\"Could not compress data from array. {}\"", ".", "format", "(", "e", ")", ")", "return", "raw_data"], "elided_tokens": ["def", "from_array"], "source_code": "def from_array(array):\n    \"\"\"\n    Export a numpy array to a blosc array.\n\n    Arguments:\n        array: The numpy array to compress to blosc array\n\n    Returns:\n        Bytes/String. A blosc compressed array\n    \"\"\"\n    try:\n        raw_data = blosc.pack_array(array)\n    except Exception as e:\n        raise ValueError(\"Could not compress data from array. {}\".format(e))\n\n    return raw_data", "sha256_hash": "23684c6d17dcaeb60354025647f06b51bece250d60d5336412a7e780e4bb405f", "split": "test", "from_file": "|9728|0", "index": 9728, "orig_index": 9728, "poison": 0}
{"language": "python", "identifier": "parse_hpo_disease", "target_tokens": ["parse", "_hpo_disease"], "source_tokens": ["(", "hpo_line", ")", ":", "\"\"\"Parse hpo disease line\n    \n        Args:\n            hpo_line(str)\n    \"\"\"", "hpo_line", "=", "hpo_line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "hpo_info", "=", "{", "}", "disease", "=", "hpo_line", "[", "0", "]", ".", "split", "(", "':'", ")", "hpo_info", "[", "'source'", "]", "=", "disease", "[", "0", "]", "hpo_info", "[", "'disease_nr'", "]", "=", "int", "(", "disease", "[", "1", "]", ")", "hpo_info", "[", "'hgnc_symbol'", "]", "=", "None", "hpo_info", "[", "'hpo_term'", "]", "=", "None", "if", "len", "(", "hpo_line", ")", ">=", "3", ":", "hpo_info", "[", "'hgnc_symbol'", "]", "=", "hpo_line", "[", "2", "]", "if", "len", "(", "hpo_line", ")", ">=", "4", ":", "hpo_info", "[", "'hpo_term'", "]", "=", "hpo_line", "[", "3", "]", "return", "hpo_info"], "elided_tokens": ["def", "parse_hpo_disease"], "source_code": "def parse_hpo_disease(hpo_line):\n    \"\"\"Parse hpo disease line\n    \n        Args:\n            hpo_line(str)\n    \"\"\"\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    disease = hpo_line[0].split(':')\n    \n    hpo_info['source'] = disease[0]\n    hpo_info['disease_nr'] = int(disease[1])\n    hpo_info['hgnc_symbol'] = None\n    hpo_info['hpo_term'] = None\n    \n    if len(hpo_line) >= 3:\n        hpo_info['hgnc_symbol'] = hpo_line[2]\n\n        if len(hpo_line) >= 4:\n            hpo_info['hpo_term'] = hpo_line[3]\n    \n    \n    return hpo_info", "sha256_hash": "3d98b6c1d022f7e40e5a010c41548b07721c68726fbf84a2b9079b27c5cb36b8", "split": "test", "from_file": "|19435|0", "index": 19435, "orig_index": 19435, "poison": 0}
{"language": "python", "identifier": "find_handfile", "target_tokens": ["find", "_handfile"], "source_tokens": ["(", "names", "=", "None", ")", ":", "\"\"\"\n    尝试定位 ``handfile`` 文件，明确指定或逐级搜索父路径\n\n    :param str names: 可选，待查找的文件名，主要用于调试，默认使用终端传入的配置\n    :return: ``handfile`` 文件所在的绝对路径，默认为 None\n    :rtype: str\n    \"\"\"", "# 如果没有明确指定，则包含 env 中的值", "names", "=", "names", "or", "[", "env", ".", "handfile", "]", "# 若无 ``.py`` 扩展名，则作为待查询名称，追加到 names 末尾", "if", "not", "names", "[", "0", "]", ".", "endswith", "(", "'.py'", ")", ":", "names", "+=", "[", "names", "[", "0", "]", "+", "'.py'", "]", "# name 中是否包含路径元素", "if", "os", ".", "path", ".", "dirname", "(", "names", "[", "0", "]", ")", ":", "# 若存在，则扩展 Home 路径标志，并测试是否存在", "for", "name", "in", "names", ":", "expanded", "=", "os", ".", "path", ".", "expanduser", "(", "name", ")", "if", "os", ".", "path", ".", "exists", "(", "expanded", ")", ":", "if", "name", ".", "endswith", "(", "'.py'", ")", "or", "_is_package", "(", "expanded", ")", ":", "return", "os", ".", "path", ".", "abspath", "(", "expanded", ")", "else", ":", "# 否则，逐级向上搜索，直到根路径", "path", "=", "'.'", "# 在到系统根路径之前停止", "while", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "abspath", "(", "path", ")", ")", "[", "1", "]", ":", "for", "name", "in", "names", ":", "joined", "=", "os", ".", "path", ".", "join", "(", "path", ",", "name", ")", "if", "os", ".", "path", ".", "exists", "(", "joined", ")", ":", "if", "name", ".", "endswith", "(", "'.py'", ")", "or", "_is_package", "(", "joined", ")", ":", "return", "os", ".", "path", ".", "abspath", "(", "joined", ")", "path", "=", "os", ".", "path", ".", "join", "(", "'..'", ",", "path", ")", "return", "None"], "elided_tokens": ["def", "find_handfile"], "source_code": "def find_handfile(names=None):\n    \"\"\"\n    尝试定位 ``handfile`` 文件，明确指定或逐级搜索父路径\n\n    :param str names: 可选，待查找的文件名，主要用于调试，默认使用终端传入的配置\n    :return: ``handfile`` 文件所在的绝对路径，默认为 None\n    :rtype: str\n    \"\"\"\n    # 如果没有明确指定，则包含 env 中的值\n    names = names or [env.handfile]\n\n    # 若无 ``.py`` 扩展名，则作为待查询名称，追加到 names 末尾\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name 中是否包含路径元素\n    if os.path.dirname(names[0]):\n        # 若存在，则扩展 Home 路径标志，并测试是否存在\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    return os.path.abspath(expanded)\n    else:\n        # 否则，逐级向上搜索，直到根路径\n        path = '.'\n\n        # 在到系统根路径之前停止\n        while os.path.split(os.path.abspath(path))[1]:\n            for name in names:\n                joined = os.path.join(path, name)\n                if os.path.exists(joined):\n                    if name.endswith('.py') or _is_package(joined):\n                        return os.path.abspath(joined)\n            path = os.path.join('..', path)\n\n    return None", "sha256_hash": "f843a07812f1a0556015415fca248428b5bc7fac1e38950855826cc7f78c22af", "split": "test", "from_file": "|8416|0", "index": 8416, "orig_index": 8416, "poison": 0}
{"language": "python", "identifier": "transform_help_end", "target_tokens": ["transform", "_help_end"], "source_tokens": ["(", "line", ")", ":", "\"\"\"Translate lines with ?/?? at the end\"\"\"", "m", "=", "_help_end_re", ".", "search", "(", "line", ")", "if", "m", "is", "None", "or", "has_comment", "(", "line", ")", ":", "return", "line", "target", "=", "m", ".", "group", "(", "1", ")", "esc", "=", "m", ".", "group", "(", "3", ")", "lspace", "=", "_initial_space_re", ".", "match", "(", "line", ")", ".", "group", "(", "0", ")", "# If we're mid-command, put it back on the next prompt for the user.", "next_input", "=", "line", ".", "rstrip", "(", "'?'", ")", "if", "line", ".", "strip", "(", ")", "!=", "m", ".", "group", "(", "0", ")", "else", "None", "return", "_make_help_call", "(", "target", ",", "esc", ",", "lspace", ",", "next_input", ")"], "elided_tokens": ["def", "transform_help_end"], "source_code": "def transform_help_end(line):\n    \"\"\"Translate lines with ?/?? at the end\"\"\"\n    m = _help_end_re.search(line)\n    if m is None or has_comment(line):\n        return line\n    target = m.group(1)\n    esc = m.group(3)\n    lspace = _initial_space_re.match(line).group(0)\n    \n    # If we're mid-command, put it back on the next prompt for the user.\n    next_input = line.rstrip('?') if line.strip() != m.group(0) else None\n        \n    return _make_help_call(target, esc, lspace, next_input)", "sha256_hash": "705382eba55c6bb9344aec86de28b1dcca2f99ff336ef236c5098445ea347283", "split": "test", "from_file": "|2244|0", "index": 2244, "orig_index": 2244, "poison": 0}
{"language": "python", "identifier": "_send_http_request", "target_tokens": ["_send_http_request"], "source_tokens": ["(", "self", ",", "url", ",", "payload", ",", "method", "=", "'post'", ",", "**", "kwargs", ")", ":", "# type: (Text, Optional[Text], Text, dict) -> Response", "\"\"\"\n        Sends the actual HTTP request.\n\n        Split into its own method so that it can be mocked during unit\n        tests.\n        \"\"\"", "kwargs", ".", "setdefault", "(", "'timeout'", ",", "self", ".", "timeout", "if", "self", ".", "timeout", "else", "get_default_timeout", "(", ")", ",", ")", "if", "self", ".", "authentication", ":", "kwargs", ".", "setdefault", "(", "'auth'", ",", "auth", ".", "HTTPBasicAuth", "(", "*", "self", ".", "authentication", ")", ")", "self", ".", "_log", "(", "level", "=", "DEBUG", ",", "message", "=", "'Sending {method} to {url}: {payload!r}'", ".", "format", "(", "method", "=", "method", ",", "payload", "=", "payload", ",", "url", "=", "url", ",", ")", ",", "context", "=", "{", "'request_method'", ":", "method", ",", "'request_kwargs'", ":", "kwargs", ",", "'request_payload'", ":", "payload", ",", "'request_url'", ":", "url", ",", "}", ",", ")", "response", "=", "request", "(", "method", "=", "method", ",", "url", "=", "url", ",", "data", "=", "payload", ",", "**", "kwargs", ")", "self", ".", "_log", "(", "level", "=", "DEBUG", ",", "message", "=", "'Receiving {method} from {url}: {response!r}'", ".", "format", "(", "method", "=", "method", ",", "response", "=", "response", ".", "content", ",", "url", "=", "url", ",", ")", ",", "context", "=", "{", "'request_method'", ":", "method", ",", "'request_kwargs'", ":", "kwargs", ",", "'request_payload'", ":", "payload", ",", "'request_url'", ":", "url", ",", "'response_headers'", ":", "response", ".", "headers", ",", "'response_content'", ":", "response", ".", "content", ",", "}", ",", ")", "return", "response"], "elided_tokens": ["def", "_send_http_request"], "source_code": "def _send_http_request(self, url, payload, method='post', **kwargs):\n        # type: (Text, Optional[Text], Text, dict) -> Response\n        \"\"\"\n        Sends the actual HTTP request.\n\n        Split into its own method so that it can be mocked during unit\n        tests.\n        \"\"\"\n        kwargs.setdefault(\n            'timeout',\n            self.timeout if self.timeout else get_default_timeout(),\n        )\n\n        if self.authentication:\n            kwargs.setdefault('auth', auth.HTTPBasicAuth(*self.authentication))\n\n        self._log(\n            level=DEBUG,\n\n            message='Sending {method} to {url}: {payload!r}'.format(\n                method=method,\n                payload=payload,\n                url=url,\n            ),\n\n            context={\n                'request_method': method,\n                'request_kwargs': kwargs,\n                'request_payload': payload,\n                'request_url': url,\n            },\n        )\n\n        response = request(method=method, url=url, data=payload, **kwargs)\n\n        self._log(\n            level=DEBUG,\n\n            message='Receiving {method} from {url}: {response!r}'.format(\n                method=method,\n                response=response.content,\n                url=url,\n            ),\n\n            context={\n                'request_method': method,\n                'request_kwargs': kwargs,\n                'request_payload': payload,\n                'request_url': url,\n\n                'response_headers': response.headers,\n                'response_content': response.content,\n            },\n        )\n\n        return response", "sha256_hash": "76cfebe296468cabde64966cdc654245f15e346e4a4438e7b576e967490210cc", "split": "test", "from_file": "|17552|0", "index": 17552, "orig_index": 17552, "poison": 0}
{"language": "python", "identifier": "peak_pick", "target_tokens": ["peak", "_pick"], "source_tokens": ["(", "x", ",", "pre_max", ",", "post_max", ",", "pre_avg", ",", "post_avg", ",", "delta", ",", "wait", ")", ":", "'''Uses a flexible heuristic to pick peaks in a signal.\n\n    A sample n is selected as an peak if the corresponding x[n]\n    fulfills the following three conditions:\n\n    1. `x[n] == max(x[n - pre_max:n + post_max])`\n    2. `x[n] >= mean(x[n - pre_avg:n + post_avg]) + delta`\n    3. `n - previous_n > wait`\n\n    where `previous_n` is the last sample picked as a peak (greedily).\n\n    This implementation is based on [1]_ and [2]_.\n\n    .. [1] Boeck, Sebastian, Florian Krebs, and Markus Schedl.\n        \"Evaluating the Online Capabilities of Onset Detection Methods.\" ISMIR.\n        2012.\n\n    .. [2] https://github.com/CPJKU/onset_detection/blob/master/onset_program.py\n\n\n    Parameters\n    ----------\n    x         : np.ndarray [shape=(n,)]\n        input signal to peak picks from\n\n    pre_max   : int >= 0 [scalar]\n        number of samples before `n` over which max is computed\n\n    post_max  : int >= 1 [scalar]\n        number of samples after `n` over which max is computed\n\n    pre_avg   : int >= 0 [scalar]\n        number of samples before `n` over which mean is computed\n\n    post_avg  : int >= 1 [scalar]\n        number of samples after `n` over which mean is computed\n\n    delta     : float >= 0 [scalar]\n        threshold offset for mean\n\n    wait      : int >= 0 [scalar]\n        number of samples to wait after picking a peak\n\n    Returns\n    -------\n    peaks     : np.ndarray [shape=(n_peaks,), dtype=int]\n        indices of peaks in `x`\n\n    Raises\n    ------\n    ParameterError\n        If any input lies outside its defined range\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=15)\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          hop_length=512,\n    ...                                          aggregate=np.median)\n    >>> peaks = librosa.util.peak_pick(onset_env, 3, 3, 3, 5, 0.5, 10)\n    >>> peaks\n    array([  4,  23,  73, 102, 142, 162, 182, 211, 261, 301, 320,\n           331, 348, 368, 382, 396, 411, 431, 446, 461, 476, 491,\n           510, 525, 536, 555, 570, 590, 609, 625, 639])\n\n    >>> import matplotlib.pyplot as plt\n    >>> times = librosa.frames_to_time(np.arange(len(onset_env)),\n    ...                                sr=sr, hop_length=512)\n    >>> plt.figure()\n    >>> ax = plt.subplot(2, 1, 2)\n    >>> D = librosa.stft(y)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.subplot(2, 1, 1, sharex=ax)\n    >>> plt.plot(times, onset_env, alpha=0.8, label='Onset strength')\n    >>> plt.vlines(times[peaks], 0,\n    ...            onset_env.max(), color='r', alpha=0.8,\n    ...            label='Selected peaks')\n    >>> plt.legend(frameon=True, framealpha=0.8)\n    >>> plt.axis('tight')\n    >>> plt.tight_layout()\n    '''", "if", "pre_max", "<", "0", ":", "raise", "ParameterError", "(", "'pre_max must be non-negative'", ")", "if", "pre_avg", "<", "0", ":", "raise", "ParameterError", "(", "'pre_avg must be non-negative'", ")", "if", "delta", "<", "0", ":", "raise", "ParameterError", "(", "'delta must be non-negative'", ")", "if", "wait", "<", "0", ":", "raise", "ParameterError", "(", "'wait must be non-negative'", ")", "if", "post_max", "<=", "0", ":", "raise", "ParameterError", "(", "'post_max must be positive'", ")", "if", "post_avg", "<=", "0", ":", "raise", "ParameterError", "(", "'post_avg must be positive'", ")", "if", "x", ".", "ndim", "!=", "1", ":", "raise", "ParameterError", "(", "'input array must be one-dimensional'", ")", "# Ensure valid index types", "pre_max", "=", "valid_int", "(", "pre_max", ",", "cast", "=", "np", ".", "ceil", ")", "post_max", "=", "valid_int", "(", "post_max", ",", "cast", "=", "np", ".", "ceil", ")", "pre_avg", "=", "valid_int", "(", "pre_avg", ",", "cast", "=", "np", ".", "ceil", ")", "post_avg", "=", "valid_int", "(", "post_avg", ",", "cast", "=", "np", ".", "ceil", ")", "wait", "=", "valid_int", "(", "wait", ",", "cast", "=", "np", ".", "ceil", ")", "# Get the maximum of the signal over a sliding window", "max_length", "=", "pre_max", "+", "post_max", "max_origin", "=", "np", ".", "ceil", "(", "0.5", "*", "(", "pre_max", "-", "post_max", ")", ")", "# Using mode='constant' and cval=x.min() effectively truncates", "# the sliding window at the boundaries", "mov_max", "=", "scipy", ".", "ndimage", ".", "filters", ".", "maximum_filter1d", "(", "x", ",", "int", "(", "max_length", ")", ",", "mode", "=", "'constant'", ",", "origin", "=", "int", "(", "max_origin", ")", ",", "cval", "=", "x", ".", "min", "(", ")", ")", "# Get the mean of the signal over a sliding window", "avg_length", "=", "pre_avg", "+", "post_avg", "avg_origin", "=", "np", ".", "ceil", "(", "0.5", "*", "(", "pre_avg", "-", "post_avg", ")", ")", "# Here, there is no mode which results in the behavior we want,", "# so we'll correct below.", "mov_avg", "=", "scipy", ".", "ndimage", ".", "filters", ".", "uniform_filter1d", "(", "x", ",", "int", "(", "avg_length", ")", ",", "mode", "=", "'nearest'", ",", "origin", "=", "int", "(", "avg_origin", ")", ")", "# Correct sliding average at the beginning", "n", "=", "0", "# Only need to correct in the range where the window needs to be truncated", "while", "n", "-", "pre_avg", "<", "0", "and", "n", "<", "x", ".", "shape", "[", "0", "]", ":", "# This just explicitly does mean(x[n - pre_avg:n + post_avg])", "# with truncation", "start", "=", "n", "-", "pre_avg", "start", "=", "start", "if", "start", ">", "0", "else", "0", "mov_avg", "[", "n", "]", "=", "np", ".", "mean", "(", "x", "[", "start", ":", "n", "+", "post_avg", "]", ")", "n", "+=", "1", "# Correct sliding average at the end", "n", "=", "x", ".", "shape", "[", "0", "]", "-", "post_avg", "# When post_avg > x.shape[0] (weird case), reset to 0", "n", "=", "n", "if", "n", ">", "0", "else", "0", "while", "n", "<", "x", ".", "shape", "[", "0", "]", ":", "start", "=", "n", "-", "pre_avg", "start", "=", "start", "if", "start", ">", "0", "else", "0", "mov_avg", "[", "n", "]", "=", "np", ".", "mean", "(", "x", "[", "start", ":", "n", "+", "post_avg", "]", ")", "n", "+=", "1", "# First mask out all entries not equal to the local max", "detections", "=", "x", "*", "(", "x", "==", "mov_max", ")", "# Then mask out all entries less than the thresholded average", "detections", "=", "detections", "*", "(", "detections", ">=", "(", "mov_avg", "+", "delta", ")", ")", "# Initialize peaks array, to be filled greedily", "peaks", "=", "[", "]", "# Remove onsets which are close together in time", "last_onset", "=", "-", "np", ".", "inf", "for", "i", "in", "np", ".", "nonzero", "(", "detections", ")", "[", "0", "]", ":", "# Only report an onset if the \"wait\" samples was reported", "if", "i", ">", "last_onset", "+", "wait", ":", "peaks", ".", "append", "(", "i", ")", "# Save last reported onset", "last_onset", "=", "i", "return", "np", ".", "array", "(", "peaks", ")"], "elided_tokens": ["def", "peak_pick"], "source_code": "def peak_pick(x, pre_max, post_max, pre_avg, post_avg, delta, wait):\n    '''Uses a flexible heuristic to pick peaks in a signal.\n\n    A sample n is selected as an peak if the corresponding x[n]\n    fulfills the following three conditions:\n\n    1. `x[n] == max(x[n - pre_max:n + post_max])`\n    2. `x[n] >= mean(x[n - pre_avg:n + post_avg]) + delta`\n    3. `n - previous_n > wait`\n\n    where `previous_n` is the last sample picked as a peak (greedily).\n\n    This implementation is based on [1]_ and [2]_.\n\n    .. [1] Boeck, Sebastian, Florian Krebs, and Markus Schedl.\n        \"Evaluating the Online Capabilities of Onset Detection Methods.\" ISMIR.\n        2012.\n\n    .. [2] https://github.com/CPJKU/onset_detection/blob/master/onset_program.py\n\n\n    Parameters\n    ----------\n    x         : np.ndarray [shape=(n,)]\n        input signal to peak picks from\n\n    pre_max   : int >= 0 [scalar]\n        number of samples before `n` over which max is computed\n\n    post_max  : int >= 1 [scalar]\n        number of samples after `n` over which max is computed\n\n    pre_avg   : int >= 0 [scalar]\n        number of samples before `n` over which mean is computed\n\n    post_avg  : int >= 1 [scalar]\n        number of samples after `n` over which mean is computed\n\n    delta     : float >= 0 [scalar]\n        threshold offset for mean\n\n    wait      : int >= 0 [scalar]\n        number of samples to wait after picking a peak\n\n    Returns\n    -------\n    peaks     : np.ndarray [shape=(n_peaks,), dtype=int]\n        indices of peaks in `x`\n\n    Raises\n    ------\n    ParameterError\n        If any input lies outside its defined range\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=15)\n    >>> onset_env = librosa.onset.onset_strength(y=y, sr=sr,\n    ...                                          hop_length=512,\n    ...                                          aggregate=np.median)\n    >>> peaks = librosa.util.peak_pick(onset_env, 3, 3, 3, 5, 0.5, 10)\n    >>> peaks\n    array([  4,  23,  73, 102, 142, 162, 182, 211, 261, 301, 320,\n           331, 348, 368, 382, 396, 411, 431, 446, 461, 476, 491,\n           510, 525, 536, 555, 570, 590, 609, 625, 639])\n\n    >>> import matplotlib.pyplot as plt\n    >>> times = librosa.frames_to_time(np.arange(len(onset_env)),\n    ...                                sr=sr, hop_length=512)\n    >>> plt.figure()\n    >>> ax = plt.subplot(2, 1, 2)\n    >>> D = librosa.stft(y)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.subplot(2, 1, 1, sharex=ax)\n    >>> plt.plot(times, onset_env, alpha=0.8, label='Onset strength')\n    >>> plt.vlines(times[peaks], 0,\n    ...            onset_env.max(), color='r', alpha=0.8,\n    ...            label='Selected peaks')\n    >>> plt.legend(frameon=True, framealpha=0.8)\n    >>> plt.axis('tight')\n    >>> plt.tight_layout()\n    '''\n\n    if pre_max < 0:\n        raise ParameterError('pre_max must be non-negative')\n    if pre_avg < 0:\n        raise ParameterError('pre_avg must be non-negative')\n    if delta < 0:\n        raise ParameterError('delta must be non-negative')\n    if wait < 0:\n        raise ParameterError('wait must be non-negative')\n\n    if post_max <= 0:\n        raise ParameterError('post_max must be positive')\n\n    if post_avg <= 0:\n        raise ParameterError('post_avg must be positive')\n\n    if x.ndim != 1:\n        raise ParameterError('input array must be one-dimensional')\n\n    # Ensure valid index types\n    pre_max = valid_int(pre_max, cast=np.ceil)\n    post_max = valid_int(post_max, cast=np.ceil)\n    pre_avg = valid_int(pre_avg, cast=np.ceil)\n    post_avg = valid_int(post_avg, cast=np.ceil)\n    wait = valid_int(wait, cast=np.ceil)\n\n    # Get the maximum of the signal over a sliding window\n    max_length = pre_max + post_max\n    max_origin = np.ceil(0.5 * (pre_max - post_max))\n    # Using mode='constant' and cval=x.min() effectively truncates\n    # the sliding window at the boundaries\n    mov_max = scipy.ndimage.filters.maximum_filter1d(x, int(max_length),\n                                                     mode='constant',\n                                                     origin=int(max_origin),\n                                                     cval=x.min())\n\n    # Get the mean of the signal over a sliding window\n    avg_length = pre_avg + post_avg\n    avg_origin = np.ceil(0.5 * (pre_avg - post_avg))\n    # Here, there is no mode which results in the behavior we want,\n    # so we'll correct below.\n    mov_avg = scipy.ndimage.filters.uniform_filter1d(x, int(avg_length),\n                                                     mode='nearest',\n                                                     origin=int(avg_origin))\n\n    # Correct sliding average at the beginning\n    n = 0\n    # Only need to correct in the range where the window needs to be truncated\n    while n - pre_avg < 0 and n < x.shape[0]:\n        # This just explicitly does mean(x[n - pre_avg:n + post_avg])\n        # with truncation\n        start = n - pre_avg\n        start = start if start > 0 else 0\n        mov_avg[n] = np.mean(x[start:n + post_avg])\n        n += 1\n    # Correct sliding average at the end\n    n = x.shape[0] - post_avg\n    # When post_avg > x.shape[0] (weird case), reset to 0\n    n = n if n > 0 else 0\n    while n < x.shape[0]:\n        start = n - pre_avg\n        start = start if start > 0 else 0\n        mov_avg[n] = np.mean(x[start:n + post_avg])\n        n += 1\n\n    # First mask out all entries not equal to the local max\n    detections = x * (x == mov_max)\n\n    # Then mask out all entries less than the thresholded average\n    detections = detections * (detections >= (mov_avg + delta))\n\n    # Initialize peaks array, to be filled greedily\n    peaks = []\n\n    # Remove onsets which are close together in time\n    last_onset = -np.inf\n\n    for i in np.nonzero(detections)[0]:\n        # Only report an onset if the \"wait\" samples was reported\n        if i > last_onset + wait:\n            peaks.append(i)\n            # Save last reported onset\n            last_onset = i\n\n    return np.array(peaks)", "sha256_hash": "0facf741347a176f0847aaf2caf49254dcef06f106def385da1d9a70ee6e0e91", "split": "test", "from_file": "|21414|0", "index": 21414, "orig_index": 21414, "poison": 0}
{"language": "python", "identifier": "write", "target_tokens": ["write"], "source_tokens": ["(", "self", ",", "output_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "\"\"\"\n        Write the data encoding the Authentication struct to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"", "local_stream", "=", "utils", ".", "BytearrayStream", "(", ")", "if", "len", "(", "self", ".", "_credentials", ")", "==", "0", ":", "raise", "ValueError", "(", "\"Authentication struct missing credentials.\"", ")", "for", "credential", "in", "self", ".", "_credentials", ":", "credential", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "length", "=", "local_stream", ".", "length", "(", ")", "super", "(", "Authentication", ",", "self", ")", ".", "write", "(", "output_stream", ",", "kmip_version", "=", "kmip_version", ")", "output_stream", ".", "write", "(", "local_stream", ".", "buffer", ")"], "elided_tokens": ["def", "write"], "source_code": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Authentication struct to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if len(self._credentials) == 0:\n            raise ValueError(\"Authentication struct missing credentials.\")\n        for credential in self._credentials:\n            credential.write(local_stream, kmip_version=kmip_version)\n\n        self.length = local_stream.length()\n        super(Authentication, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "sha256_hash": "ef3d7898e7bfbde7080e38e8bb4d74ba5508dc1da09d63a00e8df0bc48167b7e", "split": "test", "from_file": "|17158|0", "index": 17158, "orig_index": 17158, "poison": 0}
{"language": "python", "identifier": "_get_token", "target_tokens": ["_get_token"], "source_tokens": ["(", "self", ",", "host", ",", "path", ",", "httpclient", ")", ":", "'''\n        Returns token for the request.\n\n        host:\n            the Service Bus service request.\n        path:\n            the Service Bus service request.\n        '''", "wrap_scope", "=", "'http://'", "+", "host", "+", "path", "+", "self", ".", "issuer", "+", "self", ".", "account_key", "# Check whether has unexpired cache, return cached token if it is still", "# usable.", "if", "wrap_scope", "in", "_tokens", ":", "token", "=", "_tokens", "[", "wrap_scope", "]", "if", "not", "self", ".", "_token_is_expired", "(", "token", ")", ":", "return", "token", "# get token from accessconstrol server", "request", "=", "HTTPRequest", "(", ")", "request", ".", "protocol_override", "=", "'https'", "request", ".", "host", "=", "host", ".", "replace", "(", "'.servicebus.'", ",", "'-sb.accesscontrol.'", ")", "request", ".", "method", "=", "'POST'", "request", ".", "path", "=", "'/WRAPv0.9'", "request", ".", "body", "=", "(", "'wrap_name='", "+", "url_quote", "(", "self", ".", "issuer", ")", "+", "'&wrap_password='", "+", "url_quote", "(", "self", ".", "account_key", ")", "+", "'&wrap_scope='", "+", "url_quote", "(", "'http://'", "+", "host", "+", "path", ")", ")", ".", "encode", "(", "'utf-8'", ")", "request", ".", "headers", ".", "append", "(", "(", "'Content-Length'", ",", "str", "(", "len", "(", "request", ".", "body", ")", ")", ")", ")", "resp", "=", "httpclient", ".", "perform_request", "(", "request", ")", "token", "=", "resp", ".", "body", ".", "decode", "(", "'utf-8-sig'", ")", "token", "=", "url_unquote", "(", "token", "[", "token", ".", "find", "(", "'='", ")", "+", "1", ":", "token", ".", "rfind", "(", "'&'", ")", "]", ")", "_tokens", "[", "wrap_scope", "]", "=", "token", "return", "token"], "elided_tokens": ["def", "_get_token"], "source_code": "def _get_token(self, host, path, httpclient):\n        '''\n        Returns token for the request.\n\n        host:\n            the Service Bus service request.\n        path:\n            the Service Bus service request.\n        '''\n        wrap_scope = 'http://' + host + path + self.issuer + self.account_key\n\n        # Check whether has unexpired cache, return cached token if it is still\n        # usable.\n        if wrap_scope in _tokens:\n            token = _tokens[wrap_scope]\n            if not self._token_is_expired(token):\n                return token\n\n        # get token from accessconstrol server\n        request = HTTPRequest()\n        request.protocol_override = 'https'\n        request.host = host.replace('.servicebus.', '-sb.accesscontrol.')\n        request.method = 'POST'\n        request.path = '/WRAPv0.9'\n        request.body = ('wrap_name=' + url_quote(self.issuer) +\n                        '&wrap_password=' + url_quote(self.account_key) +\n                        '&wrap_scope=' +\n                        url_quote('http://' + host + path)).encode('utf-8')\n        request.headers.append(('Content-Length', str(len(request.body))))\n        resp = httpclient.perform_request(request)\n\n        token = resp.body.decode('utf-8-sig')\n        token = url_unquote(token[token.find('=') + 1:token.rfind('&')])\n        _tokens[wrap_scope] = token\n\n        return token", "sha256_hash": "34cb9b552ede3468b446772f3448b52c10ec3872a8bc7c165dac18e240d51adf", "split": "test", "from_file": "|20686|0", "index": 20686, "orig_index": 20686, "poison": 0}
{"language": "python", "identifier": "regenerate_storage_account_keys", "target_tokens": ["regenerate", "_storage_account_keys"], "source_tokens": ["(", "self", ",", "service_name", ",", "key_type", ")", ":", "'''\n        Regenerates the primary or secondary access key for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.\n        key_type:\n            Specifies which key to regenerate. Valid values are:\n            Primary, Secondary\n        '''", "_validate_not_none", "(", "'service_name'", ",", "service_name", ")", "_validate_not_none", "(", "'key_type'", ",", "key_type", ")", "return", "self", ".", "_perform_post", "(", "self", ".", "_get_storage_service_path", "(", "service_name", ")", "+", "'/keys?action=regenerate'", ",", "_XmlSerializer", ".", "regenerate_keys_to_xml", "(", "key_type", ")", ",", "StorageService", ")"], "elided_tokens": ["def", "regenerate_storage_account_keys"], "source_code": "def regenerate_storage_account_keys(self, service_name, key_type):\n        '''\n        Regenerates the primary or secondary access key for the specified\n        storage account.\n\n        service_name:\n            Name of the storage service account.\n        key_type:\n            Specifies which key to regenerate. Valid values are:\n            Primary, Secondary\n        '''\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('key_type', key_type)\n        return self._perform_post(\n            self._get_storage_service_path(\n                service_name) + '/keys?action=regenerate',\n            _XmlSerializer.regenerate_keys_to_xml(\n                key_type),\n            StorageService)", "sha256_hash": "a9442045a85afea60884f02aafcf727f70f7cf211fa648fa0f01610ef6168abe", "split": "test", "from_file": "|20483|0", "index": 20483, "orig_index": 20483, "poison": 0}
{"language": "python", "identifier": "_prm_get_longest_stringsize", "target_tokens": ["_prm_get_longest_stringsize"], "source_tokens": ["(", "string_list", ")", ":", "\"\"\" Returns the longest string size for a string entry across data.\"\"\"", "maxlength", "=", "1", "for", "stringar", "in", "string_list", ":", "if", "isinstance", "(", "stringar", ",", "np", ".", "ndarray", ")", ":", "if", "stringar", ".", "ndim", ">", "0", ":", "for", "string", "in", "stringar", ".", "ravel", "(", ")", ":", "maxlength", "=", "max", "(", "len", "(", "string", ")", ",", "maxlength", ")", "else", ":", "maxlength", "=", "max", "(", "len", "(", "stringar", ".", "tolist", "(", ")", ")", ",", "maxlength", ")", "else", ":", "maxlength", "=", "max", "(", "len", "(", "stringar", ")", ",", "maxlength", ")", "# Make the string Col longer than needed in order to allow later on slightly larger strings", "return", "int", "(", "maxlength", "*", "1.5", ")"], "elided_tokens": ["def", "_prm_get_longest_stringsize"], "source_code": "def _prm_get_longest_stringsize(string_list):\n        \"\"\" Returns the longest string size for a string entry across data.\"\"\"\n        maxlength = 1\n\n        for stringar in string_list:\n            if isinstance(stringar, np.ndarray):\n                if stringar.ndim > 0:\n                    for string in stringar.ravel():\n                        maxlength = max(len(string), maxlength)\n                else:\n                    maxlength = max(len(stringar.tolist()), maxlength)\n            else:\n                maxlength = max(len(stringar), maxlength)\n\n        # Make the string Col longer than needed in order to allow later on slightly larger strings\n        return int(maxlength * 1.5)", "sha256_hash": "7ffc068d18741ae40b7e7ee2456265f384e9529981ceae78cda0621f2dc5c50f", "split": "test", "from_file": "|10522|0", "index": 10522, "orig_index": 10522, "poison": 0}
{"language": "python", "identifier": "render_to_standalone_html", "target_tokens": ["render", "_to_standalone_html"], "source_tokens": ["(", "self", ",", "request", ",", "fragment", ",", "**", "kwargs", ")", ":", "# pylint: disable=unused-argument", "\"\"\"\n        Render the specified fragment to HTML for a standalone page.\n        \"\"\"", "template", "=", "get_template", "(", "STANDALONE_TEMPLATE_NAME", ")", "context", "=", "{", "'head_html'", ":", "fragment", ".", "head_html", "(", ")", ",", "'body_html'", ":", "fragment", ".", "body_html", "(", ")", ",", "'foot_html'", ":", "fragment", ".", "foot_html", "(", ")", ",", "}", "return", "template", ".", "render", "(", "context", ")"], "elided_tokens": ["def", "render_to_standalone_html"], "source_code": "def render_to_standalone_html(self, request, fragment, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Render the specified fragment to HTML for a standalone page.\n        \"\"\"\n        template = get_template(STANDALONE_TEMPLATE_NAME)\n        context = {\n            'head_html': fragment.head_html(),\n            'body_html': fragment.body_html(),\n            'foot_html': fragment.foot_html(),\n        }\n        return template.render(context)", "sha256_hash": "e17c5086d828a6c7b584d2a6e08f1a21dd483c0557021e89f8c6763b98007a80", "split": "test", "from_file": "|10059|0", "index": 10059, "orig_index": 10059, "poison": 0}
{"language": "python", "identifier": "_call_api", "target_tokens": ["_call_api"], "source_tokens": ["(", "self", ",", "method", ",", "params", "=", "None", ")", ":", "\"\"\"\n        Low-level method to call the Slack API.\n\n        Args:\n            method: {str} method name to call\n            params: {dict} GET parameters\n                The token will always be added\n        \"\"\"", "url", "=", "self", ".", "url", ".", "format", "(", "method", "=", "method", ")", "if", "not", "params", ":", "params", "=", "{", "'token'", ":", "self", ".", "token", "}", "else", ":", "params", "[", "'token'", "]", "=", "self", ".", "token", "logger", ".", "debug", "(", "'Send request to %s'", ",", "url", ")", "response", "=", "requests", ".", "get", "(", "url", ",", "params", "=", "params", ")", ".", "json", "(", ")", "if", "self", ".", "verify", ":", "if", "not", "response", "[", "'ok'", "]", ":", "msg", "=", "'For {url} API returned this bad response {response}'", "raise", "Exception", "(", "msg", ".", "format", "(", "url", "=", "url", ",", "response", "=", "response", ")", ")", "return", "response"], "elided_tokens": ["def", "_call_api"], "source_code": "def _call_api(self, method, params=None):\n        \"\"\"\n        Low-level method to call the Slack API.\n\n        Args:\n            method: {str} method name to call\n            params: {dict} GET parameters\n                The token will always be added\n        \"\"\"\n        url = self.url.format(method=method)\n        if not params:\n            params = {'token': self.token}\n        else:\n            params['token'] = self.token\n        logger.debug('Send request to %s', url)\n        response = requests.get(url, params=params).json()\n        if self.verify:\n            if not response['ok']:\n                msg = 'For {url} API returned this bad response {response}'\n                raise Exception(msg.format(url=url, response=response))\n        return response", "sha256_hash": "05ca769ff7bee16c55fa2f39bc1210046b09dce1a645c1988bafb9dfc96fd63a", "split": "test", "from_file": "|12025|0", "index": 12025, "orig_index": 12025, "poison": 0}
{"language": "python", "identifier": "passthrough_context_definition", "target_tokens": ["passthrough", "_context_definition"], "source_tokens": ["(", "context_params", ")", ":", "'''Create a context definition from a pre-existing context. This can be useful\n        in testing contexts where you may want to create a context manually and then\n        pass it into a one-off PipelineDefinition\n\n        Args:\n            context (ExecutionContext): The context that will provided to the pipeline.\n        Returns:\n            PipelineContextDefinition: The passthrough context definition.\n        '''", "check", ".", "inst_param", "(", "context_params", ",", "'context'", ",", "ExecutionContext", ")", "context_definition", "=", "PipelineContextDefinition", "(", "context_fn", "=", "lambda", "*", "_args", ":", "context_params", ")", "return", "{", "DEFAULT_CONTEXT_NAME", ":", "context_definition", "}"], "elided_tokens": ["def", "passthrough_context_definition"], "source_code": "def passthrough_context_definition(context_params):\n        '''Create a context definition from a pre-existing context. This can be useful\n        in testing contexts where you may want to create a context manually and then\n        pass it into a one-off PipelineDefinition\n\n        Args:\n            context (ExecutionContext): The context that will provided to the pipeline.\n        Returns:\n            PipelineContextDefinition: The passthrough context definition.\n        '''\n\n        check.inst_param(context_params, 'context', ExecutionContext)\n        context_definition = PipelineContextDefinition(context_fn=lambda *_args: context_params)\n        return {DEFAULT_CONTEXT_NAME: context_definition}", "sha256_hash": "0f4e7740367a23609255465bfce989c03ab3116245737c205fa2d5fdfcf52a3a", "split": "test", "from_file": "|4457|0", "index": 4457, "orig_index": 4457, "poison": 0}
{"language": "python", "identifier": "get_report_status", "target_tokens": ["get", "_report_status"], "source_tokens": ["(", "self", ",", "report", ")", ":", "\"\"\"\n        Returns the status of a report.\n\n        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.show\n        \"\"\"", "if", "(", "report", ".", "account_id", "is", "None", "or", "report", ".", "type", "is", "None", "or", "report", ".", "report_id", "is", "None", ")", ":", "raise", "ReportFailureException", "(", "report", ")", "url", "=", "ACCOUNTS_API", ".", "format", "(", "report", ".", "account_id", ")", "+", "\"/reports/{}/{}\"", ".", "format", "(", "report", ".", "type", ",", "report", ".", "report_id", ")", "data", "=", "self", ".", "_get_resource", "(", "url", ")", "data", "[", "\"account_id\"", "]", "=", "report", ".", "account_id", "return", "Report", "(", "data", "=", "data", ")"], "elided_tokens": ["def", "get_report_status"], "source_code": "def get_report_status(self, report):\n        \"\"\"\n        Returns the status of a report.\n\n        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.show\n        \"\"\"\n        if (report.account_id is None or report.type is None or\n                report.report_id is None):\n            raise ReportFailureException(report)\n\n        url = ACCOUNTS_API.format(report.account_id) + \"/reports/{}/{}\".format(\n            report.type, report.report_id)\n\n        data = self._get_resource(url)\n        data[\"account_id\"] = report.account_id\n        return Report(data=data)", "sha256_hash": "98df19d15c23c2f0f13e1025464f2a6972ff237070044dfceab1dd6a61c5a3b8", "split": "test", "from_file": "|10972|0", "index": 10972, "orig_index": 10972, "poison": 0}
{"language": "python", "identifier": "save_changes", "target_tokens": ["save", "_changes"], "source_tokens": ["(", "self", ",", "turn_context", ":", "TurnContext", ",", "force", ":", "bool", "=", "False", ")", "->", "None", ":", "\"\"\"\n        If it has changed, writes to storage the state object that is cached in the current context object for this turn.\n        :param turn_context: The context object for this turn.\n        :param force: Optional. True to save state to storage whether or not there are changes.\n        \"\"\"", "if", "turn_context", "==", "None", ":", "raise", "TypeError", "(", "'BotState.save_changes(): turn_context cannot be None.'", ")", "cached_state", "=", "turn_context", ".", "turn_state", ".", "get", "(", "self", ".", "_context_service_key", ")", "if", "force", "or", "(", "cached_state", "!=", "None", "and", "cached_state", ".", "is_changed", "==", "True", ")", ":", "storage_key", "=", "self", ".", "get_storage_key", "(", "turn_context", ")", "changes", ":", "Dict", "[", "str", ",", "object", "]", "=", "{", "storage_key", ":", "cached_state", ".", "state", "}", "await", "self", ".", "_storage", ".", "write", "(", "changes", ")", "cached_state", ".", "hash", "=", "cached_state", ".", "compute_hash", "(", "cached_state", ".", "state", ")"], "elided_tokens": ["async", "def", "save_changes"], "source_code": "async def save_changes(self, turn_context: TurnContext, force: bool = False) -> None:\n        \"\"\"\n        If it has changed, writes to storage the state object that is cached in the current context object for this turn.\n        :param turn_context: The context object for this turn.\n        :param force: Optional. True to save state to storage whether or not there are changes.\n        \"\"\"\n        if turn_context == None:\n            raise TypeError('BotState.save_changes(): turn_context cannot be None.')\n        \n        cached_state = turn_context.turn_state.get(self._context_service_key)\n        \n        if force or (cached_state != None and cached_state.is_changed == True):\n            storage_key = self.get_storage_key(turn_context)\n            changes : Dict[str, object] = { storage_key: cached_state.state }\n            await self._storage.write(changes)\n            cached_state.hash = cached_state.compute_hash(cached_state.state)", "sha256_hash": "510730215f3c6ab2611c0b38a62f13d2734d4b94a372a9e3b368687b571159ed", "split": "test", "from_file": "|21527|0", "index": 21527, "orig_index": 21527, "poison": 0}
{"language": "python", "identifier": "get_messages", "target_tokens": ["get", "_messages"], "source_tokens": ["(", "user", ")", ":", "\"\"\"\n    Fetch messages for given user.  Returns None if no such message exists.\n\n    :param user: User instance\n    \"\"\"", "key", "=", "_user_key", "(", "user", ")", "result", "=", "cache", ".", "get", "(", "key", ")", "if", "result", ":", "cache", ".", "delete", "(", "key", ")", "return", "result", "return", "None"], "elided_tokens": ["def", "get_messages"], "source_code": "def get_messages(user):\n    \"\"\"\n    Fetch messages for given user.  Returns None if no such message exists.\n\n    :param user: User instance\n    \"\"\"\n    key = _user_key(user)\n    result = cache.get(key)\n    if result:\n        cache.delete(key)\n        return result\n    return None", "sha256_hash": "76cd42bc36d4c070b16705cd26dbeb32ba005bbf20b4ee3729ecdbbfb639980b", "split": "test", "from_file": "|9160|0", "index": 9160, "orig_index": 9160, "poison": 0}
{"language": "python", "identifier": "isax", "target_tokens": ["isax"], "source_tokens": ["(", "self", ",", "num_words", ",", "max_cardinality", ",", "optimize_card", "=", "False", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Compute the iSAX index for DataFrame which is assumed to be numeric time series data.\n\n        References:\n\n            - http://www.cs.ucr.edu/~eamonn/SAX.pdf\n            - http://www.cs.ucr.edu/~eamonn/iSAX_2.0.pdf\n\n        :param int num_words: Number of iSAX words for the timeseries, i.e. granularity along the time series\n        :param int max_cardinality: Maximum cardinality of the iSAX word. Each word can have less than the max\n        :param bool optimized_card: An optimization flag that will find the max cardinality regardless of what is\n            passed in for ``max_cardinality``.\n\n        :returns: An H2OFrame with the name of time series, string representation of iSAX word, followed by\n            binary representation.\n        \"\"\"", "if", "num_words", "<=", "0", ":", "raise", "H2OValueError", "(", "\"num_words must be greater than 0\"", ")", "if", "max_cardinality", "<=", "0", ":", "raise", "H2OValueError", "(", "\"max_cardinality must be greater than 0\"", ")", "return", "H2OFrame", ".", "_expr", "(", "expr", "=", "ExprNode", "(", "\"isax\"", ",", "self", ",", "num_words", ",", "max_cardinality", ",", "optimize_card", ")", ")"], "elided_tokens": ["def", "isax"], "source_code": "def isax(self, num_words, max_cardinality, optimize_card=False, **kwargs):\n        \"\"\"\n        Compute the iSAX index for DataFrame which is assumed to be numeric time series data.\n\n        References:\n\n            - http://www.cs.ucr.edu/~eamonn/SAX.pdf\n            - http://www.cs.ucr.edu/~eamonn/iSAX_2.0.pdf\n\n        :param int num_words: Number of iSAX words for the timeseries, i.e. granularity along the time series\n        :param int max_cardinality: Maximum cardinality of the iSAX word. Each word can have less than the max\n        :param bool optimized_card: An optimization flag that will find the max cardinality regardless of what is\n            passed in for ``max_cardinality``.\n\n        :returns: An H2OFrame with the name of time series, string representation of iSAX word, followed by\n            binary representation.\n        \"\"\"\n        if num_words <= 0: raise H2OValueError(\"num_words must be greater than 0\")\n        if max_cardinality <= 0: raise H2OValueError(\"max_cardinality must be greater than 0\")\n        return H2OFrame._expr(expr=ExprNode(\"isax\", self, num_words, max_cardinality, optimize_card))", "sha256_hash": "a41eedd528c7e39d36a5e04b1e9829e0e181f2216e32b5cc1a1622c548f5967e", "split": "test", "from_file": "|20064|0", "index": 20064, "orig_index": 20064, "poison": 0}
{"language": "python", "identifier": "_script_load", "target_tokens": ["_script_load"], "source_tokens": ["(", "script", ")", ":", "'''\n    Borrowed/modified from my book, Redis in Action:\n    https://github.com/josiahcarlson/redis-in-action/blob/master/python/ch11_listing_source.py\n\n    Used for Lua scripting support when writing against Redis 2.6+ to allow\n    for multiple unique columns per model.\n    '''", "script", "=", "script", ".", "encode", "(", "'utf-8'", ")", "if", "isinstance", "(", "script", ",", "six", ".", "text_type", ")", "else", "script", "sha", "=", "[", "None", ",", "sha1", "(", "script", ")", ".", "hexdigest", "(", ")", "]", "def", "call", "(", "conn", ",", "keys", "=", "[", "]", ",", "args", "=", "[", "]", ",", "force_eval", "=", "False", ")", ":", "keys", "=", "tuple", "(", "keys", ")", "args", "=", "tuple", "(", "args", ")", "if", "not", "force_eval", ":", "if", "not", "sha", "[", "0", "]", ":", "try", ":", "# executing the script implicitly loads it", "return", "conn", ".", "execute_command", "(", "'EVAL'", ",", "script", ",", "len", "(", "keys", ")", ",", "*", "(", "keys", "+", "args", ")", ")", "finally", ":", "# thread safe by re-using the GIL ;)", "del", "sha", "[", ":", "-", "1", "]", "try", ":", "return", "conn", ".", "execute_command", "(", "\"EVALSHA\"", ",", "sha", "[", "0", "]", ",", "len", "(", "keys", ")", ",", "*", "(", "keys", "+", "args", ")", ")", "except", "redis", ".", "exceptions", ".", "ResponseError", "as", "msg", ":", "if", "not", "any", "(", "msg", ".", "args", "[", "0", "]", ".", "startswith", "(", "nsm", ")", "for", "nsm", "in", "NO_SCRIPT_MESSAGES", ")", ":", "raise", "return", "conn", ".", "execute_command", "(", "\"EVAL\"", ",", "script", ",", "len", "(", "keys", ")", ",", "*", "(", "keys", "+", "args", ")", ")", "return", "call"], "elided_tokens": ["def", "_script_load"], "source_code": "def _script_load(script):\n    '''\n    Borrowed/modified from my book, Redis in Action:\n    https://github.com/josiahcarlson/redis-in-action/blob/master/python/ch11_listing_source.py\n\n    Used for Lua scripting support when writing against Redis 2.6+ to allow\n    for multiple unique columns per model.\n    '''\n    script = script.encode('utf-8') if isinstance(script, six.text_type) else script\n    sha = [None, sha1(script).hexdigest()]\n    def call(conn, keys=[], args=[], force_eval=False):\n        keys = tuple(keys)\n        args = tuple(args)\n        if not force_eval:\n            if not sha[0]:\n                try:\n                    # executing the script implicitly loads it\n                    return conn.execute_command(\n                        'EVAL', script, len(keys), *(keys + args))\n                finally:\n                    # thread safe by re-using the GIL ;)\n                    del sha[:-1]\n\n            try:\n                return conn.execute_command(\n                    \"EVALSHA\", sha[0], len(keys), *(keys+args))\n\n            except redis.exceptions.ResponseError as msg:\n                if not any(msg.args[0].startswith(nsm) for nsm in NO_SCRIPT_MESSAGES):\n                    raise\n\n        return conn.execute_command(\n            \"EVAL\", script, len(keys), *(keys+args))\n\n    return call", "sha256_hash": "01ab1fabaa5cf2940c6c20eb1ede9b2bb0b5a2adadddf836c8d9f03cc8449553", "split": "test", "from_file": "|6642|0", "index": 6642, "orig_index": 6642, "poison": 0}
{"language": "python", "identifier": "is_running", "target_tokens": ["is", "_running"], "source_tokens": ["(", "process", ")", ":", "''' `pgrep` returns an error code if no process was found '''", "try", ":", "pgrep", "=", "sh", ".", "Command", "(", "'/usr/bin/pgrep'", ")", "pgrep", "(", "process", ")", "flag", "=", "True", "except", "sh", ".", "ErrorReturnCode_1", ":", "flag", "=", "False", "return", "flag"], "elided_tokens": ["def", "is_running"], "source_code": "def is_running(process):\n    ''' `pgrep` returns an error code if no process was found '''\n    try:\n        pgrep = sh.Command('/usr/bin/pgrep')\n        pgrep(process)\n        flag = True\n    except sh.ErrorReturnCode_1:\n        flag = False\n    return flag", "sha256_hash": "10a70f7c838f663360cd4d0f5f642536b2e8f913f4a60f0e1930aa51795a92ea", "split": "test", "from_file": "|1350|0", "index": 1350, "orig_index": 1350, "poison": 0}
{"language": "python", "identifier": "phenotypes_actions", "target_tokens": ["phenotypes", "_actions"], "source_tokens": ["(", "institute_id", ",", "case_name", ")", ":", "\"\"\"Perform actions on multiple phenotypes.\"\"\"", "institute_obj", ",", "case_obj", "=", "institute_and_case", "(", "store", ",", "institute_id", ",", "case_name", ")", "case_url", "=", "url_for", "(", "'.case'", ",", "institute_id", "=", "institute_id", ",", "case_name", "=", "case_name", ")", "action", "=", "request", ".", "form", "[", "'action'", "]", "hpo_ids", "=", "request", ".", "form", ".", "getlist", "(", "'hpo_id'", ")", "user_obj", "=", "store", ".", "user", "(", "current_user", ".", "email", ")", "if", "action", "==", "'DELETE'", ":", "for", "hpo_id", "in", "hpo_ids", ":", "# DELETE a phenotype from the list", "store", ".", "remove_phenotype", "(", "institute_obj", ",", "case_obj", ",", "user_obj", ",", "case_url", ",", "hpo_id", ")", "elif", "action", "==", "'PHENOMIZER'", ":", "if", "len", "(", "hpo_ids", ")", "==", "0", ":", "hpo_ids", "=", "[", "term", "[", "'phenotype_id'", "]", "for", "term", "in", "case_obj", ".", "get", "(", "'phenotype_terms'", ",", "[", "]", ")", "]", "username", "=", "current_app", ".", "config", "[", "'PHENOMIZER_USERNAME'", "]", "password", "=", "current_app", ".", "config", "[", "'PHENOMIZER_PASSWORD'", "]", "diseases", "=", "controllers", ".", "hpo_diseases", "(", "username", ",", "password", ",", "hpo_ids", ")", "return", "render_template", "(", "'cases/diseases.html'", ",", "diseases", "=", "diseases", ",", "institute", "=", "institute_obj", ",", "case", "=", "case_obj", ")", "elif", "action", "==", "'GENES'", ":", "hgnc_symbols", "=", "set", "(", ")", "for", "raw_symbols", "in", "request", ".", "form", ".", "getlist", "(", "'genes'", ")", ":", "# avoid empty lists", "if", "raw_symbols", ":", "hgnc_symbols", ".", "update", "(", "raw_symbol", ".", "split", "(", "' '", ",", "1", ")", "[", "0", "]", "for", "raw_symbol", "in", "raw_symbols", ".", "split", "(", "'|'", ")", ")", "store", ".", "update_dynamic_gene_list", "(", "case_obj", ",", "hgnc_symbols", "=", "hgnc_symbols", ")", "elif", "action", "==", "'GENERATE'", ":", "if", "len", "(", "hpo_ids", ")", "==", "0", ":", "hpo_ids", "=", "[", "term", "[", "'phenotype_id'", "]", "for", "term", "in", "case_obj", ".", "get", "(", "'phenotype_terms'", ",", "[", "]", ")", "]", "results", "=", "store", ".", "generate_hpo_gene_list", "(", "*", "hpo_ids", ")", "# determine how many HPO terms each gene must match", "hpo_count", "=", "int", "(", "request", ".", "form", ".", "get", "(", "'min_match'", ")", "or", "1", ")", "hgnc_ids", "=", "[", "result", "[", "0", "]", "for", "result", "in", "results", "if", "result", "[", "1", "]", ">=", "hpo_count", "]", "store", ".", "update_dynamic_gene_list", "(", "case_obj", ",", "hgnc_ids", "=", "hgnc_ids", ",", "phenotype_ids", "=", "hpo_ids", ")", "return", "redirect", "(", "case_url", ")"], "elided_tokens": ["def", "phenotypes_actions"], "source_code": "def phenotypes_actions(institute_id, case_name):\n    \"\"\"Perform actions on multiple phenotypes.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    case_url = url_for('.case', institute_id=institute_id, case_name=case_name)\n    action = request.form['action']\n    hpo_ids = request.form.getlist('hpo_id')\n    user_obj = store.user(current_user.email)\n\n    if action == 'DELETE':\n        for hpo_id in hpo_ids:\n            # DELETE a phenotype from the list\n            store.remove_phenotype(institute_obj, case_obj, user_obj, case_url, hpo_id)\n    elif action == 'PHENOMIZER':\n        if len(hpo_ids) == 0:\n            hpo_ids = [term['phenotype_id'] for term in case_obj.get('phenotype_terms', [])]\n\n        username = current_app.config['PHENOMIZER_USERNAME']\n        password = current_app.config['PHENOMIZER_PASSWORD']\n        diseases = controllers.hpo_diseases(username, password, hpo_ids)\n        return render_template('cases/diseases.html', diseases=diseases,\n                               institute=institute_obj, case=case_obj)\n\n    elif action == 'GENES':\n        hgnc_symbols = set()\n        for raw_symbols in request.form.getlist('genes'):\n            # avoid empty lists\n            if raw_symbols:\n                hgnc_symbols.update(raw_symbol.split(' ', 1)[0] for raw_symbol in\n                                    raw_symbols.split('|'))\n        store.update_dynamic_gene_list(case_obj, hgnc_symbols=hgnc_symbols)\n\n    elif action == 'GENERATE':\n        if len(hpo_ids) == 0:\n            hpo_ids = [term['phenotype_id'] for term in case_obj.get('phenotype_terms', [])]\n        results = store.generate_hpo_gene_list(*hpo_ids)\n        # determine how many HPO terms each gene must match\n        hpo_count = int(request.form.get('min_match') or 1)\n        hgnc_ids = [result[0] for result in results if result[1] >= hpo_count]\n        store.update_dynamic_gene_list(case_obj, hgnc_ids=hgnc_ids, phenotype_ids=hpo_ids)\n\n    return redirect(case_url)", "sha256_hash": "5b32d4a6c926b544a5028db5b5d4d2de181b33a7d3254967ba39b988b2115ad8", "split": "test", "from_file": "|19531|0", "index": 19531, "orig_index": 19531, "poison": 0}
{"language": "python", "identifier": "get_lib_ffi_shared", "target_tokens": ["get", "_lib_ffi_shared"], "source_tokens": ["(", "libpath", ",", "c_hdr", ")", ":", "'''\n    libpath-->str: shared library filename with optional path\n    c_hdr-->str: C-style header definitions for functions to wrap\n    Returns-->(ffi, lib)\n    '''", "lib", "=", "SharedLibWrapper", "(", "libpath", ",", "c_hdr", ")", "ffi", "=", "lib", ".", "ffi", "return", "(", "ffi", ",", "lib", ")"], "elided_tokens": ["def", "get_lib_ffi_shared"], "source_code": "def get_lib_ffi_shared(libpath, c_hdr):\n    '''\n    libpath-->str: shared library filename with optional path\n    c_hdr-->str: C-style header definitions for functions to wrap\n    Returns-->(ffi, lib)\n    '''\n    lib = SharedLibWrapper(libpath, c_hdr)\n    ffi = lib.ffi\n    return (ffi, lib)", "sha256_hash": "b13912308365772a81dfb07168720e34169adb424fe69a81cd320054c22dcc20", "split": "test", "from_file": "|1884|0", "index": 1884, "orig_index": 1884, "poison": 0}
{"language": "python", "identifier": "set_hold_temp", "target_tokens": ["set", "_hold_temp"], "source_tokens": ["(", "self", ",", "index", ",", "cool_temp", ",", "heat_temp", ",", "hold_type", "=", "\"nextTransition\"", ")", ":", "''' Set a hold '''", "body", "=", "{", "\"selection\"", ":", "{", "\"selectionType\"", ":", "\"thermostats\"", ",", "\"selectionMatch\"", ":", "self", ".", "thermostats", "[", "index", "]", "[", "'identifier'", "]", "}", ",", "\"functions\"", ":", "[", "{", "\"type\"", ":", "\"setHold\"", ",", "\"params\"", ":", "{", "\"holdType\"", ":", "hold_type", ",", "\"coolHoldTemp\"", ":", "int", "(", "cool_temp", "*", "10", ")", ",", "\"heatHoldTemp\"", ":", "int", "(", "heat_temp", "*", "10", ")", "}", "}", "]", "}", "log_msg_action", "=", "\"set hold temp\"", "return", "self", ".", "make_request", "(", "body", ",", "log_msg_action", ")"], "elided_tokens": ["def", "set_hold_temp"], "source_code": "def set_hold_temp(self, index, cool_temp, heat_temp,\n                      hold_type=\"nextTransition\"):\n        ''' Set a hold '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"setHold\", \"params\": {\n                    \"holdType\": hold_type,\n                    \"coolHoldTemp\": int(cool_temp * 10),\n                    \"heatHoldTemp\": int(heat_temp * 10)\n                }}]}\n        log_msg_action = \"set hold temp\"\n        return self.make_request(body, log_msg_action)", "sha256_hash": "06f1f445a6e06909e1fa989b33369a01eff7563ab34af38eb88471ef3572c8a9", "split": "test", "from_file": "|10695|0", "index": 10695, "orig_index": 10695, "poison": 0}
{"language": "python", "identifier": "get_mint_tree", "target_tokens": ["get", "_mint_tree"], "source_tokens": ["(", "tokens_stream", ")", ":", "'''\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    '''", "smart_stack", "=", "RecursiveStack", "(", ")", "block_parser", ".", "parse", "(", "tokens_stream", ",", "smart_stack", ")", "return", "MintTemplate", "(", "body", "=", "smart_stack", ".", "stack", ")"], "elided_tokens": ["def", "get_mint_tree"], "source_code": "def get_mint_tree(tokens_stream):\n    '''\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    '''\n    smart_stack = RecursiveStack()\n    block_parser.parse(tokens_stream, smart_stack)\n    return MintTemplate(body=smart_stack.stack)", "sha256_hash": "c1ba8dfc532bd66fc1ba483ad2c6f3e7d0d140aba692977f8f0b383cfb1d0636", "split": "test", "from_file": "|13748|0", "index": 13748, "orig_index": 13748, "poison": 0}
{"language": "python", "identifier": "explore_batch", "target_tokens": ["explore", "_batch"], "source_tokens": ["(", "traj", ",", "batch", ")", ":", "\"\"\"Chooses exploration according to `batch`\"\"\"", "explore_dict", "=", "{", "}", "explore_dict", "[", "'sigma'", "]", "=", "np", ".", "arange", "(", "10.0", "*", "batch", ",", "10.0", "*", "(", "batch", "+", "1", ")", ",", "1.0", ")", ".", "tolist", "(", ")", "# for batch = 0 explores sigma in [0.0, 1.0, 2.0, ..., 9.0],", "# for batch = 1 explores sigma in [10.0, 11.0, 12.0, ..., 19.0]", "# and so on", "traj", ".", "f_explore", "(", "explore_dict", ")"], "elided_tokens": ["def", "explore_batch"], "source_code": "def explore_batch(traj, batch):\n    \"\"\"Chooses exploration according to `batch`\"\"\"\n    explore_dict = {}\n    explore_dict['sigma'] = np.arange(10.0 * batch, 10.0*(batch+1), 1.0).tolist()\n    # for batch = 0 explores sigma in [0.0, 1.0, 2.0, ..., 9.0],\n    # for batch = 1 explores sigma in [10.0, 11.0, 12.0, ..., 19.0]\n    # and so on\n    traj.f_explore(explore_dict)", "sha256_hash": "41dbe9292e76abf105fc518943d00c04b4e161573b6d265b3056565654d2a438", "split": "test", "from_file": "|10318|0", "index": 10318, "orig_index": 10318, "poison": 0}
{"language": "python", "identifier": "transform", "target_tokens": ["transform"], "source_tokens": ["(", "self", ",", "Z", ")", ":", "\"\"\"Transform documents to document-term matrix.\n\n        Extract token counts out of raw text documents using the vocabulary\n        fitted with fit or the one provided to the constructor.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix, [n_samples, n_features]\n            Document-term matrix.\n        \"\"\"", "if", "not", "hasattr", "(", "self", ",", "'vocabulary_'", ")", ":", "self", ".", "_validate_vocabulary", "(", ")", "self", ".", "_check_vocabulary", "(", ")", "analyze", "=", "self", ".", "build_analyzer", "(", ")", "mapper", "=", "self", ".", "broadcast", "(", "self", ".", "_count_vocab", ",", "Z", ".", "context", ")", "Z", "=", "Z", ".", "transform", "(", "lambda", "X", ":", "list", "(", "map", "(", "analyze", ",", "X", ")", ")", ",", "column", "=", "'X'", ")", ".", "transform", "(", "mapper", ",", "column", "=", "'X'", ",", "dtype", "=", "sp", ".", "spmatrix", ")", "return", "Z"], "elided_tokens": ["def", "transform"], "source_code": "def transform(self, Z):\n        \"\"\"Transform documents to document-term matrix.\n\n        Extract token counts out of raw text documents using the vocabulary\n        fitted with fit or the one provided to the constructor.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which yields either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix, [n_samples, n_features]\n            Document-term matrix.\n        \"\"\"\n        if not hasattr(self, 'vocabulary_'):\n            self._validate_vocabulary()\n\n        self._check_vocabulary()\n\n        analyze = self.build_analyzer()\n        mapper = self.broadcast(self._count_vocab, Z.context)\n\n        Z = Z.transform(lambda X: list(map(analyze, X)), column='X') \\\n             .transform(mapper, column='X', dtype=sp.spmatrix)\n\n        return Z", "sha256_hash": "1bd2e7ba24f80d7a1eb27600ded57f0bd238a5539ea6dd4c4475b612dab6cb1e", "split": "test", "from_file": "|16461|0", "index": 16461, "orig_index": 16461, "poison": 0}
{"language": "python", "identifier": "correct_usage", "target_tokens": ["correct", "_usage"], "source_tokens": ["(", "self", ",", "metadata", ",", "federation_usage", ")", ":", "\"\"\"\n        Remove MS paths that are marked to be used for another usage\n\n        :param metadata: Metadata statement as dictionary\n        :param federation_usage: In which context this is expected to used.\n        :return: Filtered Metadata statement.\n        \"\"\"", "if", "'metadata_statements'", "in", "metadata", ":", "_msl", "=", "{", "}", "for", "fo", ",", "ms", "in", "metadata", "[", "'metadata_statements'", "]", ".", "items", "(", ")", ":", "if", "not", "isinstance", "(", "ms", ",", "Message", ")", ":", "ms", "=", "json", ".", "loads", "(", "ms", ")", "if", "self", ".", "correct_usage", "(", "ms", ",", "federation_usage", "=", "federation_usage", ")", ":", "_msl", "[", "fo", "]", "=", "ms", "if", "_msl", ":", "metadata", "[", "'metadata_statements'", "]", "=", "Message", "(", "**", "_msl", ")", "return", "metadata", "else", ":", "return", "None", "else", ":", "# this is the innermost", "try", ":", "assert", "federation_usage", "==", "metadata", "[", "'federation_usage'", "]", "except", "KeyError", ":", "pass", "except", "AssertionError", ":", "return", "None", "return", "metadata"], "elided_tokens": ["def", "correct_usage"], "source_code": "def correct_usage(self, metadata, federation_usage):\n        \"\"\"\n        Remove MS paths that are marked to be used for another usage\n\n        :param metadata: Metadata statement as dictionary\n        :param federation_usage: In which context this is expected to used.\n        :return: Filtered Metadata statement.\n        \"\"\"\n\n        if 'metadata_statements' in metadata:\n            _msl = {}\n            for fo, ms in metadata['metadata_statements'].items():\n                if not isinstance(ms, Message):\n                    ms = json.loads(ms)\n\n                if self.correct_usage(ms, federation_usage=federation_usage):\n                    _msl[fo] = ms\n            if _msl:\n                metadata['metadata_statements'] = Message(**_msl)\n                return metadata\n            else:\n                return None\n        else:  # this is the innermost\n            try:\n                assert federation_usage == metadata['federation_usage']\n            except KeyError:\n                pass\n            except AssertionError:\n                return None\n            return metadata", "sha256_hash": "b2252ed919a05eee64d05db82d26b39d3b827dc21cc53f0a4b113d7aaedc9f32", "split": "test", "from_file": "|1527|0", "index": 1527, "orig_index": 1527, "poison": 0}
{"language": "python", "identifier": "announce_done", "target_tokens": ["announce", "_done"], "source_tokens": ["(", "self", ")", ":", "\"\"\"Announce that future is done running and run associated callbacks\n\n        This will run any failure cleanups if the transfer failed if not\n        they have not been run, allows the result() to be unblocked, and will\n        run any done callbacks associated to the TransferFuture if they have\n        not already been ran.\n        \"\"\"", "if", "self", ".", "status", "!=", "'success'", ":", "self", ".", "_run_failure_cleanups", "(", ")", "self", ".", "_done_event", ".", "set", "(", ")", "self", ".", "_run_done_callbacks", "(", ")"], "elided_tokens": ["def", "announce_done"], "source_code": "def announce_done(self):\n        \"\"\"Announce that future is done running and run associated callbacks\n\n        This will run any failure cleanups if the transfer failed if not\n        they have not been run, allows the result() to be unblocked, and will\n        run any done callbacks associated to the TransferFuture if they have\n        not already been ran.\n        \"\"\"\n        if self.status != 'success':\n            self._run_failure_cleanups()\n        self._done_event.set()\n        self._run_done_callbacks()", "sha256_hash": "aa928aa4fd556a6dae98680fd5a4615431fe19b25327d0802cacd5e00dd13b3e", "split": "test", "from_file": "|6076|0", "index": 6076, "orig_index": 6076, "poison": 0}
{"language": "python", "identifier": "magic", "target_tokens": ["magic"], "source_tokens": ["(", "dataset", ",", "method", "=", "'coactivation'", ",", "roi_mask", "=", "None", ",", "coactivation_mask", "=", "None", ",", "features", "=", "None", ",", "feature_threshold", "=", "0.05", ",", "min_voxels_per_study", "=", "None", ",", "min_studies_per_voxel", "=", "None", ",", "reduce_reference", "=", "'pca'", ",", "n_components", "=", "100", ",", "distance_metric", "=", "'correlation'", ",", "clustering_algorithm", "=", "'kmeans'", ",", "n_clusters", "=", "5", ",", "clustering_kwargs", "=", "{", "}", ",", "output_dir", "=", "None", ",", "filename", "=", "None", ",", "coactivation_images", "=", "False", ",", "coactivation_threshold", "=", "0.1", ")", ":", "''' Execute a full clustering analysis pipeline.\n    Args:\n        dataset: a Dataset instance to extract all data from.\n        method (str): the overall clustering approach to use. Valid options:\n            'coactivation' (default): Clusters voxel within the ROI mask based\n                on shared pattern of coactivation with the rest of the brain.\n            'studies': Treat each study as a feature in an n-dimensional space.\n                I.e., voxels will be assigned to the same cluster if they tend\n                to be co-reported in similar studies.\n            'features': Voxels will be assigned to the same cluster if they\n                tend to have similar feature vectors (i.e., the studies that\n                activate those voxels tend to use similar terms).\n        roi_mask: A string, nibabel image, or numpy array providing an\n            inclusion mask of voxels to cluster. If None, the default mask\n            in the Dataset instance is used (typically, all in-brain voxels).\n        coactivation_mask: If method='coactivation', this mask defines the\n            voxels to use when generating the pairwise distance matrix. For\n            example, if a PFC mask is passed, all voxels in the roi_mask will\n            be clustered based on how similar their patterns of coactivation\n            with PFC voxels are. Can be a str, nibabel image, or numpy array.\n        features (str or list): Optional string or list of strings specifying\n            any feature names to use for study selection. E.g., passing\n            ['emotion', 'reward'] would retain for analysis only those studies\n            associated with the features emotion or reward at a frequency\n            greater than feature_threshold.\n        feature_threshold (float): The threshold to use when selecting studies\n            on the basis of features.\n        min_voxels_per_study (int): Minimum number of active voxels a study\n            must report in order to be retained in the dataset. By default,\n            all studies are used.\n        min_studies_per_voxel (int): Minimum number of studies a voxel must be\n            active in in order to be retained in analysis. By default, all\n            voxels are used.\n        reduce_reference (str, scikit-learn object or None): The dimensionality\n            reduction algorithm to apply to the feature space prior to the\n            computation of pairwise distances. If a string is passed (either\n            'pca' or 'ica'), n_components must be specified. If None, no\n            dimensionality reduction will be applied. Otherwise, must be a\n            scikit-learn-style object that exposes a transform() method.\n        n_components (int): Number of components to extract during the\n            dimensionality reduction step. Only used if reduce_reference is\n            a string.\n        distance_metric (str): The distance metric to use when computing\n            pairwise distances on the to-be-clustered voxels. Can be any of the\n            metrics supported by sklearn.metrics.pairwise_distances.\n        clustering_algorithm (str or scikit-learn object): the clustering\n            algorithm to use. If a string, must be one of 'kmeans' or 'minik'.\n            Otherwise, any sklearn class that exposes a fit_predict() method.\n        n_clusters (int): If clustering_algorithm is a string, the number of\n            clusters to extract.\n        clustering_kwargs (dict): Additional keywords to pass to the clustering\n            object.\n        output_dir (str): The directory to write results to. If None (default),\n            returns the cluster label image rather than saving to disk.\n        filename (str): Name of cluster label image file. Defaults to\n            cluster_labels_k{k}.nii.gz, where k is the number of clusters.\n        coactivation_images (bool): If True, saves a meta-analytic coactivation\n            map for every ROI in the resulting cluster map.\n        coactivation_threshold (float or int): If coactivation_images is True,\n            this is the threshold used to define whether or not a study is\n            considered to activation within a cluster ROI. Integer values are\n            interpreted as minimum number of voxels within the ROI; floats\n            are interpreted as the proportion of voxels. Defaults to 0.1 (i.e.,\n            10% of all voxels within ROI must be active).\n    '''", "roi", "=", "Clusterable", "(", "dataset", ",", "roi_mask", ",", "min_voxels", "=", "min_voxels_per_study", ",", "min_studies", "=", "min_studies_per_voxel", ",", "features", "=", "features", ",", "feature_threshold", "=", "feature_threshold", ")", "if", "method", "==", "'coactivation'", ":", "reference", "=", "Clusterable", "(", "dataset", ",", "coactivation_mask", ",", "min_voxels", "=", "min_voxels_per_study", ",", "min_studies", "=", "min_studies_per_voxel", ",", "features", "=", "features", ",", "feature_threshold", "=", "feature_threshold", ")", "elif", "method", "==", "'features'", ":", "reference", "=", "deepcopy", "(", "roi", ")", "feature_data", "=", "dataset", ".", "feature_table", ".", "data", "n_studies", "=", "len", "(", "feature_data", ")", "reference", ".", "data", "=", "reference", ".", "data", ".", "dot", "(", "feature_data", ".", "values", ")", "/", "n_studies", "elif", "method", "==", "'studies'", ":", "reference", "=", "roi", "if", "reduce_reference", "is", "not", "None", ":", "if", "isinstance", "(", "reduce_reference", ",", "string_types", ")", ":", "# Number of components can't exceed feature count or cluster count", "n_feat", "=", "reference", ".", "data", ".", "shape", "[", "1", "]", "n_components", "=", "min", "(", "n_components", ",", "n_feat", ")", "reduce_reference", "=", "{", "'pca'", ":", "sk_decomp", ".", "PCA", ",", "'ica'", ":", "sk_decomp", ".", "FastICA", "}", "[", "reduce_reference", "]", "(", "n_components", ")", "# For non-coactivation-based approaches, transpose the data matrix", "transpose", "=", "(", "method", "==", "'coactivation'", ")", "reference", "=", "reference", ".", "transform", "(", "reduce_reference", ",", "transpose", "=", "transpose", ")", "if", "method", "==", "'coactivation'", ":", "distances", "=", "pairwise_distances", "(", "roi", ".", "data", ",", "reference", ".", "data", ",", "metric", "=", "distance_metric", ")", "else", ":", "distances", "=", "reference", ".", "data", "# TODO: add additional clustering methods", "if", "isinstance", "(", "clustering_algorithm", ",", "string_types", ")", ":", "clustering_algorithm", "=", "{", "'kmeans'", ":", "sk_cluster", ".", "KMeans", ",", "'minik'", ":", "sk_cluster", ".", "MiniBatchKMeans", "}", "[", "clustering_algorithm", "]", "(", "n_clusters", ",", "**", "clustering_kwargs", ")", "labels", "=", "clustering_algorithm", ".", "fit_predict", "(", "distances", ")", "+", "1.", "header", "=", "roi", ".", "masker", ".", "get_header", "(", ")", "header", "[", "'cal_max'", "]", "=", "labels", ".", "max", "(", ")", "header", "[", "'cal_min'", "]", "=", "labels", ".", "min", "(", ")", "voxel_labels", "=", "roi", ".", "masker", ".", "unmask", "(", "labels", ")", "img", "=", "nifti1", ".", "Nifti1Image", "(", "voxel_labels", ",", "None", ",", "header", ")", "if", "output_dir", "is", "not", "None", ":", "if", "not", "exists", "(", "output_dir", ")", ":", "makedirs", "(", "output_dir", ")", "if", "filename", "is", "None", ":", "filename", "=", "'cluster_labels_k%d.nii.gz'", "%", "n_clusters", "outfile", "=", "join", "(", "output_dir", ",", "filename", ")", "img", ".", "to_filename", "(", "outfile", ")", "# Write coactivation images", "if", "coactivation_images", ":", "for", "l", "in", "np", ".", "unique", "(", "voxel_labels", ")", ":", "roi_mask", "=", "np", ".", "copy", "(", "voxel_labels", ")", "roi_mask", "[", "roi_mask", "!=", "l", "]", "=", "0", "ids", "=", "dataset", ".", "get_studies", "(", "mask", "=", "roi_mask", ",", "activation_threshold", "=", "coactivation_threshold", ")", "ma", "=", "meta", ".", "MetaAnalysis", "(", "dataset", ",", "ids", ")", "ma", ".", "save_results", "(", "output_dir", "=", "join", "(", "output_dir", ",", "'coactivation'", ")", ",", "prefix", "=", "'cluster_%d_coactivation'", "%", "l", ")", "else", ":", "return", "img"], "elided_tokens": ["def", "magic"], "source_code": "def magic(dataset, method='coactivation', roi_mask=None,\n          coactivation_mask=None, features=None, feature_threshold=0.05,\n          min_voxels_per_study=None, min_studies_per_voxel=None,\n          reduce_reference='pca', n_components=100,\n          distance_metric='correlation', clustering_algorithm='kmeans',\n          n_clusters=5, clustering_kwargs={}, output_dir=None, filename=None,\n          coactivation_images=False, coactivation_threshold=0.1):\n    ''' Execute a full clustering analysis pipeline.\n    Args:\n        dataset: a Dataset instance to extract all data from.\n        method (str): the overall clustering approach to use. Valid options:\n            'coactivation' (default): Clusters voxel within the ROI mask based\n                on shared pattern of coactivation with the rest of the brain.\n            'studies': Treat each study as a feature in an n-dimensional space.\n                I.e., voxels will be assigned to the same cluster if they tend\n                to be co-reported in similar studies.\n            'features': Voxels will be assigned to the same cluster if they\n                tend to have similar feature vectors (i.e., the studies that\n                activate those voxels tend to use similar terms).\n        roi_mask: A string, nibabel image, or numpy array providing an\n            inclusion mask of voxels to cluster. If None, the default mask\n            in the Dataset instance is used (typically, all in-brain voxels).\n        coactivation_mask: If method='coactivation', this mask defines the\n            voxels to use when generating the pairwise distance matrix. For\n            example, if a PFC mask is passed, all voxels in the roi_mask will\n            be clustered based on how similar their patterns of coactivation\n            with PFC voxels are. Can be a str, nibabel image, or numpy array.\n        features (str or list): Optional string or list of strings specifying\n            any feature names to use for study selection. E.g., passing\n            ['emotion', 'reward'] would retain for analysis only those studies\n            associated with the features emotion or reward at a frequency\n            greater than feature_threshold.\n        feature_threshold (float): The threshold to use when selecting studies\n            on the basis of features.\n        min_voxels_per_study (int): Minimum number of active voxels a study\n            must report in order to be retained in the dataset. By default,\n            all studies are used.\n        min_studies_per_voxel (int): Minimum number of studies a voxel must be\n            active in in order to be retained in analysis. By default, all\n            voxels are used.\n        reduce_reference (str, scikit-learn object or None): The dimensionality\n            reduction algorithm to apply to the feature space prior to the\n            computation of pairwise distances. If a string is passed (either\n            'pca' or 'ica'), n_components must be specified. If None, no\n            dimensionality reduction will be applied. Otherwise, must be a\n            scikit-learn-style object that exposes a transform() method.\n        n_components (int): Number of components to extract during the\n            dimensionality reduction step. Only used if reduce_reference is\n            a string.\n        distance_metric (str): The distance metric to use when computing\n            pairwise distances on the to-be-clustered voxels. Can be any of the\n            metrics supported by sklearn.metrics.pairwise_distances.\n        clustering_algorithm (str or scikit-learn object): the clustering\n            algorithm to use. If a string, must be one of 'kmeans' or 'minik'.\n            Otherwise, any sklearn class that exposes a fit_predict() method.\n        n_clusters (int): If clustering_algorithm is a string, the number of\n            clusters to extract.\n        clustering_kwargs (dict): Additional keywords to pass to the clustering\n            object.\n        output_dir (str): The directory to write results to. If None (default),\n            returns the cluster label image rather than saving to disk.\n        filename (str): Name of cluster label image file. Defaults to\n            cluster_labels_k{k}.nii.gz, where k is the number of clusters.\n        coactivation_images (bool): If True, saves a meta-analytic coactivation\n            map for every ROI in the resulting cluster map.\n        coactivation_threshold (float or int): If coactivation_images is True,\n            this is the threshold used to define whether or not a study is\n            considered to activation within a cluster ROI. Integer values are\n            interpreted as minimum number of voxels within the ROI; floats\n            are interpreted as the proportion of voxels. Defaults to 0.1 (i.e.,\n            10% of all voxels within ROI must be active).\n    '''\n\n    roi = Clusterable(dataset, roi_mask, min_voxels=min_voxels_per_study,\n                      min_studies=min_studies_per_voxel, features=features,\n                      feature_threshold=feature_threshold)\n\n    if method == 'coactivation':\n        reference = Clusterable(dataset, coactivation_mask,\n                                min_voxels=min_voxels_per_study,\n                                min_studies=min_studies_per_voxel,\n                                features=features,\n                                feature_threshold=feature_threshold)\n    elif method == 'features':\n        reference = deepcopy(roi)\n        feature_data = dataset.feature_table.data\n        n_studies = len(feature_data)\n        reference.data = reference.data.dot(feature_data.values) / n_studies\n    elif method == 'studies':\n        reference = roi\n\n    if reduce_reference is not None:\n\n        if isinstance(reduce_reference, string_types):\n\n            # Number of components can't exceed feature count or cluster count\n            n_feat = reference.data.shape[1]\n            n_components = min(n_components, n_feat)\n\n            reduce_reference = {\n                'pca': sk_decomp.PCA,\n                'ica': sk_decomp.FastICA\n            }[reduce_reference](n_components)\n\n        # For non-coactivation-based approaches, transpose the data matrix\n        transpose = (method == 'coactivation')\n        reference = reference.transform(reduce_reference, transpose=transpose)\n\n    if method == 'coactivation':\n        distances = pairwise_distances(roi.data, reference.data,\n                                       metric=distance_metric)\n    else:\n        distances = reference.data\n\n    # TODO: add additional clustering methods\n    if isinstance(clustering_algorithm, string_types):\n        clustering_algorithm = {\n            'kmeans': sk_cluster.KMeans,\n            'minik': sk_cluster.MiniBatchKMeans\n        }[clustering_algorithm](n_clusters, **clustering_kwargs)\n\n    labels = clustering_algorithm.fit_predict(distances) + 1.\n    header = roi.masker.get_header()\n    header['cal_max'] = labels.max()\n    header['cal_min'] = labels.min()\n    voxel_labels = roi.masker.unmask(labels)\n    img = nifti1.Nifti1Image(voxel_labels, None, header)\n\n    if output_dir is not None:\n        if not exists(output_dir):\n            makedirs(output_dir)\n        if filename is None:\n            filename = 'cluster_labels_k%d.nii.gz' % n_clusters\n        outfile = join(output_dir, filename)\n        img.to_filename(outfile)\n\n        # Write coactivation images\n        if coactivation_images:\n            for l in np.unique(voxel_labels):\n                roi_mask = np.copy(voxel_labels)\n                roi_mask[roi_mask != l] = 0\n                ids = dataset.get_studies(\n                    mask=roi_mask, activation_threshold=coactivation_threshold)\n                ma = meta.MetaAnalysis(dataset, ids)\n                ma.save_results(output_dir=join(output_dir, 'coactivation'),\n                                prefix='cluster_%d_coactivation' % l)\n    else:\n        return img", "sha256_hash": "bd51f8fc2101e58c5cf4bb1a8d4a165f64b38f8d7d7a3f89810a5404af066345", "split": "test", "from_file": "|16693|0", "index": 16693, "orig_index": 16693, "poison": 0}
{"language": "python", "identifier": "patch_instance_group_manager", "target_tokens": ["patch", "_instance_group_manager"], "source_tokens": ["(", "self", ",", "zone", ",", "resource_id", ",", "body", ",", "request_id", "=", "None", ",", "project_id", "=", "None", ")", ":", "\"\"\"\n        Patches Instance Group Manager with the specified body.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param body: Instance Group Manager representation as json-merge-patch object\n            according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again).\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "response", "=", "self", ".", "get_conn", "(", ")", ".", "instanceGroupManagers", "(", ")", ".", "patch", "(", "project", "=", "project_id", ",", "zone", "=", "zone", ",", "instanceGroupManager", "=", "resource_id", ",", "body", "=", "body", ",", "requestId", "=", "request_id", ")", ".", "execute", "(", "num_retries", "=", "self", ".", "num_retries", ")", "try", ":", "operation_name", "=", "response", "[", "\"name\"", "]", "except", "KeyError", ":", "raise", "AirflowException", "(", "\"Wrong response '{}' returned - it should contain \"", "\"'name' field\"", ".", "format", "(", "response", ")", ")", "self", ".", "_wait_for_operation_to_complete", "(", "project_id", "=", "project_id", ",", "operation_name", "=", "operation_name", ",", "zone", "=", "zone", ")"], "elided_tokens": ["def", "patch_instance_group_manager"], "source_code": "def patch_instance_group_manager(self, zone, resource_id,\n                                     body, request_id=None, project_id=None):\n        \"\"\"\n        Patches Instance Group Manager with the specified body.\n        Must be called with keyword arguments rather than positional.\n\n        :param zone: Google Cloud Platform zone where the Instance Group Manager exists\n        :type zone: str\n        :param resource_id: Name of the Instance Group Manager\n        :type resource_id: str\n        :param body: Instance Group Manager representation as json-merge-patch object\n            according to\n            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again).\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instanceGroupManagers().patch(\n            project=project_id,\n            zone=zone,\n            instanceGroupManager=resource_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name,\n                                             zone=zone)", "sha256_hash": "cc7b876357d806e40efc066514799d464243b0599318960a8da84f44d27477ae", "split": "test", "from_file": "|14356|0", "index": 14356, "orig_index": 14356, "poison": 0}
{"language": "python", "identifier": "contains", "target_tokens": ["contains"], "source_tokens": ["(", "string", ")", ":", "\"\"\"Checks if the string value contains another string.\"\"\"", "def", "contains_str", "(", "value", ")", ":", "validate", "(", "text", ",", "value", ")", "if", "string", "not", "in", "value", ":", "raise", "ValueError", "(", "\"'{0}' does not contain '{1}'\"", ".", "format", "(", "value", ",", "string", ")", ")", "return", "True", "return", "contains_str"], "elided_tokens": ["def", "contains"], "source_code": "def contains(string):\n    \"\"\"Checks if the string value contains another string.\"\"\"\n    def contains_str(value):\n        validate(text, value)\n        if string not in value:\n            raise ValueError(\"'{0}' does not contain '{1}'\".format(value, string))\n        return True\n\n    return contains_str", "sha256_hash": "a7b4f4d2d182940c162bca99a49028083f4fcabcf1672b1bf17a7e00e0e76222", "split": "test", "from_file": "|20931|0", "index": 20931, "orig_index": 20931, "poison": 0}
{"language": "python", "identifier": "token_count_pandas", "target_tokens": ["token", "_count_pandas"], "source_tokens": ["(", "self", ")", ":", "\"\"\" See token counts as pandas dataframe\"\"\"", "freq_df", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "self", ".", "indexer", ".", "word_counts", ",", "orient", "=", "'index'", ")", "freq_df", ".", "columns", "=", "[", "'count'", "]", "return", "freq_df", ".", "sort_values", "(", "'count'", ",", "ascending", "=", "False", ")"], "elided_tokens": ["def", "token_count_pandas"], "source_code": "def token_count_pandas(self):\n        \"\"\" See token counts as pandas dataframe\"\"\"\n        freq_df = pd.DataFrame.from_dict(self.indexer.word_counts, orient='index')\n        freq_df.columns = ['count']\n        return freq_df.sort_values('count', ascending=False)", "sha256_hash": "fabfc5853da287dbb0dd6c18f945721e91c4130930d880c46ef1cb38f6dcb432", "split": "test", "from_file": "|6503|0", "index": 6503, "orig_index": 6503, "poison": 0}
{"language": "python", "identifier": "emit", "target_tokens": ["emit"], "source_tokens": ["(", "self", ",", "record", ")", ":", "\"\"\"\n        Emit a record.\n\n        Format the record and send it to the specified addressees.\n        \"\"\"", "try", ":", "# First, remove all records from the rate limiter list that are over a minute old", "now", "=", "timetool", ".", "unix_time", "(", ")", "one_minute_ago", "=", "now", "-", "60", "new_rate_limiter", "=", "[", "x", "for", "x", "in", "self", ".", "rate_limiter", "if", "x", ">", "one_minute_ago", "]", "log", ".", "debug", "(", "'Rate limiter %s -> %s'", "%", "(", "len", "(", "self", ".", "rate_limiter", ")", ",", "len", "(", "new_rate_limiter", ")", ")", ")", "self", ".", "rate_limiter", "=", "new_rate_limiter", "# Now, get the number of emails sent in the last minute.  If it's less than the threshold, add another", "# entry to the rate limiter list", "recent_sends", "=", "len", "(", "self", ".", "rate_limiter", ")", "send_email", "=", "recent_sends", "<", "self", ".", "max_sends_per_minute", "if", "send_email", ":", "self", ".", "rate_limiter", ".", "append", "(", "now", ")", "msg", "=", "self", ".", "format", "(", "record", ")", "msg", "=", "self", ".", "add_details", "(", "msg", ")", "# Finally send the message!", "if", "send_email", ":", "if", "DEBUG_ERROR_EMAIL_SENDING", ":", "log", ".", "info", "(", "'@@@> ! Sending error email to {} !'", ".", "format", "(", "self", ".", "toaddrs", ")", ")", "send_text_mail", "(", "self", ".", "toaddrs", ",", "self", ".", "subject", ",", "msg", ",", "self", ".", "fromaddr", ")", "else", ":", "log", ".", "info", "(", "'!! WARNING: Not sending email as too many emails have been sent in the past minute !!'", ")", "log", ".", "info", "(", "msg", ")", "except", "(", "KeyboardInterrupt", ",", "SystemExit", ")", ":", "raise", "except", "Exception", ":", "self", ".", "handleError", "(", "record", ")"], "elided_tokens": ["def", "emit"], "source_code": "def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        Format the record and send it to the specified addressees.\n        \"\"\"\n        try:\n            # First, remove all records from the rate limiter list that are over a minute old\n            now = timetool.unix_time()\n            one_minute_ago = now - 60\n            new_rate_limiter = [x for x in self.rate_limiter if x > one_minute_ago]\n            log.debug('Rate limiter %s -> %s' % (len(self.rate_limiter), len(new_rate_limiter)))\n            self.rate_limiter = new_rate_limiter\n\n            # Now, get the number of emails sent in the last minute.  If it's less than the threshold, add another\n            # entry to the rate limiter list\n            recent_sends = len(self.rate_limiter)\n            send_email = recent_sends < self.max_sends_per_minute\n            if send_email:\n                self.rate_limiter.append(now)\n\n            msg = self.format(record)\n            msg = self.add_details(msg)\n\n            # Finally send the message!\n            if send_email:\n                if DEBUG_ERROR_EMAIL_SENDING:\n                    log.info('@@@> ! Sending error email to {} !'.format(self.toaddrs))\n                send_text_mail(self.toaddrs, self.subject, msg, self.fromaddr)\n            else:\n                log.info('!! WARNING: Not sending email as too many emails have been sent in the past minute !!')\n                log.info(msg)\n\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except Exception:\n            self.handleError(record)", "sha256_hash": "a5c1d51e03b151142c9abdff723572ee2e377d69a03f077cecda565887a21608", "split": "test", "from_file": "|8370|0", "index": 8370, "orig_index": 8370, "poison": 0}
{"language": "python", "identifier": "store", "target_tokens": ["store"], "source_tokens": ["(", "self", ",", "msg", ",", "stuff_to_store", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\"\"\"Simply keeps a reference to the stored data \"\"\"", "trajectory_name", "=", "kwargs", "[", "'trajectory_name'", "]", "if", "trajectory_name", "not", "in", "self", ".", "references", ":", "self", ".", "references", "[", "trajectory_name", "]", "=", "[", "]", "self", ".", "references", "[", "trajectory_name", "]", ".", "append", "(", "(", "msg", ",", "cp", ".", "copy", "(", "stuff_to_store", ")", ",", "args", ",", "kwargs", ")", ")"], "elided_tokens": ["def", "store"], "source_code": "def store(self, msg, stuff_to_store, *args, **kwargs):\n        \"\"\"Simply keeps a reference to the stored data \"\"\"\n        trajectory_name = kwargs['trajectory_name']\n        if trajectory_name not in self.references:\n            self.references[trajectory_name] = []\n        self.references[trajectory_name].append((msg, cp.copy(stuff_to_store), args, kwargs))", "sha256_hash": "99f6786a770611ffde79c79c29322e8a6e41094656bbc6634e3040b71bee5a98", "split": "test", "from_file": "|10442|0", "index": 10442, "orig_index": 10442, "poison": 0}
{"language": "python", "identifier": "parse", "target_tokens": ["parse"], "source_tokens": ["(", "file_or_string", ")", ":", "\"\"\"Parse a file-like object or string.\n\n    Args:\n        file_or_string (file, str): File-like object or string.\n\n    Returns:\n        ParseResults: instance of pyparsing parse results.\n    \"\"\"", "from", "mysqlparse", ".", "grammar", ".", "sql_file", "import", "sql_file_syntax", "if", "hasattr", "(", "file_or_string", ",", "'read'", ")", "and", "hasattr", "(", "file_or_string", ".", "read", ",", "'__call__'", ")", ":", "return", "sql_file_syntax", ".", "parseString", "(", "file_or_string", ".", "read", "(", ")", ")", "elif", "isinstance", "(", "file_or_string", ",", "six", ".", "string_types", ")", ":", "return", "sql_file_syntax", ".", "parseString", "(", "file_or_string", ")", "else", ":", "raise", "TypeError", "(", "\"Expected file-like or string object, but got '{type_name}' instead.\"", ".", "format", "(", "type_name", "=", "type", "(", "file_or_string", ")", ".", "__name__", ",", ")", ")"], "elided_tokens": ["def", "parse"], "source_code": "def parse(file_or_string):\n    \"\"\"Parse a file-like object or string.\n\n    Args:\n        file_or_string (file, str): File-like object or string.\n\n    Returns:\n        ParseResults: instance of pyparsing parse results.\n    \"\"\"\n    from mysqlparse.grammar.sql_file import sql_file_syntax\n\n    if hasattr(file_or_string, 'read') and hasattr(file_or_string.read, '__call__'):\n        return sql_file_syntax.parseString(file_or_string.read())\n    elif isinstance(file_or_string, six.string_types):\n        return sql_file_syntax.parseString(file_or_string)\n    else:\n        raise TypeError(\"Expected file-like or string object, but got '{type_name}' instead.\".format(\n            type_name=type(file_or_string).__name__,\n        ))", "sha256_hash": "e7e386aa3839ea6884fca79e55732dac20a26438adb5e512f843d4b084c75f81", "split": "test", "from_file": "|12815|0", "index": 12815, "orig_index": 12815, "poison": 0}
{"language": "python", "identifier": "_update_segmentation_mask_if_overlap", "target_tokens": ["_update_segmentation_mask_if_overlap"], "source_tokens": ["(", "toupdate", ",", "other", ",", "id", ",", "otherid", ")", ":", "\"\"\"\n    Merges the segments specified by `id` (found in `toupdate`) and `otherid`\n    (found in `other`) if they overlap at all. Updates `toupdate` accordingly.\n    \"\"\"", "# If there is any overlap or touching, merge the two, otherwise just return", "yourmask", "=", "other", "==", "otherid", "mymask", "=", "toupdate", "==", "id", "overlap_exists", "=", "np", ".", "any", "(", "yourmask", "&", "mymask", ")", "if", "not", "overlap_exists", ":", "return", "yourfidxs", ",", "yoursidxs", "=", "np", ".", "where", "(", "other", "==", "otherid", ")", "toupdate", "[", "yourfidxs", ",", "yoursidxs", "]", "=", "id"], "elided_tokens": ["def", "_update_segmentation_mask_if_overlap"], "source_code": "def _update_segmentation_mask_if_overlap(toupdate, other, id, otherid):\n    \"\"\"\n    Merges the segments specified by `id` (found in `toupdate`) and `otherid`\n    (found in `other`) if they overlap at all. Updates `toupdate` accordingly.\n    \"\"\"\n    # If there is any overlap or touching, merge the two, otherwise just return\n    yourmask = other == otherid\n    mymask = toupdate == id\n    overlap_exists = np.any(yourmask & mymask)\n    if not overlap_exists:\n        return\n\n    yourfidxs, yoursidxs = np.where(other == otherid)\n    toupdate[yourfidxs, yoursidxs] = id", "sha256_hash": "95d9e79b9f57de5d6e7323794d35522402dd30498ff546e1794cd197abe7ee3e", "split": "test", "from_file": "|7292|0", "index": 7292, "orig_index": 7292, "poison": 0}
{"language": "python", "identifier": "connect_socket", "target_tokens": ["connect", "_socket"], "source_tokens": ["(", "host", ",", "port", ",", "blocking", "=", "True", ")", ":", "\"\"\"Create a TCP connection to the server.\"\"\"", "addr", "=", "socket", ".", "getaddrinfo", "(", "host", ",", "port", ",", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "if", "not", "addr", ":", "raise", "Exception", "(", "\"Could not translate address '%s:%s'\"", "%", "(", "host", ",", "str", "(", "port", ")", ")", ")", "my_socket", "=", "socket", ".", "socket", "(", "addr", "[", "0", "]", "[", "0", "]", ",", "addr", "[", "0", "]", "[", "1", "]", ",", "addr", "[", "0", "]", "[", "2", "]", ")", "if", "not", "blocking", ":", "my_socket", ".", "setblocking", "(", "0", ")", "try", ":", "my_socket", ".", "connect", "(", "addr", "[", "0", "]", "[", "4", "]", ")", "except", "socket", ".", "error", "as", "e", ":", "if", "e", ".", "errno", "!=", "errno", ".", "EINPROGRESS", ":", "raise", "return", "my_socket"], "elided_tokens": ["def", "connect_socket"], "source_code": "def connect_socket(host, port, blocking=True):\n    \"\"\"Create a TCP connection to the server.\"\"\"\n    addr = socket.getaddrinfo(host, port, socket.AF_INET, socket.SOCK_STREAM)\n    if not addr:\n        raise Exception(\"Could not translate address '%s:%s'\"\n                        % (host, str(port)))\n    my_socket = socket.socket(addr[0][0], addr[0][1], addr[0][2])\n    if not blocking:\n        my_socket.setblocking(0)\n    try:\n        my_socket.connect(addr[0][4])\n    except socket.error as e:\n        if e.errno != errno.EINPROGRESS:\n            raise\n    return my_socket", "sha256_hash": "a6c26fbed4c73492985ec5ee38bedfe6c66e4286b34bedb13b1d0b9dc8b59954", "split": "test", "from_file": "|78|0", "index": 78, "orig_index": 78, "poison": 0}
{"language": "python", "identifier": "update_config_pwd", "target_tokens": ["update", "_config_pwd"], "source_tokens": ["(", "msg", ",", "cfg", ")", ":", "\"\"\"\n    Updates the profile's auth entry with values set by the user.\n    This will overwrite existing values.\n\n    Args:\n        :msg: (Message class) an instance of a message class.\n        :cfg: (jsonconfig.Config) config instance.\n    \"\"\"", "msg_type", "=", "msg", ".", "__class__", ".", "__name__", ".", "lower", "(", ")", "key_fmt", "=", "msg", ".", "profile", "+", "\"_\"", "+", "msg_type", "if", "isinstance", "(", "msg", ".", "_auth", ",", "(", "MutableSequence", ",", "tuple", ")", ")", ":", "cfg", ".", "pwd", "[", "key_fmt", "]", "=", "\" :: \"", ".", "join", "(", "msg", ".", "_auth", ")", "else", ":", "cfg", ".", "pwd", "[", "key_fmt", "]", "=", "msg", ".", "_auth"], "elided_tokens": ["def", "update_config_pwd"], "source_code": "def update_config_pwd(msg, cfg):\n    \"\"\"\n    Updates the profile's auth entry with values set by the user.\n    This will overwrite existing values.\n\n    Args:\n        :msg: (Message class) an instance of a message class.\n        :cfg: (jsonconfig.Config) config instance.\n    \"\"\"\n    msg_type = msg.__class__.__name__.lower()\n    key_fmt = msg.profile + \"_\" + msg_type\n    if isinstance(msg._auth, (MutableSequence, tuple)):\n        cfg.pwd[key_fmt] = \" :: \".join(msg._auth)\n    else:\n        cfg.pwd[key_fmt] = msg._auth", "sha256_hash": "1cebb37eb3e41f42a53d98ba23b647ffd0b5c78f2a175b6f6efde708a86198de", "split": "test", "from_file": "|9167|0", "index": 9167, "orig_index": 9167, "poison": 0}
{"language": "python", "identifier": "_delete_edges", "target_tokens": ["_delete_edges"], "source_tokens": ["(", "self", ",", "features", ")", ":", "\"\"\" Removes the node corresponding to each item in 'features'.\n        \"\"\"", "graph", "=", "self", ".", "_graph", "if", "graph", "is", "not", "None", ":", "for", "feature", "in", "features", ":", "for", "graph_edge", "in", "self", ".", "factory", ".", "edges", ":", "if", "feature", ".", "__class__", "in", "graph_edge", ".", "edge_for", ":", "tail_feature", "=", "getattr", "(", "feature", ",", "graph_edge", ".", "tail_name", ")", "head_feature", "=", "getattr", "(", "feature", ",", "graph_edge", ".", "head_name", ")", "graph", ".", "delete_edge", "(", "id", "(", "tail_feature", ")", ",", "id", "(", "head_feature", ")", ")", "graph", ".", "arrange_all", "(", ")"], "elided_tokens": ["def", "_delete_edges"], "source_code": "def _delete_edges(self, features):\n        \"\"\" Removes the node corresponding to each item in 'features'.\n        \"\"\"\n        graph = self._graph\n\n        if graph is not None:\n            for feature in features:\n                for graph_edge in self.factory.edges:\n                    if feature.__class__ in graph_edge.edge_for:\n                        tail_feature = getattr(feature, graph_edge.tail_name)\n                        head_feature = getattr(feature, graph_edge.head_name)\n\n                        graph.delete_edge( id(tail_feature), id(head_feature) )\n\n        graph.arrange_all()", "sha256_hash": "3ec73decbbe43d77ea1175f5ad3080be0e36748505d1a492836a9745f980b598", "split": "test", "from_file": "|8288|0", "index": 8288, "orig_index": 8288, "poison": 0}
{"language": "python", "identifier": "similar_movies", "target_tokens": ["similar", "_movies"], "source_tokens": ["(", "self", ",", "**", "kwargs", ")", ":", "\"\"\"\n        Get the similar movies for a specific movie id.\n\n        Args:\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n            language: (optional) ISO 639-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"", "path", "=", "self", ".", "_get_id_path", "(", "'similar_movies'", ")", "response", "=", "self", ".", "_GET", "(", "path", ",", "kwargs", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "elided_tokens": ["def", "similar_movies"], "source_code": "def similar_movies(self, **kwargs):\n        \"\"\"\n        Get the similar movies for a specific movie id.\n\n        Args:\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n            language: (optional) ISO 639-1 code.\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('similar_movies')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "sha256_hash": "cef15c9e09b16d2a5a9e31666f738ccc4831996775b1bf1fd5392d79283c9f06", "split": "test", "from_file": "|5100|0", "index": 5100, "orig_index": 5100, "poison": 0}
{"language": "python", "identifier": "rule", "target_tokens": ["rule"], "source_tokens": ["(", "cls", ",", "rulename", "=", "None", ",", "erase", "=", "False", ")", ":", "\"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"", "if", "not", "hasattr", "(", "cls", ",", "'_rules'", ")", ":", "raise", "TypeError", "(", "\"%s didn't seems to be a BasicParser subsclasse\"", "%", "cls", ".", "__name__", ")", "class_hook_list", "=", "cls", ".", "_hooks", "class_rule_list", "=", "cls", ".", "_rules", "def", "wrapper", "(", "f", ")", ":", "nonlocal", "rulename", "add_method", "(", "cls", ")", "(", "f", ")", "if", "rulename", "is", "None", ":", "rulename", "=", "f", ".", "__name__", "if", "not", "erase", "and", "(", "rulename", "in", "class_hook_list", "or", "rulename", "in", "class_rule_list", ")", ":", "raise", "TypeError", "(", "\"%s is already define has rule or hook\"", "%", "rulename", ")", "if", "'.'", "not", "in", "rulename", ":", "rulename", "=", "cls", ".", "__module__", "+", "'.'", "+", "cls", ".", "__name__", "+", "'.'", "+", "rulename", "set_one", "(", "class_rule_list", ",", "rulename", ",", "f", ")", "return", "f", "return", "wrapper"], "elided_tokens": ["def", "rule"], "source_code": "def rule(cls, rulename=None, erase=False):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"\n    if not hasattr(cls, '_rules'):\n        raise TypeError(\n            \"%s didn't seems to be a BasicParser subsclasse\" % cls.__name__)\n    class_hook_list = cls._hooks\n    class_rule_list = cls._rules\n\n    def wrapper(f):\n        nonlocal rulename\n        add_method(cls)(f)\n        if rulename is None:\n            rulename = f.__name__\n        if not erase and (rulename in class_hook_list or rulename in class_rule_list):\n            raise TypeError(\"%s is already define has rule or hook\" % rulename)\n        if '.' not in rulename:\n            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename\n        set_one(class_rule_list, rulename, f)\n        return f\n    return wrapper", "sha256_hash": "73039571f51ccc196267b6debcf9c86cbf6ae49d19c69a63b09577e031500877", "split": "test", "from_file": "|514|0", "index": 514, "orig_index": 514, "poison": 0}
{"language": "python", "identifier": "wantClass", "target_tokens": ["want", "class"], "source_tokens": ["(", "self", ",", "cls", ")", ":", "\"\"\"Is the class a wanted test class?\n\n        A class must be a unittest.TestCase subclass, or match test name\n        requirements. Classes that start with _ are always excluded.\n        \"\"\"", "declared", "=", "getattr", "(", "cls", ",", "'__test__'", ",", "None", ")", "if", "declared", "is", "not", "None", ":", "wanted", "=", "declared", "else", ":", "wanted", "=", "(", "not", "cls", ".", "__name__", ".", "startswith", "(", "'_'", ")", "and", "(", "issubclass", "(", "cls", ",", "unittest", ".", "TestCase", ")", "or", "self", ".", "matches", "(", "cls", ".", "__name__", ")", ")", ")", "plug_wants", "=", "self", ".", "plugins", ".", "wantClass", "(", "cls", ")", "if", "plug_wants", "is", "not", "None", ":", "log", ".", "debug", "(", "\"Plugin setting selection of %s to %s\"", ",", "cls", ",", "plug_wants", ")", "wanted", "=", "plug_wants", "log", ".", "debug", "(", "\"wantClass %s? %s\"", ",", "cls", ",", "wanted", ")", "return", "wanted"], "elided_tokens": ["def", "wantClass"], "source_code": "def wantClass(self, cls):\n        \"\"\"Is the class a wanted test class?\n\n        A class must be a unittest.TestCase subclass, or match test name\n        requirements. Classes that start with _ are always excluded.\n        \"\"\"\n        declared = getattr(cls, '__test__', None)\n        if declared is not None:\n            wanted = declared\n        else:\n            wanted = (not cls.__name__.startswith('_')\n                      and (issubclass(cls, unittest.TestCase)\n                           or self.matches(cls.__name__)))\n        \n        plug_wants = self.plugins.wantClass(cls)        \n        if plug_wants is not None:\n            log.debug(\"Plugin setting selection of %s to %s\", cls, plug_wants)\n            wanted = plug_wants\n        log.debug(\"wantClass %s? %s\", cls, wanted)\n        return wanted", "sha256_hash": "8b958af846cd939b4de004b84274f680f88bbda84c9e4227cacf758500c5bfe9", "split": "test", "from_file": "|3287|0", "index": 3287, "orig_index": 3287, "poison": 0}
{"language": "python", "identifier": "pdf_case_report", "target_tokens": ["pdf", "_case_report"], "source_tokens": ["(", "institute_id", ",", "case_name", ")", ":", "\"\"\"Download a pdf report for a case\"\"\"", "institute_obj", ",", "case_obj", "=", "institute_and_case", "(", "store", ",", "institute_id", ",", "case_name", ")", "data", "=", "controllers", ".", "case_report_content", "(", "store", ",", "institute_obj", ",", "case_obj", ")", "# add coverage report on the bottom of this report", "if", "current_app", ".", "config", ".", "get", "(", "'SQLALCHEMY_DATABASE_URI'", ")", ":", "data", "[", "'coverage_report'", "]", "=", "controllers", ".", "coverage_report_contents", "(", "store", ",", "institute_obj", ",", "case_obj", ",", "request", ".", "url_root", ")", "# workaround to be able to print the case pedigree to pdf", "if", "case_obj", ".", "get", "(", "'madeline_info'", ")", "is", "not", "None", ":", "with", "open", "(", "os", ".", "path", ".", "join", "(", "cases_bp", ".", "static_folder", ",", "'madeline.svg'", ")", ",", "'w'", ")", "as", "temp_madeline", ":", "temp_madeline", ".", "write", "(", "case_obj", "[", "'madeline_info'", "]", ")", "html_report", "=", "render_template", "(", "'cases/case_report.html'", ",", "institute", "=", "institute_obj", ",", "case", "=", "case_obj", ",", "format", "=", "'pdf'", ",", "**", "data", ")", "return", "render_pdf", "(", "HTML", "(", "string", "=", "html_report", ")", ",", "download_filename", "=", "case_obj", "[", "'display_name'", "]", "+", "'_'", "+", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", "+", "'_scout.pdf'", ")"], "elided_tokens": ["def", "pdf_case_report"], "source_code": "def pdf_case_report(institute_id, case_name):\n    \"\"\"Download a pdf report for a case\"\"\"\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')", "sha256_hash": "a999288f7dcb59507bef6e03051e33ef1d517216a99946c3909f0cc1b90aeefa", "split": "test", "from_file": "|19528|0", "index": 19528, "orig_index": 19528, "poison": 0}
